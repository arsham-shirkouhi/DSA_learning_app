Chapter 1. Java Primer 1.1 Getting Started Building data structures and algorithms requires that we communicate detailed instructions to a computer. An excellent way to perform such communication is using a high-level computer language, such as Java. In this chapter, we provide an overview of the Java programming language, and we continue this discussion in the next chapter, focusing on object-oriented design principles. We assume that readers are somewhat familiar with an existing high-level language, although not necessarily Java. This book does not provide a complete description of the Java language (there are numerous language references for that purpose), but it does introduce all aspects of the language that are used in code fragments later in this book. We begin our Java primer with a program that prints “Hello Universe!” on the screen, which is shown in a dissected form in Figure 1.1. Figure 1.1: A “Hello Universe!” program. In Java, executable statements are placed in functions, known as methods, that belong to class deﬁnitions. The Universe class, in our ﬁrst example, is extremely simple; its only method is a static one named main, which is the ﬁrst method to be executed when running a Java program. Any set of statements between the braces “{” and “}” deﬁne a program block. Notice that the entire Universe class deﬁnition is delimited by such braces, as is the body of the main method. The name of a class, method, or variable in Java is called an identiﬁer, which can be any string of characters as long as it begins with a letter and consists of letters, numbers, and underscore characters (where “letter” and “number” can be from any written language deﬁned in the Unicode character set). We list the exceptions to this general rule for Java identiﬁers in Table 1.1.

1.1. Getting Started Reserved Words abstract default goto package synchronized assert do if private this boolean double implements protected throw break else import public throws byte enum instanceof return transient case extends int short true catch false interface static try char ﬁnal long strictfp void class ﬁnally native super volatile const ﬂoat new switch while continue for null Table 1.1: A listing of the reserved words in Java. These names cannot be used as class, method, or variable names. Comments In addition to executable statements and declarations, Java allows a programmer to embed comments, which are annotations provided for human readers that are not processed by the Java compiler. Java allows two kinds of comments: inline comments and block comments. Java uses a “//” to begin an inline comment, ignoring everything subsequently on that line. For example: // This is an inline comment. We will intentionally color all comments in blue in this book, so that they are not confused with executable code. While inline comments are limited to one line, Java allows multiline comments in the form of block comments. Java uses a “/*” to begin a block comment and a “*/” to close it. For example: /* * This is a block comment. */ Block comments that begin with “/**” (note the second asterisk) have a special purpose, allowing a program, called Javadoc, to read these comments and automatically generate software documentation. We discuss the syntax and interpretation of Javadoc comments in Section 1.9.4.

Chapter 1. Java Primer 1.1.1 Base Types For the most commonly used data types, Java provides the following base types (also called primitive types): boolean a boolean value: true or false char 16-bit Unicode character byte 8-bit signed two’s complement integer short 16-bit signed two’s complement integer int 32-bit signed two’s complement integer long 64-bit signed two’s complement integer ﬂoat 32-bit ﬂoating-point number (IEEE 754-1985) double 64-bit ﬂoating-point number (IEEE 754-1985) A variable having one of these types simply stores a value of that type. Integer constants, like 14 or 195, are of type int, unless followed immediately by an ‘L’ or ‘l’, in which case they are of type long. Floating-point constants, like 3.1416 or 6.022e23, are of type double, unless followed immediately by an ‘F’ or ‘f’, in which case they are of type ﬂoat. Code Fragment 1.1 demonstrates the declaration, and initialization in some cases, of various base-type variables. boolean ﬂag = true; boolean verbose, debug; // two variables declared, but not yet initialized char grade = 'A'; byte b = 12; short s = 24; int i, j, k = 257; // three variables declared; only k initialized long l = 890L; // note the use of ”L” here ﬂoat pi = 3.1416F; // note the use of ”F” here double e = 2.71828, a = 6.022e23; // both variables are initialized Code Fragment 1.1: Declarations and initializations of several base-type variables. Note that it is possible to declare (and initialize) multiple variables of the same type in a single statement, as done on lines 2, 6, and 9 of this example. In this code fragment, variables verbose, debug, i, and j remain uninitialized. Variables declared locally within a block of code must be initialized before they are ﬁrst used. A nice feature of Java is that when base-type variables are declared as instance variables of a class (see next section), Java ensures initial default values if not explicitly initialized. In particular, all numeric types are initialized to zero, a boolean is initialized to false, and a character is initialized to the null character by default.

1.2. Classes and Objects 1.2 Classes and Objects In more complex Java programs, the primary “actors” are objects. Every object is an instance of a class, which serves as the type of the object and as a blueprint, deﬁning the data which the object stores and the methods for accessing and modifying that data. The critical members of a class in Java are the following: • Instance variables, which are also called ﬁelds, represent the data associated with an object of a class. Instance variables must have a type, which can either be a base type (such as int, ﬂoat, or double) or any class type (also known as a reference type for reasons we soon explain). • Methods in Java are blocks of code that can be called to perform actions (similar to functions and procedures in other high-level languages). Methods can accept parameters as arguments, and their behavior may depend on the object upon which they are invoked and the values of any parameters that are passed. A method that returns information to the caller without changing any instance variables is known as an accessor method, while an update method is one that may change one or more instance variables when called. For the purpose of illustration, Code Fragment 1.2 provides a complete definition of a very simple class named Counter, to which we will refer during the remainder of this section. public class Counter { private int count; // a simple integer instance variable public Counter() { } // default constructor (count is 0) public Counter(int initial) { count = initial; } // an alternate constructor public int getCount() { return count; } // an accessor method public void increment() { count++; } // an update method public void increment(int delta) { count += delta; } // an update method public void reset() { count = 0; } // an update method } Code Fragment 1.2: A Counter class for a simple counter, which can be queried, incremented, and reset. This class includes one instance variable, named count, which is declared at line 2. As noted on the previous page, the count will have a default value of zero, unless we otherwise initialize it. The class includes two special methods known as constructors (lines 3 and 4), one accessor method (line 5), and three update methods (lines 6–8). Unlike the original Universe class from page 2, our Counter class does not have a main method, and so it cannot be run as a complete program. Instead, the purpose of the Counter class is to create instances that might be used as part of a larger program.

Chapter 1. Java Primer 1.2.1 Creating and Using Objects Before we explore the intricacies of the syntax for our Counter class deﬁnition, we prefer to describe how Counter instances can be created and used. To this end, Code Fragment 1.3 presents a new class named CounterDemo. public class CounterDemo { public static void main(String[ ] args) { Counter c; // declares a variable; no counter yet constructed c = new Counter(); // constructs a counter; assigns its reference to c c.increment(); // increases its value by one c.increment(3); // increases its value by three more int temp = c.getCount(); // will be 4 c.reset(); // value becomes 0 Counter d = new Counter(5);// declares and constructs a counter having value 5 d.increment(); // value becomes 6 Counter e = d; // assigns e to reference the same object as d temp = e.getCount(); // will be 6 (as e and d reference the same counter) e.increment(2); // value of e (also known as d) becomes 8 } } Code Fragment 1.3: A demonstration of the use of Counter instances. There is an important distinction in Java between the treatment of base-type variables and class-type variables. At line 3 of our demonstration, a new variable c is declared with the syntax: Counter c; This establishes the identiﬁer, c, as a variable of type Counter, but it does not create a Counter instance. Classes are known as reference types in Java, and a variable of that type (such as c in our example) is known as a reference variable. A reference variable is capable of storing the location (i.e., memory address) of an object from the declared class. So we might assign it to reference an existing instance or a newly constructed instance. A reference variable can also store a special value, null, that represents the lack of an object. In Java, a new object is created by using the new operator followed by a call to a constructor for the desired class; a constructor is a method that always shares the same name as its class. The new operator returns a reference to the newly created instance; the returned reference is typically assigned to a variable for further use. In Code Fragment 1.3, a new Counter is constructed at line 4, with its reference assigned to the variable c. That relies on a form of the constructor, Counter(), that takes no arguments between the parentheses. (Such a zero-parameter constructor is known as a default constructor.) At line 9 we construct another counter using a one-parameter form that allows us to specify a nonzero initial value for the counter.

1.2. Classes and Objects Three events occur as part of the creation of a new instance of a class: • A new object is dynamically allocated in memory, and all instance variables are initialized to standard default values. The default values are null for reference variables and 0 for all base types except boolean variables (which are false by default). • The constructor for the new object is called with the parameters speciﬁed. The constructor may assign more meaningful values to any of the instance variables, and perform any additional computations that must be done due to the creation of this object. • After the constructor returns, the new operator returns a reference (that is, a memory address) to the newly created object. If the expression is in the form of an assignment statement, then this address is stored in the object variable, so the object variable refers to this newly created object. The Dot Operator One of the primary uses of an object reference variable is to access the members of the class for this object, an instance of its class. That is, an object reference variable is useful for accessing the methods and instance variables associated with an object. This access is performed with the dot (“.”) operator. We call a method associated with an object by using the reference variable name, following that by the dot operator and then the method name and its parameters. For example, in Code Fragment 1.3, we call c.increment() at line 5, c.increment(3) at line 6, c.getCount() at line 7, and c.reset() at line 8. If the dot operator is used on a reference that is currently null, the Java runtime environment will throw a NullPointerException. If there are several methods with this same name deﬁned for a class, then the Java runtime system uses the one that matches the actual number of parameters sent as arguments, as well as their respective types. For example, our Counter class supports two methods named increment: a zero-parameter form and a oneparameter form. Java determines which version to call when evaluating commands such as c.increment() versus c.increment(3). A method’s name combined with the number and types of its parameters is called a method’s signature, for it takes all of these parts to determine the actual method to perform for a certain method call. Note, however, that the signature of a method in Java does not include the type that the method returns, so Java does not allow two methods with the same signature to return different types. A reference variable v can be viewed as a “pointer” to some object o. It is as if the variable is a holder for a remote control that can be used to control the newly created object (the device). That is, the variable has a way of pointing at the object and asking it to do things or give us access to its data. We illustrate this concept in Figure 1.2. Using the remote control analogy, a null reference is a remote control holder that is empty.

Chapter 1. Java Primer Figure 1.2: Illustrating the relationship between objects and object reference variables. When we assign an object reference (that is, memory address) to a reference variable, it is as if we are storing that object’s remote control at that variable. There can, in fact, be many references to the same object, and each reference to a speciﬁc object can be used to call methods on that object. Such a situation would correspond to our having many remote controls that all work on the same device. Any of the remotes can be used to make a change to the device (like changing a channel on a television). Note that if one remote control is used to change the device, then the (single) object pointed to by all the remotes changes. Likewise, if one object reference variable is used to change the state of the object, then its state changes for all the references to it. This behavior comes from the fact that there are many references, but they all point to the same object. Returning to our CounterDemo example, the instance constructed at line 9 as Counter d = new Counter(5); is a distinct instance from the one identiﬁed as c. However, the command at line 11, Counter e = d; does not result in the construction of a new Counter instance. This declares a new reference variable named e, and assigns that variable a reference to the existing counter instance currently identiﬁed as d. At that point, both variables d and e are aliases for the same object, and so the call to d.getCount() behaves just as would e.getCount(). Similarly, the call to update method e.increment(2) is affecting the same object identiﬁed by d. It is worth noting, however, that the aliasing of two reference variables to the same object is not permanent. At any point in time, we may reassign a reference variable to a new instance, to a different existing instance, or to null.

1.2. Classes and Objects 1.2.2 Deﬁning a Class Thus far, we have provided deﬁnitions for two simple classes: the Universe class on page 2 and the Counter class on page 5. At its core, a class deﬁnition is a block of code, delimited by braces “{” and “}” , within which is included declarations of instance variables and methods that are the members of the class. In this section, we will undertake a deeper examination of class deﬁnitions in Java. Modiﬁers Immediately before the deﬁnition of a class, instance variable, or method in Java, keywords known as modiﬁers can be placed to convey additional stipulations about that deﬁnition. Access Control Modiﬁers The ﬁrst set of modiﬁers we discuss are known as access control modiﬁers, as they control the level of access (also known as visibility) that the deﬁning class grants to other classes in the context of a larger Java program. The ability to limit access among classes supports a key principle of object-orientation known as encapsulation (see Section 2.1). In general, the different access control modiﬁers and their meaning are as follows: • The public class modiﬁer designates that all classes may access the deﬁned aspect. For example, line 1 of of Code Fragment 1.2 designates public class Counter { and therefore all other classes (such as CounterDemo) are allowed to construct new instances of the Counter class, as well as to declare variables and parameters of type Counter. In Java, each public class must be deﬁned in a separate ﬁle named classname.java, where “classname” is the name of the class (for example, ﬁle Counter.java for the Counter class deﬁnition). The designation of public access for a particular method of a class allows any other class to make a call to that method. For example, line 5 of Code Fragment 1.2 designates public int getCount() { return count; } which is why the CounterDemo class may call c.getCount(). If an instance variable is declared as public, dot notation can be used to directly access the variable by code in any other class that possesses a reference to an instance of this class. For example, were the count variable of Counter to be declared as public (which it is not), then the CounterDemo would be allowed to read or modify that variable using a syntax such as c.count.

Chapter 1. Java Primer • The protected class modiﬁer designates that access to the deﬁned aspect is only granted to the following groups of other classes: ◦Classes that are designated as subclasses of the given class through inheritance. (We will discuss inheritance as the focus of Section 2.2.) ◦Classes that belong to the same package as the given class. (We will discuss packages within Section 1.8.) • The private class modiﬁer designates that access to a deﬁned member of a class be granted only to code within that class. Neither subclasses nor any other classes have access to such members. For example, we deﬁned the count instance variable of the Counter class to have private access level. We were allowed to read or edit its value from within methods of that class (such as getCount, increment, and reset), but other classes such as CounterDemo cannot directly access that ﬁeld. Of course, we did provide other public methods to grant outside classes with behaviors that depended on the current count value. • Finally, we note that if no explicit access control modiﬁer is given, the deﬁned aspect has what is known as package-private access level. This allows other classes in the same package (see Section 1.8) to have access, but not any classes or subclasses from other packages. The static Modiﬁer The static modiﬁer in Java can be declared for any variable or method of a class (or for a nested class, as we will introduce in Section 2.6). When a variable of a class is declared as static, its value is associated with the class as a whole, rather than with each individual instance of that class. Static variables are used to store “global” information about a class. (For example, a static variable could be used to maintain the total number of instances of that class that have been created.) Static variables exist even if no instance of their class exists. When a method of a class is declared as static, it too is associated with the class itself, and not with a particular instance of the class. That means that the method is not invoked on a particular instance of the class using the traditional dot notation. Instead, it is typically invoked using the name of the class as a qualiﬁer. As an example, in the java.lang package, which is part of the standard Java distribution, there is a Math class that provides many static methods, including one named sqrt that computes square roots of numbers. To compute a square root, you do not need to create an instance of the Math class; that method is called using a syntax such as Math.sqrt(2), with the class name Math as the qualiﬁer before the dot operator. Static methods can be useful for providing utility behaviors related to a class that need not rely on the state of any particular instance of that class.

1.2. Classes and Objects The abstract Modiﬁer A method of a class may be declared as abstract, in which case its signature is provided but without an implementation of the method body. Abstract methods are an advanced feature of object-oriented programming to be combined with inheritance, and the focus of Section 2.3.3. In short, any subclass of a class with abstract methods is expected to provide a concrete implementation for each abstract method. A class with one or more abstract methods must also be formally declared as abstract, because it is essentially incomplete. (It is also permissible to declare a class as abstract even if it does not contain any abstract methods.) As a result, Java will not allow any instances of an abstract class to be constructed, although reference variables may be declared with an abstract type. The ﬁnal Modiﬁer A variable that is declared with the ﬁnal modiﬁer can be initialized as part of that declaration, but can never again be assigned a new value. If it is a base type, then it is a constant. If a reference variable is ﬁnal, then it will always refer to the same object (even if that object changes its internal state). If a member variable of a class is declared as ﬁnal, it will typically be declared as static as well, because it would be unnecessarily wasteful to have every instance store the identical value when that value can be shared by the entire class. Designating a method or an entire class as ﬁnal has a completely different consequence, only relevant in the context of inheritance. A ﬁnal method cannot be overridden by a subclass, and a ﬁnal class cannot even be subclassed. Declaring Instance Variables When deﬁning a class, we can declare any number of instance variables. An important principle of object-orientation is that each instance of a class maintains its own individual set of instance variables (that is, in fact, why they are called instance variables). So in the case of the Counter class, each instance will store its own (independent) value of count. The general syntax for declaring one or more instance variables of a class is as follows (with optional portions bracketed): [modiﬁers] type identiﬁer1[=initialValue1], identiﬁer2[=initialValue2]; In the case of the Counter class, we declared private int count; where private is the modiﬁer, int is the type, and count is the identiﬁer. Because we did not declare an initial value, it automatically receives the default of zero as a base-type integer.

Chapter 1. Java Primer Declaring Methods A method deﬁnition has two parts: the signature, which deﬁnes the name and parameters for a method, and the body, which deﬁnes what the method does. The method signature speciﬁes how the method is called, and the method body speciﬁes what the object will do when it is called. The syntax for deﬁning a method is as follows: [modiﬁers] returnType methodName(type1 param1 , ..., typen paramn) { // method body . . . } Each of the pieces of this declaration has an important purpose. We have already discussed the signiﬁcance of modiﬁers such as public, private, and static. The returnType designation deﬁnes the type of value returned by the method. The methodName can be any valid Java identiﬁer. The list of parameters and their types declares the local variables that correspond to the values that are to be passed as arguments to this method. Each type declaration typei can be any Java type name and each parami can be any distinct Java identiﬁer. This list of parameters and their types can be empty, which signiﬁes that there are no values to be passed to this method when it is invoked. These parameter variables, as well as the instance variables of the class, can be used inside the body of the method. Likewise, other methods of this class can be called from inside the body of a method. When a (nonstatic) method of a class is called, it is invoked on a speciﬁc instance of that class and can change the state of that object. For example, the following method of the Counter class increases the counter’s value by the given amount. public void increment(int delta) { count += delta; } Notice that the body of this method uses count, which is an instance variable, and delta, which is a parameter. Return Types A method deﬁnition must specify the type of value the method will return. If the method does not return a value (as with the increment method of the Counter class), then the keyword void must be used. To return a value in Java, the body of the method must use the return keyword, followed by a value of the appropriate return type. Here is an example of a method (from the Counter class) with a nonvoid return type: public int getCount() { return count; }

1.2. Classes and Objects Java methods can return only one value. To return multiple values in Java, we should instead combine all the values we want to return in a compound object, whose instance variables include all the values we want to return, and then return a reference to that compound object. In addition, we can change the internal state of an object that is passed to a method as another way of “returning” multiple results. Parameters A method’s parameters are deﬁned in a comma-separated list enclosed in parentheses after the name of the method. A parameter consists of two parts, the parameter type and the parameter name. If a method has no parameters, then only an empty pair of parentheses is used. All parameters in Java are passed by value, that is, any time we pass a parameter to a method, a copy of that parameter is made for use within the method body. So if we pass an int variable to a method, then that variable’s integer value is copied. The method can change the copy but not the original. If we pass an object reference as a parameter to a method, then the reference is copied as well. Remember that we can have many different variables that all refer to the same object. Reassigning the internal reference variable inside a method will not change the reference that was passed in. For the sake of demonstration, we will assume that the following two methods were added to an arbitrary class (such as CounterDemo). public static void badReset(Counter c) { c = new Counter(); // reassigns local name c to a new counter } public static void goodReset(Counter c) { c.reset(); // resets the counter sent by the caller } Now we will assume that variable strikes refers to an existing Counter instance in some context, and that it currently has a value of 3. If we were to call badReset(strikes), this has no effect on the Counter known as strikes. The body of the badReset method reassigns the (local) parameter variable c to reference a newly created Counter instance; but this does not change the state of the existing counter that was sent by the caller (i.e., strikes). In contrast, if we were to call goodReset(strikes), this does indeed reset the caller’s counter back to a value of zero. That is because the variables c and strikes are both reference variables that refer to the same Counter instance. So when c.reset() is called, that is effectively the same as if strikes.reset() were called.

Chapter 1. Java Primer Deﬁning Constructors A constructor is a special kind of method that is used to initialize a newly created instance of the class so that it will be in a consistent and stable initial state. This is typically achieved by initializing each instance variable of the object (unless the default value will sufﬁce), although a constructor can perform more complex computation. The general syntax for declaring a constructor in Java is as follows: modiﬁers name(type0 parameter0 , ..., typen−1 parametern−1) { // constructor body . . . } Constructors are deﬁned in a very similar way as other methods of a class, but there are a few important distinctions: 1. Constructors cannot be static, abstract, or ﬁnal, so the only modiﬁers that are allowed are those that affect visibility (i.e., public, protected, private, or the default package-level visibility). 2. The name of the constructor must be identical to the name of the class it constructs. For example, when deﬁning the Counter class, a constructor must be named Counter as well. 3. We don’t specify a return type for a constructor (not even void). Nor does the body of a constructor explicitly return anything. When a user of a class creates an instance using a syntax such as Counter d = new Counter(5); the new operator is responsible for returning a reference to the new instance to the caller; the responsibility of the constructor method is only to initialize the state of the new instance. A class can have many constructors, but each must have a different signature, that is, each must be distinguished by the type and number of the parameters it takes. If no constructors are explicitly deﬁned, Java provides an implicit default constructor for the class, having zero arguments and leaving all instance variables initialized to their default values. However, if a class deﬁnes one or more nondefault constructors, no default constructor will be provided. As an example, our Counter class deﬁnes the following pair of constructors: public Counter() { } public Counter(int initial) { count = initial; } The ﬁrst of these has a trivial body, { }, as the goal for this default constructor is to create a counter with value zero, and that is already the default value of the integer instance variable, count. However, it is still important that we declared such an explicit constructor, because otherwise none would have been provided, given the existence of the nondefault constructor. In that scenario, a user would have been unable to use the syntax, new Counter().

1.2. Classes and Objects The Keyword this Within the body of a (nonstatic) method in Java, the keyword this is automatically deﬁned as a reference to the instance upon which the method was invoked. That is, if a caller uses a syntax such as thing.foo(a, b, c), then within the body of method foo for that call, the keyword this refers to the object known as thing in the caller’s context. There are three common reasons why this reference is needed from within a method body: 1. To store the reference in a variable, or send it as a parameter to another method that expects an instance of that type as an argument. 2. To differentiate between an instance variable and a local variable with the same name. If a local variable is declared in a method having the same name as an instance variable for the class, that name will refer to the local variable within that method body. (We say that the local variable masks the instance variable.) In this case, the instance variable can still be accessed by explicitly using the dot notation with this as the qualiﬁer. For example, some programmers prefer to use the following style for a constructor, with a parameter having the same name as the underlying variable. public Counter(int count) { this.count = count; // set the instance variable equal to parameter } 3. To allow one constructor body to invoke another constructor body. When one method of a class invokes another method of that same class on the current instance, that is typically done by using the (unqualiﬁed) name of the other method. But the syntax for calling a constructor is special. Java allows use of the keyword this to be used as a method within the body of one constructor, so as to invoke another constructor with a different signature. This is often useful because all of the initialization steps of one constructor can be reused with appropriate parameterization. As a trivial demonstration of the syntax, we could reimplement the zero-argument version of our Counter constructor to have it invoke the one-argument version sending 0 as an explicit parameter. This would be written as follows: public Counter() { this(0); // invoke one-parameter constructor with value zero } We will provide a more meaningful demonstration of this technique in a later example of a CreditCard class in Section 1.7.

Chapter 1. Java Primer The main Method Some Java classes, such as our Counter class, are meant to be used by other classes, but are not intended to serve as a self-standing program. The primary control for an application in Java must begin in some class with the execution of a special method named main. This method must be declared as follows: public static void main(String[ ] args) { // main method body... } The args parameter is an array of String objects, that is, a collection of indexed strings, with the ﬁrst string being args[0], the second being args[1], and so on. (We say more about strings and arrays in Section 1.3.) Those represent what are known as command-line arguments that are given by a user when the program is executed. Java programs can be called from the command line using the java command (in a Windows, Linux, or Unix shell), followed by the name of the Java class whose main method we want to run, plus any optional arguments. For example, to execute the main method of a class named Aquarium, we could issue the following command: java Aquarium In this case, the Java runtime system looks for a compiled version of the Aquarium class, and then invokes the special main method in that class. If we had deﬁned the Aquarium program to take an optional argument that speciﬁes the number of ﬁsh in the aquarium, then we might invoke the program by typing the following in a shell window: java Aquarium 45 to specify that we want an aquarium with 45 ﬁsh in it. In this case, args[0] would refer to the string "45". It would be up to the body of the main method to interpret that string as the desired number of ﬁsh. Programmers who use an integrated development environment (IDE), such as Eclipse, can optionally specify command-line arguments when executing the program through the IDE. Unit Testing When deﬁning a class, such as Counter, that is meant to be used by other classes rather than as a self-standing program, there is no need to deﬁne a main method. However, a nice feature of Java’s design is that we could provide such a method as a way to test the functionality of that class in isolation, knowing that it would not be run unless we speciﬁcally invoke the java command on that isolated class. However, for more robust testing, frameworks such as JUnit are preferred.

1.3. Strings, Wrappers, Arrays, and Enum Types 1.3 Strings, Wrappers, Arrays, and Enum Types The String Class Java’s char base type stores a value that represents a single text character. In Java, the set of all possible characters, known as an alphabet, is the Unicode international character set, a 16-bit character encoding that covers most used written languages. (Some programming languages use the smaller ASCII character set, which is a proper subset of the Unicode alphabet based on a 7-bit encoding.) The form for expressing a character literal in Java is using single quotes, such as 'G'. Because it is common to work with sequences of text characters in programs (e.g., for user interactions or data processing), Java provides support in the form of a String class. A string instance represents a sequence of zero or more characters. The class provides extensive support for various text-processing tasks, and in Chapter 13 we will examine several of the underlying algorithms for text processing. For now, we will only highlight the most central aspects of the String class. Java uses double quotes to designate string literals. Therefore, we might declare and initialize a String instance as follows: String title = "Data Structures & Algorithms in Java" Character Indexing Each character c within a string s can be referenced by using an index, which is equal to the number of characters that come before c in s. By this convention, the ﬁrst character is at index 0, and the last is at index n−1, where n is the length of the string. For example, the string title, deﬁned above, has length 36. The character at index 2 is 't' (the third character), and the character at index 4 is ' ' (the space character). Java’s String class supports a method length(), which returns the length of a string instance, and a method charAt(k), which returns the character at index k. Concatenation The primary operation for combining strings is called concatenation, which takes a string P and a string Q combines them into a new string, denoted P+ Q, which consists of all the characters of P followed by all the characters of Q. In Java, the “+” operation performs concatenation when acting on two strings, as follows: String term = "over" + "load"; This statement deﬁnes a variable named term that references a string with value "overload". (We will discuss assignment statements and expressions such as that above in more detail later in this chapter.)

Chapter 1. Java Primer The StringBuilder Class An important trait of Java’s String class is that its instances are immutable; once an instance is created and initialized, the value of that instance cannot be changed. This is an intentional design, as it allows for great efﬁciencies and optimizations within the Java Virtual Machine. However, because String is a class in Java, it is a reference type. Therefore, variables of type String can be reassigned to another string instance (even if the current string instance cannot be changed), as in the following: String greeting = "Hello"; greeting = "Ciao"; // we changed our mind It is also quite common in Java to use string concatenation to build a new string that is subsequently used to replace one of the operands of concatenation, as in: greeting = greeting + '!'; // now it is ”Ciao!” However, it is important to remember that this operation does create a new string instance, copying all the characters of the existing string in the process. For long string (such as DNA sequences), this can be very time consuming. (In fact, we will experiment with the efﬁciency of string concatenation to begin Chapter 4.) In order to support more efﬁcient editing of character strings, Java provides a StringBuilder class, which is effectively a mutable version of a string. This class combines some of the accessor methods of the String class, while supporting additional methods including the following (and more): setCharAt(k,c): Change the character at index k to character c. insert(k,s): Insert a copy of string s starting at index k of the sequence, shifting existing characters further back to make room. append(s): Append string s to the end of the sequence. reverse(): Reverse the current sequence. toString(): Return a traditional String instance based on the current character sequence. An error condition occurs, for both String and StringBuilder classes, if an index k is out of the bounds of the indices of the character sequence. The StringBuilder class can be very useful, and it serves as an interesting case study for data structures and algorithms. We will further explore the empirical efﬁciency of the StringBuilder class in Section 4.1 and the theoretical underpinnings of its implementation in Section 7.2.4.

1.3. Strings, Wrappers, Arrays, and Enum Types Wrapper Types There are many data structures and algorithms in Java’s libraries that are specifically designed so that they only work with object types (not primitives). To get around this obstacle, Java deﬁnes a wrapper class for each base type. An instance of each wrapper type stores a single value of the corresponding base type. In Table 1.2, we show the base types and their corresponding wrapper class, along with examples of how objects are created and accessed. Base Type Class Name Creation Example Access Example boolean Boolean obj = new Boolean(true); obj.booleanValue() char Character obj = new Character(’Z’); obj.charValue() byte Byte obj = new Byte((byte) 34); obj.byteValue() short Short obj = new Short((short) 100); obj.shortValue() int Integer obj = new Integer(1045); obj.intValue() long Long obj = new Long(10849L); obj.longValue() ﬂoat Float obj = new Float(3.934F); obj.ﬂoatValue() double Double obj = new Double(3.934); obj.doubleValue() Table 1.2: Java’s wrapper classes. Each class is given with its corresponding base type and example expressions for creating and accessing such objects. For each row, we assume the variable obj is declared with the corresponding class name. Automatic Boxing and Unboxing Java provides additional support for implicitly converting between base types and their wrapper types through a process known as automatic boxing and unboxing. In any context for which an Integer is expected (for example, as a parameter), an int value k can be expressed, in which case Java automatically boxes the int, with an implicit call to new Integer(k). In reverse, in any context for which an int is expected, an Integer value v can be given in which case Java automatically unboxes it with an implicit call to v.intValue(). Similar conversions are made with the other base-type wrappers. Finally, all of the wrapper types provide support for converting back and forth between string literals. Code Fragment 1.4 demonstrates many such features. int j = 8; Integer a = new Integer(12); int k = a; // implicit call to a.intValue() int m = j + a; // a is automatically unboxed before the addition a = 3 ∗m; // result is automatically boxed before assignment Integer b = new Integer("-135"); // constructor accepts a String int n = Integer.parseInt("2013"); // using static method of Integer class Code Fragment 1.4: A demonstration of the use of the Integer wrapper class.

Chapter 1. Java Primer Arrays A common programming task is to keep track of an ordered sequence of related values or objects. For example, we may want a video game to keep track of the top ten scores for that game. Rather than using ten different variables for this task, we would prefer to use a single name for the group and use index numbers to refer to the high scores in that group. Similarly, we may want a medical information system to keep track of the patients currently assigned to beds in a certain hospital. Again, we would rather not have to introduce 200 variables in our program just because the hospital has 200 beds. In such cases, we can save programming effort by using an array, which is a sequenced collection of variables all of the same type. Each variable, or cell, in an array has an index, which uniquely refers to the value stored in that cell. The cells of an array a are numbered 0, 1, 2, and so on. We illustrate an array of high scores for a video game in Figure 1.3. indices High scores Figure 1.3: An illustration of an array of ten (int) high scores for a video game. Array Elements and Capacities Each value stored in an array is called an element of that array. Since the length of an array determines the maximum number of things that can be stored in the array, we will sometimes refer to the length of an array as its capacity. In Java, the length of an array named a can be accessed using the syntax a.length. Thus, the cells of an array a are numbered 0, 1, 2, and so on, up through a.length−1, and the cell with index k can be accessed with syntax a[k]. Out of Bounds Errors It is a dangerous mistake to attempt to index into an array a using a number outside the range from 0 to a.length−1. Such a reference is said to be out of bounds. Out of bounds references have been exploited numerous times by hackers using a method called the buffer overﬂow attack to compromise the security of computer systems written in languages other than Java. As a safety feature, array indices are always checked in Java to see if they are ever out of bounds. If an array index is out of bounds, the runtime Java environment signals an error condition. The name of this condition is the ArrayIndexOutOfBoundsException. This check helps Java avoid a number of security problems, such as buffer overﬂow attacks.

1.3. Strings, Wrappers, Arrays, and Enum Types Declaring and Constructing Arrays Arrays in Java are somewhat unusual, in that they are not technically a base type nor are they instances of a particular class. With that said, an instance of an array is treated as an object by Java, and variables of an array type are reference variables. To declare a variable (or parameter) to have an array type, we use an empty pair of square brackets just after the type of element that the array will store. For example, we might declare: int[ ] primes; Because arrays are a reference type, this declares the variable primes to be a reference to an array of integer values, but it does not immediately construct any such array. There are two ways for creating an array. The ﬁrst way to create an array is to use an assignment to a literal form when initially declaring the array, using a syntax as: elementType[] arrayName = {initialValue0, initialValue1, . . . , initialValueN−1}; The elementType can be any Java base type or class name, and arrayName can be any valid Java identiﬁer. The initial values must be of the same type as the array. For example, we could initialize the array of primes to contain the ﬁrst ten prime numbers as: int[ ] primes = {2, 3, 5, 7, 11, 13, 17, 19, 23, 29}; When using an initializer, an array is created having precisely the capacity needed to store the indicated values. The second way to create an array is to use the new operator. However, because an array is not an instance of a class, we do not use a typical constructor syntax. Instead we use the syntax: new elementType[length] where length is a positive integer denoting the length of the new array. The new operator returns a reference to the new array, and typically this would be assigned to an array variable. For example, the following statement declares an array variable named measurements, and immediately assigns it a new array of 1000 cells. double[ ] measurements = new double[1000]; When arrays are created using the new operator, all of their elements are automatically assigned the default value for the element type. That is, if the element type is numeric, all cells of the array are initialized to zero, if the element type is boolean, all cells are false, and if the element type is a reference type (such as with an array of String instances), all cells are initialized to null.

Chapter 1. Java Primer Enum Types In olden times, programmers would often deﬁne a series of constant integer values to be used for representing a ﬁnite set of choices. For example, in representing a day of the week, they might declare variable today as an int and then set it with value 0 for Monday, 1 for Tuesday, and so on. A slightly better programming style is to deﬁne static constants (with the ﬁnal keyword), to make the associations, such as: static ﬁnal int MON = 0; static ﬁnal int TUE = 1; static ﬁnal int WED = 2; ... because then it becomes possible to make assignments such as today = TUE, rather than the more obscure today = 1. Unfortunately, the variable today is still declared as an int using such a programming style, and it may not be clear that you intend for it to represent a day of the week when storing it as an instance variable or sending it as a parameter. Java supports a more elegant approach to representing choices from a ﬁnite set by deﬁning what is known as an enumerated type, or enum for short. These are types that are only allowed to take on values that come from a speciﬁed set of names. They are declared as follows: modiﬁer enum name { valueName0 , valueName1 , ..., valueNamen−1 }; where the modiﬁer can be blank, public, protected, or private. The name of this enum, name, can be any legal Java identiﬁer. Each of the value identiﬁers, valueNamei, is the name of a possible value that variables of this enum type can take on. Each of these name values can also be any legal Java identiﬁer, but the Java convention is that these should usually be capitalized words. For example, an enumerated type deﬁnition for days of the weak might appear as: public enum Day { MON, TUE, WED, THU, FRI, SAT, SUN }; Once deﬁned, Day becomes an ofﬁcial type and we may declare variables or parameters with type Day. A variable of that type can be declared as: Day today; and an assignment of a value to that variable can appear as: today = Day.TUE;

1.4. Expressions 1.4 Expressions Variables and constants are used in expressions to deﬁne new values and to modify variables. In this section, we discuss how expressions work in Java in more detail. Expressions involve the use of literals, variables, and operators. Since we have already discussed variables, let us brieﬂy focus on literals and then discuss operators in some detail. 1.4.1 Literals A literal is any “constant” value that can be used in an assignment or other expression. Java allows the following kinds of literals: • The null object reference (this is the only object literal, and it is allowed to be any reference type). • Boolean: true and false. • Integer: The default for an integer like 176, or -52 is that it is of type int, which is a 32-bit integer. A long integer literal must end with an “L” or “l”, for example, 176L or -52l, and deﬁnes a 64-bit integer. • Floating Point: The default for ﬂoating-point numbers, such as 3.1415 and 135.23, is that they are double. To specify that a literal is a ﬂoat, it must end with an “F” or “f”. Floating-point literals in exponential notation are also allowed, such as 3.14E2 or .19e10; the base is assumed to be 10. • Character: In Java, character constants are assumed to be taken from the Unicode alphabet. Typically, a character is deﬁned as an individual symbol enclosed in single quotes. For example, ’a’ and ’?’ are character constants. In addition, Java deﬁnes the following special character constants: '\n' (newline) '\t' (tab) '\b' (backspace) '\r' (return) '\f' (form feed) '\\' (backslash) '\'' (single quote) '\"' (double quote). • String Literal: A string literal is a sequence of characters enclosed in double quotes, for example, the following is a string literal: "dogs cannot climb trees"

Chapter 1. Java Primer 1.4.2 Operators Java expressions involve composing literals and variables with operators. We will survey the operators in Java in this section. Arithmetic Operators The following are binary arithmetic operators in Java: + addition − subtraction ∗ multiplication / division % the modulo operator This last operator, modulo, is also known as the “remainder” operator, because it is the remainder left after an integer division. We often use “mod” to denote the modulo operator, and we deﬁne it formally as n mod m = r, such that n = mq+r, for an integer q and 0 ≤r < m. Java also provides a unary minus (−), which can be placed in front of an arithmetic expression to invert its sign. Parentheses can be used in any expression to deﬁne the order of evaluation. Java also uses a fairly intuitive operator precedence rule to determine the order of evaluation when parentheses are not used. Unlike C++, Java does not allow operator overloading for class types. String Concatenation With strings, the (+) operator performs concatenation, so that the code String rug = "carpet"; String dog = "spot"; String mess = rug + dog; String answer = mess + " will cost me " + 5 + " hours!"; would have the effect of making answer refer to the string "carpetspot will cost me 5 hours!" This example also shows how Java converts nonstring values (such as 5) into strings, when they are involved in a string concatenation operation.

1.4. Expressions Increment and Decrement Operators Like C and C++, Java provides increment and decrement operators. Speciﬁcally, it provides the plus-one increment (++) and decrement (−−) operators. If such an operator is used in front of a variable reference, then 1 is added to (or subtracted from) the variable and its value is read into the expression. If it is used after a variable reference, then the value is ﬁrst read and then the variable is incremented or decremented by 1. So, for example, the code fragment int i = 8; int j = i++; // j becomes 8 and then i becomes 9 int k = ++i; // i becomes 10 and then k becomes 10 int m = i−−; // m becomes 10 and then i becomes 9 int n = 9 + −−i; // i becomes 8 and then n becomes 17 assigns 8 to j, 10 to k, 10 to m, 17 to n, and returns i to value 8, as noted. Logical Operators Java supports the standard comparisons operators between numbers: < less than <= less than or equal to == equal to != not equal to >= greater than or equal to > greater than The type of the result of any of these comparison is a boolean. Comparisons may also be performed on char values, with inequalities determined according to the underlying character codes. For reference types, it is important to know that the operators == and != are deﬁned so that expression a == b is true if a and b both refer to the identical object (or are both null). Most object types support an equals method, such that a.equals(b) is true if a and b refer to what are deemed as “equivalent” instances for that class (even if not the same instance); see Section 3.5 for further discussion. Operators deﬁned for boolean values are the following: ! not (preﬁx) && conditional and || conditional or The boolean operators && and || will not evaluate the second operand (to the right) in their expression if it is not needed to determine the value of the expression. This “short circuiting” feature is useful for constructing boolean expressions where we ﬁrst test that a certain condition holds (such as an array index being valid) and then test a condition that could have otherwise generated an error condition had the prior test not succeeded.

Chapter 1. Java Primer Bitwise Operators Java also provides the following bitwise operators for integers and booleans: ∼ bitwise complement (preﬁx unary operator) & bitwise and | bitwise or ˆ bitwise exclusive-or << shift bits left, ﬁlling in with zeros >> shift bits right, ﬁlling in with sign bit >>> shift bits right, ﬁlling in with zeros The Assignment Operator The standard assignment operator in Java is “=”. It is used to assign a value to an instance variable or local variable. Its syntax is as follows: variable = expression where variable refers to a variable that is allowed to be referenced by the statement block containing this expression. The value of an assignment operation is the value of the expression that was assigned. Thus, if j and k are both declared as type int, it is correct to have an assignment statement like the following: j = k = 25; // works because ’=’ operators are evaluated right-to-left Compound Assignment Operators Besides the standard assignment operator (=), Java also provides a number of other assignment operators that combine a binary operation with an assignment. These other kinds of operators are of the following form: variable op= expression where op is any binary operator. The above expression is generally equivalent to variable = variable op expression so that x ∗= 2 is equivalent to x = x ∗2. However, if variable contains an expression (for example, an array index), the expression is evaluated only once. Thus, the code fragment a[5] = 10; j = 5; a[j++] += 2; // not the same as a[j++] = a[j++] + 2 leaves a[5] with value 12 and j with value 6.

1.4. Expressions Operator Precedence Operators in Java are given preferences, or precedence, that determine the order in which operations are performed when the absence of parentheses brings up evaluation ambiguities. For example, we need a way of deciding if the expression, “5+2*3,” has value 21 or 11 (Java says it is 11). We show the precedence of the operators in Java (which, incidentally, is the same as in C and C++) in Table 1.3. Operator Precedence Type Symbols array index [ ] method call () dot operator . postﬁx ops exp++ exp−− preﬁx ops ++exp −−exp +exp −exp ˜exp !exp cast (type) exp mult./div. ∗/ % add./subt. + − shift << >> >>> comparison < <= > >= instanceof equality == != bitwise-and & bitwise-xor ˆ bitwise-or | and && or || conditional booleanExpression ? valueIfTrue : valueIfFalse assignment = += −= ∗= /= %= <<= >>= >>>= &= ˆ= |= Table 1.3: The Java precedence rules. Operators in Java are evaluated according to the ordering above if parentheses are not used to determine the order of evaluation. Operators on the same line are evaluated in left-to-right order (except for assignment and preﬁx operations, which are evaluated in right-to-left order), subject to the conditional evaluation rule for boolean && and || operations. The operations are listed from highest to lowest precedence (we use exp to denote an atomic or parenthesized expression). Without parenthesization, higher precedence operators are performed before lower precedence operators. We have now discussed almost all of the operators listed in Table 1.3. A notable exception is the conditional operator, which involves evaluating a boolean expression and then taking on the appropriate value depending on whether this boolean expression is true or false. (We discuss the use of the instanceof operator in the next chapter.)

Chapter 1. Java Primer 1.4.3 Type Conversions Casting is an operation that allows us to change the type of a value. In essence, we can take a value of one type and cast it into an equivalent value of another type. There are two forms of casting in Java: explicit casting and implicit casting. Explicit Casting Java supports an explicit casting syntax with the following form: (type) exp where type is the type that we would like the expression exp to have. This syntax may only be used to cast from one primitive type to another primitive type, or from one reference type to another reference type. We will discuss its use between primitives here, and between reference types in Section 2.5.1. Casting from an int to a double is known as a widening cast, as the double type is more broad than the int type, and a conversion can be performed without losing information. But a cast from a double to an int is a narrowing cast; we may lose precision, as any fractional portion of the value will be truncated. For example, consider the following: double d1 = 3.2; double d2 = 3.9999; int i1 = (int) d1; // i1 gets value 3 int i2 = (int) d2; // i2 gets value 3 double d3 = (double) i2; // d3 gets value 3.0 Although explicit casting cannot directly convert a primitive type to a reference type, or vice versa, there are other means for performing such type conversions. We already discussed, as part of Section 1.3, conversions between Java’s primitive types and corresponding wrapper classes (such as int and Integer). For convenience, those wrapper classes also provide static methods that convert between their corresponding primitive type and String values. For example, the Integer.toString method accepts an int parameter and returns a String representation of that integer, while the Integer.parseInt method accepts a String as a parameter and returns the corresponding int value that the string represents. (If that string does not represent an integer, a NumberFormatException results.) We demonstrate their use as follows: String s1 = "2014"; int i1 = Integer.parseInt(s1); // i1 gets value 2014 int i2 = −35; String s2 = Integer.toString(i2); // s2 gets value ”-35” Similar methods are supported by other wrapper types, such as Double.

1.4. Expressions Implicit Casting There are cases where Java will perform an implicit cast based upon the context of an expression. For example, you can perform a widening cast between primitive types (such as from an int to a double), without explicit use of the casting operator. However, if attempting to do an implicit narrowing cast, a compiler error results. For example, the following demonstrates both a legal and an illegal implicit cast via assignment statements: int i1 = 42; double d1 = i1; // d1 gets value 42.0 i1 = d1; // compile error: possible loss of precision Implicit casting also occurs when performing arithmetic operations involving a mixture of numeric types. Most notably, when performing an operation with an integer type as one operand and a ﬂoating-point type as the other operand, the integer value is implicitly converted to a ﬂoating-point type before the operation is performed. For example, the expression 3 + 5.7 is implicitly converted to 3.0 + 5.7 before computing the resulting double value of 8.7. It is common to combine an explicit cast and an implicit cast to perform a ﬂoating-point division on two integer operands. The expression (double) 7 / 4 produces the result 1.75, because operator precedence dictates that the cast happens ﬁrst, as ( (double) 7) / 4, and thus 7.0 / 4, which implicitly becomes 7.0 / 4.0. Note however that the expression, (double) (7 / 4) produces the result 1.0. Incidentally, there is one situation in Java when only implicit casting is allowed, and that is in string concatenation. Any time a string is concatenated with any object or base type, that object or base type is automatically converted to a string. Explicit casting of an object or base type to a string is not allowed, however. Thus, the following assignments are incorrect: String s = 22; // this is wrong! String t = (String) 4.5; // this is wrong! String u = "Value = " + (String) 13; // this is wrong! To perform a conversion to a string, we must use the appropriate toString method or perform an implicit cast via the concatenation operation. Thus, the following statements are correct: String s = Integer.toString(22); // this is good String t = "" + 4.5; // correct, but poor style String u = "Value = " + 13; // this is good

Chapter 1. Java Primer 1.5 Control Flow Control ﬂow in Java is similar to that of other high-level languages. We review the basic structure and syntax of control ﬂow in Java in this section, including method returns, if statements, switch statements, loops, and restricted forms of “jumps” (the break and continue statements). 1.5.1 The If and Switch Statements In Java, conditionals work similarly to the way they work in other languages. They provide a way to make a decision and then execute one or more different statement blocks based on the outcome of that decision. The If Statement The syntax of a simple if statement is as follows: if (booleanExpression) trueBody else falseBody where booleanExpression is a boolean expression and trueBody and falseBody are each either a single statement or a block of statements enclosed in braces (“{” and “}”). Note that, unlike some similar languages, the value tested by an if statement in Java must be a boolean expression. In particular, it is deﬁnitely not an integer expression. Nevertheless, as in other similar languages, the else part (and its associated statement) in a Java if statement is optional. There is also a way to group a number of boolean tests, as follows: if (ﬁrstBooleanExpression) ﬁrstBody else if (secondBooleanExpression) secondBody else thirdBody If the ﬁrst boolean expression is false, the second boolean expression will be tested, and so on. An if statement can have an arbitrary number of else if parts. Braces can be used for any or all statement bodies to deﬁne their extent.

1.5. Control Flow As a simple example, a robot controller might have the following logic: if (door.isClosed()) door.open(); advance(); Notice that the ﬁnal command, advance(), is not part of the conditional body; it will be executed unconditionally (although after opening a closed door). We may nest one control structure within another, relying on explicit braces to mark the extent of the various bodies if needed. Revisiting our robot example, here is a more complex control that accounts for unlocking a closed door. if (door.isClosed()) { if (door.isLocked()) door.unlock(); door.open(); } advance(); The logic expressed by this example can be diagrammed as a traditional ﬂowchart, as portrayed in Figure 1.4. false advance() door.unlock() door.open() true false true door.isClosed() door.isLocked() Figure 1.4: A ﬂowchart describing the logic of nested conditional statements.

Chapter 1. Java Primer The following is an example of the nesting of if and else clauses. if (snowLevel < 2) { goToClass(); comeHome(); } else if (snowLevel < 5) { goSledding(); haveSnowballFight(); } else stayAtHome(); // single-statement body needs no { } braces Switch Statements Java provides for multiple-value control ﬂow using the switch statement, which is especially useful with enum types. The following is an indicative example (based on a variable d of the Day enum type of Section 1.3). switch (d) { case MON: System.out.println("This is tough."); break; case TUE: System.out.println("This is getting better."); break; case WED: System.out.println("Half way there."); break; case THU: System.out.println("I can see the light."); break; case FRI: System.out.println("Now we are talking."); break; default: System.out.println("Day off!"); } The switch statement evaluates an integer, string, or enum expression and causes control ﬂow to jump to the code location labeled with the value of this expression. If there is no matching label, then control ﬂow jumps to the location labeled “default.” This is the only explicit jump performed by the switch statement, however, so ﬂow of control “falls through” to the next case if the code for a case is not ended with a break statement (which causes control ﬂow to jump to the end).

1.5. Control Flow 1.5.2 Loops Another important control ﬂow mechanism in a programming language is looping. Java provides for three types of loops. While Loops The simplest kind of loop in Java is a while loop. Such a loop tests that a certain condition is satisﬁed and will perform the body of the loop each time this condition is evaluated to be true. The syntax for such a conditional test before a loop body is executed is as follows: while (booleanExpression) loopBody As with an if statement, booleanExpression, can be an arbitrary boolean expression, and the body of the loop can be an arbitrary block of code (including nested control structures). The execution of a while loop begins with a test of the boolean condition. If that condition evaluates to true, the body of the loop is performed. After each execution of the body, the loop condition is retested and if it evaluates to true, another iteration of the body is performed. If the condition evaluates to false when tested (assuming it ever does), the loop is exited and the ﬂow of control continues just beyond the body of the loop. As an example, here is a loop that advances an index through an array named data until ﬁnding an entry with value target or reaching the end of the array. int j = 0; while ((j < data.length) && (data[j] != target)) j++; When this loop terminates, variable j’s value will be the index of the leftmost occurrence of target, if found, or otherwise the length of the array (which is recognizable as an invalid index to indicate failure of the search). The correctness of the loop relies on the short-circuiting behavior of the logical && operator, as described on page 25. We intentionally test j < data.length to ensure that j is a valid index, prior to accessing element data[j]. Had we written that compound condition with the opposite order, the evaluation of data[j] would eventually throw an ArrayIndexOutOfBoundsException if the target is not found. (See Section 2.4 for discussion of exceptions.) We note that a while loop will execute its body zero times in the case that the initial condition fails. For example, our above loop will not increment the value of j if data[0] matches the target (or if the array has length 0).

Chapter 1. Java Primer Do-While Loops Java has another form of the while loop that allows the boolean condition to be checked at the end of each pass of the loop rather than before each pass. This form is known as a do-while loop, and has syntax shown below: do loopBody while (booleanExpression) A consequence of the do-while loop is that its body always executes at least once. (In contrast, a while loop will execute zero times if the initial condition fails.) This form is most useful for a situation in which the condition is ill-deﬁned until after at least one pass. Consider, for example, that we want to prompt the user for input and then do something useful with that input. (We discuss Java input and output in more detail in Section 1.6.) A possible condition, in this case, for exiting the loop is when the user enters an empty string. However, even in this case, we may want to handle that input and inform the user that he or she has quit. The following example illustrates this case: String input; do { input = getInputString(); handleInput(input); } while (input.length() > 0); For Loops Another kind of loop is the for loop. Java supports two different styles of for loop. The ﬁrst, which we will refer to as the “traditional” style, is patterned after a similar syntax as for loops in the C and C++ languages. The second style, which is known as the “for-each” loop, was introduced into Java in 2004 as part of the SE 5 release. This style provides a more succinct syntax for iterating through elements of an array or an appropriate container type. The traditional for-loop syntax consists of four sections—an initialization, a boolean condition, an increment statement, and the body—although any of those can be empty. The structure is as follows: for (initialization; booleanCondition; increment) loopBody For example, the most common use of a for loop provides repetition based on an integer index, such as the following: for (int j=0; j < n; j++) // do something

1.5. Control Flow The behavior of a for loop is very similar to the following while loop equivalent: { initialization; while (booleanCondition) { loopBody; increment; } } The initialization section will be executed once, before any other portion of the loop begins. Traditionally, it is used to either initialize existing variables, or to declare and initialize new variables. Note that any variables declared in the initialization section only exist in scope for the duration of the for loop. The booleanCondition will be evaluated immediately before each potential iteration of the loop. It should be expressed similar to a while-loop condition, in that if it is true, the loop body is executed, and if false, the loop is exited and the program continues to the next statement beyond the for-loop body. The increment section is executed immediately after each iteration of the formal loop body, and is traditionally used to update the value of the primary loop variable. However, the incrementing statement can be any legal statement, allowing signiﬁcant ﬂexibility in coding. As a concrete example, here is a method that computes the sum of an array of double values using a for loop: public static double sum(double[ ] data) { double total = 0; for (int j=0; j < data.length; j++) // note the use of length total += data[j]; return total; } As one further example, the following method computes the maximum value within a (nonempty) array. public static double max(double[ ] data) { double currentMax = data[0]; // assume ﬁrst is biggest (for now) for (int j=1; j < data.length; j++) // consider all other entries if (data[j] > currentMax) // if data[j] is biggest thus far... currentMax = data[j]; // record it as the current max return currentMax; } Notice that a conditional statement is nested within the body of the loop, and that no explicit “{” and “}” braces are needed for the loop body, as the entire conditional construct serves as a single statement.

Chapter 1. Java Primer For-Each Loop Since looping through elements of a collection is such a common construct, Java provides a shorthand notation for such loops, called the for-each loop. The syntax for such a loop is as follows: for (elementType name : container) loopBody where container is an array of the given elementType (or a collection that implements the Iterable interface, as we will later discuss in Section 7.4.1). Revisiting a previous example, the traditional loop for computing the sum of elements in an array of double values can be written as: public static double sum(double[ ] data) { double total = 0; for (double val : data) // Java’s for-each loop style total += val; return total; } When using a for-each loop, there is no explicit use of array indices. The loop variable represents one particular element of the array. However, within the body of the loop, there is no designation as to which element it is. It is also worth emphasizing that making an assignment to the loop variable has no effect on the underlying array. Therefore, the following method is an invalid attempt to scale all values of a numeric array. public static void scaleBad(double[ ] data, double factor) { for (double val : data) val ∗= factor; // changes local variable only } In order to overwrite the values in the cells of an array, we must make use of indices. Therefore, this task is best solved with a traditional for loop, such as the following: public static void scaleGood(double[ ] data, double factor) { for (int j=0; j < data.length; j++) data[j] ∗= factor; // overwrites cell of the array }

1.5. Control Flow 1.5.3 Explicit Control-Flow Statements Java also provides statements that cause explicit change in the ﬂow of control of a program. Returning from a Method If a Java method is declared with a return type of void, then ﬂow of control returns when it reaches the last line of code in the method or when it encounters a return statement (with no argument). If a method is declared with a return type, however, the method must exit by returning an appropriate value as an argument to a return statement. It follows that the return statement must be the last statement executed in a method, as the rest of the code will never be reached. Note that there is a signiﬁcant difference between a statement being the last line of code that is executed in a method and the last line of code in the method itself. The following (correct) example illustrates returning from a method: public double abs(double value) { if (value < 0) // value is negative, return −value; // so return its negation return value; // return the original nonnegative value } In the example above, the line return −value; is clearly not the last line of code that is written in the method, but it may be the last line that is executed (if the original value is negative). Such a statement explicitly interrupts the ﬂow of control in the method. There are two other such explicit control-ﬂow statements, which are used in conjunction with loops and switch statements. The break Statement We ﬁrst introduced use of the break command, in Section 1.5.1, to exit from the body of a switch statement. More generally, it can be used to “break” out of the innermost switch, for, while, or do-while statement body. When it is executed, a break statement causes the ﬂow of control to jump to the next line after the loop or switch to the body containing the break. The continue Statement A continue statement can be used within a loop. It causes the execution to skip over the remaining steps of the current iteration of the loop body, but then, unlike the break statement, the ﬂow of control returns to the top of the loop, assuming its condition remains satisﬁed.

Chapter 1. Java Primer 1.6 Simple Input and Output Java provides a rich set of classes and methods for performing input and output within a program. There are classes in Java for doing graphical user interface design, complete with pop-up windows and pull-down menus, as well as methods for the display and input of text and numbers. Java also provides methods for dealing with graphical objects, images, sounds, Web pages, and mouse events (such as clicks, mouse overs, and dragging). Moreover, many of these input and output methods can be used in either stand-alone programs or in applets. Unfortunately, going into the details on how all of the methods work for constructing sophisticated graphical user interfaces is beyond the scope of this book. Still, for the sake of completeness, we describe how simple input and output can be done in Java in this section. Simple input and output in Java occurs within the Java console window. Depending on the Java environment we are using, this window is either a special pop-up window that can be used for displaying and inputting text, or a window used to issue commands to the operating system (such windows are referred to as shell windows, command windows, or terminal windows). Simple Output Methods Java provides a built-in static object, called System.out, that performs output to the “standard output” device. Most operating system shells allow users to redirect standard output to ﬁles or even as input to other programs, but the default output is to the Java console window. The System.out object is an instance of the java.io.PrintStream class. This class deﬁnes methods for a buffered output stream, meaning that characters are put in a temporary location, called a buffer, which is then emptied when the console window is ready to print characters. Speciﬁcally, the java.io.PrintStream class provides the following methods for performing simple output (we use baseType here to refer to any of the possible base types): print(String s): Print the string s. print(Object o): Print the object o using its toString method. print(baseType b): Print the base type value b. println(String s): Print the string s, followed by the newline character. println(Object o): Similar to print(o), followed by the newline character. println(baseType b): Similar to print(b), followed by the newline character.

1.6. Simple Input and Output An Output Example Consider, for example, the following code fragment: System.out.print("Java values: "); System.out.print(3.1416); System.out.print(','); System.out.print(15); System.out.println(" (double,char,int)."); When executed, this fragment will output the following in the Java console window: Java values: 3.1416,15 (double,char,int). Simple Input Using the java.util.Scanner Class Just as there is a special object for performing output to the Java console window, there is also a special object, called System.in, for performing input from the Java console window. Technically, the input is actually coming from the “standard input” device, which by default is the computer keyboard echoing its characters in the Java console. The System.in object is an object associated with the standard input device. A simple way of reading input with this object is to use it to create a Scanner object, using the expression new Scanner(System.in) The Scanner class has a number of convenient methods that read from the given input stream, one of which is demonstrated in the following program: import java.util.Scanner; // loads Scanner deﬁnition for our use public class InputExample { public static void main(String[ ] args) { Scanner input = new Scanner(System.in); System.out.print("Enter your age in years: "); double age = input.nextDouble(); System.out.print("Enter your maximum heart rate: "); double rate = input.nextDouble(); double fb = (rate −age) ∗0.65; System.out.println("Your ideal fat-burning heart rate is " + fb); } } When executed, this program could produce the following on the Java console: Enter your age in years: 21 Enter your maximum heart rate: 220 Your ideal fat-burning heart rate is 129.35

Chapter 1. Java Primer java.util.Scanner Methods The Scanner class reads the input stream and divides it into tokens, which are strings of characters separated by delimiters. A delimiter is a special separating string, and the default delimiter is whitespace. That is, tokens are separated by strings of spaces, tabs, and newlines, by default. Tokens can either be read immediately as strings or a Scanner object can convert a token to a base type, if the token has the right form. Speciﬁcally, the Scanner class includes the following methods for dealing with tokens: hasNext(): Return true if there is another token in the input stream. next(): Return the next token string in the input stream; generate an error if there are no more tokens left. hasNextType(): Return true if there is another token in the input stream and it can be interpreted as the corresponding base type, Type, where Type can be Boolean, Byte, Double, Float, Int, Long, or Short. nextType(): Return the next token in the input stream, returned as the base type corresponding to Type; generate an error if there are no more tokens left or if the next token cannot be interpreted as a base type corresponding to Type. Additionally, Scanner objects can process input line by line, ignoring delimiters, and even look for patterns within lines while doing so. The methods for processing input in this way include the following: hasNextLine(): Returns true if the input stream has another line of text. nextLine(): Advances the input past the current line ending and returns the input that was skipped. ﬁndInLine(String s): Attempts to ﬁnd a string matching the (regular expression) pattern s in the current line. If the pattern is found, it is returned and the scanner advances to the ﬁrst character after this match. If the pattern is not found, the scanner returns null and doesn’t advance. These methods can be used with those above, as in the following: Scanner input = new Scanner(System.in); System.out.print("Please enter an integer: "); while (!input.hasNextInt()) { input.nextLine(); System.out.print("Invalid integer; please enter an integer: "); } int i = input.nextInt();

1.7. An Example Program 1.7 An Example Program In this section, we present another example of a Java class, which illustrates many of the constructs deﬁned thus far in this chapter. This CreditCard class deﬁnes credit card objects that model a simpliﬁed version of traditional credit cards. They store information about the customer, issuing bank, account identiﬁer, credit limit, and current balance. They do not charge interest or late payments, but they do restrict charges that would cause a card’s balance to go over its credit limit. We also provide a static main method as part of this class to demonstrate its use. The primary deﬁnition of the CreditCard class is given in Code Fragment 1.5. We defer until Code Fragment 1.6 the presentation of the main method, and in Code Fragment 1.7 we show the output produced by the main method. Highlights of this class, and underlying techniques that are demonstrated, include: • The class deﬁnes ﬁve instance variables (lines 3–7), four of which are declared as private and one that is protected. (We will take advantage of the protected balance member when introducing inheritance in the next chapter.) • The class deﬁnes two different constructor forms. The ﬁrst version (beginning at line 9) requires ﬁve parameters, including an explicit initial balance for the account. The second constructor (beginning at line 16) accepts only four parameters; it relies on use of the special this keyword to invoke the ﬁve-parameter version, with an explicit initial balance of zero (a reasonable default for most new accounts). • The class deﬁnes ﬁve basic accessor methods (lines 20–24), and two update methods (charge and makePayment). The charge method relies on conditional logic to ensure that a charge is rejected if it would have resulted in the balance exceeding the credit limit on the card. • We provide a static utility method, named printSummary, in lines 37–43. • The main method includes an array, named wallet, storing CreditCard instances. The main method also demonstrates a while loop, a traditional for loop, and a for-each loop over the contents of the wallet. • The main method demonstrates the syntax for calling traditional (nonstatic) methods—charge, getBalance, and makePayment—as well as the syntax for invoking the static printSummary method.

Chapter 1. Java Primer public class CreditCard { // Instance variables: private String customer; // name of the customer (e.g., ”John Bowman”) private String bank; // name of the bank (e.g., ”California Savings”) private String account; // account identiﬁer (e.g., ”5391 0375 9387 5309”) private int limit; // credit limit (measured in dollars) protected double balance; // current balance (measured in dollars) // Constructors: public CreditCard(String cust, String bk, String acnt, int lim, double initialBal) { customer = cust; bank = bk; account = acnt; limit = lim; balance = initialBal; } public CreditCard(String cust, String bk, String acnt, int lim) { this(cust, bk, acnt, lim, 0.0); // use a balance of zero as default } // Accessor methods: public String getCustomer() { return customer; } public String getBank() { return bank; } public String getAccount() { return account; } public int getLimit() { return limit; } public double getBalance() { return balance; } // Update methods: public boolean charge(double price) { // make a charge if (price + balance > limit) // if charge would surpass limit return false; // refuse the charge // at this point, the charge is successful balance += price; // update the balance return true; // announce the good news } public void makePayment(double amount) { // make a payment balance −= amount; } // Utility method to print a card's information public static void printSummary(CreditCard card) { System.out.println("Customer = " + card.customer); System.out.println("Bank = " + card.bank); System.out.println("Account = " + card.account); System.out.println("Balance = " + card.balance); // implicit cast System.out.println("Limit = " + card.limit); // implicit cast } // main method shown on next page... } Code Fragment 1.5: The CreditCard class.

1.7. An Example Program public static void main(String[ ] args) { CreditCard[ ] wallet = new CreditCard[3]; wallet[0] = new CreditCard("John Bowman", "California Savings", "5391 0375 9387 5309", 5000); wallet[1] = new CreditCard("John Bowman", "California Federal", "3485 0399 3395 1954", 3500); wallet[2] = new CreditCard("John Bowman", "California Finance", "5391 0375 9387 5309", 2500, 300); for (int val = 1; val <= 16; val++) { wallet[0].charge(3∗val); wallet[1].charge(2∗val); wallet[2].charge(val); } for (CreditCard card : wallet) { CreditCard.printSummary(card); // calling static method while (card.getBalance() > 200.0) { card.makePayment(200); System.out.println("New balance = " + card.getBalance()); } } } Code Fragment 1.6: The main method of the CreditCard class. Customer = John Bowman Bank = California Savings Account = 5391 0375 9387 5309 Balance = 408.0 Limit = 5000 New balance = 208.0 New balance = 8.0 Customer = John Bowman Bank = California Federal Account = 3485 0399 3395 1954 Balance = 272.0 Limit = 3500 New balance = 72.0 Customer = John Bowman Bank = California Finance Account = 5391 0375 9387 5309 Balance = 436.0 Limit = 2500 New balance = 236.0 New balance = 36.0 Code Fragment 1.7: Output from the Test class.

Chapter 1. Java Primer 1.8 Packages and Imports The Java language takes a general and useful approach to the organization of classes into programs. Every stand-alone public class deﬁned in Java must be given in a separate ﬁle. The ﬁle name is the name of the class with a .java extension. So a class declared as public class Window is deﬁned in a ﬁle Window.java. That ﬁle may contain deﬁnitions for other stand-alone classes, but none of them may be declared with public visibility. To aid in the organization of large code repository, Java allows a group of related type deﬁnitions (such as classes and enums) to be grouped into what is known as a package. For types to belong to a package named packageName, their source code must all be located in a directory named packageName and each ﬁle must begin with the line: package packageName; By convention, most package names are lowercased. For example, we might deﬁne an architecture package that deﬁnes classes such as Window, Door, and Room. Public deﬁnitions within a ﬁle that does not have an explicit package declaration are placed into what is known as the default package. To refer to a type within a named package, we may use a fully qualiﬁed name based on dot notation, with that type treated as an attribute of the package. For example, we might declare a variable with architecture.Window as its type. Packages can be further organized hierarchically into subpackages. Code for classes in a subpackage must be located within a subdirectory of the package’s directory, and qualiﬁed names for subpackages rely on further use of dot notation. For example, there is a java.util.zip subpackage (with support for working with ZIP compression) within the java.util package, and the Deﬂater class within that subpackage is fully qualiﬁed as java.util.zip.Deﬂater. There are many advantages to organizing classes into packages, most notably: • Packages help us avoid the pitfalls of name conﬂicts. If all type deﬁnitions were in a single package, there could be only one public class named Window. But with packages, we can have an architecture.Window class that is independent from a gui.Window class for graphical user interfaces. • It is much easier to distribute a comprehensive set of classes for other programmers to use when those classes are packaged. • When type deﬁnitions have a related purpose, it is often easier for other programmers to ﬁnd them in a large library and to better understand their coordinated use when they are grouped as a package. • Classes within the same package have access to any of each others’ members having public, protected, or default visibility (i.e., anything but private).

1.8. Packages and Imports Import Statements As noted on the previous page, we may refer to a type within a package using its fully qualiﬁed name. For example, the Scanner class, introduced in Section 1.6, is deﬁned in the java.util package, and so we may refer to it as java.util.Scanner. We could declare and construct a new instance of that class in a project using the following statement: java.util.Scanner input = new java.util.Scanner(System.in); However, all the extra typing needed to refer to a class outside of the current package can get tiring. In Java, we can use the import keyword to include external classes or entire packages in the current ﬁle. To import an individual class from a speciﬁc package, we type the following at the beginning of the ﬁle: import packageName.className; For example, in Section 1.6 we imported the Scanner class from the java.util package with the command: import java.util.Scanner; and then we were allowed to use the less burdensome syntax: Scanner input = new Scanner(System.in); Note that it is illegal to import a class with the above syntax if a similarly named class is deﬁned elsewhere in the current ﬁle, or has already been imported from another package. For example, we could not simultaneously import both architecture.Window and gui.Window to use with the unqualiﬁed name Window. Importing a Whole Package If we know we will be using many deﬁnitions from the same package, we can import all of them using an asterisk character (∗) to denote a wildcard, as in the following syntax: import packageName.∗; If a locally deﬁned name conﬂicts with one in a package being imported in this way, the locally deﬁned one retains the unqualiﬁed name. If there is a name conﬂict between deﬁnitions in two different packages being imported this way, neither of the conﬂicting names can be used without qualiﬁcation. For example, if we import the following hypothetical packages: import architecture.∗; // which we assume includes a Window class import gui.∗; // which we assume includes a Window class we must still use the qualiﬁed names architecture.Window and gui.Window in the rest of our program.

Chapter 1. Java Primer 1.9 Software Development Traditional software development involves several phases. Three major steps are: 1. Design 2. Coding 3. Testing and Debugging In this section, we brieﬂy discuss the role of these phases, and we introduce several good practices for programming in Java, including coding style, naming conventions, formal documentation, and testing. 1.9.1 Design For object-oriented programming, the design step is perhaps the most important phase in the process of developing software. It is in the design step that we decide how to divide the workings of our program into classes, when we decide how these classes will interact, what data each will store, and what actions each will perform. Indeed, one of the main challenges that beginning programmers face is deciding what classes to deﬁne to do the work of their program. While general prescriptions are hard to come by, there are some rules of thumb that we can apply when determining how to deﬁne our classes: • Responsibilities: Divide the work into different actors, each with a different responsibility. Try to describe responsibilities using action verbs. These actors will form the classes for the program. • Independence: Deﬁne the work for each class to be as independent from other classes as possible. Subdivide responsibilities between classes so that each class has autonomy over some aspect of the program. Give data (as instance variables) to the class that has jurisdiction over the actions that require access to this data. • Behaviors: Deﬁne the behaviors for each class carefully and precisely, so that the consequences of each action performed by a class will be well understood by other classes that interact with it. These behaviors will deﬁne the methods that this class performs, and the set of behaviors for a class form the protocol by which other pieces of code will interact with objects from the class. Deﬁning the classes, together with their instance variables and methods, are key to the design of an object-oriented program. A good programmer will naturally develop greater skill in performing these tasks over time, as experience teaches him or her to notice patterns in the requirements of a program that match patterns that he or she has seen before.

1.9. Software Development A common tool for developing an initial high-level design for a project is the use of CRC cards. Class-Responsibility-Collaborator (CRC) cards are simple index cards that subdivide the work required of a program. The main idea behind this tool is to have each card represent a component, which will ultimately become a class in the program. We write the name of each component on the top of an index card. On the left-hand side of the card, we begin writing the responsibilities for this component. On the right-hand side, we list the collaborators for this component, that is, the other components that this component will have to interact with to perform its duties. The design process iterates through an action/actor cycle, where we ﬁrst identify an action (that is, a responsibility), and we then determine an actor (that is, a component) that is best suited to perform that action. The design is complete when we have assigned all actions to actors. In using index cards for this process (rather than larger pieces of paper), we are relying on the fact that each component should have a small set of responsibilities and collaborators. Enforcing this rule helps keep the individual classes manageable. As the design takes form, a standard approach to explain and document the design is the use of UML (Uniﬁed Modeling Language) diagrams to express the organization of a program. UML diagrams are a standard visual notation to express object-oriented software designs. Several computer-aided tools are available to build UML diagrams. One type of UML ﬁgure is known as a class diagram. An example of a class diagram is given in Figure 1.5, corresponding to our CreditCard class from Section 1.7. The diagram has three portions, with the ﬁrst designating the name of the class, the second designating the recommended instance variables, and the third designating the recommended methods of the class. The type declarations of variables, parameters, and return values are speciﬁed in the appropriate place following a colon, and the visibility of each member is designated on its left, with the “+” symbol for public visibility, the “#” symbol for protected visibility, and the “−” symbol for private visibility. fields: methods: class: −limit : int # balance : double + getBalance() : double + getLimit() : int + getAccount() : String −customer : String −account : String −bank : String + getCustomer() : String + getBank() : String + charge(price : double) : boolean + makePayment(amount : double) CreditCard Figure 1.5: A UML Class diagram for the CreditCard class from Section 1.7.

Chapter 1. Java Primer 1.9.2 Pseudocode As an intermediate step before the implementation of a design, programmers are often asked to describe algorithms in a way that is intended for human eyes only. Such descriptions are called pseudocode. Pseudocode is not a computer program, but is more structured than usual prose. It is a mixture of natural language and high-level programming constructs that describe the main ideas behind a generic implementation of a data structure or algorithm. Because pseudocode is designed for a human reader, not a computer, we can communicate high-level ideas without being burdened by low-level implementation details. At the same time, we should not gloss over important steps. Like many forms of human communication, ﬁnding the right balance is an important skill that is reﬁned through practice. There really is no precise deﬁnition of the pseudocode language. At the same time, to help achieve clarity, pseudocode mixes natural language with standard programming language constructs. The programming language constructs that we choose are those consistent with modern high-level languages such as C, C++, and Java. These constructs include the following: • Expressions: We use standard mathematical symbols to express numeric and boolean expressions. To be consistent with Java, we use the equal sign “=” as the assignment operator in assignment statements, and the “==” relation to test equivalence in boolean expressions. • Method declarations: Algorithm name(param1, param2,...) declares new method “name” and its parameters. • Decision structures: if condition then true-actions [else false-actions]. We use indentation to indicate what actions should be included in the true-actions and false-actions. • While-loops: while condition do actions. We use indentation to indicate what actions should be included in the loop actions. • Repeat-loops: repeat actions until condition. We use indentation to indicate what actions should be included in the loop actions. • For-loops: for variable-increment-deﬁnition do actions. We use indentation to indicate what actions should be included among the loop actions. • Array indexing: A[i] represents the ith cell in the array A. The cells of an n-celled array A are indexed from A[0] to A[n−1] (consistent with Java). • Method calls: object.method(args); object is optional if it is understood. • Method returns: return value. This operation returns the value speciﬁed to the method that called this one. • Comments: { Comment goes here. }. We enclose comments in braces.

1.9. Software Development 1.9.3 Coding One of the key steps in implementing an object-oriented program is coding the descriptions of classes and their respective data and methods. In order to accelerate the development of this skill, we will discuss various design patterns for designing object-oriented programs (see Section 2.1.3) at various points throughout this text. These patterns provide templates for deﬁning classes and the interactions between these classes. Once we have settled on a design for the classes or our program and their responsibilities, and perhaps drafted pseudocode for their behaviors, we are ready to begin the actual coding on a computer. We type the Java source code for the classes of our program by using either an independent text editor (such as emacs, WordPad, or vi), or the editor embedded in an integrated development environment (IDE), such as Eclipse. Once we have completed coding for a class (or package), we compile this ﬁle into working code by invoking a compiler. If we are not using an IDE, then we compile our program by calling a program, such as javac, on our ﬁle. If we are using an IDE, then we compile our program by clicking the appropriate compilation button. If we are fortunate, and our program has no syntax errors, then this compilation process will create ﬁles with a “.class” extension. If our program contains syntax errors, then these will be identiﬁed, and we will have to go back into our editor to ﬁx the offending lines of code. Once we have eliminated all syntax errors, and created the appropriate compiled code, we can run our program by either invoking a command, such as “java” (outside an IDE), or by clicking on the appropriate “run” button (within an IDE). When a Java program is run in this way, the runtime environment locates the directories containing the named class and any other classes that are referenced from this class according to a special operating system environment variable named “CLASSPATH.” This variable deﬁnes an order of directories in which to search, given as a list of directories, which are separated by colons in Unix/Linux or semicolons in DOS/Windows. An example CLASSPATH assignment in the DOS/Windows operating system could be the following: SET CLASSPATH=.;C:\java;C:\Program Files\Java\ Whereas an example CLASSPATH assignment in the Unix/Linux operating system could be the following: setenv CLASSPATH ".:/usr/local/java/lib:/usr/netscape/classes" In both cases, the dot (“.”) refers to the current directory in which the runtime environment is invoked.

Chapter 1. Java Primer 1.9.4 Documentation and Style Javadoc In order to encourage good use of block comments and the automatic production of documentation, the Java programming environment comes with a documentation production program called javadoc. This program takes a collection of Java source ﬁles that have been commented using certain keywords, called tags, and it produces a series of HTML documents that describe the classes, methods, variables, and constants contained in these ﬁles. As an example, Figure 1.6 shows a portion of the documentation generated for our CreditCard class. Each javadoc comment is a block comment that starts with “/**” and ends with “*/”, and each line between these two can begin with a single asterisk, “*”, which is ignored. The block comment is assumed to start with a descriptive sentence, which is followed by special lines that begin with javadoc tags. A block comment that comes just before a class deﬁnition, instance variable declaration, or method deﬁnition is processed by javadoc into a comment about that class, variable, or method. The primary javadoc tags that we use are the following: • @author text: Identiﬁes each author (one per line) for a class. • @throws exceptionName description: Identiﬁes an error condition that is signaled by this method (see Section 2.4). • @param parameterName description: Identiﬁes a parameter accepted by this method. • @return description: Describes the return type and its range of values for a method. There are other tags as well; the interested reader is referred to online documentation for javadoc for further information. For space reasons, we cannot always include javadoc-style comments in all the example programs included in this book, but we include such a sample in Code Fragment 1.8, and within the online code at the website that accompanies this book. Figure 1.6: Documentation rendered by javadoc for the CreditCard.charge method.

1.9. Software Development /∗∗ ∗A simple model for a consumer credit card. ∗ ∗@author Michael T. Goodrich ∗@author Roberto Tamassia ∗@author Michael H. Goldwasser ∗/ public class CreditCard { /∗∗ ∗Constructs a new credit card instance. ∗@param cust the name of the customer (e.g., ”John Bowman”) ∗@param bk the name of the bank (e.g., ”California Savings”) ∗@param acnt the account identiﬁer (e.g., ”5391 0375 9387 5309”) ∗@param lim the credit limit (measured in dollars) ∗@param initialBal the initial balance (measured in dollars) ∗/ public CreditCard(String cust, String bk, String acnt, int lim, double initialBal) { customer = cust; bank = bk; account = acnt; limit = lim; balance = initialBal; } /∗∗ ∗Charges the given price to the card, assuming suﬃcient credit limit. ∗@param price the amount to be charged ∗@return true if charge was accepted; false if charge was denied ∗/ public boolean charge(double price) { // make a charge if (price + balance > limit) // if charge would surpass limit return false; // refuse the charge // at this point, the charge is successful balance += price; // update the balance return true; // announce the good news } /∗∗ ∗Processes customer payment that reduces balance. ∗@param amount the amount of payment made ∗/ public void makePayment(double amount) { // make a payment balance −= amount; } // remainder of class omitted... Code Fragment 1.8: A portion of the CreditCard class deﬁnition, originally from Code Fragment 1.5, with javadoc-style comments included.

Chapter 1. Java Primer Readability and Programming Conventions Programs should be made easy to read and understand. Good programmers should therefore be mindful of their coding style, and develop a style that communicates the important aspects of a program’s design for both humans and computers. Much has been written about good coding style, with some of the main principles being the following: • Use meaningful names for identiﬁers. Try to choose names that can be read aloud, and choose names that reﬂect the action, responsibility, or data each identiﬁer is naming. The tradition in most Java circles is to capitalize the ﬁrst letter of each word in an identiﬁer, except for the ﬁrst word for a variable or method name. By this convention, “Date,” “Vector,” “DeviceManager” would identify classes, and “isFull(),” “insertItem(),” “studentName,” and “studentHeight” would respectively identify methods and variables. • Use named constants or enum types instead of literals. Readability, robustness, and modiﬁability are enhanced if we include a series of deﬁnitions of named constant values in a class deﬁnition. These can then be used within this class and others to refer to special values for this class. The tradition in Java is to fully capitalize such constants, as shown below: public class Student { public static ﬁnal int MIN CREDITS = 12; // min credits per term public static ﬁnal int MAX CREDITS = 24; // max credits per term public enum Year {FRESHMAN, SOPHOMORE, JUNIOR, SENIOR}; // Instance variables, constructors, and method deﬁnitions go here... } • Indent statement blocks. Typically programmers indent each statement block by 4 spaces; in this book we typically use 2 spaces, however, to avoid having our code overrun the book’s margins. • Organize each class in the following order: 1. Constants 2. Instance variables 3. Constructors 4. Methods We note that some Java programmers prefer to put instance variable deﬁnitions last. We put them earlier so that we can read each class sequentially and understand the data each method is working with. • Use comments that add meaning to a program and explain ambiguous or confusing constructs. In-line comments are good for quick explanations and do not need to be sentences. Block comments are good for explaining the purpose of a method and complex code sections.

1.9. Software Development 1.9.5 Testing and Debugging Testing is the process of experimentally checking the correctness of a program, while debugging is the process of tracking the execution of a program and discovering the errors in it. Testing and debugging are often the most time-consuming activity in the development of a program. Testing A careful testing plan is an essential part of writing a program. While verifying the correctness of a program over all possible inputs is usually infeasible, we should aim at executing the program on a representative subset of inputs. At the very minimum, we should make sure that every method of a program is tested at least once (method coverage). Even better, each code statement in the program should be executed at least once (statement coverage). Programs often tend to fail on special cases of the input. Such cases need to be carefully identiﬁed and tested. For example, when testing a method that sorts (that is, puts in order) an array of integers, we should consider the following inputs: • The array has zero length (no elements). • The array has one element. • All the elements of the array are the same. • The array is already sorted. • The array is reverse sorted. In addition to special inputs to the program, we should also consider special conditions for the structures used by the program. For example, if we use an array to store data, we should make sure that boundary cases, such as inserting or removing at the beginning or end of the subarray holding data, are properly handled. While it is essential to use handcrafted test suites, it is also advantageous to run the program on a large collection of randomly generated inputs. The Random class in the java.util package provides several means for generating pseudorandom numbers. There is a hierarchy among the classes and methods of a program induced by the caller-callee relationship. Namely, a method A is above a method B in the hierarchy if A calls B. There are two main testing strategies, top-down testing and bottom-up testing, which differ in the order in which methods are tested. Top-down testing proceeds from the top to the bottom of the program hierarchy. It is typically used in conjunction with stubbing, a boot-strapping technique that replaces a lower-level method with a stub, a replacement for the method that simulates the functionality of the original. For example, if method A calls method B to get the ﬁrst line of a ﬁle, when testing A we can replace B with a stub that returns a ﬁxed string.

Chapter 1. Java Primer Bottom-up testing proceeds from lower-level methods to higher-level methods. For example, bottom-level methods, which do not invoke other methods, are tested ﬁrst, followed by methods that call only bottom-level methods, and so on. Similarly a class that does not depend upon any other classes can be tested before another class that depends on the former. This form of testing is usually described as unit testing, as the functionality of a speciﬁc component is tested in isolation of the larger software project. If used properly, this strategy better isolates the cause of errors to the component being tested, as lower-level components upon which it relies should have already been thoroughly tested. Java provides several forms of support for automated testing. We have already discussed how a class’s static main method can be repurposed to perform tests of the functionality of that class (as was done in Code 1.6 for the CreditCard class). Such a test can be executed by invoking the Java virtual machine directly on this secondary class, rather than on the primary class for the entire application. When Java is started on the primary class, any code within such secondary main methods will be ignored. More robust support for automation of unit testing is provided by the JUnit framework, which is not part of the standard Java toolkit but freely available at www.junit.org. This framework allows the grouping of individual test cases into larger test suites, and provides support for executing those suites, and reporting or analyzing the results of those tests. As software is maintained, regression testing should be performed, whereby automation is used to re-execute all previous tests to ensure that changes to the software do not introduce new bugs in previously tested components. Debugging The simplest debugging technique consists of using print statements to track the values of variables during the execution of the program. A problem with this approach is that eventually the print statements need to be removed or commented out, so they are not executed when the software is ﬁnally released. A better approach is to run the program within a debugger, which is a specialized environment for controlling and monitoring the execution of a program. The basic functionality provided by a debugger is the insertion of breakpoints within the code. When the program is executed within the debugger, it stops at each breakpoint. While the program is stopped, the current value of variables can be inspected. In addition to ﬁxed breakpoints, advanced debuggers allow speciﬁcation of conditional breakpoints, which are triggered only if a given expression is satisﬁed. The standard Java toolkit includes a basic debugger named jdb, which has a command-line interface. Most IDEs for Java programming provide advanced debugging environments with graphical user interfaces.

Chapter 2. Object-Oriented Design 2.1 Goals, Principles, and Patterns As the name implies, the main “actors” in the object-oriented paradigm are called objects. Each object is an instance of a class. Each class presents to the outside world a concise and consistent view of the objects that are instances of this class, without going into too much unnecessary detail or giving others access to the inner workings of the objects. The class deﬁnition typically speciﬁes the data ﬁelds, also known as instance variables, that an object contains, as well as the methods (operations) that an object can execute. This view of computing fulﬁll several goals and incorporates design principles, which we will discuss in this chapter. 2.1.1 Object-Oriented Design Goals Software implementations should achieve robustness, adaptability, and reusability. (See Figure 2.1.) Figure 2.1: Goals of object-oriented design. Robustness Every good programmer wants to develop software that is correct, which means that a program produces the right output for all the anticipated inputs in the program’s application. In addition, we want software to be robust, that is, capable of handling unexpected inputs that are not explicitly deﬁned for its application. For example, if a program is expecting a positive integer (perhaps representing the price of an item) and instead is given a negative integer, then the program should be able to recover gracefully from this error. More importantly, in life-critical applications, where a software error can lead to injury or loss of life, software that is not robust could be deadly. This point was driven home in the late 1980s in accidents involving Therac-25, a radiation-therapy machine, which severely overdosed six patients between 1985 and 1987, some of whom died from complications resulting from their radiation overdose. All six accidents were traced to software errors.

2.1. Goals, Principles, and Patterns Adaptability Modern software applications, such as Web browsers and Internet search engines, typically involve large programs that are used for many years. Software, therefore, needs to be able to evolve over time in response to changing conditions in its environment. Thus, another important goal of quality software is that it achieves adaptability (also called evolvability). Related to this concept is portability, which is the ability of software to run with minimal change on different hardware and operating system platforms. An advantage of writing software in Java is the portability provided by the language itself. Reusability Going hand in hand with adaptability is the desire that software be reusable, that is, the same code should be usable as a component of different systems in various applications. Developing quality software can be an expensive enterprise, and its cost can be offset somewhat if the software is designed in a way that makes it easily reusable in future applications. Such reuse should be done with care, however, for one of the major sources of software errors in the Therac-25 came from inappropriate reuse of Therac-20 software (which was not object-oriented and not designed for the hardware platform used with the Therac-25). 2.1.2 Object-Oriented Design Principles Chief among the principles of the object-oriented approach, which are intended to facilitate the goals outlined above, are the following (see Figure 2.2): • Abstraction • Encapsulation • Modularity Abstraction Encapsulation Modularity Figure 2.2: Principles of object-oriented design.

Chapter 2. Object-Oriented Design Abstraction The notion of abstraction is to distill a complicated system down to its most fundamental parts. Typically, describing the parts of a system involves naming them and explaining their functionality. Applying the abstraction paradigm to the design of data structures gives rise to abstract data types (ADTs). An ADT is a mathematical model of a data structure that speciﬁes the type of data stored, the operations supported on them, and the types of parameters of the operations. An ADT speciﬁes what each operation does, but not how it does it. In Java, an ADT can be expressed by an interface, which is simply a list of method declarations, where each method has an empty body. (We will say more about Java interfaces in Section 2.3.1.) An ADT is realized by a concrete data structure, which is modeled in Java by a class. A class deﬁnes the data being stored and the operations supported by the objects that are instances of the class. Also, unlike interfaces, classes specify how the operations are performed in the body of each method. A Java class is said to implement an interface if its methods include all the methods declared in the interface, thus providing a body for them. However, a class can have more methods than those of the interface. Encapsulation Another important principle of object-oriented design is encapsulation; different components of a software system should not reveal the internal details of their respective implementations. One of the main advantages of encapsulation is that it gives one programmer freedom to implement the details of a component, without concern that other programmers will be writing code that intricately depends on those internal decisions. The only constraint on the programmer of a component is to maintain the public interface for the component, as other programmers will be writing code that depends on that interface. Encapsulation yields robustness and adaptability, for it allows the implementation details of parts of a program to change without adversely affecting other parts, thereby making it easier to ﬁx bugs or add new functionality with relatively local changes to a component. Modularity Modern software systems typically consist of several different components that must interact correctly in order for the entire system to work properly. Keeping these interactions straight requires that these different components be well organized. Modularity refers to an organizing principle in which different components of a software system are divided into separate functional units. Robustness is greatly increased because it is easier to test and debug separate components before they are integrated into a larger software system.

2.1. Goals, Principles, and Patterns 2.1.3 Design Patterns Object-oriented design facilitates reusable, robust, and adaptable software. Designing good code takes more than simply understanding object-oriented methodologies, however. It requires the effective use of object-oriented design techniques. Computing researchers and practitioners have developed a variety of organizational concepts and methodologies for designing quality object-oriented software that is concise, correct, and reusable. Of special relevance to this book is the concept of a design pattern, which describes a solution to a “typical” software design problem. A pattern provides a general template for a solution that can be applied in many different situations. It describes the main elements of a solution in an abstract way that can be specialized for a speciﬁc problem at hand. It consists of a name, which identiﬁes the pattern; a context, which describes the scenarios for which this pattern can be applied; a template, which describes how the pattern is applied; and a result, which describes and analyzes what the pattern produces. We present several design patterns in this book, and we show how they can be consistently applied to implementations of data structures and algorithms. These design patterns fall into two groups—patterns for solving algorithm design problems and patterns for solving software engineering problems. Some of the algorithm design patterns we discuss include the following: • Recursion (Chapter 5) • Amortization (Sections 7.2.3, 11.4.4, and 14.7.3) • Divide-and-conquer (Section 12.1.1) • Prune-and-search, also known as decrease-and-conquer (Section 12.5.1) • Brute force (Section 13.2.1) • The greedy method (Sections 13.4.2, 14.6.2, and 14.7) • Dynamic programming (Section 13.5) Likewise, some of the software engineering design patterns we discuss include: • Template method (Sections 2.3.3, 10.5.1, and 11.2.1) • Composition (Sections 2.5.2, 2.6, and 9.2.1) • Adapter (Section 6.1.3) • Position (Sections 7.3, 8.1.2, and 14.7.3) • Iterator (Section 7.4) • Factory Method (Sections 8.3.1 and 11.2.1) • Comparator (Sections 9.2.2, 10.3, and Chapter 12) • Locator (Section 9.5.1) Rather than explain each of these concepts here, however, we will introduce them throughout the text as noted above. For each pattern, be it for algorithm engineering or software engineering, we explain its general use and we illustrate it with at least one concrete example.

Chapter 2. Object-Oriented Design 2.2 Inheritance A natural way to organize various structural components of a software package is in a hierarchical fashion, with similar abstract deﬁnitions grouped together in a level-by-level manner that goes from speciﬁc to more general as one traverses up the hierarchy. An example of such a hierarchy is shown in Figure 2.3. Using mathematical notations, the set of houses is a subset of the set of buildings, but a superset of the set of ranches. The correspondence between levels is often referred to as an “is a” relationship, as a house is a building, and a ranch is a house. Building Low-rise Apartment High-rise Apartment Two-story House Ranch Skyscraper Commercial Building House Apartment Figure 2.3: An example of an “is a” hierarchy involving architectural buildings. A hierarchical design is useful in software development, as common functionality can be grouped at the most general level, thereby promoting reuse of code, while differentiated behaviors can be viewed as extensions of the general case. In object-oriented programming, the mechanism for a modular and hierarchical organization is a technique known as inheritance. This allows a new class to be deﬁned based upon an existing class as the starting point. In object-oriented terminology, the existing class is typically described as the base class, parent class, or superclass, while the newly deﬁned class is known as the subclass or child class. We say that the subclass extends the superclass. When inheritance is used, the subclass automatically inherits, as its starting point, all methods from the superclass (other than constructors). The subclass can differentiate itself from its superclass in two ways. It may augment the superclass by adding new ﬁelds and new methods. It may also specialize existing behaviors by providing a new implementation that overrides an existing method.

2.2. Inheritance 2.2.1 Extending the CreditCard Class As an introduction to the use of inheritance, we revisit the CreditCard class of Section 1.7, designing a new subclass that, for lack of a better name, we name PredatoryCreditCard. The new class will differ from the original in two ways: (1) if an attempted charge is rejected because it would have exceeded the credit limit, a $5 fee will be charged, and (2) there will be a mechanism for assessing a monthly interest charge on the outstanding balance, using an annual percentage rate (APR) speciﬁed as a constructor parameter. Figure 2.4 provides a UML diagram that serves as an overview of our design for the new PredatoryCreditCard class as a subclass of the existing CreditCard class. The hollow arrow in that diagram indicates the use of inheritance, with the arrow oriented from the subclass to the superclass. The PredatoryCreditCard class augments the original CreditCard class, adding a new instance variable named apr to store the annual percentage rate, and adding a new method named processMonth that will assess interest charges. The new class also specializes its superclass by overriding the original charge method in order to provide a new implementation that assess a $5 fee for an attempted overcharge. class: fields: methods: class: fields: methods: −apr : double PredatoryCreditCard + charge(price : double) : boolean + processMonth() −limit : int + getBalance() : double + getLimit() : int + getAccount() : String # balance : double −customer : String −account : String −bank : String + getCustomer() : String + getBank() : String + charge(price : double) : boolean + makePayment(amount : double) CreditCard Figure 2.4: A UML diagram showing PredatoryCreditCard as a subclass of CreditCard. (See Figure 1.5 for the original CreditCard design.)

Chapter 2. Object-Oriented Design To demonstrate the mechanisms for inheritance in Java, Code Fragment 2.1 presents a complete implementation of the new PredatoryCreditCard class. We wish to draw attention to several aspects of the Java implementation. We begin with the ﬁrst line of the class deﬁnition, which indicates that the new class inherits from the existing CreditCard class by using Java’s extends keyword followed by the name of its superclass. In Java, each class can extend exactly one other class. Because of this property, Java is said to allow only single inheritance among classes. We should also note that even if a class deﬁnition makes no explicit use of the extends clause, it automatically inherits from a class, java.lang.Object, which serves as the universal superclass in Java. We next consider the declaration of the new apr instance variable, at line 3 of the code. Each instance of the PredatoryCreditCard class will store each of the variables inherited from the CreditCard deﬁnition (customer, bank, account, limit, and balance) in addition to the new apr variable. Yet we are only responsible for declaring the new instance variable within the subclass deﬁnition. public class PredatoryCreditCard extends CreditCard { // Additional instance variable private double apr; // annual percentage rate // Constructor for this class public PredatoryCreditCard(String cust, String bk, String acnt, int lim, double initialBal, double rate) { super(cust, bk, acnt, lim, initialBal); // initialize superclass attributes apr = rate; } // A new method for assessing monthly interest charges public void processMonth() { if (balance > 0) { // only charge interest on a positive balance double monthlyFactor = Math.pow(1 + apr, 1.0/12); // compute monthly rate balance ∗= monthlyFactor; // assess interest } } // Overriding the charge method deﬁned in the superclass public boolean charge(double price) { boolean isSuccess = super.charge(price); // call inherited method if (!isSuccess) balance += 5; // assess a $5 penalty return isSuccess; } } Code Fragment 2.1: A subclass of CreditCard that assesses interest and fees.

2.2. Inheritance Constructors are never inherited in Java. Lines 6–10 of Code Fragment 2.1 deﬁne a constructor for the new class. When a PredatoryCreditCard instance is created, all of its ﬁelds must be properly initialized, including any inherited ﬁelds. For this reason, the ﬁrst operation performed within the body of a constructor must be to invoke a constructor of the superclass, which is responsible for properly initializing the ﬁelds deﬁned in the superclass. In Java, a constructor of the superclass is invoked by using the keyword super with appropriate parameterization, as demonstrated at line 8 of our implementation: super(cust, mk, acnt, lim, initialBal); This use of the super keyword is very similar to use of the keyword this when invoking a different constructor within the same class (as described on page 15 of Section 1.2.2). If a constructor for a subclass does not make an explicit call to super or this as its ﬁrst command, then an implicit call to super(), the zero-parameter version of the superclass constructor, will be made. Returning our attention to the constructor for PredatoryCreditCard, after calling the superclass constructor with appropriate parameters, line 9 initializes the newly declared apr ﬁeld. (That ﬁeld was unknown to the superclass.) The processMonth method is a new behavior, so there is no inherited version upon which to rely. In our model, this method should be invoked by the bank, once each month, to add new interest charges to the customer’s balance. From a technical aspect, we note that this method accesses the value of the inherited balance ﬁeld (at line 14), and potentially modiﬁes that balance at line 16. This is permitted precisely because the balance attributed was declared with protected visibility in the original CreditCard class. (See Code Fragment 1.5.) The most challenging aspect in implementing the processMonth method is making sure we have working knowledge of how an annual percentage rate translates to a monthly rate. We do not simply divide the annual rate by twelve to get a monthly rate (that would be too predatory, as it would result in a higher APR than advertised). The correct computation is to take the twelfth-root of 1 + apr, and use that as a multiplicative factor. For example, if the APR is 0.0825 (representing 8.25%), we compute 12√ 1.0825 ≈1.006628, and therefore charge 0.6628% interest per month. In this way, each $100 of debt will amass $8.25 of compounded interest in a year. Notice that we use the Math.pow method from Java’s libraries. Finally, we consider the new implementation of the charge method provided for the PredatoryCreditCard class (lines 21–27). This deﬁnition overrides the inherited method. Yet, our implementation of the new method relies on a call to the inherited method, with syntax super.charge(price) at line 22. The return value of that call designates whether the charge was successful. We examine that return value to decide whether to assess a fee, and in either case return that boolean to the caller, so that the new version of charge maintains a similar outward interface as the original.

Chapter 2. Object-Oriented Design 2.2.2 Polymorphism and Dynamic Dispatch The word polymorphism literally means “many forms.” In the context of objectoriented design, it refers to the ability of a reference variable to take different forms. Consider, for example, the declaration of a variable having CreditCard as its type: CreditCard card; Because this is a reference variable, the statement declares the new variable, which does not yet refer to any card instance. While we have already seen that we can assign it to a newly constructed instance of the CreditCard class, Java also allows us to assign that variable to refer to an instance of the PredatoryCreditCard subclass. That is, we can do the following: CreditCard card = new PredatoryCreditCard(...); // parameters omitted This is a demonstration of what is known as the Liskov Substitution Principle, which states that a variable (or parameter) with a declared type can be assigned an instance from any direct or indirect subclass of that type. Informally, this is a manifestation of the “is a” relationship modeled by inheritance, as a predatory credit card is a credit card (but a credit card is not necessarily predatory). We say that the variable, card, is polymorphic; it may take one of many forms, depending on the speciﬁc class of the object to which it refers. Because card has been declared with type CreditCard, that variable may only be used to call methods that are declared as part of the CreditCard deﬁnition. So we can call card.makePayment(50) and card.charge(100), but a compilation error would be reported for the call card.processMonth() because a CreditCard is not guaranteed to have such a behavior. (That call could be made if the variable were originally declared to have PredatoryCreditCard as its type.) An interesting (and important) issue is how Java handles a call such as card.charge(100) when the variable card has a declared type of CreditCard. Recall that the object referenced by card might be an instance of the CreditCard class or an instance of the PredatoryCreditCard class, and that there are distinct implementations of the charge method: CreditCard.charge and PredatoryCreditCard.charge. Java uses a process known as dynamic dispatch, deciding at runtime to call the version of the method that is most speciﬁc to the actual type of the referenced object (not the declared type). So, if the object is a PredatoryCreditCard instance, it will execute the PredatoryCreditCard.charge method, even if the reference variable has a declared type of CreditCard. Java also provides an instanceof operator that tests, at runtime, whether an instance satisﬁes as a particular type. For example, the evaluation of the boolean condition, (card instanceof PredatoryCreditCard), produces true if the object currently referenced by the variable card belongs to the PredatoryCreditCard class, or any further subclass of that class. (See Section 2.5.1 for further discusion.)

2.2. Inheritance 2.2.3 Inheritance Hierarchies Although a subclass may not inherit from multiple superclasses in Java, a superclass may have many subclasses. In fact, it is quite common in Java to develop complex inheritance hierarchies to maximize the reusability of code. As a second example of the use of inheritance, we develop a hierarchy of classes for iterating numeric progressions. A numeric progression is a sequence of numbers, where each number depends on one or more of the previous numbers. For example, an arithmetic progression determines the next number by adding a ﬁxed constant to the previous value, and a geometric progression determines the next number by multiplying the previous value by a ﬁxed constant. In general, a progression requires a ﬁrst value, and a way of identifying a new value based on one or more previous values. Our hierarchy stems from a general base class that we name Progression. This class produces the progression of whole numbers: 0, 1, 2, . . . . More importantly, this class has been designed so that it can easily be specialized by other progression types, producing a hierarchy given in Figure 2.5. FibonacciProgression Progression ArithmeticProgression GeometricProgression Figure 2.5: An overview of our hierarchy of progression classes. Our implementation of the basic Progression class is provided in Code Fragment 2.2. This class has a single ﬁeld, named current. It deﬁnes two constructors, one accepting an arbitrary starting value for the progression and the other using 0 as the default value. The remainder of the class includes three methods: nextValue(): A public method that returns the next value of the progression, implicitly advancing the value each time. advance(): A protected method that is responsible for advancing the value of current in the progression. printProgression(n): A public utility that advances the progression n times while displaying each value. Our decision to factor out the protected advance() method, which is called during the execution of nextValue(), is to minimize the burden on subclasses, which are solely responsible for overriding the advance method to update the current ﬁeld.

Chapter 2. Object-Oriented Design /∗∗Generates a simple progression. By default: 0, 1, 2, ... ∗/ public class Progression { // instance variable protected long current; /∗∗Constructs a progression starting at zero. ∗/ public Progression() { this(0); } /∗∗Constructs a progression with given start value. ∗/ public Progression(long start) { current = start; } /∗∗Returns the next value of the progression. ∗/ public long nextValue() { long answer = current; advance(); // this protected call is responsible for advancing the current value return answer; } /∗∗Advances the current value to the next value of the progression. ∗/ protected void advance() { current++; } /∗∗Prints the next n values of the progression, separated by spaces. ∗/ public void printProgression(int n) { System.out.print(nextValue()); // print ﬁrst value without leading space for (int j=1; j < n; j++) System.out.print(" " + nextValue()); // print leading space before others System.out.println(); // end the line } } Code Fragment 2.2: General numeric progression class. The body of the nextValue method temporarily records the current value of the progression, which will soon be returned, and then calls the protected advance method in order to update the value in preparation for a subsequent call. The implementation of the advance method in our Progression class simply increments the current value. This method is the one that will be overridden by our specialized subclasses in order to alter the progression of numbers. In the remainder of this section, we present three subclasses of the Progression class—ArithmeticProgression, GeometricProgression, and FibonacciProgression– which respectively produce arithmetic, geometric, and Fibonacci progressions.

2.2. Inheritance An Arithmetic Progression Class Our ﬁrst example of a specialized progression is an arithmetic progression. While the default progression increases its value by one in each step, an arithmetic progression adds a ﬁxed constant to one term of the progression to produce the next. For example, using an increment of 4 for an arithmetic progression that starts at 0 results in the sequence 0,4,8,12,... . Code Fragment 2.3 presents our implementation of an ArithmeticProgression class, which relies on Progression as its base class. We include three constructor forms, with the most general (at lines 12–15) accepting an increment value and a start value, such that ArithmeticProgression(4, 2) produces the sequence 2,6,10,14,... . The body of that constructor invokes the superclass constructor, with syntax super(start), to initialize current to the given start value, and then it initializes the increment ﬁeld introduced by this subclass. For convenience, we offer two additional constructors, so that the default progression produces 0,1,2,3,... , and a one-parameter constructor produces an arithmetic progression with a given increment value (but a default starting value of 0). Finally (and most importantly), we override the protected advance method so that the given increment is added to each successive value of the progression. public class ArithmeticProgression extends Progression { protected long increment; /∗∗Constructs progression 0, 1, 2, ... ∗/ public ArithmeticProgression() { this(1, 0); } // start at 0 with increment of 1 /∗∗Constructs progression 0, stepsize, 2∗stepsize, ... ∗/ public ArithmeticProgression(long stepsize) { this(stepsize, 0); } // start at 0 /∗∗Constructs arithmetic progression with arbitrary start and increment. ∗/ public ArithmeticProgression(long stepsize, long start) { super(start); increment = stepsize; } /∗∗Adds the arithmetic increment to the current value. ∗/ protected void advance() { current += increment; } } Code Fragment 2.3: Class for arithmetic progressions, which inherits from the general progression class shown in Code Fragment 2.2.

Chapter 2. Object-Oriented Design A Geometric Progression Class Our second example of a specialized progression is a geometric progression, in which each value is produced by multiplying the preceding value by a ﬁxed constant, known as the base of the geometric progression. The starting point of a geometric progression is traditionally 1, rather than 0, because multiplying 0 by any factor results in 0. As an example, a geometric progression with base 2, starting at value 1, produces the sequence 1,2,4,8,16,... . Code Fragment 2.4 presents our implementation of a GeometricProgression class. It is quite similar to the ArithmeticProgression class in terms of the programming techniques used. In particular, it introduces one new ﬁeld (the base of the geometric progression), provides three forms of a constructor for convenience, and overrides the protected advance method so that the current value of the progression is multiplied by the base at each step. In the case of a geometric progression, we have chosen to have the default (zeroparameter) constructor use a starting value of 1 and a base of 2 so that it produces the progression 1,2,4,8,... . The one-parameter version of the constructor accepts an arbitrary base and uses 1 as the starting value, thus GeometricProgression(3) produces the sequence 1,3,9,27,... . Finally, we offer a two-parameter version accepting both a base and start value, such that GeometricProgression(3,2) produces the sequence 2,6,18,54,... . public class GeometricProgression extends Progression { protected long base; /∗∗Constructs progression 1, 2, 4, 8, 16, ... ∗/ public GeometricProgression() { this(2, 1); } // start at 1 with base of 2 /∗∗Constructs progression 1, b, bˆ2, bˆ3, bˆ4, ... for base b. ∗/ public GeometricProgression(long b) { this(b, 1); } // start at 1 /∗∗Constructs geometric progression with arbitrary base and start. ∗/ public GeometricProgression(long b, long start) { super(start); base = b; } /∗∗Multiplies the current value by the geometric base. ∗/ protected void advance() { current ∗= base; // multiply current by the geometric base } } Code Fragment 2.4: Class for geometric progressions.

2.2. Inheritance A Fibonacci Progression Class As our ﬁnal example, we demonstrate how to use our progression framework to produce a Fibonacci progression. Each value of a Fibonacci series is the sum of the two most recent values. To begin the series, the ﬁrst two values are conventionally 0 and 1, leading to the Fibonacci series 0,1,1,2,3,5,8,... . More generally, such a series can be generated from any two starting values. For example, if we start with values 4 and 6, the series proceeds as 4,6,10,16,26,42,... . Code Fragment 2.5 presents an implementation of the FibonacciProgression class. This class is markedly different from those for the arithmetic and geometric progressions because we cannot determine the next value of a Fibonacci series solely from the current one. We must maintain knowledge of the two most recent values. Our FibonacciProgression class introduces a new member, named prev, to store the value that proceeded the current one (which is stored in the inherited current ﬁeld). However, the question arises as to how to initialize the previous value in the constructor, when provided with the desired ﬁrst and second values as parameters. The ﬁrst should be stored as current so that it is reported by the ﬁrst call to nextValue(). Within that method call, an assignment will set the new current value (which will be the second value reported) equal to the ﬁrst value plus the “previous.” By initializing the previous value to (second −ﬁrst), the initial advancement will set the new current value to ﬁrst + (second −ﬁrst) = second, as desired. public class FibonacciProgression extends Progression { protected long prev; /∗∗Constructs traditional Fibonacci, starting 0, 1, 1, 2, 3, ... ∗/ public FibonacciProgression() { this(0, 1); } /∗∗Constructs generalized Fibonacci, with give ﬁrst and second values. ∗/ public FibonacciProgression(long ﬁrst, long second) { super(ﬁrst); prev = second −ﬁrst; // ﬁctitious value preceding the ﬁrst } /∗∗Replaces (prev,current) with (current, current+prev). ∗/ protected void advance() { long temp = prev; prev = current; current += temp; } } Code Fragment 2.5: Class for the Fibonacci progression.

Chapter 2. Object-Oriented Design class: fields: methods: ArithmeticProgression # advance() # base : long GeometricProgression # advance() # prev : long FibonacciProgression # advance() # current : long + nextValue() : long + printProgression(n : int) # advance() Progression # increment : long Figure 2.6: Detailed inheritance diagram for class Progression and its subclasses. As a summary, Figure 2.6 presents a more detailed version of our inheritance design than was originally given in Figure 2.5. Notice that each of these classes introduces an additional ﬁeld that allows it to properly implement the advance() method in an appropriate manner for its progression. Testing Our Progression Hierarchy To complete our example, we deﬁne a class TestProgression, shown in Code Fragment 2.6, which performs a simple test of each of the three classes. In this class, variable prog is polymorphic during the execution of the main method, since it references objects of class ArithmeticProgression, GeometricProgression, and FibonacciProgression in turn. When the main method of the TestProgression class is invoked by the Java runtime system, the output shown in Code Fragment 2.7 is produced. The example presented in this section is admittedly simple, but it provides an illustration of an inheritance hierarchy in Java. As an interesting aside, we consider how quickly the numbers grow in the three progressions, and how long it would be before the long integers used for computations overﬂow. With the default increment of one, an arithmetic progression would not overﬂow for 263 steps (that is approximately 10 billion billions). In contrast, a geometric progression with base b = 3 will overﬂow a long integer after 40 iterations, as 340 > 263. Likewise, the 94th Fibonacci number is greater than 263; hence, the Fibonacci progression will overﬂow a long integer after 94 iterations.

2.2. Inheritance /∗∗Test program for the progression hierarchy. ∗/ public class TestProgression { public static void main(String[ ] args) { Progression prog; // test ArithmeticProgression System.out.print("Arithmetic progression with default increment: "); prog = new ArithmeticProgression(); prog.printProgression(10); System.out.print("Arithmetic progression with increment 5: "); prog = new ArithmeticProgression(5); prog.printProgression(10); System.out.print("Arithmetic progression with start 2: "); prog = new ArithmeticProgression(5, 2); prog.printProgression(10); // test GeometricProgression System.out.print("Geometric progression with default base: "); prog = new GeometricProgression(); prog.printProgression(10); System.out.print("Geometric progression with base 3: "); prog = new GeometricProgression(3); prog.printProgression(10); // test FibonacciProgression System.out.print("Fibonacci progression with default start values: "); prog = new FibonacciProgression(); prog.printProgression(10); System.out.print("Fibonacci progression with start values 4 and 6: "); prog = new FibonacciProgression(4, 6); prog.printProgression(8); } } Code Fragment 2.6: Program for testing the progression classes. Arithmetic progression with default increment: 0 1 2 3 4 5 6 7 8 9 Arithmetic progression with increment 5: 0 5 10 15 20 25 30 35 40 45 Arithmetic progression with start 2: 2 7 12 17 22 27 32 37 42 47 Geometric progression with default base: 1 2 4 8 16 32 64 128 256 512 Geometric progression with base 3: 1 3 9 27 81 243 729 2187 6561 19683 Fibonacci progression with default start values: 0 1 1 2 3 5 8 13 21 34 Fibonacci progression with start values 4 and 6: 4 6 10 16 26 42 68 110 Code Fragment 2.7: Output of the TestProgression program of Code Fragment 2.6.

Chapter 2. Object-Oriented Design 2.3 Interfaces and Abstract Classes In order for two objects to interact, they must “know” about the various messages that each will accept, that is, the methods each object supports. To enforce this “knowledge,” the object-oriented design paradigm asks that classes specify the application programming interface (API), or simply interface, that their objects present to other objects. In the ADT-based approach (see Section 2.1.2) to data structures followed in this book, an interface deﬁning an ADT is speciﬁed as a type deﬁnition and a collection of methods for this type, with the arguments for each method being of speciﬁed types. This speciﬁcation is, in turn, enforced by the compiler or runtime system, which requires that the types of parameters that are actually passed to methods rigidly conform with the type speciﬁed in the interface. This requirement is known as strong typing. Having to deﬁne interfaces and then having those deﬁnitions enforced by strong typing admittedly places a burden on the programmer, but this burden is offset by the rewards it provides, for it enforces the encapsulation principle and often catches programming errors that would otherwise go unnoticed. 2.3.1 Interfaces in Java The main structural element in Java that enforces an API is an interface. An interface is a collection of method declarations with no data and no bodies. That is, the methods of an interface are always empty; they are simply method signatures. Interfaces do not have constructors and they cannot be directly instantiated. When a class implements an interface, it must implement all of the methods declared in the interface. In this way, interfaces enforce requirements that an implementing class has methods with certain speciﬁed signatures. Suppose, for example, that we want to create an inventory of antiques we own, categorized as objects of various types and with various properties. We might, for instance, wish to identify some of our objects as sellable, in which case they could implement the Sellable interface shown in Code Fragment 2.8. We can then deﬁne a concrete class, Photograph, shown in Code Fragment 2.9, that implements the Sellable interface, indicating that we would be willing to sell any of our Photograph objects. This class deﬁnes an object that implements each of the methods of the Sellable interface, as required. In addition, it adds a method, isColor, which is specialized for Photograph objects. Another kind of object in our collection might be something we could transport. For such objects, we deﬁne the interface shown in Code Fragment 2.10.

2.3. Interfaces and Abstract Classes /∗∗Interface for objects that can be sold. ∗/ public interface Sellable { /∗∗Returns a description of the object. ∗/ public String description(); /∗∗Returns the list price in cents. ∗/ public int listPrice(); /∗∗Returns the lowest price in cents we will accept. ∗/ public int lowestPrice(); } Code Fragment 2.8: Interface Sellable. /∗∗Class for photographs that can be sold. ∗/ public class Photograph implements Sellable { private String descript; // description of this photo private int price; // the price we are setting private boolean color; // true if photo is in color public Photograph(String desc, int p, boolean c) { // constructor descript = desc; price = p; color = c; } public String description() { return descript; } public int listPrice() { return price; } public int lowestPrice() { return price/2; } public boolean isColor() { return color; } } Code Fragment 2.9: Class Photograph implementing the Sellable interface. /∗∗Interface for objects that can be transported. ∗/ public interface Transportable { /∗∗Returns the weight in grams. ∗/ public int weight(); /∗∗Returns whether the object is hazardous. ∗/ public boolean isHazardous(); } Code Fragment 2.10: Interface Transportable.

Chapter 2. Object-Oriented Design We could then deﬁne the class BoxedItem, shown in Code Fragment 2.11, for miscellaneous antiques that we can sell, pack, and ship. Thus, the class BoxedItem implements the methods of the Sellable interface and the Transportable interface, while also adding specialized methods to set an insured value for a boxed shipment and to set the dimensions of a box for shipment. /∗∗Class for objects that can be sold, packed, and shipped. ∗/ public class BoxedItem implements Sellable, Transportable { private String descript; // description of this item private int price; // list price in cents private int weight; // weight in grams private boolean haz; // true if object is hazardous private int height=0; // box height in centimeters private int width=0; // box width in centimeters private int depth=0; // box depth in centimeters /∗∗Constructor ∗/ public BoxedItem(String desc, int p, int w, boolean h) { descript = desc; price = p; weight = w; haz = h; } public String description() { return descript; } public int listPrice() { return price; } public int lowestPrice() { return price/2; } public int weight() { return weight; } public boolean isHazardous() { return haz; } public int insuredValue() { return price∗2; } public void setBox(int h, int w, int d) { height = h; width = w; depth = d; } } Code Fragment 2.11: Class BoxedItem. The class BoxedItem shows another feature of classes and interfaces in Java, as well—that a class can implement multiple interfaces (even though it may only extend one other class). This allows us a great deal of ﬂexibility when deﬁning classes that should conform to multiple APIs.

2.3. Interfaces and Abstract Classes 2.3.2 Multiple Inheritance for Interfaces The ability of extending from more than one type is known as multiple inheritance. In Java, multiple inheritance is allowed for interfaces but not for classes. The reason for this rule is that interfaces do not deﬁne ﬁelds or method bodies, yet classes typically do. Thus, if Java were to allow multiple inheritance for classes, there could be a confusion if a class tried to extend from two classes that contained ﬁelds with the same name or methods with the same signatures. Since there is no such confusion for interfaces, and there are times when multiple inheritance of interfaces is useful, Java allows interfaces to use multiple inheritance. One use for multiple inheritance of interfaces is to approximate a multiple inheritance technique called the mixin. Unlike Java, some object-oriented languages, such as Smalltalk and C++, allow multiple inheritance of concrete classes, not just interfaces. In such languages, it is common to deﬁne classes, called mixin classes, that are never intended to be created as stand-alone objects, but are instead meant to provide additional functionality to existing classes. Such inheritance is not allowed in Java, however, so programmers must approximate it with interfaces. In particular, we can use multiple inheritance of interfaces as a mechanism for “mixing” the methods from two or more unrelated interfaces to deﬁne an interface that combines their functionality, possibly adding more methods of its own. Returning to our example of the antique objects, we could deﬁne an interface for insurable items as follows: public interface Insurable extends Sellable, Transportable { /∗∗Returns insured value in cents ∗/ public int insuredValue(); } This interface combines the methods of the Transportable interface with the methods of the Sellable interface, and adds an extra method, insuredValue. Such an interface could allow us to deﬁne the BoxedItem alternately as follows: public class BoxedItem2 implements Insurable { // ... same code as class BoxedItem } In this case, note that the method insuredValue is not optional, whereas it was optional in the declaration of BoxedItem given previously. Java interfaces that approximate the mixin include java.lang.Cloneable, which adds a copy feature to a class; java.lang.Comparable, which adds a comparability feature to a class (imposing a natural order on its instances); and java.util.Observer, which adds an update feature to a class that wishes to be notiﬁed when certain “observable” objects change state.

Chapter 2. Object-Oriented Design 2.3.3 Abstract Classes In Java, an abstract class serves a role somewhat between that of a traditional class and that of an interface. Like an interface, an abstract class may deﬁne signatures for one or more methods without providing an implementation of those method bodies; such methods are known as abstract methods. However, unlike an interface, an abstract class may deﬁne one or more ﬁelds and any number of methods with implementation (so-called concrete methods). An abstract class may also extend another class and be extended by further subclasses. As is the case with interfaces, an abstract class may not be instantiated, that is, no object can be created directly from an abstract class. In a sense, it remains an incomplete class. A subclass of an abstract class must provide an implementation for the abstract methods of its superclass, or else remain abstract. To distinguish from abstract classes, we will refer to nonabstract classes as concrete classes. In comparing the use of interfaces and abstract classes, it is clear that abstract classes are more powerful, as they can provide some concrete functionality. However, the use of abstract classes in Java is limited to single inheritance, so a class may have at most one superclass, whether concrete or abstract (see Section 2.3.2). We will take great advantage of abstract classes in our study of data structures, as they support greater reusability of code (one of our object-oriented design goals from Section 2.1.1). The commonality between a family of classes can be placed within an abstract class, which serves as a superclass to multiple concrete classes. In this way, the concrete subclasses need only implement the additional functionality that differentiates themselves from each other. As a tangible example, we reconsider the progression hierarchy introduced in Section 2.2.3. Although we did not formally declare the Progression base class as abstract in that presentation, it would have been a reasonable design to have done so. We did not intend for users to directly create instances of the Progression class; in fact, the sequence that it produces is simply a special case of an arithmetic progression with increment one. The primary purpose of the Progression class is to provide common functionality to all three subclasses: the declaration and initialization of the current ﬁeld, and the concrete implementations of the nextValue and printProgression methods. The most important aspect in specializing that class was in overriding the protected advance method. Although we gave a simple implementation of that method within the Progression class to increment the current value, none of our three subclasses rely on that behavior. On the next page, we demonstrate the mechanics of abstract classes in Java by redesigning the progression base class into an AbstractProgression base class. In that design, we leave the advance method as truly abstract, leaving the burden of an implementation to the various subclasses.

2.3. Interfaces and Abstract Classes Mechanics of Abstract Classes in Java In Code Fragment 2.12, we give a Java implementation of a new abstract base class for our progression hierarchy. We name the new class AbstractProgression rather than Progression, only to differentiate it in our discussion. The deﬁnitions are almost identical; there are only two key differences that we highlight. The ﬁrst is the use of the abstract modiﬁer on line 1, when declaring the class. (See Section 1.2.2 for a discussion of class modiﬁers.) As with our original class, the new class declares the current ﬁeld and provides constructors that initialize it. Although our abstract class cannot be instantiated, the constructors can be invoked within the subclass constructors using the super keyword. (We do just that, within all three of our progression subclasses.) The new class has the same concrete implementations of methods nextValue and printProgression as did our original. However, we explicitly deﬁne the advance method with the abstract modiﬁer at line 19, and without any method body. Even though we have not implemented the advance method as part of the AbstractProgression class, it is legal to call it from within the body of nextValue. This is an example of an object-oriented design pattern known as the template method pattern, in which an abstract base class provides a concrete behavior that relies upon calls to other abstract behaviors. Once a subclass provides deﬁnitions for the missing abstract behaviors, the inherited concrete behavior is well deﬁned. public abstract class AbstractProgression { protected long current; public AbstractProgression() { this(0); } public AbstractProgression(long start) { current = start; } public long nextValue() { // this is a concrete method long answer = current; advance(); // this protected call is responsible for advancing the current value return answer; } public void printProgression(int n) { // this is a concrete method System.out.print(nextValue()); // print ﬁrst value without leading space for (int j=1; j < n; j++) System.out.print(" " + nextValue()); // print leading space before others System.out.println(); // end the line } protected abstract void advance(); // notice the lack of a method body } Code Fragment 2.12: An abstract version of the progression base class, originally given in Code Fragment 2.2. (We omit documentation for brevity.)

Chapter 2. Object-Oriented Design 2.4 Exceptions Exceptions are unexpected events that occur during the execution of a program. An exception might result due to an unavailable resource, unexpected input from a user, or simply a logical error on the part of the programmer. In Java, exceptions are objects that can be thrown by code that encounters an unexpected situation, or by the Java Virtual Machine, for example, if running out of memory. An exception may also be caught by a surrounding block of code that “handles” the problem in an appropriate fashion. If uncaught, an exception causes the virtual machine to stop executing the program and to report an appropriate message to the console. In this section, we discuss common exception types in Java, as well as the syntax for throwing and catch exceptions within user-deﬁned blocks of code. 2.4.1 Catching Exceptions If an exception occurs and is not handled, then the Java runtime system will terminate the program after printing an appropriate message together with a trace of the runtime stack. The stack trace shows the series of nested method calls that were active at the time the exception occurred, as in the following example: Exception in thread "main" java.lang.NullPointerException at java.util.ArrayList.toArray(ArrayList.java:358) at net.datastructures.HashChainMap.bucketGet(HashChainMap.java:35) at net.datastructures.AbstractHashMap.get(AbstractHashMap.java:62) at dsaj.design.Demonstration.main(Demonstration.java:12) However, before a program is terminated, each method on the stack trace has an opportunity to catch the exception. Starting with the most deeply nested method in which the exception occurs, each method may either catch the exception, or allow it to pass through to the method that called it. For example, in the above stack trace, the ArrayList.java method had the ﬁrst opportunity to catch the exception. Since it did not do so, the exception was passed upward to the HashChainMap.bucketGet method, which in turn ignored the exception, causing it to pass further upward to the AbstractHashMap.get method. The ﬁnal opportunity to catch the exception was in the Demonstration.main method, but since it did not do so, the program terminated with the above diagnostic message. The general methodology for handling exceptions is a try-catch construct in which a guarded fragment of code that might throw an exception is executed. If it throws an exception, then that exception is caught by having the ﬂow of control jump to a predeﬁned catch block that contains the code to analyze the exception and apply an appropriate resolution. If no exception occurs in the guarded code, all catch blocks are ignored.

2.4. Exceptions A typical syntax for a try-catch statement in Java is as follows: try { guardedBody } catch (exceptionType1 variable1) { remedyBody1 } catch (exceptionType2 variable2) { remedyBody2 } ... ... Each exceptionTypei is the type of some exception, and each variablei is a valid Java variable name. The Java runtime environment begins performing a try-catch statement such as this by executing the block of statements, guardedBody. If no exceptions are generated during this execution, the ﬂow of control continues with the ﬁrst statement beyond the last line of the entire try-catch statement. If, on the other hand, the block, guardedBody, generates an exception at some point, the execution of that block immediate terminates and execution jumps to the catch block whose exceptionType most closely matches the exception thrown (if any). The variable for this catch statement references the exception object itself, which can be used in the block of the matching catch statement. Once execution of that catch block completes, control ﬂow continues with the ﬁrst statement beyond the entire try-catch construct. If an exception occurs during the execution of the block, guardedBody, that does not match any of the exception types declared in the catch statements, that exception is rethrown in the surrounding context. There are several possible reactions when an exception is caught. One possibility is to print out an error message and terminate the program. There are also some interesting cases in which the best way to handle an exception is to quietly catch and ignore it (this can be done by having an empty body as a catch block). Another legitimate way of handling exceptions is to create and throw another exception, possibly one that speciﬁes the exceptional condition more precisely. We note brieﬂy that try-catch statements in Java support a few advanced techniques that we will not use in this book. There can be an optional ﬁnally clause with a body that will be executed whether or not an exception happens in the original guarded body; this can be useful, for example, to close a ﬁle before proceeding onward. Java SE 7 introduced a new syntax known as a “try with resource” that provides even more advanced cleanup techniques for resources such as open ﬁles that must be properly cleaned up. Also as of Java SE 7, each catch statement can designate multiple exception types that it handles; previously, a separate clause would be needed for each one, even if the same remedy were applied in each case.

Chapter 2. Object-Oriented Design public static void main(String[ ] args) { int n = DEFAULT; try { n = Integer.parseInt(args[0]); if (n <= 0) { System.out.println("n must be positive. Using default."); n = DEFAULT; } } catch (ArrayIndexOutOfBoundsException e) { System.out.println("No argument specified for n. Using default."); } catch (NumberFormatException e) { System.out.println("Invalid integer argument. Using default."); } } Code Fragment 2.13: A demonstration of catching an exception. As a tangible example of a try-catch statement, we consider the simple application presented in Code Fragment 2.13. This main method attempts to interpret the ﬁrst command-line argument as a positive integer. (Command-line arguments were introduced on page 16.) The statement at risk of throwing an exception, at line 4, is the command n = Integer.parseInt(args[0]). That command may fail for one of two reasons. First, the attempt to access args[0] will fail if the user did not specify any arguments, and thus, the array args is empty. An ArrayIndexOutOfBoundsException will be thrown in that case (and caught by us at line 9). The second potential exception is when calling the Integer.parseInt method. That command succeeds so long as the parameter is a string that is a legitimate integer representation, such as "2013". Of course, since a command-line argument can be any string, the user might provide an invalid integer representation, in which case the parseInt method throws a NumberFormatException (caught by us at line 11). A ﬁnal condition we wish to enforce is that the integer speciﬁed by the user is positive. To test this property, we rely on a traditional conditional statement (lines 5–8). However, notice that we have placed that conditional statement within the primary body of the try-catch statement. That conditional statement will only be evaluated if the command at line 4 succeeded without exception; had an exception occurred at line 4, the primary try block is terminated, and control proceeds directly to the exception handling for the appropriate catch statement. As an aside, if we had been willing to use the same error message for the two exceptional cases, we can use a single catch clause with the following syntax: } catch (ArrayIndexOutOfBoundsException | NumberFormatException e) { System.out.println("Using default value for n."); }

2.4. Exceptions 2.4.2 Throwing Exceptions Exceptions originate when a piece of Java code ﬁnds some sort of problem during execution and throws an exception object. This is done by using the throw keyword followed by an instance of the exception type to be thrown. It is often convenient to instantiate an exception object at the time the exception has to be thrown. Thus, a throw statement is typically written as follows: throw new exceptionType(parameters); where exceptionType is the type of the exception and the parameters are sent to that type’s constructor; most exception types offer a version of a constructor that accepts an error message string as a parameter. As an example, the following method takes an integer parameter, which it expects to be positive. If a negative integer is sent, an IllegalArgumentException is thrown. public void ensurePositive(int n) { if (n < 0) throw new IllegalArgumentException("That's not positive!"); // ... } The execution of a throw statement immediately terminates the body of a method. The Throws Clause When a method is declared, it is possible to explicitly declare, as part of its signature, the possibility that a particular exception type may be thrown during a call to that method. It does not matter whether the exception is directly from a throw statement in that method body, or propagated upward from a secondary method call made from within the body. The syntax for declaring possible exceptions in a method signature relies on the keyword throws (not to be confused with an actual throw statement). For example, the parseInt method of the Integer class has the following formal signature: public static int parseInt(String s) throws NumberFormatException; The designation “throws NumberFormatException” warns users about the possibility of an exceptional case, so that they might be better prepared to handle an exception that may arise. If one of many exception types may possibly be thrown, all such types can be listed, separated with commas. Alternatively, it may be possible to list an appropriate superclass that encompasses all speciﬁc exceptions that may be thrown.

Chapter 2. Object-Oriented Design The use of a throws clause in a method signature does not take away the responsibility of properly documenting all possible exceptions through the use of the @throws tag within a javadoc comment (see Section 1.9.4). The type and reasons for any potential exceptions should always be properly declared in the documentation for a method. In contrast, the use of the throws clause in a method signature is optional for many types of exceptions. For example, the documentation for the nextInt() method of the Scanner class makes clear that three different exception types may arise: • An IllegalStateException, if the scanner has been closed • A NoSuchElementException, if the scanner is active, but there is currently no token available for input • An InputMismatchException, if the next available token does not represent an integer However, no potential exceptions are formally declared within the method signature; they are only noted in the documentation. To better understand the functional purpose of the throws declaration in a method signature, it is helpful to know more about the way Java organizes its hierarchy of exception types. 2.4.3 Java’s Exception Hierarchy Java deﬁnes a rich inheritance hierarchy of all objects that are deemed Throwable. We show a small portion of this hierarchy in Figure 2.7. The hierarchy is intentionally divided into two subclasses: Error and Exception. Errors are typically thrown only by the Java Virtual Machine and designate the most serious situations that are unlikely to be recoverable, such as when the virtual machine is asked to execute a corrupt class ﬁle, or when the system runs out of memory. In contrast, exceptions designate situations in which a running program might reasonably be able to recover, for example, when unable to open a data ﬁle. Checked and Unchecked Exceptions Java provides further reﬁnement by declaring the RuntimeException class as an important subclass of Exception. All subtypes of RuntimeException in Java are ofﬁcially treated as unchecked exceptions, and any exception type that is not part of the RuntimeException is a checked exception. The intent of the design is that runtime exceptions occur entirely due to mistakes in programming logic, such as using a bad index with an array, or sending an inappropriate value as a parameter to a method. While such programming errors

2.4. Exceptions OutofMemoryError RuntimeException IOError VirtualMachineError NullPointerException Throwable IOException IllegalArgumentException NoSuchElementException . . . EOFException IndexOutOfBoundsException Exception ArrayIndexOutOfBoundsException FileNotFoundException . . . NumberFormatException Error ClassCastException Figure 2.7: A small portion of Java’s hierarchy of Throwable types. will certainly occur as part of the software development process, they should presumably be resolved before software reaches production quality. Therefore, it is not in the interest of efﬁciency to explicitly check for each such mistake at runtime, and thus these are designated as “unchecked” exceptions. In contrast, other exceptions occur because of conditions that cannot easily be detected until a program is executing, such as an unavailable ﬁle or a failed network connection. Those are typically designated as “checked” exceptions in Java (and thus, not a subtype of RuntimeException). The designation between checked and unchecked exceptions plays a signiﬁcant role in the syntax of the language. In particular, all checked exceptions that might propagate upward from a method must be explicitly declared in its signature. A consequence is that if one method calls a second method declaring checked exceptions, then the call to that second method must either be guarded within a try-catch statement, or else the calling method must itself declare the checked exceptions in its signature, since there is risk that such an exception might propagate upward from the calling method. Deﬁning New Exception Types In this book, we will rely entirely on existing RuntimeException types to designate various requirements on the use of our data structures. However, some libraries deﬁne new classes of exceptions to describe more speciﬁc conditions. Specialized exceptions should inherit either from the Exception class (if checked), from the RuntimeException class (if unchecked), or from an existing Exception subtype that is more relevant.

Chapter 2. Object-Oriented Design 2.5 Casting and Generics In this section, we discuss casting among reference variables, as well as a technique, called generics, that allows us to deﬁne methods and classes that work with a variety of data types without the need for explicit casting. 2.5.1 Casting We begin our discussion with methods for type conversions for objects. Widening Conversions A widening conversion occurs when a type T is converted into a “wider” type U. The following are common cases of widening conversions: • T and U are class types and U is a superclass of T. • T and U are interface types and U is a superinterface of T. • T is a class that implements interface U. Widening conversions are automatically performed to store the result of an expression into a variable, without the need for an explicit cast. Thus, we can directly assign the result of an expression of type T into a variable v of type U when the conversion from T to U is a widening conversion. When discussing polymorphism on page 68, we gave the following example of an implicit widening cast, assigning an instance of the narrower PredatoryCreditCard class to a variable of the wider CreditCard type: CreditCard card = new PredatoryCreditCard(...); // parameters omitted The correctness of a widening conversion can be checked by the compiler and its validity does not require testing by the Java runtime environment during program execution. Narrowing Conversions A narrowing conversion occurs when a type T is converted into a “narrower” type S. The following are common cases of narrowing conversions: • T and S are class types and S is a subclass of T. • T and S are interface types and S is a subinterface of T. • T is an interface implemented by class S. In general, a narrowing conversion of reference types requires an explicit cast. Also, the correctness of a narrowing conversion may not be veriﬁable by the compiler. Thus, its validity should be tested by the Java runtime environment during program execution.

2.5. Casting and Generics The example code fragment below shows how to use a cast to perform a narrowing conversion from type PredatoryCreditCard to type CreditCard. CreditCard card = new PredatoryCreditCard(...); // widening PredatoryCreditCard pc = (PredatoryCreditCard) card; // narrowing Although variable card happens to reference an instance of a PredatoryCreditCard, the variable has declared type, CreditCard. Therefore, the assignment pc = card is a narrowing conversion and requires an explicit cast that will be evaluated at runtime (as not all cards are predatory). Casting Exceptions In Java, we can cast an object reference o of type T into a type S, provided the object o is referring to is actually of type S. If, on the other hand, object o is not also of type S, then attempting to cast o to type S will throw an exception called ClassCastException. We illustrate this rule in the following code fragment, using Java’s Number abstract class, which is a superclass of both Integer and Double. Number n; Integer i; n = new Integer(3); i = (Integer) n; // This is legal n = new Double(3.1415); i = (Integer) n; // This is illegal To avoid problems such as this and to avoid peppering our code with try-catch blocks every time we perform a cast, Java provides a way to make sure an object cast will be correct. Namely, it provides an operator, instanceof, that allows us to test whether an object variable is referring to an object that belongs to a particular type. The syntax for this operator is objectReference instanceof referenceType, where objectReference is an expression that evaluates to an object reference and referenceType is the name of some existing class, interface, or enum (Section 1.3). If objectReference is indeed an instance satisfying referenceType, then the operator returns true; otherwise, it returns false. Thus, we can avoid a ClassCastException from being thrown in the code fragment above by modifying it as follows: Number n; Integer i; n = new Integer(3); if (n instanceof Integer) i = (Integer) n; // This is legal n = new Double(3.1415); if (n instanceof Integer) i = (Integer) n; // This will not be attempted

Chapter 2. Object-Oriented Design Casting with Interfaces Interfaces allow us to enforce that objects implement certain methods, but using interface variables with concrete objects sometimes requires casting. Suppose we declare a Person interface as shown in Code Fragment 2.14. Note that method equals of the Person interface takes one parameter of type Person. Thus, we can pass an object of any class implementing the Person interface to this method. public interface Person { public boolean equals(Person other); // is this the same person? public String getName(); // get this person’s name public int getAge(); // get this person’s age } Code Fragment 2.14: Interface Person. In Code Fragment 2.15, we show a class, Student, that implements Person. Because the parameter to equals is a Person, the implementation must not assume that it is necessarily of type Student. Instead, it ﬁrst uses the instanceof operator at line 15, returning false if the argument is not a student (since it surely is not the student in question). Only after verifying that the parameter is a student, is it explicitly cast to a Student, at which point its id ﬁeld can be accessed. public class Student implements Person { String id; String name; int age; public Student(String i, String n, int a) { // simple constructor id = i; name = n; age = a; } protected int studyHours() { return age/2;} // just a guess public String getID() { return id;} // ID of the student public String getName() { return name; } // from Person interface public int getAge() { return age; } // from Person interface public boolean equals(Person other) { // from Person interface if (!(other instanceof Student)) return false; // cannot possibly be equal Student s = (Student) other; // explicit cast now safe return id.equals(s.id); // compare IDs } public String toString() { // for printing return "Student(ID:" + id + ", Name:" + name + ", Age:" + age + ")"; } } Code Fragment 2.15: Class Student implementing interface Person.

2.5. Casting and Generics 2.5.2 Generics Java includes support for writing generic classes and methods that can operate on a variety of data types while often avoiding the need for explicit casts. The generics framework allows us to deﬁne a class in terms of a set of formal type parameters, which can then be used as the declared type for variables, parameters, and return values within the class deﬁnition. Those formal type parameters are later speciﬁed when using the generic class as a type elsewhere in a program. To better motivate the use of generics, we consider a simple case study. Often, we wish to treat a pair of related values as a single object, for example, so that the pair can be returned from a method. A solution is to deﬁne a new class whose instances store both values. This is our ﬁrst example of an object-oriented design pattern known as the composition design pattern. If we know, for example, that we want a pair to store a string and a ﬂoating-point number, perhaps to store a stock ticker label and a price, we could easily design a custom class for that purpose. However, for another purpose, we might want to store a pair that consists of a Book object and an integer that represents a quantity. The goal of generic programming is to be able to write a single class that can represent all such pairs. The generics framework was not a part of the original Java language; it was added as part of Java SE 5. Prior to that, generic programming was implemented by relying heavily on Java’s Object class, which is the universal supertype of all objects (including the wrapper types corresponding to primitives). In that “classic” style, a generic pair might be implemented as shown in Code Fragment 2.16. public class ObjectPair { Object ﬁrst; Object second; public ObjectPair(Object a, Object b) { // constructor ﬁrst = a; second = b; } public Object getFirst() { return ﬁrst; } public Object getSecond() { return second;} } Code Fragment 2.16: Representing a generic pair of objects using a classic style. An ObjectPair instance stores the two objects that are sent to the constructor, and provides individual accessors for each component of the pair. With this deﬁnition, a pair can be declared and instantiated with the following command: ObjectPair bid = new ObjectPair("ORCL", 32.07); This instantiation is legal because the parameters to the constructor undergo widening conversions. The ﬁrst parameter, "ORCL", is a String, and thus also an Object.

Chapter 2. Object-Oriented Design The second parameter is a double, but it is automatically boxed into a Double, which then qualiﬁes as an Object. (For the record, this is not quite the “classic” style, as automatic boxing was not introduced until Java SE 5.) The drawback of the classic approach involves use of the accessors, both of which formally return an Object reference. Even if we know that the ﬁrst object is a string in our application, we cannot legally make the following assignment: String stock = bid.getFirst(); // illegal; compile error This represents a narrowing conversion from the declared return type of Object to the variable of type String. Instead, an explicit cast is required, as follows: String stock = (String) bid.getFirst(); // narrowing cast: Object to String With the classic style for generics, code became rampant with such explicit casts. Using Java’s Generics Framework With Java’s generics framework, we can implement a pair class using formal type parameters to represent the two relevant types in our composition. An implementation using this framework is given in Code Fragment 2.17. public class Pair<A,B> { A ﬁrst; B second; public Pair(A a, B b) { // constructor ﬁrst = a; second = b; } public A getFirst() { return ﬁrst; } public B getSecond() { return second;} } Code Fragment 2.17: Representing a pair of objects with generic type parameters. Angle brackets are used at line 1 to enclose the sequence of formal type parameters. Although any valid identiﬁer can be used for a formal type parameter, single-letter uppercase names are conventionally used (in this example, A and B). We may then use these type parameters within the body of the class deﬁnition. For example, we declare instance variable, ﬁrst, to have type A; we similarly use A as the declared type for the ﬁrst constructor parameter and for the return type of method, getFirst. When subsequently declaring a variable with such a parameterize type, we must explicitly specify actual type parameters that will take the place of the generic formal type parameters. For example, to declare a variable that is a pair holding a stock-ticker string and a price, we write the following: Pair<String,Double> bid;

2.5. Casting and Generics Effectively, we have stated that we wish to have String serve in place of type A, and Double serve in place of type B for the pair known as bid. The actual types for generic programming must be object types, which is why we use the wrapper class Double instead of the primitive type double. (Fortunately, the automatic boxing and unboxing will work in our favor.) We can subsequently instantiate the generic class using the following syntax: bid = new Pair<>("ORCL", 32.07); // rely on type inference After the new operator, we provide the name of the generic class, then an empty set of angle brackets (known as the “diamond”), and ﬁnally the parameters to the constructor. An instance of the generic class is created, with the actual types for the formal type parameters determined based upon the original declaration of the variable to which it is assigned (bid in this example). This process is known as type inference, and was introduced to the generics framework in Java SE 7. It is also possible to use a style that existed prior to Java SE 7, in which the generic type parameters are explicitly speciﬁed between angle brackets during instantiation. Using that style, our previous example would be implemented as: bid = new Pair<String,Double>("ORCL", 32.07); // give explicit types However, it is important that one of the two above styles be used. If angle brackets are entirely omitted, as in the following example, bid = new Pair("ORCL", 32.07); // classic style this reverts to the classic style, with Object automatically used for all generic type parameters, and resulting in a compiler warning when assigning to a variable with more speciﬁc types. Although the syntax for the declaration and instantiation of objects using the generics framework is slightly more cluttered than the classic style, the advantage is that there is no longer any need for explicit narrowing casts from Object to a more speciﬁc type. Continuing with our example, since bid was declared with actual type parameters <String,Double>, the return type of the getFirst() method is String, and the return type of the getSecond() method is Double. Unlike the classic style, we can make the following assignments without any explicit casting (although there is still an automatic unboxing of the Double): String stock = bid.getFirst(); double price = bid.getSecond();

Chapter 2. Object-Oriented Design Generics and Arrays There is an important caveat related to generic types and the use of arrays. Although Java allows the declaration of an array storing a parameterized type, it does not technically allow the instantiation of new arrays involving those types. Fortunately, it allows an array deﬁned with a parameterized type to be initialized with a newly created, nonparametric array, which can then be cast to the parameterized type. Even so, this latter mechanism causes the Java compiler to issue a warning, because it is not 100% type-safe. We will see this issue arise in two ways: • Code outside a generic class may wish to declare an array storing instances of the generic class with actual type parameters. • A generic class may wish to declare an array storing objects that belong to one of the formal parameter types. As an example of the ﬁrst use case, we continue with our stock market example and presume that we would like to keep an array of Pair<String,Double> objects. Such an array can be declared with a parameterized type, but it must be instantiated with an unparameterized type and then cast back to the parameterized type. We demonstrate this usage in the following: Pair<String,Double>[ ] holdings; holdings = new Pair<String,Double>[25]; // illegal; compile error holdings = new Pair[25]; // correct, but warning about unchecked cast holdings[0] = new Pair<>("ORCL", 32.07); // valid element assignment As an example of the second use case, assume that we want to create a generic Portfolio class that can store a ﬁxed number of generic entries in an array. If the class uses <T> as a parameterized type, it can declare an array of type T[ ], but it cannot directly instantiate such an array. Instead, a common approach is to instantiate an array of type Object[ ], and then make a narrowing cast to type T[ ], as shown in the following: public class Portfolio<T> { T[ ] data; public Portfolio(int capacity) { data = new T[capacity]; // illegal; compiler error data = (T[ ]) new Object[capacity]; // legal, but compiler warning } public T get(int index) { return data[index]; } public void set(int index, T element) { data[index] = element; } }

2.5. Casting and Generics Generic Methods The generics framework allows us to deﬁne generic versions of individual methods (as opposed to generic versions of entire classes). To do so, we include a generic formal type declaration among the method modiﬁers. For example, we show below a nonparametric GenericDemo class with a parameterized static method that can reverse an array containing elements of any object type. public class GenericDemo { public static <T> void reverse(T[ ] data) { int low = 0, high = data.length −1; while (low < high) { // swap data[low] and data[high] T temp = data[low]; data[low++] = data[high]; // post-increment of low data[high−−] = temp; // post-decrement of high } } } Note the use of the <T> modiﬁer to declare the method to be generic, and the use of the type T within the method body, when declaring the local variable, temp. The method can be called using the syntax, GenericDemo.reverse(books), with type inference determining the generic type, assuming books is an array of some object type. (This generic method cannot be applied to primitive arrays, because autoboxing does not apply to entire arrays.) As an aside, we note that we could have implemented a reverse method equally well using a classic style, acting upon an Object[ ] array. Bounded Generic Types By default, when using a type name such as T in a generic class or method, a user can specify any object type as the actual type of the generic. A formal parameter type can be restricted by using the extends keyword followed by a class or interface. In that case, only a type that satisﬁes the stated condition is allowed to substitute for the parameter. The advantage of such a bounded type is that it becomes possible to call any methods that are guaranteed by the stated bound. As an example, we might declare a generic ShoppingCart that could only be instantiated with a type that satisﬁed the Sellable interface (from Code Fragment 2.8 on page 77). Such a class would be declared beginning with the line: public class ShoppingCart<T extends Sellable> { Within that class deﬁnition, we would then be allowed to call methods such as description() and lowestPrice() on any instances of type T.

Chapter 2. Object-Oriented Design 2.6 Nested Classes Java allows a class deﬁnition to be nested inside the deﬁnition of another class. The main use for nesting classes is when deﬁning a class that is strongly afﬁliated with another class. This can help increase encapsulation and reduce undesired name conﬂicts. Nested classes are a valuable technique when implementing data structures, as an instance of a nested use can be used to represent a small portion of a larger data structure, or an auxiliary class that helps navigate a primary data structure. We will use nested classes in many implementations within this book. To demonstrate the mechanics of a nested class, we consider a new Transaction class to support logging of transactions associated with a credit card. That new class deﬁnition can be nested within the CreditCard class using a style as follows: public class CreditCard { private static class Transaction { /∗details omitted ∗/ } // instance variable for a CreditCard Transaction[ ] history; // keep log of all transactions for this card } The containing class is known as the outer class. The nested class is formally a member of the outer class, and its fully qualiﬁed name is OuterName.NestedName. For example, with the above deﬁnition the nested class is CreditCard.Transaction, although we may refer to it simply as Transaction from within the CreditCard class. Much like packages (see Section 1.8), the use of nested classes can help reduce name collisions, as it is perfectly acceptable to have another class named Transaction nested within some other class (or as a self-standing class). A nested class has an independent set of modiﬁers from the outer class. Visibility modiﬁers (e.g., public, private) effect whether the nested class deﬁnition is accessible beyond the outer class deﬁnition. For example, a private nested class can be used by the outer class, but by no other classes. A nested class can also be designated as either static or (by default) nonstatic, with signiﬁcant consequences. A static nested class is most like a traditional class; its instances have no association with any speciﬁc instance of the outer class. A nonstatic nested class is more commonly known as an inner class in Java. An instance of an inner class can only be created from within a nonstatic method of the outer class, and that inner instance becomes associated with the outer instance that creates it. Each instance of an inner class implicitly stores a reference to its associated outer instance, accessible from within the inner class methods using the syntax OuterName.this (as opposed to this, which refers to the inner instance). The inner instance also has private access to all members of its associated outer instance, and can rely on the formal type parameters of the outer class, if generic.

Chapter Fundamental Data Structures Contents 3.1 Using Arrays . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 Storing Game Entries in an Array . . . . . . . . . . . . . . 104 3.1.2 Sorting an Array . . . . . . . . . . . . . . . . . . . . . . . 110 3.1.3 java.util Methods for Arrays and Random Numbers . . . . 112 3.1.4 Simple Cryptography with Character Arrays . . . . . . . . 115 3.1.5 Two-Dimensional Arrays and Positional Games . . . . . . 118 3.2 Singly Linked Lists . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Implementing a Singly Linked List Class . . . . . . . . . . 126 3.3 Circularly Linked Lists . . . . . . . . . . . . . . . . . . . . . 3.3.1 Round-Robin Scheduling . . . . . . . . . . . . . . . . . . 128 3.3.2 Designing and Implementing a Circularly Linked List . . . 129 3.4 Doubly Linked Lists . . . . . . . . . . . . . . . . . . . . . . 3.4.1 Implementing a Doubly Linked List Class . . . . . . . . . 135 3.5 Equivalence Testing . . . . . . . . . . . . . . . . . . . . . . 3.5.1 Equivalence Testing with Arrays . . . . . . . . . . . . . . 139 3.5.2 Equivalence Testing with Linked Lists . . . . . . . . . . . 140 3.6 Cloning Data Structures . . . . . . . . . . . . . . . . . . . 3.6.1 Cloning Arrays . . . . . . . . . . . . . . . . . . . . . . . . 142 3.6.2 Cloning Linked Lists . . . . . . . . . . . . . . . . . . . . . 144 3.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Chapter 3. Fundamental Data Structures 3.1 Using Arrays In this section, we explore a few applications of arrays—the concrete data structures introduced in Section 1.3 that access their entries using integer indices. 3.1.1 Storing Game Entries in an Array The ﬁrst application we study is storing a sequence of high score entries for a video game in an array. This is representative of many applications in which a sequence of objects must be stored. We could just as easily have chosen to store records for patients in a hospital or the names of players on a football team. Nevertheless, let us focus on storing high score entries, which is a simple application that is already rich enough to present some important data-structuring concepts. To begin, we consider what information to include in an object representing a high score entry. Obviously, one component to include is an integer representing the score itself, which we identify as score. Another useful thing to include is the name of the person earning this score, which we identify as name. We could go on from here, adding ﬁelds representing the date the score was earned or game statistics that led to that score. However, we omit such details to keep our example simple. A Java class, GameEntry, representing a game entry, is given in Code Fragment 3.1. public class GameEntry { private String name; // name of the person earning this score private int score; // the score value /∗∗Constructs a game entry with given parameters.. ∗/ public GameEntry(String n, int s) { name = n; score = s; } /∗∗Returns the name ﬁeld. ∗/ public String getName() { return name; } /∗∗Returns the score ﬁeld. ∗/ public int getScore() { return score; } /∗∗Returns a string representation of this entry. ∗/ public String toString() { return "(" + name + ", " + score + ")"; } } Code Fragment 3.1: Java code for a simple GameEntry class. Note that we include methods for returning the name and score for a game entry object, as well as a method for returning a string representation of this entry.

3.1. Using Arrays A Class for High Scores To maintain a sequence of high scores, we develop a class named Scoreboard. A scoreboard is limited to a certain number of high scores that can be saved; once that limit is reached, a new score only qualiﬁes for the scoreboard if it is strictly higher than the lowest “high score” on the board. The length of the desired scoreboard may depend on the game, perhaps 10, 50, or 500. Since that limit may vary, we allow it to be speciﬁed as a parameter to our Scoreboard constructor. Internally, we will use an array named board to manage the GameEntry instances that represent the high scores. The array is allocated with the speciﬁed maximum capacity, but all entries are initially null. As entries are added, we will maintain them from highest to lowest score, starting at index 0 of the array. We illustrate a typical state of the data structure in Figure 3.1, and give Java code to construct such a data structure in Code Fragment 3.2. Figure 3.1: An illustration of an array of length ten storing references to six GameEntry objects in the cells with indices 0 to 5; the rest are null references. /∗∗Class for storing high scores in an array in nondecreasing order. ∗/ public class Scoreboard { private int numEntries = 0; // number of actual entries private GameEntry[ ] board; // array of game entries (names & scores) /∗∗Constructs an empty scoreboard with the given capacity for storing entries. ∗/ public Scoreboard(int capacity) { board = new GameEntry[capacity]; } ... // more methods will go here } Code Fragment 3.2: The beginning of a Scoreboard class for maintaining a set of scores as GameEntry objects. (Completed in Code Fragments 3.3 and 3.4.)

Chapter 3. Fundamental Data Structures Adding an Entry One of the most common updates we might want to make to a Scoreboard is to add a new entry. Keep in mind that not every entry will necessarily qualify as a high score. If the board is not yet full, any new entry will be retained. Once the board is full, a new entry is only retained if it is strictly better than one of the other scores, in particular, the last entry of the scoreboard, which is the lowest of the high scores. Code Fragment 3.3 provides an implementation of an update method for the Scoreboard class that considers the addition of a new game entry. /∗∗Attempt to add a new score to the collection (if it is high enough) ∗/ public void add(GameEntry e) { int newScore = e.getScore(); // is the new entry e really a high score? if (numEntries < board.length || newScore > board[numEntries−1].getScore()) { if (numEntries < board.length) // no score drops from the board numEntries++; // so overall number increases // shift any lower scores rightward to make room for the new entry int j = numEntries −1; while (j > 0 && board[j−1].getScore() < newScore) { board[j] = board[j−1]; // shift entry from j-1 to j j−−; // and decrement j } board[j] = e; // when done, add new entry } } Code Fragment 3.3: Java code for inserting a GameEntry object into a Scoreboard. When a new score is considered, the ﬁrst goal is to determine whether it qualiﬁes as a high score. This will be the case (see line 13) if the scoreboard is below its capacity, or if the new score is strictly higher than the lowest score on the board. Once it has been determined that a new entry should be kept, there are two remaining tasks: (1) properly update the number of entries, and (2) place the new entry in the appropriate location, shifting entries with inferior scores as needed. The ﬁrst of these tasks is easily handled at lines 14 and 15, as the total number of entries can only be increased if the board is not yet at full capacity. (When full, the addition of a new entry will be counteracted by the removal of the entry with lowest score.) The placement of the new entry is implemented by lines 17–22. Index j is initially set to numEntries −1, which is the index at which the last GameEntry will reside after completing the operation. Either j is the correct index for the newest entry, or one or more immediately before it will have lesser scores. The while loop checks the compound condition, shifting entries rightward and decrementing j, as long as there is another entry at index j −1 with a score less than the new score.

3.1. Using Arrays Figure 3.2: Preparing to add Jill’s GameEntry object to the board array. In order to make room for the new reference, we have to shift any references to game entries with smaller scores than the new one to the right by one cell. Figure 3.2 shows an example of the process, just after the shifting of existing entries, but before adding the new entry. When the loop completes, j will be the correct index for the new entry. Figure 3.3 shows the result of a complete operation, after the assignment of board[j] = e, accomplished by line 22 of the code. In Exercise C-3.19, we explore how game entry addition might be simpliﬁed for the case when we don’t need to preserve relative orders. Figure 3.3: Adding a reference to Jill’s GameEntry object to the board array. The reference can now be inserted at index 2, since we have shifted all references to GameEntry objects with scores less than the new one to the right.

Chapter 3. Fundamental Data Structures Removing an Entry Suppose some hot shot plays our video game and gets his or her name on our high score list, but we later learn that cheating occurred. In this case, we might want to have a method that lets us remove a game entry from the list of high scores. Therefore, let us consider how we might remove a reference to a GameEntry object from a Scoreboard. We choose to add a method to the Scoreboard class, with signature remove(i), where i designates the current index of the entry that should be removed and returned. When a score is removed, any lower scores will be shifted upward, to ﬁll in for the removed entry. If index i is outside the range of current entries, the method will throw an IndexOutOfBoundsException. Our implementation for remove will involve a loop for shifting entries, much like our algorithm for addition, but in reverse. To remove the reference to the object at index i, we start at index i and move all the references at indices higher than i one cell to the left. (See Figure 3.4.) Figure 3.4: An illustration of the removal of Paul’s score from index 3 of an array storing references to GameEntry objects. Our implementation of the remove method for the Scoreboard class is given in Code Fragment 3.4. The details for doing the remove operation contain a few subtle points. The ﬁrst is that, in order to remove and return the game entry (let’s call it e) at index i in our array, we must ﬁrst save e in a temporary variable. We will use this variable to return e when we are done removing it.

3.1. Using Arrays /∗∗Remove and return the high score at index i. ∗/ public GameEntry remove(int i) throws IndexOutOfBoundsException { if (i < 0 || i >= numEntries) throw new IndexOutOfBoundsException("Invalid index: " + i); GameEntry temp = board[i]; // save the object to be removed for (int j = i; j < numEntries −1; j++) // count up from i (not down) board[j] = board[j+1]; // move one cell to the left board[numEntries −1 ] = null; // null out the old last score numEntries−−; return temp; // return the removed object } Code Fragment 3.4: Java code for performing the Scoreboard.remove operation. The second subtle point is that, in moving references higher than i one cell to the left, we don’t go all the way to the end of the array. First, we base our loop on the number of current entries, not the capacity of the array, because there is no reason for “shifting” a series of null references that may be at the end of the array. We also carefully deﬁne the loop condition, j < numEntries −1, so that the last iteration of the loop assigns board[numEntries−2] = board[numEntries−1]. There is no entry to shift into cell board[numEntries−1], so we return that cell to null just after the loop. We conclude by returning a reference to the removed entry (which no longer has any reference pointing to it within the board array). Conclusions In the version of the Scoreboard class that is available online, we include an implementation of the toString() method, which allows us to display the contents of the current scoreboard, separated by commas. We also include a main method that performs a basic test of the class. The methods for adding and removing objects in an array of high scores are simple. Nevertheless, they form the basis of techniques that are used repeatedly to build more sophisticated data structures. These other structures may be more general than the array structure above, of course, and often they will have a lot more operations that they can perform than just add and remove. But studying the concrete array data structure, as we are doing now, is a great starting point to understanding these other structures, since every data structure has to be implemented using concrete means. In fact, later in this book, we will study a Java collections class, ArrayList, which is more general than the array structure we are studying here. The ArrayList has methods to operate on an underlying array; yet it also eliminates the error that occurs when adding an object to a full array by automatically copying the objects into a larger array when necessary. We will discuss the ArrayList class in far more detail in Section 7.2.

Chapter 3. Fundamental Data Structures 3.1.2 Sorting an Array In the previous subsection, we considered an application for which we added an object to an array at a given position while shifting other elements so as to keep the previous order intact. In this section, we use a similar technique to solve the sorting problem, that is, starting with an unordered array of elements and rearranging them into nondecreasing order. The Insertion-Sort Algorithm We study several sorting algorithms in this book, most of which are described in Chapter 12. As a warm-up, in this section we describe a simple sorting algorithm known as insertion-sort. The algorithm proceeds by considering one element at a time, placing the element in the correct order relative to those before it. We start with the ﬁrst element in the array, which is trivially sorted by itself. When considering the next element in the array, if it is smaller than the ﬁrst, we swap them. Next we consider the third element in the array, swapping it leftward until it is in its proper order relative to the ﬁrst two elements. We continue in this manner with the fourth element, the ﬁfth, and so on, until the whole array is sorted. We can express the insertion-sort algorithm in pseudocode, as shown in Code Fragment 3.5. Algorithm InsertionSort(A): Input: An array A of n comparable elements Output: The array A with elements rearranged in nondecreasing order for k from 1 to n−1 do Insert A[k] at its proper location within A[0], A[1], ..., A[k]. Code Fragment 3.5: High-level description of the insertion-sort algorithm. This is a simple, high-level description of insertion-sort. If we look back to Code Fragment 3.3 in Section 3.1.1, we see that the task of inserting a new entry into the list of high scores is almost identical to the task of inserting a newly considered element in insertion-sort (except that game scores were ordered from high to low). We provide a Java implementation of insertion-sort in Code Fragment 3.6, using an outer loop to consider each element in turn, and an inner loop that moves a newly considered element to its proper location relative to the (sorted) subarray of elements that are to its left. We illustrate an example run of the insertion-sort algorithm in Figure 3.5. We note that if an array is already sorted, the inner loop of insertion-sort does only one comparison, determines that there is no swap needed, and returns back to the outer loop. Of course, we might have to do a lot more work than this if the input array is extremely out of order. In fact, we will have to do the most work if the input array is in decreasing order.

3.1. Using Arrays /∗∗Insertion-sort of an array of characters into nondecreasing order ∗/ public static void insertionSort(char[ ] data) { int n = data.length; for (int k = 1; k < n; k++) { // begin with second character char cur = data[k]; // time to insert cur=data[k] int j = k; // ﬁnd correct index j for cur while (j > 0 && data[j−1] > cur) { // thus, data[j-1] must go after cur data[j] = data[j−1]; // slide data[j-1] rightward j−−; // and consider previous j for cur } data[j] = cur; // this is the proper place for cur } } Code Fragment 3.6: Java code for performing insertion-sort on a character array. Figure 3.5: Execution of the insertion-sort algorithm on an array of eight characters. Each row corresponds to an iteration of the outer loop, and each copy of the sequence in a row corresponds to an iteration of the inner loop. The current element that is being inserted is highlighted in the array, and shown as the cur value. insert insert insert Done! C E H G F B C A E H G F D B E H G F C D A H G F B C D E A F B C D E H A G F B C D E H E H G F D C B B E C F F G H G F D C A F B C D E H C D E H G B A D A B C D E G H A B C A G H E A D D E H G H G F E D A C B B A cur move move move no move no move no move no move move no move move move no move

Chapter 3. Fundamental Data Structures 3.1.3 java.util Methods for Arrays and Random Numbers Because arrays are so important, Java provides a class, java.util.Arrays, with a number of built-in static methods for performing common tasks on arrays. Later in this book, we will describe the algorithms that several of these methods are based upon. For now, we provide an overview of the most commonly used methods of that class, as follows (more discussion is in Section 3.5.1): equals(A, B): Returns true if and only if the array A and the array B are equal. Two arrays are considered equal if they have the same number of elements and every corresponding pair of elements in the two arrays are equal. That is, A and B have the same values in the same order. ﬁll(A, x): Stores value x in every cell of array A, provided the type of array A is deﬁned so that it is allowed to store the value x. copyOf(A, n): Returns an array of size n such that the ﬁrst k elements of this array are copied from A, where k = min{n,A.length}. If n > A.length, then the last n −A.length elements in this array will be padded with default values, e.g., 0 for an array of int and null for an array of objects. copyOfRange(A, s, t): Returns an array of size t −s such that the elements of this array are copied in order from A[s] to A[t −1], where s < t, padded as with copyOf() if t > A.length. toString(A): Returns a String representation of the array A, beginning with [, ending with ], and with elements of A displayed separated by string ", ". The string representation of an element A[i] is obtained using String.valueOf(A[i]), which returns the string "null" for a null reference and otherwise calls A[i].toString(). sort(A): Sorts the array A based on a natural ordering of its elements, which must be comparable. Sorting algorithms are the focus of Chapter 12. binarySearch(A, x): Searches the sorted array A for value x, returning the index where it is found, or else the index of where it could be inserted while maintaining the sorted order. The binary-search algorithm is described in Section 5.1.3. As static methods, these are invoked directly on the java.util.Arrays class, not on a particular instance of the class. For example, if data were an array, we could sort it with syntax, java.util.Arrays.sort(data), or with the shorter syntax Arrays.sort(data) if we ﬁrst import the Arrays class (see Section 1.8).

3.1. Using Arrays PseudoRandom Number Generation Another feature built into Java, which is often useful when testing programs dealing with arrays, is the ability to generate pseudorandom numbers, that is, numbers that appear to be random (but are not necessarily truly random). In particular, Java has a built-in class, java.util.Random, whose instances are pseudorandom number generators, that is, objects that compute a sequence of numbers that are statistically random. These sequences are not actually random, however, in that it is possible to predict the next number in the sequence given the past list of numbers. Indeed, a popular pseudorandom number generator is to generate the next number, next, from the current number, cur, according to the formula (in Java syntax): next = (a ∗cur + b) % n; where a, b, and n are appropriately chosen integers, and % is the modulus operator. Something along these lines is, in fact, the method used by java.util.Random objects, with n = 248. It turns out that such a sequence can be proven to be statistically uniform, which is usually good enough for most applications requiring random numbers, such as games. For applications, such as computer security settings, where unpredictable random sequences are needed, this kind of formula should not be used. Instead, ideally a sample from a source that is actually random should be used, such as radio static coming from outer space. Since the next number in a pseudorandom generator is determined by the previous number(s), such a generator always needs a place to start, which is called its seed. The sequence of numbers generated for a given seed will always be the same. The seed for an instance of the java.util.Random class can be set in its constructor or with its setSeed() method. One common trick to get a different sequence each time a program is run is to use a seed that will be different for each run. For example, we could use some timed input from a user or we could set the seed to the current time in milliseconds since January 1, 1970 (provided by method System.currentTimeMillis). Methods of the java.util.Random class include the following: nextBoolean(): Returns the next pseudorandom boolean value. nextDouble(): Returns the next pseudorandom double value, between 0.0 and 1.0. nextInt(): Returns the next pseudorandom int value. nextInt(n): Returns the next pseudorandom int value in the range from 0 up to but not including n. setSeed(s): Sets the seed of this pseudorandom number generator to the long s.

Chapter 3. Fundamental Data Structures An Illustrative Example We provide a short (but complete) illustrative program in Code Fragment 3.7. import java.util.Arrays; import java.util.Random; /∗∗Program showing some array uses. ∗/ public class ArrayTest { public static void main(String[ ] args) { int data[ ] = new int[10]; Random rand = new Random(); // a pseudo-random number generator rand.setSeed(System.currentTimeMillis()); // use current time as a seed // ﬁll the data array with pseudo-random numbers from 0 to 99, inclusive for (int i = 0; i < data.length; i++) data[i] = rand.nextInt(100); // the next pseudo-random number int[ ] orig = Arrays.copyOf(data, data.length); // make a copy of the data array System.out.println("arrays equal before sort: "+Arrays.equals(data, orig)); Arrays.sort(data); // sorting the data array (orig is unchanged) System.out.println("arrays equal after sort: " + Arrays.equals(data, orig)); System.out.println("orig = " + Arrays.toString(orig)); System.out.println("data = " + Arrays.toString(data)); } } Code Fragment 3.7: A simple test of some built-in methods in java.util.Arrays. We show a sample output of this program below: arrays equal before sort: true arrays equal after sort: false orig = [41, 38, 48, 12, 28, 46, 33, 19, 10, 58] data = [10, 12, 19, 28, 33, 38, 41, 46, 48, 58] In another run, we got the following output: arrays equal before sort: true arrays equal after sort: false orig = [87, 49, 70, 2, 59, 37, 63, 37, 95, 1] data = [1, 2, 37, 37, 49, 59, 63, 70, 87, 95] By using a pseudorandom number generator to determine program values, we get a different input to our program each time we run it. This feature is, in fact, what makes pseudorandom number generators useful for testing code, particularly when dealing with arrays. Even so, we should not use random test runs as a replacement for reasoning about our code, as we might miss important special cases in test runs. Note, for example, that there is a slight chance that the orig and data arrays will be equal even after data is sorted, namely, if orig is already ordered. The odds of this occurring are less than 1 in 3 million, so it’s unlikely to happen during even a few thousand test runs; however, we need to reason that this is possible.

3.1. Using Arrays 3.1.4 Simple Cryptography with Character Arrays An important application of character arrays and strings is cryptography, which is the science of secret messages. This ﬁeld involves the process of encryption, in which a message, called the plaintext, is converted into a scrambled message, called the ciphertext. Likewise, cryptography studies corresponding ways of performing decryption, turning a ciphertext back into its original plaintext. Arguably the earliest encryption scheme is the Caesar cipher, which is named after Julius Caesar, who used this scheme to protect important military messages. (All of Caesar’s messages were written in Latin, of course, which already makes them unreadable for most of us!) The Caesar cipher is a simple way to obscure a message written in a language that forms words with an alphabet. The Caesar cipher involves replacing each letter in a message with the letter that is a certain number of letters after it in the alphabet. So, in an English message, we might replace each A with D, each B with E, each C with F, and so on, if shifting by three characters. We continue this approach all the way up to W, which is replaced with Z. Then, we let the substitution pattern wrap around, so that we replace X with A, Y with B, and Z with C. Converting Between Strings and Character Arrays Given that strings are immutable, we cannot directly edit an instance to encrypt it. Instead, our goal will be to generate a new string. A convenient technique for performing string transformations is to create an equivalent array of characters, edit the array, and then reassemble a (new) string based on the array. Java has support for conversions from strings to character arrays and vice versa. Given a string S, we can create a new character array matching S by using the method, S.toCharArray(). For example, if s="bird", the method returns the character array A={'b', 'i', 'r', 'd'}. Conversely, there is a form of the String constructor that accepts a character array as a parameter. For example, with character array A={'b', 'i', 'r', 'd'}, the syntax new String(A) produces "bird". Using Character Arrays as Replacement Codes If we were to number our letters like array indices, so that A is 0, B is 1, C is 2, then we can represent the replacement rule as a character array, encoder, such that A is mapped to encoder[0], B is mapped to encoder[1], and so on. Then, in order to ﬁnd a replacement for a character in our Caesar cipher, we need to map the characters from A to Z to the respective numbers from 0 to 25. Fortunately, we can rely on the fact that characters are represented in Unicode by integer code points, and the code points for the uppercase letters of the Latin alphabet are consecutive (for simplicity, we restrict our encryption to uppercase letters).

Chapter 3. Fundamental Data Structures Java allows us to “subtract” two characters from each other, with an integer result equal to their separation distance in the encoding. Given a variable c that is known to be an uppercase letter, the Java computation, j = c −'A' produces the desired index j. As a sanity check, if character c is 'A', then j = 0. When c is 'B', the difference is 1. In general, the integer j that results from such a calculation can be used as an index into our precomputed encoder array, as illustrated in Figure 3.6. M O P Q R S T U V W X Y Z A B C D E F G H I J K L N Here is the replacement for 'T' = In Unicode Using 'T' as an index encoder array = 'T' −'A' 84 − Figure 3.6: Illustrating the use of uppercase characters as indices, in this case to perform the replacement rule for Caesar cipher encryption. The process of decrypting the message can be implemented by simply using a different character array to represent the replacement rule—one that effectively shifts characters in the opposite direction. In Code Fragment 3.8, we present a Java class that performs the Caesar cipher with an arbitrary rotational shift. The constructor for the class builds the encoder and decoder translation arrays for the given rotation. We rely heavily on modular arithmetic, as a Caesar cipher with a rotation of r encodes the letter having index k with the letter having index (k + r) mod 26, where mod is the modulo operator, which returns the remainder after performing an integer division. This operator is denoted with % in Java, and it is exactly the operator we need to easily perform the wraparound at the end of the alphabet, for 26 mod 26 is 0, 27 mod 26 is 1, and 28 mod 26 is 2. The decoder array for the Caesar cipher is just the opposite— we replace each letter with the one r places before it, with wraparound; to avoid subtleties involving negative numbers and the modulus operator, we will replace the letter having code k with the letter having code (k −r +26) mod 26. With the encoder and decoder arrays in hand, the encryption and decryption algorithms are essentially the same, and so we perform both by means of a private utility method named transform. This method converts a string to a character array, performs the translation diagrammed in Figure 3.6 for any uppercase alphabet symbols, and ﬁnally returns a new string, constructed from the updated array. The main method of the class, as a simple test, produces the following output: Encryption code = DEFGHIJKLMNOPQRSTUVWXYZABC Decryption code = XYZABCDEFGHIJKLMNOPQRSTUVW Secret: WKH HDJOH LV LQ SODB; PHHW DW MRH’V. Message: THE EAGLE IS IN PLAY; MEET AT JOE’S.

3.1. Using Arrays /∗∗Class for doing encryption and decryption using the Caesar Cipher. ∗/ public class CaesarCipher { protected char[ ] encoder = new char[26]; // Encryption array protected char[ ] decoder = new char[26]; // Decryption array /∗∗Constructor that initializes the encryption and decryption arrays ∗/ public CaesarCipher(int rotation) { for (int k=0; k < 26; k++) { encoder[k] = (char) ('A' + (k + rotation) % 26); decoder[k] = (char) ('A' + (k −rotation + 26) % 26); } } /∗∗Returns String representing encrypted message. ∗/ public String encrypt(String message) { return transform(message, encoder); // use encoder array } /∗∗Returns decrypted message given encrypted secret. ∗/ public String decrypt(String secret) { return transform(secret, decoder); // use decoder array } /∗∗Returns transformation of original String using given code. ∗/ private String transform(String original, char[ ] code) { char[ ] msg = original.toCharArray(); for (int k=0; k < msg.length; k++) if (Character.isUpperCase(msg[k])) { // we have a letter to change int j = msg[k] −'A'; // will be value from 0 to 25 msg[k] = code[j]; // replace the character } return new String(msg); } /∗∗Simple main method for testing the Caesar cipher ∗/ public static void main(String[ ] args) { CaesarCipher cipher = new CaesarCipher(3); System.out.println("Encryption code = " + new String(cipher.encoder)); System.out.println("Decryption code = " + new String(cipher.decoder)); String message = "THE EAGLE IS IN PLAY; MEET AT JOE'S."; String coded = cipher.encrypt(message); System.out.println("Secret: " + coded); String answer = cipher.decrypt(coded); System.out.println("Message: " + answer); // should be plaintext again } } Code Fragment 3.8: A complete Java class for performing the Caesar cipher.

Chapter 3. Fundamental Data Structures 3.1.5 Two-Dimensional Arrays and Positional Games Many computer games, be they strategy games, simulation games, or ﬁrst-person conﬂict games, involve objects that reside in a two-dimensional space. Software for such positional games needs a way of representing objects in a two-dimensional space. A natural way to do this is with a two-dimensional array, where we use two indices, say i and j, to refer to the cells in the array. The ﬁrst index usually refers to a row number and the second to a column number. Given such an array, we can maintain two-dimensional game boards and perform other kinds of computations involving data stored in rows and columns. Arrays in Java are one-dimensional; we use a single index to access each cell of an array. Nevertheless, there is a way we can deﬁne two-dimensional arrays in Java—we can create a two-dimensional array as an array of arrays. That is, we can deﬁne a two-dimensional array to be an array with each of its cells being another array. Such a two-dimensional array is sometimes also called a matrix. In Java, we may declare a two-dimensional array as follows: int[ ][ ] data = new int[8][10]; This statement creates a two-dimensional “array of arrays,” data, which is 8×10, having 8 rows and 10 columns. That is, data is an array of length 8 such that each element of data is an array of length 10 of integers. (See Figure 3.7.) The following would then be valid uses of array data and int variables i, j, and k: data[i][i+1] = data[i][i] + 3; j = data.length; // j is 8 k = data[4].length; // k is 10 Two-dimensional arrays have many applications to numerical analysis. Rather than going into the details of such applications, however, we explore an application of two-dimensional arrays for implementing a simple positional game. Figure 3.7: Illustration of a two-dimensional integer array, data, which has 8 rows and 10 columns. The value of data[3][5] is 100 and the value of data[6][2] is 632.

3.1. Using Arrays Tic-Tac-Toe As most school children know, Tic-Tac-Toe is a game played in a three-by-three board. Two players—X and O—alternate in placing their respective marks in the cells of this board, starting with player X. If either player succeeds in getting three of his or her marks in a row, column, or diagonal, then that player wins. This is admittedly not a sophisticated positional game, and it’s not even that much fun to play, since a good player O can always force a tie. Tic-Tac-Toe’s saving grace is that it is a nice, simple example showing how two-dimensional arrays can be used for positional games. Software for more sophisticated positional games, such as checkers, chess, or the popular simulation games, are all based on the same approach we illustrate here for using a two-dimensional array for Tic-Tac-Toe. The basic idea is to use a two-dimensional array, board, to maintain the game board. Cells in this array store values that indicate if that cell is empty or stores an X or O. That is, board is a three-by-three matrix, whose middle row consists of the cells board[1][0], board[1][1], and board[1][2]. In our case, we choose to make the cells in the board array be integers, with a 0 indicating an empty cell, a 1 indicating an X, and a −1 indicating an O. This encoding allows us to have a simple way of testing if a given board conﬁguration is a win for X or O, namely, if the values of a row, column, or diagonal add up to 3 or −3, respectively. We illustrate this approach in Figure 3.8. Figure 3.8: An illustration of a Tic-Tac-Toe board and the two-dimensional integer array, board, representing it. We give a complete Java class for maintaining a Tic-Tac-Toe board for two players in Code Fragments 3.9 and 3.10. We show a sample output in Figure 3.9. Note that this code is just for maintaining the Tic-Tac-Toe board and registering moves; it doesn’t perform any strategy or allow someone to play Tic-Tac-Toe against the computer. The details of such a program are beyond the scope of this chapter, but it might nonetheless make a good course project (see Exercise P-8.67).

Chapter 3. Fundamental Data Structures /∗∗Simulation of a Tic-Tac-Toe game (does not do strategy). ∗/ public class TicTacToe { public static ﬁnal int X = 1, O = −1; // players public static ﬁnal int EMPTY = 0; // empty cell private int board[ ][ ] = new int[3][3]; // game board private int player; // current player /∗∗Constructor ∗/ public TicTacToe() { clearBoard(); } /∗∗Clears the board ∗/ public void clearBoard() { for (int i = 0; i < 3; i++) for (int j = 0; j < 3; j++) board[i][j] = EMPTY; // every cell should be empty player = X; // the ﬁrst player is 'X' } /∗∗Puts an X or O mark at position i,j. ∗/ public void putMark(int i, int j) throws IllegalArgumentException { if ((i < 0) || (i > 2) || (j < 0) || (j > 2)) throw new IllegalArgumentException("Invalid board position"); if (board[i][j] != EMPTY) throw new IllegalArgumentException("Board position occupied"); board[i][j] = player; // place the mark for the current player player = −player; // switch players (uses fact that O = - X) } /∗∗Checks whether the board conﬁguration is a win for the given player. ∗/ public boolean isWin(int mark) { return ((board[0][0] + board[0][1] + board[0][2] == mark∗3) // row 0 || (board[1][0] + board[1][1] + board[1][2] == mark∗3) // row 1 || (board[2][0] + board[2][1] + board[2][2] == mark∗3) // row 2 || (board[0][0] + board[1][0] + board[2][0] == mark∗3) // column 0 || (board[0][1] + board[1][1] + board[2][1] == mark∗3) // column 1 || (board[0][2] + board[1][2] + board[2][2] == mark∗3) // column 2 || (board[0][0] + board[1][1] + board[2][2] == mark∗3) // diagonal || (board[2][0] + board[1][1] + board[0][2] == mark∗3)); // rev diag } /∗∗Returns the winning player's code, or 0 to indicate a tie (or unﬁnished game).∗/ public int winner() { if (isWin(X)) return(X); else if (isWin(O)) return(O); else return(0); } Code Fragment 3.9: A simple, complete Java class for playing Tic-Tac-Toe between two players. (Continues in Code Fragment 3.10.)

3.1. Using Arrays /∗∗Returns a simple character string showing the current board. ∗/ public String toString() { StringBuilder sb = new StringBuilder(); for (int i=0; i<3; i++) { for (int j=0; j<3; j++) { switch (board[i][j]) { case X: sb.append("X"); break; case O: sb.append("O"); break; case EMPTY: sb.append(" "); break; } if (j < 2) sb.append("|"); // column boundary } if (i < 2) sb.append("\n-----\n"); // row boundary } return sb.toString(); } /∗∗Test run of a simple game ∗/ public static void main(String[ ] args) { TicTacToe game = new TicTacToe(); /∗X moves: ∗/ /∗O moves: ∗/ game.putMark(1,1); game.putMark(0,2); game.putMark(2,2); game.putMark(0,0); game.putMark(0,1); game.putMark(2,1); game.putMark(1,2); game.putMark(1,0); game.putMark(2,0); System.out.println(game); int winningPlayer = game.winner(); String[ ] outcome = {"O wins", "Tie", "X wins"}; // rely on ordering System.out.println(outcome[1 + winningPlayer]); } } Code Fragment 3.10: A simple, complete Java class for playing Tic-Tac-Toe between two players. (Continued from Code Fragment 3.9.) O|X|O ----- O|X|X ----- X|O|X Tie Figure 3.9: Sample output of a Tic-Tac-Toe game.

Chapter 3. Fundamental Data Structures 3.2 Singly Linked Lists In the previous section, we presented the array data structure and discussed some of its applications. Arrays are great for storing things in a certain order, but they have drawbacks. The capacity of the array must be ﬁxed when it is created, and insertions and deletions at interior positions of an array can be time consuming if many elements must be shifted. In this section, we introduce a data structure known as a linked list, which provides an alternative to an array-based structure. A linked list, in its simplest form, is a collection of nodes that collectively form a linear sequence. In a singly linked list, each node stores a reference to an object that is an element of the sequence, as well as a reference to the next node of the list (see Figure 3.10). MSP element next Figure 3.10: Example of a node instance that forms part of a singly linked list. The node’s element ﬁeld refers to an object that is an element of the sequence (the airport code MSP, in this example), while the next ﬁeld refers to the subsequent node of the linked list (or null if there is no further node). A linked list’s representation relies on the collaboration of many objects (see Figure 3.11). Minimally, the linked list instance must keep a reference to the ﬁrst node of the list, known as the head. Without an explicit reference to the head, there would be no way to locate that node (or indirectly, any others). The last node of the list is known as the tail. The tail of a list can be found by traversing the linked list— starting at the head and moving from one node to another by following each node’s next reference. We can identify the tail as the node having null as its next reference. This process is also known as link hopping or pointer hopping. However, storing an explicit reference to the tail node is a common efﬁciency to avoid such a traversal. In similar regard, it is common for a linked list instance to keep a count of the total number of nodes that comprise the list (also known as the size of the list), to avoid traversing the list to count the nodes. LAX MSP BOS ATL head tail Figure 3.11: Example of a singly linked list whose elements are strings indicating airport codes. The list instance maintains a member named head that refers to the ﬁrst node of the list, and another member named tail that refers to the last node of the list. The null value is denoted as Ø.

3.2. Singly Linked Lists Inserting an Element at the Head of a Singly Linked List An important property of a linked list is that it does not have a predetermined ﬁxed size; it uses space proportional to its current number of elements. When using a singly linked list, we can easily insert an element at the head of the list, as shown in Figure 3.12, and described with pseudocode in Code Fragment 3.11. The main idea is that we create a new node, set its element to the new element, set its next link to refer to the current head, and set the list’s head to point to the new node. ATL BOS MSP head (a) BOS newest MSP ATL head LAX (b) LAX MSP ATL BOS head newest (c) Figure 3.12: Insertion of an element at the head of a singly linked list: (a) before the insertion; (b) after a new node is created and linked to the existing head; (c) after reassignment of the head reference to the newest node. Algorithm addFirst(e): newest = Node(e) {create new node instance storing reference to element e} newest.next = head {set new node’s next to reference the old head node} head = newest {set variable head to reference the new node} size = size+1 {increment the node count} Code Fragment 3.11: Inserting a new element at the beginning of a singly linked list. Note that we set the next pointer of the new node before we reassign variable head to it. If the list were initially empty (i.e., head is null), then a natural consequence is that the new node has its next reference set to null.

Chapter 3. Fundamental Data Structures Inserting an Element at the Tail of a Singly Linked List We can also easily insert an element at the tail of the list, provided we keep a reference to the tail node, as shown in Figure 3.13. In this case, we create a new node, assign its next reference to null, set the next reference of the tail to point to this new node, and then update the tail reference itself to this new node. We give pseudocode for the process in Code Fragment 3.12. ATL BOS MSP tail (a) MIA ATL BOS MSP tail newest (b) MSP MIA tail newest ATL BOS (c) Figure 3.13: Insertion at the tail of a singly linked list: (a) before the insertion; (b) after creation of a new node; (c) after reassignment of the tail reference. Note that we must set the next link of the tail node in (b) before we assign the tail variable to point to the new node in (c). Algorithm addLast(e): newest = Node(e) {create new node instance storing reference to element e} newest.next = null {set new node’s next to reference the null object} tail.next = newest {make old tail node point to new node} tail = newest {set variable tail to reference the new node} size = size+1 {increment the node count} Code Fragment 3.12: Inserting a new node at the end of a singly linked list. Note that we set the next pointer for the old tail node before we make variable tail point to the new node. This code would need to be adjusted for inserting onto an empty list, since there would not be an existing tail node.

3.2. Singly Linked Lists Removing an Element from a Singly Linked List Removing an element from the head of a singly linked list is essentially the reverse operation of inserting a new element at the head. This operation is illustrated in Figure 3.14 and described in detail in Code Fragment 3.13. head MSP ATL BOS LAX (a) BOS head MSP ATL LAX (b) ATL BOS MSP head (c) Figure 3.14: Removal of an element at the head of a singly linked list: (a) before the removal; (b) after “linking out” the old head; (c) ﬁnal conﬁguration. Algorithm removeFirst(): if head == null then the list is empty. head = head.next {make head point to next node (or null)} size = size−1 {decrement the node count} Code Fragment 3.13: Removing the node at the beginning of a singly linked list. Unfortunately, we cannot easily delete the last node of a singly linked list. Even if we maintain a tail reference directly to the last node of the list, we must be able to access the node before the last node in order to remove the last node. But we cannot reach the node before the tail by following next links from the tail. The only way to access this node is to start from the head of the list and search all the way through the list. But such a sequence of link-hopping operations could take a long time. If we want to support such an operation efﬁciently, we will need to make our list doubly linked (as we do in Section 3.4).

Chapter 3. Fundamental Data Structures 3.2.1 Implementing a Singly Linked List Class In this section, we present a complete implementation of a SinglyLinkedList class, supporting the following methods: size(): Returns the number of elements in the list. isEmpty(): Returns true if the list is empty, and false otherwise. ﬁrst(): Returns (but does not remove) the ﬁrst element in the list. last(): Returns (but does not remove) the last element in the list. addFirst(e): Adds a new element to the front of the list. addLast(e): Adds a new element to the end of the list. removeFirst(): Removes and returns the ﬁrst element of the list. If ﬁrst(), last(), or removeFirst() are called on a list that is empty, we will simply return a null reference and leave the list unchanged. Because it does not matter to us what type of elements are stored in the list, we use Java’s generics framework (see Section 2.5.2) to deﬁne our class with a formal type parameter E that represents the user’s desired element type. Our implementation also takes advantage of Java’s support for nested classes (see Section 2.6), as we deﬁne a private Node class within the scope of the public SinglyLinkedList class. Code Fragment 3.14 presents the Node class deﬁnition, and Code Fragment 3.15 the rest of the SinglyLinkedList class. Having Node as a nested class provides strong encapsulation, shielding users of our class from the underlying details about nodes and links. This design also allows Java to differentiate this node type from forms of nodes we may deﬁne for use in other structures. public class SinglyLinkedList<E> { //---------------- nested Node class ---------------- private static class Node<E> { private E element; // reference to the element stored at this node private Node<E> next; // reference to the subsequent node in the list public Node(E e, Node<E> n) { element = e; next = n; } public E getElement() { return element; } public Node<E> getNext() { return next; } public void setNext(Node<E> n) { next = n; } } //----------- end of nested Node class ----------- ... rest of SinglyLinkedList class will follow ... Code Fragment 3.14: A nested Node class within the SinglyLinkedList class. (The remainder of the SinglyLinkedList class will be given in Code Fragment 3.15.)

3.2. Singly Linked Lists public class SinglyLinkedList<E> { ... (nested Node class goes here) // instance variables of the SinglyLinkedList private Node<E> head = null; // head node of the list (or null if empty) private Node<E> tail = null; // last node of the list (or null if empty) private int size = 0; // number of nodes in the list public SinglyLinkedList() { } // constructs an initially empty list // access methods public int size() { return size; } public boolean isEmpty() { return size == 0; } public E ﬁrst() { // returns (but does not remove) the ﬁrst element if (isEmpty()) return null; return head.getElement(); } public E last() { // returns (but does not remove) the last element if (isEmpty()) return null; return tail.getElement(); } // update methods public void addFirst(E e) { // adds element e to the front of the list head = new Node<>(e, head); // create and link a new node if (size == 0) tail = head; // special case: new node becomes tail also size++; } public void addLast(E e) { // adds element e to the end of the list Node<E> newest = new Node<>(e, null); // node will eventually be the tail if (isEmpty()) head = newest; // special case: previously empty list else tail.setNext(newest); // new node after existing tail tail = newest; // new node becomes the tail size++; } public E removeFirst() { // removes and returns the ﬁrst element if (isEmpty()) return null; // nothing to remove E answer = head.getElement(); head = head.getNext(); // will become null if list had only one node size−−; if (size == 0) tail = null; // special case as list is now empty return answer; } } Code Fragment 3.15: The SinglyLinkedList class deﬁnition (when combined with the nested Node class of Code Fragment 3.14).

Chapter 3. Fundamental Data Structures 3.3 Circularly Linked Lists Linked lists are traditionally viewed as storing a sequence of items in a linear order, from ﬁrst to last. However, there are many applications in which data can be more naturally viewed as having a cyclic order, with well-deﬁned neighboring relationships, but no ﬁxed beginning or end. For example, many multiplayer games are turn-based, with player A taking a turn, then player B, then player C, and so on, but eventually back to player A again, and player B again, with the pattern repeating. As another example, city buses and subways often run on a continuous loop, making stops in a scheduled order, but with no designated ﬁrst or last stop per se. We next consider another important example of a cyclic order in the context of computer operating systems. 3.3.1 Round-Robin Scheduling One of the most important roles of an operating system is in managing the many processes that are currently active on a computer, including the scheduling of those processes on one or more central processing units (CPUs). In order to support the responsiveness of an arbitrary number of concurrent processes, most operating systems allow processes to effectively share use of the CPUs, using some form of an algorithm known as round-robin scheduling. A process is given a short turn to execute, known as a time slice, but it is interrupted when the slice ends, even if its job is not yet complete. Each active process is given its own time slice, taking turns in a cyclic order. New processes can be added to the system, and processes that complete their work can be removed. A round-robin scheduler could be implemented with a traditional linked list, by repeatedly performing the following steps on linked list L (see Figure 3.15): 1. process p = L.removeFirst() 2. Give a time slice to process p 3. L.addLast(p) Unfortunately, there are drawbacks to the use of a traditional linked list for this purpose. It is unnecessarily inefﬁcient to repeatedly throw away a node from one end of the list, only to create a new node for the same element when reinserting it, not to mention the various updates that are performed to decrement and increment the list’s size and to unlink and relink nodes. In the remainder of this section, we demonstrate how a slight modiﬁcation to our singly linked list implementation can be used to provide a more efﬁcient data structure for representing a cyclic order.

3.3. Circularly Linked Lists 1. Remove the next 3. Add process to end of waiting pool 2. Give current process a time slice on CPU waiting processes waiting process CPU Figure 3.15: The three iterative steps for round-robin scheduling. 3.3.2 Designing and Implementing a Circularly Linked List In this section, we design a structure known as a circularly linked list, which is essentially a singularly linked list in which the next reference of the tail node is set to refer back to the head of the list (rather than null), as shown in Figure 3.16. BOS head tail LAX MSP ATL Figure 3.16: Example of a singly linked list with circular structure. We use this model to design and implement a new CircularlyLinkedList class, which supports all of the public behaviors of our SinglyLinkedList class and one additional update method: rotate(): Moves the ﬁrst element to the end of the list. With this new operation, round-robin scheduling can be efﬁciently implemented by repeatedly performing the following steps on a circularly linked list C: 1. Give a time slice to process C.ﬁrst() 2. C.rotate() Additional Optimization In implementing a new class, we make one additional optimization—we no longer explicitly maintain the head reference. So long as we maintain a reference to the tail, we can locate the head as tail.getNext(). Maintaining only the tail reference not only saves a bit on memory usage, it makes the code simpler and more efﬁcient, as it removes the need to perform additional operations to keep a head reference current. In fact, our new implementation is arguably superior to our original singly linked list implementation, even if we are not interested in the new rotate method.

Chapter 3. Fundamental Data Structures Operations on a Circularly Linked List Implementing the new rotate method is quite trivial. We do not move any nodes or elements, we simply advance the tail reference to point to the node that follows it (the implicit head of the list). Figure 3.17 illustrates this operation using a more symmetric visualization of a circularly linked list. ATL tail (head) MSP LAX BOS ATL tail (head) MSP LAX BOS (a) (b) Figure 3.17: The rotation operation on a circularly linked list: (a) before the rotation, representing sequence { LAX, MSP, ATL, BOS }; (b) after the rotation, representing sequence { MSP, ATL, BOS, LAX }. We display the implicit head reference, which is identiﬁed only as tail.getNext() within the implementation. We can add a new element at the front of the list by creating a new node and linking it just after the tail of the list, as shown in Figure 3.18. To implement the addLast method, we can rely on the use of a call to addFirst and then immediately advance the tail reference so that the newest node becomes the last. Removing the ﬁrst node from a circularly linked list can be accomplished by simply updating the next ﬁeld of the tail node to bypass the implicit head. A Java implementation of all methods of the CircularlyLinkedList class is given in Code Fragment 3.16. BOS newest LAX ATL STL MSP tail Figure 3.18: Effect of a call to addFirst(STL) on the circularly linked list of Figure 3.17(b). The variable newest has local scope during the execution of the method. Notice that when the operation is complete, STL is the ﬁrst element of the list, as it is stored within the implicit head, tail.getNext().

3.3. Circularly Linked Lists public class CircularlyLinkedList<E> { ... (nested node class identical to that of the SinglyLinkedList class) // instance variables of the CircularlyLinkedList private Node<E> tail = null; // we store tail (but not head) private int size = 0; // number of nodes in the list public CircularlyLinkedList() { } // constructs an initially empty list // access methods public int size() { return size; } public boolean isEmpty() { return size == 0; } public E ﬁrst() { // returns (but does not remove) the ﬁrst element if (isEmpty()) return null; return tail.getNext().getElement(); // the head is *after* the tail } public E last() { // returns (but does not remove) the last element if (isEmpty()) return null; return tail.getElement(); } // update methods public void rotate() { // rotate the ﬁrst element to the back of the list if (tail != null) // if empty, do nothing tail = tail.getNext(); // the old head becomes the new tail } public void addFirst(E e) { // adds element e to the front of the list if (size == 0) { tail = new Node<>(e, null); tail.setNext(tail); // link to itself circularly } else { Node<E> newest = new Node<>(e, tail.getNext()); tail.setNext(newest); } size++; } public void addLast(E e) { // adds element e to the end of the list addFirst(e); // insert new element at front of list tail = tail.getNext(); // now new element becomes the tail } public E removeFirst() { // removes and returns the ﬁrst element if (isEmpty()) return null; // nothing to remove Node<E> head = tail.getNext(); if (head == tail) tail = null; // must be the only node left else tail.setNext(head.getNext()); // removes ”head” from the list size−−; return head.getElement(); } } Code Fragment 3.16: Implementation of the CircularlyLinkedList class.

Chapter 3. Fundamental Data Structures 3.4 Doubly Linked Lists In a singly linked list, each node maintains a reference to the node that is immediately after it. We have demonstrated the usefulness of such a representation when managing a sequence of elements. However, there are limitations that stem from the asymmetry of a singly linked list. In Section 3.2, we demonstrated that we can efﬁciently insert a node at either end of a singly linked list, and can delete a node at the head of a list, but we are unable to efﬁciently delete a node at the tail of the list. More generally, we cannot efﬁciently delete an arbitrary node from an interior position of the list if only given a reference to that node, because we cannot determine the node that immediately precedes the node to be deleted (yet, that node needs to have its next reference updated). To provide greater symmetry, we deﬁne a linked list in which each node keeps an explicit reference to the node before it and a reference to the node after it. Such a structure is known as a doubly linked list. These lists allow a greater variety of O(1)-time update operations, including insertions and deletions at arbitrary positions within the list. We continue to use the term “next” for the reference to the node that follows another, and we introduce the term “prev” for the reference to the node that precedes it. Header and Trailer Sentinels In order to avoid some special cases when operating near the boundaries of a doubly linked list, it helps to add special nodes at both ends of the list: a header node at the beginning of the list, and a trailer node at the end of the list. These “dummy” nodes are known as sentinels (or guards), and they do not store elements of the primary sequence. A doubly linked list with such sentinels is shown in Figure 3.19. SFO JFK PVD next next next prev prev prev prev header trailer next Figure 3.19: A doubly linked list representing the sequence { JFK, PVD, SFO }, using sentinels header and trailer to demarcate the ends of the list. When using sentinel nodes, an empty list is initialized so that the next ﬁeld of the header points to the trailer, and the prev ﬁeld of the trailer points to the header; the remaining ﬁelds of the sentinels are irrelevant (presumably null, in Java). For a nonempty list, the header’s next will refer to a node containing the ﬁrst real element of a sequence, just as the trailer’s prev references the node containing the last element of a sequence.

3.4. Doubly Linked Lists Advantage of Using Sentinels Although we could implement a doubly linked list without sentinel nodes (as we did with our singly linked list in Section 3.2), the slight extra memory devoted to the sentinels greatly simpliﬁes the logic of our operations. Most notably, the header and trailer nodes never change—only the nodes between them change. Furthermore, we can treat all insertions in a uniﬁed manner, because a new node will always be placed between a pair of existing nodes. In similar fashion, every element that is to be deleted is guaranteed to be stored in a node that has neighbors on each side. For contrast, we look at our SinglyLinkedList implementation from Section 3.2. Its addLast method required a conditional (lines 39–42 of Code Fragment 3.15) to manage the special case of inserting into an empty list. In the general case, the new node was linked after the existing tail. But when adding to an empty list, there is no existing tail; instead it is necessary to reassign head to reference the new node. The use of a sentinel node in that implementation would eliminate the special case, as there would always be an existing node (possibly the header) before a new node. Inserting and Deleting with a Doubly Linked List Every insertion into our doubly linked list representation will take place between a pair of existing nodes, as diagrammed in Figure 3.20. For example, when a new element is inserted at the front of the sequence, we will simply add the new node between the header and the node that is currently after the header. (See Figure 3.21.) JFK BWI SFO trailer header (a) BWI PVD SFO JFK trailer header (b) BWI PVD SFO JFK trailer header (c) Figure 3.20: Adding an element to a doubly linked list with header and trailer sentinels: (a) before the operation; (b) after creating the new node; (c) after linking the neighbors to the new node.

Chapter 3. Fundamental Data Structures JFK BWI SFO trailer header (a) PVD BWI JFK SFO trailer header (b) PVD JFK SFO BWI trailer header (c) Figure 3.21: Adding an element to the front of a sequence represented by a doubly linked list with header and trailer sentinels: (a) before the operation; (b) after creating the new node; (c) after linking the neighbors to the new node. The deletion of a node, portrayed in Figure 3.22, proceeds in the opposite fashion of an insertion. The two neighbors of the node to be deleted are linked directly to each other, thereby bypassing the original node. As a result, that node will no longer be considered part of the list and it can be reclaimed by the system. Because of our use of sentinels, the same implementation can be used when deleting the ﬁrst or the last element of a sequence, because even such an element will be stored at a node that lies between two others. BWI PVD SFO JFK trailer header (a) BWI PVD SFO JFK trailer header (b) JFK BWI SFO trailer header (c) Figure 3.22: Removing the element PVD from a doubly linked list: (a) before the removal; (b) after linking out the old node; (c) after the removal (and garbage collection).

3.4. Doubly Linked Lists 3.4.1 Implementing a Doubly Linked List Class In this section, we present a complete implementation of a DoublyLinkedList class, supporting the following public methods: size(): Returns the number of elements in the list. isEmpty(): Returns true if the list is empty, and false otherwise. ﬁrst(): Returns (but does not remove) the ﬁrst element in the list. last(): Returns (but does not remove) the last element in the list. addFirst(e): Adds a new element to the front of the list. addLast(e): Adds a new element to the end of the list. removeFirst(): Removes and returns the ﬁrst element of the list. removeLast(): Removes and returns the last element of the list. If ﬁrst(), last(), removeFirst(), or removeLast() are called on a list that is empty, we will return a null reference and leave the list unchanged. Although we have seen that it is possible to add or remove an element at an internal position of a doubly linked list, doing so requires knowledge of one or more nodes, to identify the position at which the operation should occur. In this chapter, we prefer to maintain encapsulation, with a private, nested Node class. In Chapter 7, we will revisit the use of doubly linked lists, offering a more advanced interface that supports internal insertions and deletions while maintaining encapsulation. Code Fragments 3.17 and 3.18 present the DoublyLinkedList class implementation. As we did with our SinglyLinkedList class, we use the generics framework to accept any type of element. The nested Node class for the doubly linked list is similar to that of the singly linked list, except with support for an additional prev reference to the preceding node. Our use of sentinel nodes, header and trailer, impacts the implementation in several ways. We create and link the sentinels when constructing an empty list (lines 25–29). We also keep in mind that the ﬁrst element of a nonempty list is stored in the node just after the header (not in the header itself), and similarly that the last element is stored in the node just before the trailer. The sentinels greatly ease our implementation of the various update methods. We will provide a private method, addBetween, to handle the general case of an insertion, and then we will rely on that utility as a straightforward method to implement both addFirst and addLast. In similar fashion, we will deﬁne a private remove method that can be used to easily implement both removeFirst and removeLast.

Chapter 3. Fundamental Data Structures /∗∗A basic doubly linked list implementation. ∗/ public class DoublyLinkedList<E> { //---------------- nested Node class ---------------- private static class Node<E> { private E element; // reference to the element stored at this node private Node<E> prev; // reference to the previous node in the list private Node<E> next; // reference to the subsequent node in the list public Node(E e, Node<E> p, Node<E> n) { element = e; prev = p; next = n; } public E getElement() { return element; } public Node<E> getPrev() { return prev; } public Node<E> getNext() { return next; } public void setPrev(Node<E> p) { prev = p; } public void setNext(Node<E> n) { next = n; } } //----------- end of nested Node class ----------- // instance variables of the DoublyLinkedList private Node<E> header; // header sentinel private Node<E> trailer; // trailer sentinel private int size = 0; // number of elements in the list /∗∗Constructs a new empty list. ∗/ public DoublyLinkedList() { header = new Node<>(null, null, null); // create header trailer = new Node<>(null, header, null); // trailer is preceded by header header.setNext(trailer); // header is followed by trailer } /∗∗Returns the number of elements in the linked list. ∗/ public int size() { return size; } /∗∗Tests whether the linked list is empty. ∗/ public boolean isEmpty() { return size == 0; } /∗∗Returns (but does not remove) the ﬁrst element of the list. ∗/ public E ﬁrst() { if (isEmpty()) return null; return header.getNext().getElement(); // ﬁrst element is beyond header } /∗∗Returns (but does not remove) the last element of the list. ∗/ public E last() { if (isEmpty()) return null; return trailer.getPrev().getElement(); // last element is before trailer } Code Fragment 3.17: Implementation of the DoublyLinkedList class. (Continues in Code Fragment 3.18.)

3.4. Doubly Linked Lists // public update methods /∗∗Adds element e to the front of the list. ∗/ public void addFirst(E e) { addBetween(e, header, header.getNext()); // place just after the header } /∗∗Adds element e to the end of the list. ∗/ public void addLast(E e) { addBetween(e, trailer.getPrev(), trailer); // place just before the trailer } /∗∗Removes and returns the ﬁrst element of the list. ∗/ public E removeFirst() { if (isEmpty()) return null; // nothing to remove return remove(header.getNext()); // ﬁrst element is beyond header } /∗∗Removes and returns the last element of the list. ∗/ public E removeLast() { if (isEmpty()) return null; // nothing to remove return remove(trailer.getPrev()); // last element is before trailer } // private update methods /∗∗Adds element e to the linked list in between the given nodes. ∗/ private void addBetween(E e, Node<E> predecessor, Node<E> successor) { // create and link a new node Node<E> newest = new Node<>(e, predecessor, successor); predecessor.setNext(newest); successor.setPrev(newest); size++; } /∗∗Removes the given node from the list and returns its element. ∗/ private E remove(Node<E> node) { Node<E> predecessor = node.getPrev(); Node<E> successor = node.getNext(); predecessor.setNext(successor); successor.setPrev(predecessor); size−−; return node.getElement(); } } //----------- end of DoublyLinkedList class ----------- Code Fragment 3.18: Implementation of the public and private update methods for the DoublyLinkedList class. (Continued from Code Fragment 3.17.)

Chapter 3. Fundamental Data Structures 3.5 Equivalence Testing When working with reference types, there are many different notions of what it means for one expression to be equal to another. At the lowest level, if a and b are reference variables, then expression a == b tests whether a and b refer to the same object (or if both are set to the null value). However, for many types there is a higher-level notion of two variables being considered “equivalent” even if they do not actually refer to the same instance of the class. For example, we typically want to consider two String instances to be equivalent to each other if they represent the identical sequence of characters. To support a broader notion of equivalence, all object types support a method named equals. Users of reference types should rely on the syntax a.equals(b), unless they have a speciﬁc need to test the more narrow notion of identity. The equals method is formally deﬁned in the Object class, which serves as a superclass for all reference types, but that implementation reverts to returning the value of expression a == b. Deﬁning a more meaningful notion of equivalence requires knowledge about a class and its representation. The author of each class has a responsibility to provide an implementation of the equals method, which overrides the one inherited from Object, if there is a more relevant deﬁnition for the equivalence of two instances. For example, Java’s String class redeﬁnes equals to test character-for-character equivalence. Great care must be taken when overriding the notion of equality, as the consistency of Java’s libraries depends upon the equals method deﬁning what is known as an equivalence relation in mathematics, satisfying the following properties: Treatment of null: For any nonnull reference variable x, the call x.equals(null) should return false (that is, nothing equals null except null). Reﬂexivity: For any nonnull reference variable x, the call x.equals(x) should return true (that is, an object should equal itself). Symmetry: For any nonnull reference variables x and y, the calls x.equals(y) and y.equals(x) should return the same value. Transitivity: For any nonnull reference variables x, y, and z, if both calls x.equals(y) and y.equals(z) return true, then call x.equals(z) must return true as well. While these properties may seem intuitive, it can be challenging to properly implement equals for some data structures, especially in an object-oriented context, with inheritance and generics. For most of the data structures in this book, we omit the implementation of a valid equals method (leaving it as an exercise). However, in this section, we consider the treatment of equivalence testing for both arrays and linked lists, including a concrete example of a proper implementation of the equals method for our SinglyLinkedList class.

3.5. Equivalence Testing 3.5.1 Equivalence Testing with Arrays As we mentioned in Section 1.3, arrays are a reference type in Java, but not technically a class. However, the java.util.Arrays class, introduced in Section 3.1.3, provides additional static methods that are useful when processing arrays. The following provides a summary of the treatment of equivalence for arrays, assuming that variables a and b refer to array objects: a == b: Tests if a and b refer to the same underlying array instance. a.equals(b): Interestingly, this is identical to a == b. Arrays are not a true class type and do not override the Object.equals method. Arrays.equals(a,b): This provides a more intuitive notion of equivalence, returning true if the arrays have the same length and all pairs of corresponding elements are “equal” to each other. More speciﬁcally, if the array elements are primitives, then it uses the standard == to compare values. If elements of the arrays are a reference type, then it makes pairwise comparisons a[k].equals(b[k]) in evaluating the equivalence. For most applications, the Arrays.equals behavior captures the appropriate notion of equivalence. However, there is an additional complication when using multidimensional arrays. The fact that two-dimensional arrays in Java are really one-dimensional arrays nested inside a common one-dimensional array raises an interesting issue with respect to how we think about compound objects, which are objects—like a two-dimensional array—that are made up of other objects. In particular, it brings up the question of where a compound object begins and ends. Thus, if we have a two-dimensional array, a, and another two-dimensional array, b, that has the same entries as a, we probably want to think that a is equal to b. But the one-dimensional arrays that make up the rows of a and b (such as a[0] and b[0]) are stored in different memory locations, even though they have the same internal content. Therefore, a call to the method java.util.Arrays.equals(a,b) will return false in this case, because it tests a[k].equals(b[k]), which invokes the Object class’s deﬁnition of equals. To support the more natural notion of multidimensional arrays being equal if they have equal contents, the class provides an additional method: Arrays.deepEquals(a,b): Identical to Arrays.equals(a,b) except when the elements of a and b are themselves arrays, in which case it calls Arrays.deepEquals(a[k],b[k]) for corresponding entries, rather than a[k].equals(b[k]).

Chapter 3. Fundamental Data Structures 3.5.2 Equivalence Testing with Linked Lists In this section, we develop an implementation of the equals method in the context of the SinglyLinkedList class of Section 3.2.1. Using a deﬁnition very similar to the treatment of arrays by the java.util.Arrays.equals method, we consider two lists to be equivalent if they have the same length and contents that are element-by-element equivalent. We can evaluate such equivalence by simultaneously traversing two lists, verifying that x.equals(y) for each pair of corresponding elements x and y. The implementation of the SinglyLinkedList.equals method is given in Code Fragment 3.19. Although we are focused on comparing two singly linked lists, the equals method must take an arbitrary Object as a parameter. We take a conservative approach, demanding that two objects be instances of the same class to have any possibility of equivalence. (For example, we do not consider a singly linked list to be equivalent to a doubly linked list with the same sequence of elements.) After ensuring, at line 2, that parameter o is nonnull, line 3 uses the getClass() method supported by all objects to test whether the two instances belong to the same class. When reaching line 4, we have ensured that the parameter was an instance of the SinglyLinkedList class (or an appropriate subclass), and so we can safely cast it to a SinglyLinkedList, so that we may access its instance variables size and head. There is subtlety involving the treatment of Java’s generics framework. Although our SinglyLinkedList class has a declared formal type parameter <E>, we cannot detect at runtime whether the other list has a matching type. (For those interested, look online for a discussion of erasure in Java.) So we revert to using a more classic approach with nonparameterized type SinglyLinkedList at line 4, and nonparameterized Node declarations at lines 6 and 7. If the two lists have incompatible types, this will be detected when calling the equals method on corresponding elements. public boolean equals(Object o) { if (o == null) return false; if (getClass() != o.getClass()) return false; SinglyLinkedList other = (SinglyLinkedList) o; // use nonparameterized type if (size != other.size) return false; Node walkA = head; // traverse the primary list Node walkB = other.head; // traverse the secondary list while (walkA != null) { if (!walkA.getElement().equals(walkB.getElement())) return false; //mismatch walkA = walkA.getNext(); walkB = walkB.getNext(); } return true; // if we reach this, everything matched successfully } Code Fragment 3.19: Implementation of the SinglyLinkedList.equals method.

3.6. Cloning Data Structures 3.6 Cloning Data Structures The beauty of object-oriented programming is that abstraction allows for a data structure to be treated as a single object, even though the encapsulated implementation of the structure might rely on a more complex combination of many objects. In this section, we consider what it means to make a copy of such a structure. In a programming environment, a common expectation is that a copy of an object has its own state and that, once made, the copy is independent of the original (for example, so that changes to one do not directly affect the other). However, when objects have ﬁelds that are reference variables pointing to auxiliary objects, it is not always obvious whether a copy should have a corresponding ﬁeld that refers to the same auxiliary object, or to a new copy of that auxiliary object. For example, if a hypothetical AddressBook class has instances that represent an electronic address book—with contact information (such as phone numbers and email addresses) for a person’s friends and acquaintances—how might we envision a copy of an address book? Should an entry added to one book appear in the other? If we change a person’s phone number in one book, would we expect that change to be synchronized in the other? There is no one-size-ﬁts-all answer to questions like this. Instead, each class in Java is responsible for deﬁning whether its instances can be copied, and if so, precisely how the copy is constructed. The universal Object superclass deﬁnes a method named clone, which can be used to produce what is known as a shallow copy of an object. This uses the standard assignment semantics to assign the value of each ﬁeld of the new object equal to the corresponding ﬁeld of the existing object that is being copied. The reason this is known as a shallow copy is because if the ﬁeld is a reference type, then an initialization of the form duplicate.ﬁeld = original.ﬁeld causes the ﬁeld of the new object to refer to the same underlying instance as the ﬁeld of the original object. A shallow copy is not always appropriate for all classes, and therefore, Java intentionally disables use of the clone() method by declaring it as protected, and by having it throw a CloneNotSupportedException when called. The author of a class must explicitly declare support for cloning by formally declaring that the class implements the Cloneable interface, and by declaring a public version of the clone() method. That public method can simply call the protected one to do the ﬁeld-by-ﬁeld assignment that results in a shallow copy, if appropriate. However, for many classes, the class may choose to implement a deeper version of cloning, in which some of the referenced objects are themselves cloned. For most of the data structures in this book, we omit the implementation of a valid clone method (leaving it as an exercise). However, in this section, we consider approaches for cloning both arrays and linked lists, including a concrete implementation of the clone method for the SinglyLinkedList class.

Chapter 3. Fundamental Data Structures 3.6.1 Cloning Arrays Although arrays support some special syntaxes such as a[k] and a.length, it is important to remember that they are objects, and that array variables are reference variables. This has important consequences. As a ﬁrst example, consider the following code: int[ ] data = {2, 3, 5, 7, 11, 13, 17, 19}; int[ ] backup; backup = data; // warning; not a copy The assignment of variable backup to data does not create any new array; it simply creates a new alias for the same array, as portrayed in Figure 3.23. backup data Figure 3.23: The result of the command backup = data for int arrays. Instead, if we want to make a copy of the array, data, and assign a reference to the new array to variable, backup, we should write: backup = data.clone(); The clone method, when executed on an array, initializes each cell of the new array to the value that is stored in the corresponding cell of the original array. This results in an independent array, as shown in Figure 3.24. data backup Figure 3.24: The result of the command backup = data.clone() for int arrays. If we subsequently make an assignment such as data[4] = 23 in this conﬁguration, the backup array is unaffected. There are more considerations when copying an array that stores reference types rather than primitive types. The clone() method produces a shallow copy of the array, producing a new array whose cells refer to the same objects referenced by the ﬁrst array.

3.6. Cloning Data Structures For example, if the variable contacts refers to an array of hypothetical Person instances, the result of the command guests = contacts.clone() produces a shallow copy, as portrayed in Figure 3.25. guests contacts Figure 3.25: A shallow copy of an array of objects, resulting from the command guests = contacts.clone(). A deep copy of the contact list can be created by iteratively cloning the individual elements, as follows, but only if the Person class is declared as Cloneable. Person[ ] guests = new Person[contacts.length]; for (int k=0; k < contacts.length; k++) guests[k] = (Person) contacts[k].clone(); // returns Object type Because a two-dimensional array is really a one-dimensional array storing other one-dimensional arrays, the same distinction between a shallow and deep copy exists. Unfortunately, the java.util.Arrays class does not provide any “deepClone” method. However, we can implement our own method by cloning the individual rows of an array, as shown in Code Fragment 3.20, for a two-dimensional array of integers. public static int[ ][ ] deepClone(int[ ][ ] original) { int[ ][ ] backup = new int[original.length][ ]; // create top-level array of arrays for (int k=0; k < original.length; k++) backup[k] = original[k].clone(); // copy row k return backup; } Code Fragment 3.20: A method for creating a deep copy of a two-dimensional array of integers.

Chapter 3. Fundamental Data Structures 3.6.2 Cloning Linked Lists In this section, we add support for cloning instances of the SinglyLinkedList class from Section 3.2.1. The ﬁrst step to making a class cloneable in Java is declaring that it implements the Cloneable interface. Therefore, we adjust the ﬁrst line of the class deﬁnition to appear as follows: public class SinglyLinkedList<E> implements Cloneable { The remaining task is implementing a public version of the clone() method of the class, which we present in Code Fragment 3.21. By convention, that method should begin by creating a new instance using a call to super.clone(), which in our case invokes the method from the Object class (line 3). Because the inherited version returns an Object, we perform a narrowing cast to type SinglyLinkedList<E>. At this point in the execution, the other list has been created as a shallow copy of the original. Since our list class has two ﬁelds, size and head, the following assignments have been made: other.size = this.size; other.head = this.head; While the assignment of the size variable is correct, we cannot allow the new list to share the same head value (unless it is null). For a nonempty list to have an independent state, it must have an entirely new chain of nodes, each storing a reference to the corresponding element from the original list. We therefore create a new head node at line 5 of the code, and then perform a walk through the remainder of the original list (lines 8–13) while creating and linking new nodes for the new list. public SinglyLinkedList<E> clone() throws CloneNotSupportedException { // always use inherited Object.clone() to create the initial copy SinglyLinkedList<E> other = (SinglyLinkedList<E>) super.clone(); // safe cast if (size > 0) { // we need independent chain of nodes other.head = new Node<>(head.getElement(), null); Node<E> walk = head.getNext(); // walk through remainder of original list Node<E> otherTail = other.head; // remember most recently created node while (walk != null) { // make a new node storing same element Node<E> newest = new Node<>(walk.getElement(), null); otherTail.setNext(newest); // link previous node to this one otherTail = newest; walk = walk.getNext(); } } return other; } Code Fragment 3.21: Implementation of the SinglyLinkedList.clone method.

Chapter 4. Algorithm Analysis In a classic story, the famous mathematician Archimedes was asked to determine if a golden crown commissioned by the king was indeed pure gold, and not part silver, as an informant had claimed. Archimedes discovered a way to perform this analysis while stepping into a bath. He noted that water spilled out of the bath in proportion to the amount of him that went in. Realizing the implications of this fact, he immediately got out of the bath and ran naked through the city shouting, “Eureka, eureka!” for he had discovered an analysis tool (displacement), which, when combined with a simple scale, could determine if the king’s new crown was good or not. That is, Archimedes could dip the crown and an equal-weight amount of gold into a bowl of water to see if they both displaced the same amount. This discovery was unfortunate for the goldsmith, however, for when Archimedes did his analysis, the crown displaced more water than an equal-weight lump of pure gold, indicating that the crown was not, in fact, pure gold. In this book, we are interested in the design of “good” data structures and algorithms. Simply put, a data structure is a systematic way of organizing and accessing data, and an algorithm is a step-by-step procedure for performing some task in a ﬁnite amount of time. These concepts are central to computing, but to be able to classify some data structures and algorithms as “good,” we must have precise ways of analyzing them. The primary analysis tool we will use in this book involves characterizing the running times of algorithms and data structure operations, with space usage also being of interest. Running time is a natural measure of “goodness,” since time is a precious resource—computer solutions should run as fast as possible. In general, the running time of an algorithm or data structure operation increases with the input size, although it may also vary for different inputs of the same size. Also, the running time is affected by the hardware environment (e.g., the processor, clock rate, memory, disk) and software environment (e.g., the operating system, programming language) in which the algorithm is implemented and executed. All other factors being equal, the running time of the same algorithm on the same input data will be smaller if the computer has, say, a much faster processor or if the implementation is done in a program compiled into native machine code instead of an interpreted implementation run on a virtual machine. We begin this chapter by discussing tools for performing experimental studies, yet also limitations to the use of experiments as a primary means for evaluating algorithm efﬁciency. Focusing on running time as a primary measure of goodness requires that we be able to use a few mathematical tools. In spite of the possible variations that come from different environmental factors, we would like to focus on the relationship between the running time of an algorithm and the size of its input. We are interested in characterizing an algorithm’s running time as a function of the input size. But what is the proper way of measuring it? In this chapter, we “roll up our sleeves” and develop a mathematical way of analyzing algorithms.

4.1. Experimental Studies 4.1 Experimental Studies One way to study the efﬁciency of an algorithm is to implement it and experiment by running the program on various test inputs while recording the time spent during each execution. A simple mechanism for collecting such running times in Java is based on use of the currentTimeMillis method of the System class. That method reports the number of milliseconds that have passed since a benchmark time known as the epoch (January 1, 1970 UTC). It is not that we are directly interested in the time since the epoch; the key is that if we record the time immediately before executing the algorithm and then immediately after, we can measure the elapsed time of an algorithm’s execution by computing the difference of those times. A typical way to automate this process is shown in Code Fragment 4.1. long startTime = System.currentTimeMillis(); // record the starting time /∗(run the algorithm) ∗/ long endTime = System.currentTimeMillis(); // record the ending time long elapsed = endTime −startTime; // compute the elapsed time Code Fragment 4.1: Typical approach for timing an algorithm in Java. Measuring elapsed time in this fashion provides a reasonable reﬂection of an algorithm’s efﬁciency; for extremely quick operations, Java provides a method, nanoTime, that measures in nanoseconds rather than milliseconds. Because we are interested in the general dependence of running time on the size and structure of the input, we should perform independent experiments on many different test inputs of various sizes. We can then visualize the results by plotting the performance of each run of the algorithm as a point with x-coordinate equal to the input size, n, and y-coordinate equal to the running time, t. Such a visualization provides some intuition regarding the relationship between problem size and execution time for the algorithm. This may be followed by a statistical analysis that seeks to ﬁt the best function of the input size to the experimental data. To be meaningful, this analysis requires that we choose good sample inputs and test enough of them to be able to make sound statistical claims about the algorithm’s running time. However, the measured times reported by both methods currentTimeMillis and nanoTime will vary greatly from machine to machine, and may likely vary from trial to trial, even on the same machine. This is because many processes share use of a computer’s central processing unit (or CPU) and memory system; therefore, the elapsed time will depend on what other processes are running on the computer when a test is performed. While the precise running time may not be dependable, experiments are quite useful when comparing the efﬁciency of two or more algorithms, so long as they gathered under similar circumstances.

Chapter 4. Algorithm Analysis As a tangible example of experimental analysis, we consider two algorithms for constructing long strings in Java. Our goal will be to have a method, with a calling signature such as repeat('*', 40), that produces a string composed of 40 asterisks: "****************************************". The ﬁrst algorithm we consider performs repeated string concatenation, based on the + operator. It is implemented as method repeat1 in Code Fragment 4.2. The second algorithm relies on Java’s StringBuilder class (see Section 1.3), and is implemented as method repeat2 in Code Fragment 4.2. /∗∗Uses repeated concatenation to compose a String with n copies of character c. ∗/ public static String repeat1(char c, int n) { String answer = ""; for (int j=0; j < n; j++) answer += c; return answer; } /∗∗Uses StringBuilder to compose a String with n copies of character c. ∗/ public static String repeat2(char c, int n) { StringBuilder sb = new StringBuilder(); for (int j=0; j < n; j++) sb.append(c); return sb.toString(); } Code Fragment 4.2: Two algorithms for composing a string of repeated characters. As an experiment, we used System.currentTimeMillis(), in the style of Code Fragment 4.1, to measure the efﬁciency of both repeat1 and repeat2 for very large strings. We executed trials to compose strings of increasing lengths to explore the relationship between the running time and the string length. The results of our experiments are shown in Table 4.1 and charted on a log-log scale in Figure 4.1. n repeat1 (in ms) repeat2 (in ms) 50,000 2,884 100,000 7,437 200,000 39,158 400,000 170,173 800,000 690,836 1,600,000 2,874,968 3,200,000 12,809,631 6,400,000 59,594,275 12,800,000 265,696,421 Table 4.1: Results of timing experiment on the methods from Code Fragment 4.2.

4.1. Experimental Studies n repeat1 repeat2 Running Time (ms) Figure 4.1: Chart of the results of the timing experiment from Code Fragment 4.2, displayed on a log-log scale. The divergent slopes demonstrate an order of magnitude difference in the growth of the running times. The most striking outcome of these experiments is how much faster the repeat2 algorithm is relative to repeat1. While repeat1 is already taking more than 3 days to compose a string of 12.8 million characters, repeat2 is able to do the same in a fraction of a second. We also see some interesting trends in how the running times of the algorithms each depend upon the size of n. As the value of n is doubled, the running time of repeat1 typically increases more than fourfold, while the running time of repeat2 approximately doubles. Challenges of Experimental Analysis While experimental studies of running times are valuable, especially when ﬁnetuning production-quality code, there are three major limitations to their use for algorithm analysis: • Experimental running times of two algorithms are difﬁcult to directly compare unless the experiments are performed in the same hardware and software environments. • Experiments can be done only on a limited set of test inputs; hence, they leave out the running times of inputs not included in the experiment (and these inputs may be important). • An algorithm must be fully implemented in order to execute it to study its running time experimentally. This last requirement is the most serious drawback to the use of experimental studies. At early stages of design, when considering a choice of data structures or algorithms, it would be foolish to spend a signiﬁcant amount of time implementing an approach that could easily be deemed inferior by a higher-level analysis.

Chapter 4. Algorithm Analysis 4.1.1 Moving Beyond Experimental Analysis Our goal is to develop an approach to analyzing the efﬁciency of algorithms that: 1. Allows us to evaluate the relative efﬁciency of any two algorithms in a way that is independent of the hardware and software environment. 2. Is performed by studying a high-level description of the algorithm without need for implementation. 3. Takes into account all possible inputs. Counting Primitive Operations To analyze the running time of an algorithm without performing experiments, we perform an analysis directly on a high-level description of the algorithm (either in the form of an actual code fragment, or language-independent pseudocode). We deﬁne a set of primitive operations such as the following: • Assigning a value to a variable • Following an object reference • Performing an arithmetic operation (for example, adding two numbers) • Comparing two numbers • Accessing a single element of an array by index • Calling a method • Returning from a method Formally, a primitive operation corresponds to a low-level instruction with an execution time that is constant. Ideally, this might be the type of basic operation that is executed by the hardware, although many of our primitive operations may be translated to a small number of instructions. Instead of trying to determine the speciﬁc execution time of each primitive operation, we will simply count how many primitive operations are executed, and use this number t as a measure of the running time of the algorithm. This operation count will correlate to an actual running time in a speciﬁc computer, for each primitive operation corresponds to a constant number of instructions, and there are only a ﬁxed number of primitive operations. The implicit assumption in this approach is that the running times of different primitive operations will be fairly similar. Thus, the number, t, of primitive operations an algorithm performs will be proportional to the actual running time of that algorithm. Measuring Operations as a Function of Input Size To capture the order of growth of an algorithm’s running time, we will associate, with each algorithm, a function f(n) that characterizes the number of primitive operations that are performed as a function of the input size n. Section 4.2 will introduce the seven most common functions that arise, and Section 4.3 will introduce a mathematical framework for comparing functions to each other.

4.1. Experimental Studies Focusing on the Worst-Case Input An algorithm may run faster on some inputs than it does on others of the same size. Thus, we may wish to express the running time of an algorithm as the function of the input size obtained by taking the average over all possible inputs of the same size. Unfortunately, such an average-case analysis is typically quite challenging. It requires us to deﬁne a probability distribution on the set of inputs, which is often a difﬁcult task. Figure 4.2 schematically shows how, depending on the input distribution, the running time of an algorithm can be anywhere between the worst-case time and the best-case time. For example, what if inputs are really only of types “A” or “D”? An average-case analysis usually requires that we calculate expected running times based on a given input distribution, which usually involves sophisticated probability theory. Therefore, for the remainder of this book, unless we specify otherwise, we will characterize running times in terms of the worst case, as a function of the input size, n, of the algorithm. Worst-case analysis is much easier than average-case analysis, as it requires only the ability to identify the worst-case input, which is often simple. Also, this approach typically leads to better algorithms. Making the standard of success for an algorithm to perform well in the worst case necessarily requires that it will do well on every input. That is, designing for the worst case leads to stronger algorithmic “muscles,” much like a track star who always practices by running up an incline. Running Time B C D E F G best-case time A  Input Instance 1 ms 2 ms 3 ms 4 ms 5 ms worst-case time average-case time? Figure 4.2: The difference between best-case and worst-case time. Each bar represents the running time of some algorithm on a different possible input.

Chapter 4. Algorithm Analysis 4.2 The Seven Functions Used in This Book In this section, we will brieﬂy discuss the seven most important functions used in the analysis of algorithms. We will use only these seven simple functions for almost all the analysis we do in this book. In fact, a section that uses a function other than one of these seven will be marked with a star (⋆) to indicate that it is optional. In addition to these seven fundamental functions, an appendix (available on the companion website) contains a list of other useful mathematical facts that apply in the analysis of data structures and algorithms. The Constant Function The simplest function we can think of is the constant function, that is, f(n) = c, for some ﬁxed constant c, such as c = 5, c = 27, or c = 210. That is, for any argument n, the constant function f(n) assigns the value c. In other words, it does not matter what the value of n is; f(n) will always be equal to the constant value c. Because we are most interested in integer functions, the most fundamental constant function is g(n) = 1, and this is the typical constant function we use in this book. Note that any other constant function, f(n) = c, can be written as a constant c times g(n). That is, f(n) = cg(n) in this case. As simple as it is, the constant function is useful in algorithm analysis because it characterizes the number of steps needed to do a basic operation on a computer, like adding two numbers, assigning a value to a variable, or comparing two numbers. The Logarithm Function One of the interesting and sometimes even surprising aspects of the analysis of data structures and algorithms is the ubiquitous presence of the logarithm function, f(n) = logb n, for some constant b > 1. This function is deﬁned as the inverse of a power, as follows: x = logb n if and only if bx = n. The value b is known as the base of the logarithm. Note that by the above deﬁnition, for any base b > 0, we have that logb 1 = 0. The most common base for the logarithm function in computer science is 2 as computers store integers in binary. In fact, this base is so common that we will typically omit it from the notation when it is 2. That is, for us, logn = log2 n. We note that most handheld calculators have a button marked LOG, but this is typically for calculating the logarithm base-10, not base-two.

4.2. The Seven Functions Used in This Book Computing the logarithm function exactly for any integer n involves the use of calculus, but we can use an approximation that is good enough for our purposes without calculus. We recall that the ceiling of a real number, x, is the smallest integer greater than or equal to x, denoted with ⌈x⌉. The ceiling of x can be viewed as an integer approximation of x since we have x ≤⌈x⌉< x + 1. For a positive integer, n, we repeatedly divide n by b and stop when we get a number less than or equal to 1. The number of divisions performed is equal to ⌈logb n⌉. We give below three examples of the computation of ⌈logb n⌉by repeated divisions: • ⌈log3 27⌉= 3, because ((27/3)/3)/3 = 1; • ⌈log4 64⌉= 3, because ((64/4)/4)/4 = 1; • ⌈log2 12⌉= 4, because (((12/2)/2)/2)/2 = 0.75 ≤1. The following proposition describes several important identities that involve logarithms for any base greater than 1. Proposition 4.1 (Logarithm Rules): Given real numbers a > 0, b > 1, c > 0, and d > 1, we have: 1. logb(ac) = logb a+logb c 2. logb(a/c) = logb a−logb c 3. logb(ac) = clogb a 4. logb a = logd a/logd b 5. blogd a = alogd b By convention, the unparenthesized notation lognc denotes the value log(nc). We use a notational shorthand, logcn, to denote the quantity, (logn)c, in which the result of the logarithm is raised to a power. The above identities can be derived from converse rules for exponentiation that we will present on page 161. We illustrate these identities with a few examples. Example 4.2: We demonstrate below some interesting applications of the logarithm rules from Proposition 4.1 (using the usual convention that the base of a logarithm is 2 if it is omitted). • log(2n) = log2+logn = 1+logn, by rule 1 • log(n/2) = logn−log2 = logn−1, by rule 2 • logn3 = 3logn, by rule 3 • log2n = nlog2 = n·1 = n, by rule 3 • log4 n = (logn)/log4 = (logn)/2, by rule 4 • 2logn = nlog2 = n1 = n, by rule 5. As a practical matter, we note that rule 4 gives us a way to compute the base-two logarithm on a calculator that has a base-10 logarithm button, LOG, for log2 n = LOG n/LOG 2.

Chapter 4. Algorithm Analysis The Linear Function Another simple yet important function is the linear function, f(n) = n. That is, given an input value n, the linear function f assigns the value n itself. This function arises in algorithm analysis any time we have to do a single basic operation for each of n elements. For example, comparing a number x to each element of an array of size n will require n comparisons. The linear function also represents the best running time we can hope to achieve for any algorithm that processes each of n objects that are not already in the computer’s memory, because reading in the n objects already requires n operations. The N-Log-N Function The next function we discuss in this section is the n-log-n function, f(n) = nlogn, that is, the function that assigns to an input n the value of n times the logarithm base-two of n. This function grows a little more rapidly than the linear function and a lot less rapidly than the quadratic function; therefore, we would greatly prefer an algorithm with a running time that is proportional to nlogn, than one with quadratic running time. We will see several important algorithms that exhibit a running time proportional to the n-log-n function. For example, the fastest possible algorithms for sorting n arbitrary values require time proportional to nlogn. The Quadratic Function Another function that appears often in algorithm analysis is the quadratic function, f(n) = n2. That is, given an input value n, the function f assigns the product of n with itself (in other words, “n squared”). The main reason why the quadratic function appears in the analysis of algorithms is that there are many algorithms that have nested loops, where the inner loop performs a linear number of operations and the outer loop is performed a linear number of times. Thus, in such cases, the algorithm performs n · n = n2 operations.

4.2. The Seven Functions Used in This Book Nested Loops and the Quadratic Function The quadratic function can also arise in the context of nested loops where the ﬁrst iteration of a loop uses one operation, the second uses two operations, the third uses three operations, and so on. That is, the number of operations is 1+2+3+···+(n−2)+(n−1)+n. In other words, this is the total number of operations that will be performed by the nested loop if the number of operations performed inside the loop increases by one with each iteration of the outer loop. This quantity also has an interesting history. In 1787, a German schoolteacher decided to keep his 9- and 10-year-old pupils occupied by adding up the integers from 1 to 100. But almost immediately one of the children claimed to have the answer! The teacher was suspicious, for the student had only the answer on his slate. But the answer, 5050, was correct and the student, Carl Gauss, grew up to be one of the greatest mathematicians of his time. We presume that young Gauss used the following identity. Proposition 4.3: For any integer n ≥1, we have: 1+2+3+···+(n−2)+(n−1)+n = n(n+1) . We give two “visual” justiﬁcations of Proposition 4.3 in Figure 4.3. n n ... n/2 n n+1 ... (a) (b) Figure 4.3: Visual justiﬁcations of Proposition 4.3. Both illustrations visualize the identity in terms of the total area covered by n unit-width rectangles with heights 1,2,...,n. In (a), the rectangles are shown to cover a big triangle of area n2/2 (base n and height n) plus n small triangles of area 1/2 each (base 1 and height 1). In (b), which applies only when n is even, the rectangles are shown to cover a big rectangle of base n/2 and height n+1.

Chapter 4. Algorithm Analysis The lesson to be learned from Proposition 4.3 is that if we perform an algorithm with nested loops such that the operations in the inner loop increase by one each time, then the total number of operations is quadratic in the number of times, n, we perform the outer loop. To be fair, the number of operations is n2/2 + n/2, and so this is just over half the number of operations than an algorithm that uses n operations each time the inner loop is performed. But the order of growth is still quadratic in n. The Cubic Function and Other Polynomials Continuing our discussion of functions that are powers of the input, we consider the cubic function, f(n) = n3, which assigns to an input value n the product of n with itself three times. The cubic function appears less frequently in the context of algorithm analysis than the constant, linear, and quadratic functions previously mentioned, but it does appear from time to time. Polynomials The linear, quadratic and cubic functions can each be viewed as being part of a larger class of functions, the polynomials. A polynomial function has the form, f(n) = a0 +a1n+a2n2 +a3n3 +···+adnd, where a0,a1,...,ad are constants, called the coefﬁcients of the polynomial, and ad ̸= 0. Integer d, which indicates the highest power in the polynomial, is called the degree of the polynomial. For example, the following functions are all polynomials: • f(n) = 2+5n+n2 • f(n) = 1+n3 • f(n) = 1 • f(n) = n • f(n) = n2 Therefore, we could argue that this book presents just four important functions used in algorithm analysis, but we will stick to saying that there are seven, since the constant, linear, and quadratic functions are too important to be lumped in with other polynomials. Running times that are polynomials with small degree are generally better than polynomial running times with larger degree.

4.2. The Seven Functions Used in This Book Summations A notation that appears again and again in the analysis of data structures and algorithms is the summation, which is deﬁned as follows: b ∑ i=a f(i) = f(a)+ f(a+1)+ f(a+2)+···+ f(b), where a and b are integers and a ≤b. Summations arise in data structure and algorithm analysis because the running times of loops naturally give rise to summations. Using a summation, we can rewrite the formula of Proposition 4.3 as n ∑ i=1 i = n(n+1) . Likewise, we can write a polynomial f(n) of degree d with coefﬁcients a0,...,ad as f(n) = d ∑ i=0 aini. Thus, the summation notation gives us a shorthand way of expressing sums of increasing terms that have a regular structure. The Exponential Function Another function used in the analysis of algorithms is the exponential function, f(n) = bn, where b is a positive constant, called the base, and the argument n is the exponent. That is, function f(n) assigns to the input argument n the value obtained by multiplying the base b by itself n times. As was the case with the logarithm function, the most common base for the exponential function in algorithm analysis is b = 2. For example, an integer word containing n bits can represent all the nonnegative integers less than 2n. If we have a loop that starts by performing one operation and then doubles the number of operations performed with each iteration, then the number of operations performed in the nth iteration is 2n. We sometimes have other exponents besides n, however; hence, it is useful for us to know a few handy rules for working with exponents. In particular, the following exponent rules are quite helpful. Proposition 4.4 (Exponent Rules): Given positive integers a, b, and c, we have 1. (ba)c = bac 2. babc = ba+c 3. ba/bc = ba−c

Chapter 4. Algorithm Analysis For example, we have the following: • 256 = 162 = (24)2 = 24·2 = 28 = 256 (Exponent Rule 1) • 243 = 35 = 32+3 = 3233 = 9·27 = 243 (Exponent Rule 2) • 16 = 1024/64 = 210/26 = 210−6 = 24 = 16 (Exponent Rule 3) We can extend the exponential function to exponents that are fractions or real numbers and to negative exponents, as follows. Given a positive integer k, we deﬁne b1/k to be k th root of b, that is, the number r such that rk = b. For example, 251/2 = 5, since 52 = 25. Likewise, 271/3 = 3 and 161/4 = 2. This approach allows us to deﬁne any power whose exponent can be expressed as a fraction, for ba/c = (ba)1/c, by Exponent Rule 1. For example, 93/2 = (93)1/2 = 7291/2 = 27. Thus, ba/c is really just the cth root of the integral exponent ba. We can further extend the exponential function to deﬁne bx for any real number x, by computing a series of numbers of the form ba/c for fractions a/c that get progressively closer and closer to x. Any real number x can be approximated arbitrarily closely by a fraction a/c; hence, we can use the fraction a/c as the exponent of b to get arbitrarily close to bx. For example, the number 2π is well deﬁned. Finally, given a negative exponent d, we deﬁne bd = 1/b−d, which corresponds to applying Exponent Rule 3 with a = 0 and c = −d. For example, 2−3 = 1/23 = 1/8. Geometric Sums Suppose we have a loop for which each iteration takes a multiplicative factor longer than the previous one. This loop can be analyzed using the following proposition. Proposition 4.5: For any integer n ≥0 and any real number a such that a > 0 and a ̸= 1, consider the summation n ∑ i=0 ai = 1+a+a2 +···+an (remembering that a0 = 1 if a > 0). This summation is equal to an+1 −1 a−1 . Summations as shown in Proposition 4.5 are called geometric summations, because each term is geometrically larger than the previous one if a > 1. For example, everyone working in computing should know that 1+2+4+8+···+2n−1 = 2n −1, for this is the largest unsigned integer that can be represented in binary notation using n bits.

4.2. The Seven Functions Used in This Book 4.2.1 Comparing Growth Rates To sum up, Table 4.2 shows, in order, each of the seven common functions used in algorithm analysis. constant logarithm linear n-log-n quadratic cubic exponential logn n nlogn n2 n3 an Table 4.2: Seven functions commonly used in the analysis of algorithms. We recall that logn = log2 n. Also, we denote with a a constant greater than 1. Ideally, we would like data structure operations to run in times proportional to the constant or logarithm function, and we would like our algorithms to run in linear or n-log-n time. Algorithms with quadratic or cubic running times are less practical, and algorithms with exponential running times are infeasible for all but the smallest sized inputs. Plots of the seven functions are shown in Figure 4.4. f(n) n Linear Exponential Constant Logarithmic N-Log-N Quadratic Cubic Figure 4.4: Growth rates for the seven fundamental functions used in algorithm analysis. We use base a = 2 for the exponential function. The functions are plotted on a log-log chart to compare the growth rates primarily as slopes. Even so, the exponential function grows too fast to display all its values on the chart. The Ceiling and Floor Functions When discussing logarithms, we noted that the value is generally not an integer, yet the running time of an algorithm is usually expressed by means of an integer quantity, such as the number of operations performed. Thus, the analysis of an algorithm may sometimes involve the use of the ﬂoor function and ceiling function, which are deﬁned respectively as follows: • ⌊x⌋= the largest integer less than or equal to x. (e.g., ⌊3.7⌋= 3.) • ⌈x⌉= the smallest integer greater than or equal to x. (e.g., ⌈5.2⌉= 6.)

Chapter 4. Algorithm Analysis 4.3 Asymptotic Analysis In algorithm analysis, we focus on the growth rate of the running time as a function of the input size n, taking a “big-picture” approach. For example, it is often enough just to know that the running time of an algorithm grows proportionally to n. We analyze algorithms using a mathematical notation for functions that disregards constant factors. Namely, we characterize the running times of algorithms by using functions that map the size of the input, n, to values that correspond to the main factor that determines the growth rate in terms of n. This approach reﬂects that each basic step in a pseudocode description or a high-level language implementation may correspond to a small number of primitive operations. Thus, we can perform an analysis of an algorithm by estimating the number of primitive operations executed up to a constant factor, rather than getting bogged down in language-speciﬁc or hardware-speciﬁc analysis of the exact number of operations that execute on the computer. 4.3.1 The “Big-Oh” Notation Let f(n) and g(n) be functions mapping positive integers to positive real numbers. We say that f(n) is O(g(n)) if there is a real constant c > 0 and an integer constant n0 ≥1 such that f(n) ≤c·g(n), for n ≥n0. This deﬁnition is often referred to as the “big-Oh” notation, for it is sometimes pronounced as “ f(n) is big-Oh of g(n).” Figure 4.5 illustrates the general deﬁnition. Input Size Running Time cg(n) f(n) n0 Figure 4.5: Illustrating the “big-Oh” notation. The function f(n) is O(g(n)), since f(n) ≤c·g(n) when n ≥n0.

4.3. Asymptotic Analysis Example 4.6: The function 8n+5 is O(n). Justiﬁcation: By the big-Oh deﬁnition, we need to ﬁnd a real constant c > 0 and an integer constant n0 ≥1 such that 8n+5 ≤cn for every integer n ≥n0. It is easy to see that a possible choice is c = 9 and n0 = 5. Indeed, this is one of inﬁnitely many choices available because there is a trade-off between c and n0. For example, we could rely on constants c = 13 and n0 = 1. The big-Oh notation allows us to say that a function f(n) is “less than or equal to” another function g(n) up to a constant factor and in the asymptotic sense as n grows toward inﬁnity. This ability comes from the fact that the deﬁnition uses “≤” to compare f(n) to a g(n) times a constant, c, for the asymptotic cases when n ≥n0. However, it is considered poor taste to say “f(n) ≤O(g(n)),” since the big-Oh already denotes the “less-than-or-equal-to” concept. Likewise, although common, it is not fully correct to say “ f(n) = O(g(n)),” with the usual understanding of the “=” relation, because there is no way to make sense of the symmetric statement, “O(g(n)) = f(n).” It is best to say, “ f(n) is O(g(n)).” Alternatively, we can say “ f(n) is order of g(n).” For the more mathematically inclined, it is also correct to say, “ f(n) ∈O(g(n)),” for the big-Oh notation, technically speaking, denotes a whole collection of functions. In this book, we will stick to presenting big-Oh statements as “ f(n) is O(g(n)).” Even with this interpretation, there is considerable freedom in how we can use arithmetic operations with the bigOh notation, and with this freedom comes a certain amount of responsibility. Some Properties of the Big-Oh Notation The big-Oh notation allows us to ignore constant factors and lower-order terms and focus on the main components of a function that affect its growth. Example 4.7: 5n4 +3n3 +2n2 +4n+1 is O(n4). Justiﬁcation: Note that 5n4 +3n3 +2n2 +4n+1 ≤(5+3+2+4+1)n4 = cn4, for c = 15, when n ≥n0 = 1. In fact, we can characterize the growth rate of any polynomial function. Proposition 4.8: If f(n) is a polynomial of degree d, that is, f(n) = a0 +a1n+···+adnd, and ad > 0, then f(n) is O(nd). Justiﬁcation: Note that, for n ≥1, we have 1 ≤n ≤n2 ≤··· ≤nd; hence, a0 +a1n+a2n2 +···+adnd ≤(|a0|+|a1|+|a2|+···+|ad|)nd. We show that f(n) is O(nd) by deﬁning c = |a0|+|a1|+···+|ad| and n0 = 1.

Chapter 4. Algorithm Analysis Thus, the highest-degree term in a polynomial is the term that determines the asymptotic growth rate of that polynomial. We consider some additional properties of the big-Oh notation in the exercises. Let us consider some further examples here, focusing on combinations of the seven fundamental functions used in algorithm design. We rely on the mathematical fact that logn ≤n for n ≥1. Example 4.9: 5n2 +3nlogn+2n+5 is O(n2). Justiﬁcation: 5n2 +3nlogn+2n+5 ≤(5+3+2+5)n2 = cn2, for c = 15, when n ≥n0 = 1. Example 4.10: 20n3 +10nlogn+5 is O(n3). Justiﬁcation: 20n3 +10nlogn+5 ≤35n3, for n ≥1. Example 4.11: 3logn+2 is O(logn). Justiﬁcation: 3logn+ 2 ≤5logn, for n ≥2. Note that logn is zero for n = 1. That is why we use n ≥n0 = 2 in this case. Example 4.12: 2n+2 is O(2n). Justiﬁcation: 2n+2 = 2n ·22 = 4·2n; hence, we can take c = 4 and n0 = 1 in this case. Example 4.13: 2n+100logn is O(n). Justiﬁcation: 2n+100logn ≤102n, for n ≥n0 = 1; hence, we can take c = 102 in this case. Characterizing Functions in Simplest Terms In general, we should use the big-Oh notation to characterize a function as closely as possible. While it is true that the function f(n) = 4n3 + 3n2 is O(n5) or even O(n4), it is more accurate to say that f(n) is O(n3). Consider, by way of analogy, a scenario where a hungry traveler driving along a long country road happens upon a local farmer walking home from a market. If the traveler asks the farmer how much longer he must drive before he can ﬁnd some food, it may be truthful for the farmer to say, “certainly no longer than 12 hours,” but it is much more accurate (and helpful) for him to say, “you can ﬁnd a market just a few minutes drive up this road.” Thus, even with the big-Oh notation, we should strive as much as possible to tell the whole truth. It is also considered poor taste to include constant factors and lower-order terms in the big-Oh notation. For example, it is not fashionable to say that the function 2n2 is O(4n2 + 6nlogn), although this is completely correct. We should strive instead to describe the function in the big-Oh in simplest terms.

4.3. Asymptotic Analysis The seven functions listed in Section 4.2 are the most common functions used in conjunction with the big-Oh notation to characterize the running times and space usage of algorithms. Indeed, we typically use the names of these functions to refer to the running times of the algorithms they characterize. So, for example, we would say that an algorithm that runs in worst-case time 4n2 +nlogn is a quadratic-time algorithm, since it runs in O(n2) time. Likewise, an algorithm running in time at most 5n+20logn+4 would be called a linear-time algorithm. Big-Omega Just as the big-Oh notation provides an asymptotic way of saying that a function is “less than or equal to” another function, the following notations provide an asymptotic way of saying that a function grows at a rate that is “greater than or equal to” that of another. Let f(n) and g(n) be functions mapping positive integers to positive real numbers. We say that f(n) is Ω(g(n)), pronounced “f(n) is big-Omega of g(n),” if g(n) is O( f(n)), that is, there is a real constant c > 0 and an integer constant n0 ≥1 such that f(n) ≥cg(n), for n ≥n0. This deﬁnition allows us to say asymptotically that one function is greater than or equal to another, up to a constant factor. Example 4.14: 3nlogn−2n is Ω(nlogn). Justiﬁcation: 3nlogn−2n = nlogn+ 2n(logn−1) ≥nlogn for n ≥2; hence, we can take c = 1 and n0 = 2 in this case. Big-Theta In addition, there is a notation that allows us to say that two functions grow at the same rate, up to constant factors. We say that f(n) is Θ(g(n)), pronounced “f(n) is big-Theta of g(n),” if f(n) is O(g(n)) and f(n) is Ω(g(n)), that is, there are real constants c′ > 0 and c′′ > 0, and an integer constant n0 ≥1 such that c′g(n) ≤f(n) ≤c′′g(n), for n ≥n0. Example 4.15: 3nlogn+4n+5logn is Θ(nlogn). Justiﬁcation: 3nlogn ≤3nlogn+4n+5logn ≤(3+4+5)nlogn for n ≥2.

Chapter 4. Algorithm Analysis 4.3.2 Comparative Analysis The big-Oh notation is widely used to characterize running times and space bounds in terms of some parameter n, which is deﬁned as a chosen measure of the “size” of the problem. Suppose two algorithms solving the same problem are available: an algorithm A, which has a running time of O(n), and an algorithm B, which has a running time of O(n2). Which algorithm is better? We know that n is O(n2), which implies that algorithm A is asymptotically better than algorithm B, although for a small value of n, B may have a lower running time than A. We can use the big-Oh notation to order classes of functions by asymptotic growth rate. Our seven functions are ordered by increasing growth rate in the following sequence, such that f(n) is O(g(n)) if function f(n) precedes function g(n): 1, logn, n, nlogn, n2, n3, 2n. We illustrate the growth rates of the seven functions in Table 4.3. (See also Figure 4.4 from Section 4.2.1.) n logn n nlogn n2 n3 2n 4,096 65,536 1,024 32,768 4,294,967,296 4,096 262,144 1.84× 1019 16,384 2,097,152 3.40× 1038 2,048 65,536 16,777,216 1.15× 1077 4,608 262,144 134,217,728 1.34× 10154 Table 4.3: Selected values of fundamental functions in algorithm analysis. We further illustrate the importance of the asymptotic viewpoint in Table 4.4. This table explores the maximum size allowed for an input instance that is processed by an algorithm in 1 second, 1 minute, and 1 hour. It shows the importance of good algorithm design, because an asymptotically slow algorithm is beaten in the long run by an asymptotically faster algorithm, even if the constant factor for the asymptotically faster algorithm is worse. Running Maximum Problem Size (n) Time (µs) 1 second 1 minute 1 hour 400n 2,500 150,000 9,000,000 2n2 5,477 42,426 2n Table 4.4: Maximum size of a problem that can be solved in 1 second, 1 minute, and 1 hour, for various running times measured in microseconds.

4.3. Asymptotic Analysis The importance of good algorithm design goes beyond just what can be solved effectively on a given computer, however. As shown in Table 4.5, even if we achieve a dramatic speedup in hardware, we still cannot overcome the handicap of an asymptotically slow algorithm. This table shows the new maximum problem size achievable for any ﬁxed amount of time, assuming algorithms with the given running times are now run on a computer 256 times faster than the previous one. Running Time New Maximum Problem Size 400n 256m 2n2 16m 2n m+8 Table 4.5: Increase in the maximum size of a problem that can be solved in a ﬁxed amount of time, by using a computer that is 256 times faster than the previous one. Each entry is a function of m, the previous maximum problem size. Some Words of Caution A few words of caution about asymptotic notation are in order at this point. First, note that the use of the big-Oh and related notations can be somewhat misleading should the constant factors they “hide” be very large. For example, while it is true that the function 10100n is O(n), if this is the running time of an algorithm being compared to one whose running time is 10nlogn, we should prefer the O(nlogn)- time algorithm, even though the linear-time algorithm is asymptotically faster. This preference is because the constant factor, 10100, which is called “one googol,” is believed by many astronomers to be an upper bound on the number of atoms in the observable universe. So we are unlikely to ever have a real-world problem that has this number as its input size. The observation above raises the issue of what constitutes a “fast” algorithm. Generally speaking, any algorithm running in O(nlogn) time (with a reasonable constant factor) should be considered efﬁcient. Even an O(n2)-time function may be fast enough in some contexts, that is, when n is small. But an algorithm whose running time is an exponential function, e.g., O(2n), should almost never be considered efﬁcient. Exponential Running Times To see how fast the function 2n grows, consider the famous story about the inventor of the game of chess. He asked only that his king pay him 1 grain of rice for the ﬁrst square on the board, 2 grains for the second, 4 grains for the third, 8 for the fourth, and so on. The number of grains in the 64th square would be 263 = 9,223,372,036,854,775,808, which is about nine billion billions!

Chapter 4. Algorithm Analysis If we must draw a line between efﬁcient and inefﬁcient algorithms, therefore, it is natural to make this distinction be that between those algorithms running in polynomial time and those running in exponential time. That is, make the distinction between algorithms with a running time that is O(nc), for some constant c > 1, and those with a running time that is O(bn), for some constant b > 1. Like so many notions we have discussed in this section, this too should be taken with a “grain of salt,” for an algorithm running in O(n100) time should probably not be considered “efﬁcient.” Even so, the distinction between polynomial-time and exponential-time algorithms is considered a robust measure of tractability. 4.3.3 Examples of Algorithm Analysis Now that we have the big-Oh notation for doing algorithm analysis, let us give some examples by characterizing the running time of some simple algorithms using this notation. Moreover, in keeping with our earlier promise, we will illustrate below how each of the seven functions given earlier in this chapter can be used to characterize the running time of an example algorithm. Constant-Time Operations All of the primitive operations, originally described on page 154, are assumed to run in constant time; formally, we say they run in O(1) time. We wish to emphasize several important constant-time operations that involve arrays. Assume that variable A is an array of n elements. The expression A.length in Java is evaluated in constant time, because arrays are represented internally with an explicit variable that records the length of the array. Another central behavior of arrays is that for any valid index j, the individual element, A[ j], can be accessed in constant time. This is because an array uses a consecutive block of memory. The j th element can be found, not by iterating through the array one element at a time, but by validating the index, and using it as an offset from the beginning of the array in determining the appropriate memory address. Therefore, we say that the expression A[ j] is evaluated in O(1) time for an array. Finding the Maximum of an Array As a classic example of an algorithm with a running time that grows proportional to n, we consider the goal of ﬁnding the largest element of an array. A typical strategy is to loop through elements of the array while maintaining as a variable the largest element seen thus far. Code Fragment 4.3 presents a method named arrayMax implementing this strategy.

4.3. Asymptotic Analysis /∗∗Returns the maximum value of a nonempty array of numbers. ∗/ public static double arrayMax(double[ ] data) { int n = data.length; double currentMax = data[0]; // assume ﬁrst entry is biggest (for now) for (int j=1; j < n; j++) // consider all other entries if (data[j] > currentMax) // if data[j] is biggest thus far... currentMax = data[j]; // record it as the current max return currentMax; } Code Fragment 4.3: A method that returns the maximum value of an array. Using the big-Oh notation, we can write the following mathematically precise statement on the running time of algorithm arrayMax for any computer. Proposition 4.16: The algorithm, arrayMax, for computing the maximum element of an array of n numbers, runs in O(n) time. Justiﬁcation: The initialization at lines 3 and 4 and the return statement at line 8 require only a constant number of primitive operations. Each iteration of the loop also requires only a constant number of primitive operations, and the loop executes n −1 times. Therefore, we account for the number of primitive operations being c′·(n−1)+c′′ for appropriate constants c′ and c′′ that reﬂect, respectively, the work performed inside and outside the loop body. Because each primitive operation runs in constant time, we have that the running time of algorithm arrayMax on an input of size n is at most c′ ·(n−1)+c′′ = c′ ·n+(c′′ −c′) ≤c′ ·n if we assume, without loss of generality, that c′′ ≤c′. We conclude that the running time of algorithm arrayMax is O(n). Further Analysis of the Maximum-Finding Algorithm A more interesting question about arrayMax is how many times we might update the current “biggest” value. In the worst case, if the data is given to us in increasing order, the biggest value is reassigned n −1 times. But what if the input is given to us in random order, with all orders equally likely; what would be the expected number of times we update the biggest value in this case? To answer this question, note that we update the current biggest in an iteration of the loop only if the current element is bigger than all the elements that precede it. If the sequence is given to us in random order, the probability that the j th element is the largest of the ﬁrst j elements is 1/j (assuming uniqueness). Hence, the expected number of times we update the biggest (including initialization) is Hn = ∑n j=1 1/j, which is known as the nth Harmonic number. It can be shown that Hn is O(logn). Therefore, the expected number of times the biggest value is updated by arrayMax on a randomly ordered sequence is O(logn).

Chapter 4. Algorithm Analysis Composing Long Strings As our next example, we revisit the experimental study from Section 4.1, in which we examined two different implementations for composing a long string (see Code Fragment 4.2). Our ﬁrst algorithm was based on repeated use of the string concatenation operator; for convenience, that method is also given in Code Fragment 4.4. /∗∗Uses repeated concatenation to compose a String with n copies of character c. ∗/ public static String repeat1(char c, int n) { String answer = ""; for (int j=0; j < n; j++) answer += c; return answer; } Code Fragment 4.4: Composing a string using repeated concatenation. The most important aspect of this implementation is that strings in Java are immutable objects. Once created, an instance cannot be modiﬁed. The command, answer += c, is shorthand for answer = (answer + c). This command does not cause a new character to be added to the existing String instance; instead it produces a new String with the desired sequence of characters, and then it reassigns the variable, answer, to refer to that new string. In terms of efﬁciency, the problem with this interpretation is that the creation of a new string as a result of a concatenation, requires time that is proportional to the length of the resulting string. The ﬁrst time through this loop, the result has length 1, the second time through the loop the result has length 2, and so on, until we reach the ﬁnal string of length n. Therefore, the overall time taken by this algorithm is proportional to 1+2+···+n, which we recognize as the familiar O(n2) summation from Proposition 4.3. Therefore, the total time complexity of the repeat1 algorithm is O(n2). We see this theoretical analysis reﬂected in the experimental results. The running time of a quadratic algorithm should theoretically quadruple if the size of the problem doubles, as (2n)2 = 4· n2. (We say “theoretically,” because this does not account for lower-order terms that are hidden by the asymptotic notation.) We see such an approximate fourfold increase in the running time of repeat1 in Table 4.1 on page 152. In contrast, the running times in that table for the repeat2 algorithm, which uses Java’s StringBuilder class, demonstrate a trend of approximately doubling each time the problem size doubles. The StringBuilder class relies on an advanced technique with a worst-case running time of O(n) for composing a string of length n; we will later explore that technique as the focus of Section 7.2.1.

4.3. Asymptotic Analysis Three-Way Set Disjointness Suppose we are given three sets, A, B, and C, stored in three different integer arrays. We will assume that no individual set contains duplicate values, but that there may be some numbers that are in two or three of the sets. The three-way set disjointness problem is to determine if the intersection of the three sets is empty, namely, that there is no element x such that x ∈A, x ∈B, and x ∈C. A simple Java method to determine this property is given in Code Fragment 4.5. /∗∗Returns true if there is no element common to all three arrays. ∗/ public static boolean disjoint1(int[ ] groupA, int[ ] groupB, int[ ] groupC) { for (int a : groupA) for (int b : groupB) for (int c : groupC) if ((a == b) && (b == c)) return false; // we found a common value return true; // if we reach this, sets are disjoint } Code Fragment 4.5: Algorithm disjoint1 for testing three-way set disjointness. This simple algorithm loops through each possible triple of values from the three sets to see if those values are equivalent. If each of the original sets has size n, then the worst-case running time of this method is O(n3). We can improve upon the asymptotic performance with a simple observation. Once inside the body of the loop over B, if selected elements a and b do not match each other, it is a waste of time to iterate through all values of C looking for a matching triple. An improved solution to this problem, taking advantage of this observation, is presented in Code Fragment 4.6. /∗∗Returns true if there is no element common to all three arrays. ∗/ public static boolean disjoint2(int[ ] groupA, int[ ] groupB, int[ ] groupC) { for (int a : groupA) for (int b : groupB) if (a == b) // only check C when we ﬁnd match from A and B for (int c : groupC) if (a == c) // and thus b == c as well return false; // we found a common value return true; // if we reach this, sets are disjoint } Code Fragment 4.6: Algorithm disjoint2 for testing three-way set disjointness. In the improved version, it is not simply that we save time if we get lucky. We claim that the worst-case running time for disjoint2 is O(n2). There are quadratically many pairs (a,b) to consider. However, if A and B are each sets of distinct

Chapter 4. Algorithm Analysis elements, there can be at most O(n) such pairs with a equal to b. Therefore, the innermost loop, over C, executes at most n times. To account for the overall running time, we examine the time spent executing each line of code. The management of the for loop over A requires O(n) time. The management of the for loop over B accounts for a total of O(n2) time, since that loop is executed n different times. The test a == b is evaluated O(n2) times. The rest of the time spent depends upon how many matching (a,b) pairs exist. As we have noted, there are at most n such pairs; therefore, the management of the loop over C and the commands within the body of that loop use at most O(n2) time. By our standard application of Proposition 4.8, the total time spent is O(n2). Element Uniqueness A problem that is closely related to the three-way set disjointness problem is the element uniqueness problem. In the former, we are given three sets and we presumed that there were no duplicates within a single set. In the element uniqueness problem, we are given an array with n elements and asked whether all elements of that collection are distinct from each other. Our ﬁrst solution to this problem uses a straightforward iterative algorithm. The unique1 method, given in Code Fragment 4.7, solves the element uniqueness problem by looping through all distinct pairs of indices j < k, checking if any of those pairs refer to elements that are equivalent to each other. It does this using two nested for loops, such that the ﬁrst iteration of the outer loop causes n−1 iterations of the inner loop, the second iteration of the outer loop causes n −2 iterations of the inner loop, and so on. Thus, the worst-case running time of this method is proportional to (n−1)+(n−2)+···+2+1, which we recognize as the familiar O(n2) summation from Proposition 4.3. /∗∗Returns true if there are no duplicate elements in the array. ∗/ public static boolean unique1(int[ ] data) { int n = data.length; for (int j=0; j < n−1; j++) for (int k=j+1; k < n; k++) if (data[j] == data[k]) return false; // found duplicate pair return true; // if we reach this, elements are unique } Code Fragment 4.7: Algorithm unique1 for testing element uniqueness.

4.3. Asymptotic Analysis Using Sorting as a Problem-Solving Tool An even better algorithm for the element uniqueness problem is based on using sorting as a problem-solving tool. In this case, by sorting the array of elements, we are guaranteed that any duplicate elements will be placed next to each other. Thus, to determine if there are any duplicates, all we need to do is perform a single pass over the sorted array, looking for consecutive duplicates. A Java implementation of this algorithm is given in Code Fragment 4.8. (See Section 3.1.3 for discussion of the java.util.Arrays class.) /∗∗Returns true if there are no duplicate elements in the array. ∗/ public static boolean unique2(int[ ] data) { int n = data.length; int[ ] temp = Arrays.copyOf(data, n); // make copy of data Arrays.sort(temp); // and sort the copy for (int j=0; j < n−1; j++) if (temp[j] == temp[j+1]) // check neighboring entries return false; // found duplicate pair return true; // if we reach this, elements are unique } Code Fragment 4.8: Algorithm unique2 for testing element uniqueness. Sorting algorithms will be the focus of Chapter 12. The best sorting algorithms (including those used by Array.sort in Java) guarantee a worst-case running time of O(nlogn). Once the data is sorted, the subsequent loop runs in O(n) time, and so the entire unique2 algorithm runs in O(nlogn) time. Exercise C-4.35 explores the use of sorting to solve the three-way set disjointness problem in O(nlogn) time. Preﬁx Averages The next problem we consider is computing what are known as preﬁx averages of a sequence of numbers. Namely, given a sequence x consisting of n numbers, we want to compute a sequence a such that aj is the average of elements x0,...,xj, for j = 0,...,n−1, that is, aj = ∑j i=0 xi j +1 . Preﬁx averages have many applications in economics and statistics. For example, given the year-by-year returns of a mutual fund, ordered from recent to past, an investor will typically want to see the fund’s average annual returns for the most recent year, the most recent three years, the most recent ﬁve years, and so on. Likewise, given a stream of daily Web usage logs, a website manager may wish to track average usage trends over various time periods. We present two implementation for computing preﬁx averages, yet with signiﬁcantly different running times.

Chapter 4. Algorithm Analysis A Quadratic-Time Algorithm Our ﬁrst algorithm for computing preﬁx averages, denoted as preﬁxAverage1, is shown in Code Fragment 4.9. It computes each element aj independently, using an inner loop to compute that partial sum. /∗∗Returns an array a such that, for all j, a[j] equals the average of x[0], ..., x[j]. ∗/ public static double[ ] preﬁxAverage1(double[ ] x) { int n = x.length; double[ ] a = new double[n]; // ﬁlled with zeros by default for (int j=0; j < n; j++) { double total = 0; // begin computing x[0] + ... + x[j] for (int i=0; i <= j; i++) total += x[i]; a[j] = total / (j+1); // record the average } return a; } Code Fragment 4.9: Algorithm preﬁxAverage1. Let us analyze the preﬁxAverage1 algorithm. • The initialization of n = x.length at line 3 and the eventual return of a reference to array a at line 11 both execute in O(1) time. • Creating and initializing the new array, a, at line 4 can be done with in O(n) time, using a constant number of primitive operations per element. • There are two nested for loops, which are controlled, respectively, by counters j and i. The body of the outer loop, controlled by counter j, is executed n times, for j = 0,...,n −1. Therefore, statements total = 0 and a[j] = total / (j+1) are executed n times each. This implies that these two statements, plus the management of counter j in the loop, contribute a number of primitive operations proportional to n, that is, O(n) time. • The body of the inner loop, which is controlled by counter i, is executed j+1 times, depending on the current value of the outer loop counter j. Thus, statement total += x[i], in the inner loop, is executed 1+ 2+ 3+ ··· + n times. By recalling Proposition 4.3, we know that 1+2+3+···+n = n(n+1)/2, which implies that the statement in the inner loop contributes O(n2) time. A similar argument can be done for the primitive operations associated with maintaining counter i, which also take O(n2) time. The running time of implementation preﬁxAverage1 is given by the sum of these terms. The ﬁrst term is O(1), the second and third terms are O(n), and the fourth term is O(n2). By a simple application of Proposition 4.8, the running time of preﬁxAverage1 is O(n2).

4.3. Asymptotic Analysis A Linear-Time Algorithm An intermediate value in the computation of the preﬁx average is the preﬁx sum x0 + x1 + ··· + xj, denoted as total in our ﬁrst implementation; this allows us to compute the preﬁx average a[j] = total / (j + 1). In our ﬁrst algorithm, the preﬁx sum is computed anew for each value of j. That contributed O( j) time for each j, leading to the quadratic behavior. For greater efﬁciency, we can maintain the current preﬁx sum dynamically, effectively computing x0 +x1 +··· +xj as total + xj, where value total is equal to the sum x0+x1+···+xj−1, when computed by the previous pass of the loop over j. Code Fragment 4.10 provides a new implementation, denoted as preﬁxAverage2, using this approach. /∗∗Returns an array a such that, for all j, a[j] equals the average of x[0], ..., x[j]. ∗/ public static double[ ] preﬁxAverage2(double[ ] x) { int n = x.length; double[ ] a = new double[n]; // ﬁlled with zeros by default double total = 0; // compute preﬁx sum as x[0] + x[1] + ... for (int j=0; j < n; j++) { total += x[j]; // update preﬁx sum to include x[j] a[j] = total / (j+1); // compute average based on current sum } return a; } Code Fragment 4.10: Algorithm preﬁxAverage2. The analysis of the running time of algorithm preﬁxAverage2 follows: • Initializing variables n and total uses O(1) time. • Initializing the array a uses O(n) time. • There is a single for loop, which is controlled by counter j. The maintenance of that loop contributes a total of O(n) time. • The body of the loop is executed n times, for j = 0,...,n −1. Thus, statements total += x[j] and a[j] = total / (j+1) are executed n times each. Since each of these statements uses O(1) time per iteration, their overall contribution is O(n) time. • The eventual return of a reference to array A uses O(1) time. The running time of algorithm preﬁxAverage2 is given by the sum of the ﬁve terms. The ﬁrst and last are O(1) and the remaining three are O(n). By a simple application of Proposition 4.8, the running time of preﬁxAverage2 is O(n), which is much better than the quadratic time of algorithm preﬁxAverage1.

Chapter 4. Algorithm Analysis 4.4 Simple Justiﬁcation Techniques Sometimes, we will want to make claims about an algorithm, such as showing that it is correct or that it runs fast. In order to rigorously make such claims, we must use mathematical language, and in order to back up such claims, we must justify or prove our statements. Fortunately, there are several simple ways to do this. 4.4.1 By Example Some claims are of the generic form, “There is an element x in a set S that has property P.” To justify such a claim, we only need to produce a particular x in S that has property P. Likewise, some hard-to-believe claims are of the generic form, “Every element x in a set S has property P.” To justify that such a claim is false, we only need to produce a particular x from S that does not have property P. Such an instance is called a counterexample. Example 4.17: Professor Amongus claims that every number of the form 2i −1 is a prime, when i is an integer greater than 1. Professor Amongus is wrong. Justiﬁcation: To prove Professor Amongus is wrong, we ﬁnd a counterexample. Fortunately, we need not look too far, for 24 −1 = 15 = 3·5. 4.4.2 The “Contra” Attack Another set of justiﬁcation techniques involves the use of the negative. The two primary such methods are the use of the contrapositive and the contradiction. To justify the statement “if p is true, then q is true,” we establish that “if q is not true, then p is not true” instead. Logically, these two statements are the same, but the latter, which is called the contrapositive of the ﬁrst, may be easier to think about. Example 4.18: Let a and b be integers. If ab is even, then a is even or b is even. Justiﬁcation: To justify this claim, consider the contrapositive, “If a is odd and b is odd, then ab is odd.” So, suppose a = 2j +1 and b = 2k +1, for some integers j and k. Then ab = 4jk +2j +2k +1 = 2(2jk + j +k)+1; hence, ab is odd. Besides showing a use of the contrapositive justiﬁcation technique, the previous example also contains an application of de Morgan’s law. This law helps us deal with negations, for it states that the negation of a statement of the form “p or q” is “not p and not q.” Likewise, it states that the negation of a statement of the form “p and q” is “not p or not q.”

4.4. Simple Justiﬁcation Techniques Contradiction Another negative justiﬁcation technique is justiﬁcation by contradiction, which also often involves using de Morgan’s law. In applying the justiﬁcation by contradiction technique, we establish that a statement q is true by ﬁrst supposing that q is false and then showing that this assumption leads to a contradiction (such as 2 ̸= 2 or 1 > 3). By reaching such a contradiction, we show that no consistent situation exists with q being false, so q must be true. Of course, in order to reach this conclusion, we must be sure our situation is consistent before we assume q is false. Example 4.19: Let a and b be integers. If ab is odd, then a is odd and b is odd. Justiﬁcation: Let ab be odd. We wish to show that a is odd and b is odd. So, with the hope of leading to a contradiction, let us assume the opposite, namely, suppose a is even or b is even. In fact, without loss of generality, we can assume that a is even (since the case for b is symmetric). Then a = 2j for some integer j. Hence, ab = (2j)b = 2( jb), that is, ab is even. But this is a contradiction: ab cannot simultaneously be odd and even. Therefore, a is odd and b is odd. 4.4.3 Induction and Loop Invariants Most of the claims we make about a running time or a space bound involve an integer parameter n (usually denoting an intuitive notion of the “size” of the problem). Moreover, most of these claims are equivalent to saying some statement q(n) is true “for all n ≥1.” Since this is making a claim about an inﬁnite set of numbers, we cannot justify this exhaustively in a direct fashion. Induction We can often justify claims such as those above as true, however, by using the technique of induction. This technique amounts to showing that, for any particular n ≥1, there is a ﬁnite sequence of implications that starts with something known to be true and ultimately leads to showing that q(n) is true. Speciﬁcally, we begin a justiﬁcation by induction by showing that q(n) is true for n = 1 (and possibly some other values n = 2,3,...,k, for some constant k). Then we justify that the inductive “step” is true for n > k, namely, we show “if q( j) is true for all j < n, then q(n) is true.” The combination of these two pieces completes the justiﬁcation by induction.

Chapter 4. Algorithm Analysis Proposition 4.20: Consider the Fibonacci function F(n), which is deﬁned such that F(1) = 1, F(2) = 2, and F(n) = F(n −2) + F(n −1) for n > 2. (See Section 2.2.3.) We claim that F(n) < 2n. Justiﬁcation: We will show our claim is correct by induction. Base cases: (n ≤2). F(1) = 1 < 2 = 21 and F(2) = 2 < 4 = 22. Induction step: (n > 2). Suppose our claim is true for all j < n. Since both n−2 and n−1 are less than n, we can apply the inductive assumption (sometimes called the “inductive hypothesis”) to imply that F(n) = F(n−2)+F(n−1) < 2n−2 +2n−1. Since 2n−2 +2n−1 < 2n−1 +2n−1 = 2·2n−1 = 2n, we have that F(n) < 2n, thus showing the inductive hypothesis for n. Let us do another inductive argument, this time for a fact we have seen before. Proposition 4.21: (which is the same as Proposition 4.3) n ∑ i=1 i = n(n+1) . Justiﬁcation: We will justify this equality by induction. Base case: n = 1. Trivial, for 1 = n(n+1)/2, if n = 1. Induction step: n ≥2. Assume the inductive hypothesis is true for any j < n. Therefore, for j = n−1, we have n−1 ∑ i=1 i = (n−1)(n−1+1) = (n−1)n . Hence, we obtain n ∑ i=1 i = n+ n−1 ∑ i=1 i = n+ (n−1)n = 2n+n2 −n = n2 +n = n(n+1) , thereby proving the inductive hypothesis for n. We may sometimes feel overwhelmed by the task of justifying something true for all n ≥1. We should remember, however, the concreteness of the inductive technique. It shows that, for any particular n, there is a ﬁnite step-by-step sequence of implications that starts with something true and leads to the truth about n. In short, the inductive argument is a template for building a sequence of direct justiﬁcations.

4.4. Simple Justiﬁcation Techniques Loop Invariants The ﬁnal justiﬁcation technique we discuss in this section is the loop invariant. To prove some statement L about a loop is correct, deﬁne L in terms of a series of smaller statements L0,L1,...,Lk, where: 1. The initial claim, L0, is true before the loop begins. 2. If L j−1 is true before iteration j, then L j will be true after iteration j. 3. The ﬁnal statement, Lk, implies the desired statement L to be true. Let us give a simple example of using a loop-invariant argument to justify the correctness of an algorithm. In particular, we use a loop invariant to justify that the method arrayFind (see Code Fragment 4.11) ﬁnds the smallest index at which element val occurs in array A. /∗∗Returns index j such that data[j] == val, or −1 if no such element. ∗/ public static int arrayFind(int[ ] data, int val) { int n = data.length; int j = 0; while (j < n) { // val is not equal to any of the ﬁrst j elements of data if (data[j] == val) return j; // a match was found at index j j++; // continue to next index // val is not equal to any of the ﬁrst j elements of data } return −1; // if we reach this, no match found } Code Fragment 4.11: Algorithm arrayFind for ﬁnding the ﬁrst index at which a given element occurs in an array. To show that arrayFind is correct, we inductively deﬁne a series of statements, L j, that lead to the correctness of our algorithm. Speciﬁcally, we claim the following is true at the beginning of iteration j of the while loop: L j: val is not equal to any of the ﬁrst j elements of data. This claim is true at the beginning of the ﬁrst iteration of the loop, because j is 0 and there are no elements among the ﬁrst 0 in data (this kind of a trivially true claim is said to hold vacuously). In iteration j, we compare element val to element data[ j]; if these two elements are equivalent, we return the index j, which is clearly correct since no earlier elements equal val. If the two elements val and data[ j] are not equal, then we have found one more element not equal to val and we increment the index j. Thus, the claim L j will be true for this new value of j; hence, it is true at the beginning of the next iteration. If the while loop terminates without ever returning an index in data, then we have j = n. That is, Ln is true—there are no elements of data equal to val. Therefore, the algorithm correctly returns −1 to indicate that val is not in data.

Chapter 5. Recursion One way to describe repetition within a computer program is the use of loops, such as Java’s while-loop and for-loop constructs described in Section 1.5.2. An entirely different way to achieve repetition is through a process known as recursion. Recursion is a technique by which a method makes one or more calls to itself during execution, or by which a data structure relies upon smaller instances of the very same type of structure in its representation. There are many examples of recursion in art and nature. For example, fractal patterns are naturally recursive. A physical example of recursion used in art is in the Russian Matryoshka dolls. Each doll is either made of solid wood, or is hollow and contains another Matryoshka doll inside it. In computing, recursion provides an elegant and powerful alternative for performing repetitive tasks. In fact, a few programming languages (e.g., Scheme, Smalltalk) do not explicitly support looping constructs and instead rely directly on recursion to express repetition. Most modern programming languages support functional recursion using the identical mechanism that is used to support traditional forms of method calls. When one invocation of the method makes a recursive call, that invocation is suspended until the recursive call completes. Recursion is an important technique in the study of data structures and algorithms. We will use it prominently in several later chapters of this book (most notably, Chapters 8 and 12). In this chapter, we begin with the following four illustrative examples of the use of recursion, providing a Java implementation for each. • The factorial function (commonly denoted as n!) is a classic mathematical function that has a natural recursive deﬁnition. • An English ruler has a recursive pattern that is a simple example of a fractal structure. • Binary search is among the most important computer algorithms. It allows us to efﬁciently locate a desired value in a data set with upwards of billions of entries. • The ﬁle system for a computer has a recursive structure in which directories can be nested arbitrarily deeply within other directories. Recursive algorithms are widely used to explore and manage these ﬁle systems. We then describe how to perform a formal analysis of the running time of a recursive algorithm, and we discuss some potential pitfalls when deﬁning recursions. In the balance of the chapter, we provide many more examples of recursive algorithms, organized to highlight some common forms of design.

5.1. Illustrative Examples 5.1 Illustrative Examples 5.1.1 The Factorial Function To demonstrate the mechanics of recursion, we begin with a simple mathematical example of computing the value of the factorial function. The factorial of a positive integer n, denoted n!, is deﬁned as the product of the integers from 1 to n. If n = 0, then n! is deﬁned as 1 by convention. More formally, for any integer n ≥0, n! =  1 if n = 0 n·(n−1)·(n−2)···3·2·1 if n ≥1. For example, 5! = 5·4·3·2·1 = 120. The factorial function is important because it is known to equal the number of ways in which n distinct items can be arranged into a sequence, that is, the number of permutations of n items. For example, the three characters a, b, and c can be arranged in 3! = 3 · 2 · 1 = 6 ways: abc, acb, bac, bca, cab, and cba. There is a natural recursive deﬁnition for the factorial function. To see this, observe that 5! = 5· (4· 3· 2· 1) = 5· 4!. More generally, for a positive integer n, we can deﬁne n! to be n·(n−1)!. This recursive deﬁnition can be formalized as n! =  1 if n = 0 n·(n−1)! if n ≥1. This deﬁnition is typical of many recursive deﬁnitions of functions. First, we have one or more base cases, which refer to ﬁxed values of the function. The above deﬁnition has one base case stating that n! = 1 for n = 0. Second, we have one or more recursive cases, which deﬁne the function in terms of itself. In the above deﬁnition, there is one recursive case, which indicates that n! = n·(n−1)! for n ≥1. A Recursive Implementation of the Factorial Function Recursion is not just a mathematical notation; we can use recursion to design a Java implementation of the factorial function, as shown in Code Fragment 5.1. public static int factorial(int n) throws IllegalArgumentException { if (n < 0) throw new IllegalArgumentException(); // argument must be nonnegative else if (n == 0) return 1; // base case else return n ∗factorial(n−1); // recursive case } Code Fragment 5.1: A recursive implementation of the factorial function.

Chapter 5. Recursion This method does not use any explicit loops. Repetition is achieved through repeated recursive invocations of the method. The process is ﬁnite because each time the method is invoked, its argument is smaller by one, and when a base case is reached, no further recursive calls are made. We illustrate the execution of a recursive method using a recursion trace. Each entry of the trace corresponds to a recursive call. Each new recursive method call is indicated by a downward arrow to a new invocation. When the method returns, an arrow showing this return is drawn and the return value may be indicated alongside this arrow. An example of such a trace for the factorial function is shown in Figure 5.1. return 4 ∗6 = 24 factorial(1) factorial(0) factorial(3) factorial(2) factorial(5) factorial(4) return 1 return 1 ∗1 = 1 return 2 ∗1 = 2 return 3 ∗2 = 6 return 5 ∗24 = 120 Figure 5.1: A recursion trace for the call factorial(5). A recursion trace closely mirrors a programming language’s execution of the recursion. In Java, each time a method (recursive or otherwise) is called, a structure known as an activation record or activation frame is created to store information about the progress of that invocation of the method. This frame stores the parameters and local variables speciﬁc to a given call of the method, and information about which command in the body of the method is currently executing. When the execution of a method leads to a nested method call, the execution of the former call is suspended and its frame stores the place in the source code at which the ﬂow of control should continue upon return of the nested call. A new frame is then created for the nested method call. This process is used both in the standard case of one method calling a different method, or in the recursive case where a method invokes itself. The key point is to have a separate frame for each active call.

5.1. Illustrative Examples 5.1.2 Drawing an English Ruler In the case of computing the factorial function, there is no compelling reason for preferring recursion over a direct iteration with a loop. As a more complex example of the use of recursion, consider how to draw the markings of a typical English ruler. For each inch, we place a tick with a numeric label. We denote the length of the tick designating a whole inch as the major tick length. Between the marks for whole inches, the ruler contains a series of minor ticks, placed at intervals of 1/2 inch, 1/4 inch, and so on. As the size of the interval decreases by half, the tick length decreases by one. Figure 5.2 demonstrates several such rulers with varying major tick lengths (although not drawn to scale). ---- 0 ----- 0 --- 0 - - - -- -- -- - - - --- --- --- 1 - - - -- -- -- - - - ---- 1 ---- --- 2 - - - -- -- -- - - - --- --- --- 3 - - -- -- - - ---- 2 ----- 1 (a) (b) (c) Figure 5.2: Three sample outputs of an English ruler drawing: (a) a 2-inch ruler with major tick length 4; (b) a 1-inch ruler with major tick length 5; (c) a 3-inch ruler with major tick length 3. A Recursive Approach to Ruler Drawing The English ruler pattern is a simple example of a fractal, that is, a shape that has a self-recursive structure at various levels of magniﬁcation. Consider the rule with major tick length 5 shown in Figure 5.2(b). Ignoring the lines containing 0 and 1, let us consider how to draw the sequence of ticks lying between these lines. The central tick (at 1/2 inch) has length 4. Observe that the two patterns of ticks above and below this central tick are identical, and each has a central tick of length 3.

Chapter 5. Recursion In general, an interval with a central tick length L ≥1 is composed of: • An interval with a central tick length L−1 • A single tick of length L • An interval with a central tick length L−1 Although it is possible to draw such a ruler using an iterative process (see Exercise P-5.29), the task is considerably easier to accomplish with recursion. Our implementation consists of three methods, as shown in Code Fragment 5.2. The main method, drawRuler, manages the construction of the entire ruler. Its arguments specify the total number of inches in the ruler and the major tick length. The utility method, drawLine, draws a single tick with a speciﬁed number of dashes (and an optional integer label that is printed to the right of the tick). The interesting work is done by the recursive drawInterval method. This method draws the sequence of minor ticks within some interval, based upon the length of the interval’s central tick. We rely on the intuition shown at the top of this page, and with a base case when L = 0 that draws nothing. For L ≥1, the ﬁrst and last steps are performed by recursively calling drawInterval(L−1). The middle step is performed by calling method drawLine(L). /∗∗Draws an English ruler for the given number of inches and major tick length. ∗/ public static void drawRuler(int nInches, int majorLength) { drawLine(majorLength, 0); // draw inch 0 line and label for (int j = 1; j <= nInches; j++) { drawInterval(majorLength −1); // draw interior ticks for inch drawLine(majorLength, j); // draw inch j line and label } } private static void drawInterval(int centralLength) { if (centralLength >= 1) { // otherwise, do nothing drawInterval(centralLength −1); // recursively draw top interval drawLine(centralLength); // draw center tick line (without label) drawInterval(centralLength −1); // recursively draw bottom interval } } private static void drawLine(int tickLength, int tickLabel) { for (int j = 0; j < tickLength; j++) System.out.print("-"); if (tickLabel >= 0) System.out.print(" " + tickLabel); System.out.print("\n"); } /∗∗Draws a line with the given tick length (but no label). ∗/ private static void drawLine(int tickLength) { drawLine(tickLength, −1); } Code Fragment 5.2: A recursive implementation of a method that draws a ruler.

5.1. Illustrative Examples Illustrating Ruler Drawing Using a Recursion Trace The execution of the recursive drawInterval method can be visualized using a recursion trace. The trace for drawInterval is more complicated than in the factorial example, however, because each instance makes two recursive calls. To illustrate this, we will show the recursion trace in a form that is reminiscent of an outline for a document. See Figure 5.3. (previous pattern repeats) drawInterval(3) drawInterval(2) drawInterval(1) drawInterval(1) drawInterval(0) drawLine(1) drawInterval(0) drawInterval(0) drawLine(1) drawInterval(0) drawLine(3) drawInterval(2) drawLine(2) Output Figure 5.3: A partial recursion trace for the call drawInterval(3). The second pattern of calls for drawInterval(2) is not shown, but it is identical to the ﬁrst.

Chapter 5. Recursion 5.1.3 Binary Search In this section, we describe a classic recursive algorithm, binary search, used to efﬁciently locate a target value within a sorted sequence of n elements stored in an array. This is among the most important of computer algorithms, and it is the reason that we so often store data in sorted order (as in Figure 5.4). 10 11 12 13 14 15 12 14 17 19 22 25 27 28 33 Figure 5.4: Values stored in sorted order within an array. The numbers at top are the indices. When the sequence is unsorted, the standard approach to search for a target value is to use a loop to examine every element, until either ﬁnding the target or exhausting the data set. This algorithm is known as linear search, or sequential search, and it runs in O(n) time (i.e., linear time) since every element is inspected in the worst case. When the sequence is sorted and indexable, there is a more efﬁcient algorithm. (For intuition, think about how you would accomplish this task by hand!) If we consider an arbitrary element of the sequence with value v, we can be sure that all elements prior to that in the sequence have values less than or equal to v, and that all elements after that element in the sequence have values greater than or equal to v. This observation allows us to quickly “home in” on a search target using a variant of the children’s game “high-low.” We call an element of the sequence a candidate if, at the current stage of the search, we cannot rule out that this item matches the target. The algorithm maintains two parameters, low and high, such that all the candidate elements have index at least low and at most high. Initially, low = 0 and high = n−1. We then compare the target value to the median candidate, that is, the element with index mid = ⌊(low +high)/2⌋. We consider three cases: • If the target equals the median candidate, then we have found the item we are looking for, and the search terminates successfully. • If the target is less than the median candidate, then we recur on the ﬁrst half of the sequence, that is, on the interval of indices from low to mid−1. • If the target is greater than the median candidate, then we recur on the second half of the sequence, that is, on the interval of indices from mid+1 to high. An unsuccessful search occurs if low > high, as the interval [low,high] is empty.

5.1. Illustrative Examples This algorithm is known as binary search. We give a Java implementation in Code Fragment 5.3, and an illustration of the execution of the algorithm in Figure 5.5. Whereas sequential search runs in O(n) time, the more efﬁcient binary search runs in O(logn) time. This is a signiﬁcant improvement, given that if n is 1 billion, logn is only 30. (We defer our formal analysis of binary search’s running time to Proposition 5.2 in Section 5.2.) /∗∗ ∗Returns true if the target value is found in the indicated portion of the data array. ∗This search only considers the array portion from data[low] to data[high] inclusive. ∗/ public static boolean binarySearch(int[ ] data, int target, int low, int high) { if (low > high) return false; // interval empty; no match else { int mid = (low + high) / 2; if (target == data[mid]) return true; // found a match else if (target < data[mid]) return binarySearch(data, target, low, mid −1); // recur left of the middle else return binarySearch(data, target, mid + 1, high); // recur right of the middle } } Code Fragment 5.3: An implementation of the binary search algorithm on a sorted array. mid high high low low mid low mid low=mid=high high 19 22 25 27 28 33 37 10 11 12 13 14 15 12 14 17 12 14 17 19 22 25 27 28 33 37 19 22 25 27 28 33 37 Figure 5.5: Example of a binary search for target value 22 on a sorted array with 16 elements.

Chapter 5. Recursion 5.1.4 File Systems Modern operating systems deﬁne ﬁle-system directories (also called “folders”) in a recursive way. Namely, a ﬁle system consists of a top-level directory, and the contents of this directory consists of ﬁles and other directories, which in turn can contain ﬁles and other directories, and so on. The operating system allows directories to be nested arbitrarily deeply (as long as there is enough memory), although by necessity there must be some base directories that contain only ﬁles, not further subdirectories. A representation of a portion of such a ﬁle system is given in Figure 5.6. /user/rt/courses/ cs016/ cs252/ programs/ homeworks/ projects/ papers/ demos/ hw1 hw2 hw3 pr1 pr2 pr3 grades market buylow sellhigh grades Figure 5.6: A portion of a ﬁle system demonstrating a nested organization. Given the recursive nature of the ﬁle-system representation, it should not come as a surprise that many common behaviors of an operating system, such as copying a directory or deleting a directory, are implemented with recursive algorithms. In this section, we consider one such algorithm: computing the total disk usage for all ﬁles and directories nested within a particular directory. For illustration, Figure 5.7 portrays the disk space being used by all entries in our sample ﬁle system. We differentiate between the immediate disk space used by each entry and the cumulative disk space used by that entry and all nested features. For example, the cs016 directory uses only 2K of immediate space, but a total of 249K of cumulative space.

5.1. Illustrative Examples /user/rt/courses/ cs016/ cs252/ programs/ homeworks/ projects/ papers/ demos/ hw1 3K hw2 2K hw3 4K pr1 57K pr2 97K pr3 74K grades 8K market 4786K buylow 26K sellhigh 55K grades 3K 2K 1K 1K 1K 1K 1K 1K 1K 10K 229K 4870K 82K 4787K 5124K 249K 4874K Figure 5.7: The same portion of a ﬁle system given in Figure 5.6, but with additional annotations to describe the amount of disk space that is used. Within the icon for each ﬁle or directory is the amount of space directly used by that artifact. Above the icon for each directory is an indication of the cumulative disk space used by that directory and all its (recursive) contents. The cumulative disk space for an entry can be computed with a simple recursive algorithm. It is equal to the immediate disk space used by the entry plus the sum of the cumulative disk space usage of any entries that are stored directly within the entry. For example, the cumulative disk space for cs016 is 249K because it uses 2K itself, 8K cumulatively in grades, 10K cumulatively in homeworks, and 229K cumulatively in programs. Pseudocode for this algorithm is given in Code Fragment 5.4. Algorithm DiskUsage( path): Input: A string designating a path to a ﬁle-system entry Output: The cumulative disk space used by that entry and any nested entries total = size( path) {immediate disk space used by the entry} if path represents a directory then for each child entry stored within directory path do total = total + DiskUsage( child) {recursive call} return total Code Fragment 5.4: An algorithm for computing the cumulative disk space usage nested at a ﬁle-system entry. We presume that method size returns the immediate disk space of an entry.

Chapter 5. Recursion The java.io.File Class To implement a recursive algorithm for computing disk usage in Java, we rely on the java.io.File class. An instance of this class represents an abstract pathname in the operating system and allows for properties of that operating system entry to be queried. We will rely on the following methods of the class: • new File(pathString) or new File(parentFile, childString) A new File instance can be constructed either by providing the full path as a string, or by providing an existing File instance that represents a directory and a string that designates the name of a child entry within that directory. • ﬁle.length() Returns the immediate disk usage (measured in bytes) for the operating system entry represented by the File instance (e.g., /user/rt/courses). • ﬁle.isDirectory() Returns true if the File instance represents a directory; false otherwise. • ﬁle.list() Returns an array of strings designating the names of all entries within the given directory. In our sample ﬁle system, if we call this method on the File associated with path /user/rt/courses/cs016, it returns an array with contents: {"grades", "homeworks", "programs"}. Java Implementation With use of the File class, we now convert the algorithm from Code Fragment 5.4 into the Java implementation of Code Fragment 5.5. /∗∗ ∗Calculates the total disk usage (in bytes) of the portion of the ﬁle system rooted ∗at the given path, while printing a summary akin to the standard 'du' Unix tool. ∗/ public static long diskUsage(File root) { long total = root.length(); // start with direct disk usage if (root.isDirectory()) { // and if this is a directory, for (String childname : root.list()) { // then for each child File child = new File(root, childname); // compose full path to child total += diskUsage(child); // add child’s usage to total } } System.out.println(total + "\t" + root); // descriptive output return total; // return the grand total } Code Fragment 5.5: A recursive method for reporting disk usage of a ﬁle system.

5.1. Illustrative Examples Recursion Trace To produce a different form of a recursion trace, we have included an extraneous print statement within our Java implementation (line 13 of Code Fragment 5.5). The precise format of that output intentionally mirrors the output that is produced by a classic Unix/Linux utility named du (for “disk usage”). It reports the amount of disk space used by a directory and all contents nested within, and can produce a verbose report, as given in Figure 5.8. When executed on the sample ﬁle system portrayed in Figure 5.7, our implementation of the diskUsage method produces the result given in Figure 5.8. During the execution of the algorithm, exactly one recursive call is made for each entry in the portion of the ﬁle system that is considered. Because each line is printed just before returning from a recursive call, the lines of output reﬂect the order in which the recursive calls are completed. Notice that it computes and reports the cumulative disk space for a nested entry before computing and reporting the cumulative disk space for the directory that contains it. For example, the recursive calls regarding entries grades, homeworks, and programs are computed before the cumulative total for the directory /user/rt/courses/cs016 that contains them. /user/rt/courses/cs016/grades /user/rt/courses/cs016/homeworks/hw1 /user/rt/courses/cs016/homeworks/hw2 /user/rt/courses/cs016/homeworks/hw3 /user/rt/courses/cs016/homeworks /user/rt/courses/cs016/programs/pr1 /user/rt/courses/cs016/programs/pr2 /user/rt/courses/cs016/programs/pr3 /user/rt/courses/cs016/programs /user/rt/courses/cs016 /user/rt/courses/cs252/projects/papers/buylow /user/rt/courses/cs252/projects/papers/sellhigh /user/rt/courses/cs252/projects/papers /user/rt/courses/cs252/projects/demos/market /user/rt/courses/cs252/projects/demos /user/rt/courses/cs252/projects /user/rt/courses/cs252/grades /user/rt/courses/cs252 /user/rt/courses/ Figure 5.8: A report of the disk usage for the ﬁle system shown in Figure 5.7, as generated by our diskUsage method from Code Fragment 5.5, or equivalently by the Unix/Linux command du with option -a (which lists both directories and ﬁles).

Chapter 5. Recursion 5.2 Analyzing Recursive Algorithms In Chapter 4, we introduced mathematical techniques for analyzing the efﬁciency of an algorithm, based upon an estimate of the number of primitive operations that are executed by the algorithm. We use notations such as big-Oh to summarize the relationship between the number of operations and the input size for a problem. In this section, we demonstrate how to perform this type of running-time analysis to recursive algorithms. With a recursive algorithm, we will account for each operation that is performed based upon the particular activation of the method that manages the ﬂow of control at the time it is executed. Stated another way, for each invocation of the method, we only account for the number of operations that are performed within the body of that activation. We can then account for the overall number of operations that are executed as part of the recursive algorithm by taking the sum, over all activations, of the number of operations that take place during each individual activation. (As an aside, this is also the way we analyze a nonrecursive method that calls other methods from within its body.) To demonstrate this style of analysis, we revisit the four recursive algorithms presented in Sections 5.1.1 through 5.1.4: factorial computation, drawing an English ruler, binary search, and computation of the cumulative size of a ﬁle system. In general, we may rely on the intuition afforded by a recursion trace in recognizing how many recursive activations occur, and how the parameterization of each activation can be used to estimate the number of primitive operations that occur within the body of that activation. However, each of these recursive algorithms has a unique structure and form. Computing Factorials It is relatively easy to analyze the efﬁciency of our method for computing factorials, as described in Section 5.1.1. A sample recursion trace for our factorial method was given in Figure 5.1. To compute factorial(n), we see that there are a total of n+1 activations, as the parameter decreases from n in the ﬁrst call, to n−1 in the second call, and so on, until reaching the base case with parameter 0. It is also clear, given an examination of the method body in Code Fragment 5.1, that each individual activation of factorial executes a constant number of operations. Therefore, we conclude that the overall number of operations for computing factorial(n) is O(n), as there are n+1 activations, each of which accounts for O(1) operations.

5.2. Analyzing Recursive Algorithms Drawing an English Ruler In analyzing the English ruler application from Section 5.1.2, we consider the fundamental question of how many total lines of output are generated by an initial call to drawInterval(c), where c denotes the center length. This is a reasonable benchmark for the overall efﬁciency of the algorithm as each line of output is based upon a call to the drawLine utility, and each recursive call to drawInterval with nonzero parameter makes exactly one direct call to drawLine. Some intuition may be gained by examining the source code and the recursion trace. We know that a call to drawInterval(c) for c > 0 spawns two calls to drawInterval(c−1) and a single call to drawLine. We will rely on this intuition to prove the following claim. Proposition 5.1: For c ≥0, a call to drawInterval(c) results in precisely 2c −1 lines of output. Justiﬁcation: We provide a formal proof of this claim by induction (see Section 4.4.3). In fact, induction is a natural mathematical technique for proving the correctness and efﬁciency of a recursive process. In the case of the ruler, we note that an application of drawInterval(0) generates no output, and that 20−1 = 1−1 = 0. This serves as a base case for our claim. More generally, the number of lines printed by drawInterval(c) is one more than twice the number generated by a call to drawInterval(c−1), as one center line is printed between two such recursive calls. By induction, we have that the number of lines is thus 1+2·(2c−1 −1) = 1+2c −2 = 2c −1. This proof is indicative of a more mathematically rigorous tool, known as a recurrence equation, that can be used to analyze the running time of a recursive algorithm. That technique is discussed in Section 12.1.4, in the context of recursive sorting algorithms. Performing a Binary Search When considering the running time of the binary search algorithm, as presented in Section 5.1.3, we observe that a constant number of primitive operations are executed during each recursive call of the binary search method. Hence, the running time is proportional to the number of recursive calls performed. We will show that at most ⌊logn⌋+ 1 recursive calls are made during a binary search of a sequence having n elements, leading to the following claim. Proposition 5.2: The binary search algorithm runs in O(logn) time for a sorted array with n elements.

Chapter 5. Recursion Justiﬁcation: To prove this claim, a crucial fact is that with each recursive call the number of candidate elements still to be searched is given by the value high−low +1. Moreover, the number of remaining candidates is reduced by at least one-half with each recursive call. Speciﬁcally, from the deﬁnition of mid, the number of remaining candidates is either (mid−1)−low +1 = low +high  −low ≤high−low +1 or high−(mid+1)+1 = high− low +high  ≤high−low +1 . Initially, the number of candidates is n; after the ﬁrst call in a binary search, it is at most n/2; after the second call, it is at most n/4; and so on. In general, after the j th call in a binary search, the number of candidate elements remaining is at most n/2j. In the worst case (an unsuccessful search), the recursive calls stop when there are no more candidate elements. Hence, the maximum number of recursive calls performed, is the smallest integer r such that n 2r < 1. In other words (recalling that we omit a logarithm’s base when it is 2), r is the smallest integer such that r > logn. Thus, we have r = ⌊logn⌋+1, which implies that binary search runs in O(logn) time. Computing Disk Space Usage Our ﬁnal recursive algorithm from Section 5.1 was that for computing the overall disk space usage in a speciﬁed portion of a ﬁle system. To characterize the “problem size” for our analysis, we let n denote the number of ﬁle-system entries in the portion of the ﬁle system that is considered. (For example, the ﬁle system portrayed in Figure 5.6 has n = 19 entries.) To characterize the cumulative time spent for an initial call to diskUsage, we must analyze the total number of recursive invocations that are made, as well as the number of operations that are executed within those invocations. We begin by showing that there are precisely n recursive invocations of the method, in particular, one for each entry in the relevant portion of the ﬁle system. Intuitively, this is because a call to diskUsage for a particular entry e of the ﬁle system is only made from within the for loop of Code Fragment 5.5 when processing the entry for the unique directory that contains e, and that entry will only be explored once.

5.2. Analyzing Recursive Algorithms To formalize this argument, we can deﬁne the nesting level of each entry such that the entry on which we begin has nesting level 0, entries stored directly within it have nesting level 1, entries stored within those entries have nesting level 2, and so on. We can prove by induction that there is exactly one recursive invocation of diskUsage upon each entry at nesting level k. As a base case, when k = 0, the only recursive invocation made is the initial one. As the inductive step, once we know there is exactly one recursive invocation for each entry at nesting level k, we can claim that there is exactly one invocation for each entry e at nesting level k + 1, made within the for loop for the entry at level k that contains e. Having established that there is one recursive call for each entry of the ﬁle system, we return to the question of the overall computation time for the algorithm. It would be great if we could argue that we spend O(1) time in any single invocation of the method, but that is not the case. While there is a constant number of steps reﬂected in the call to root.length() to compute the disk usage directly at that entry, when the entry is a directory, the body of the diskUsage method includes a for loop that iterates over all entries that are contained within that directory. In the worst case, it is possible that one entry includes n−1 others. Based on this reasoning, we could conclude that there are O(n) recursive calls, each of which runs in O(n) time, leading to an overall running time that is O(n2). While this upper bound is technically true, it is not a tight upper bound. Remarkably, we can prove the stronger bound that the recursive algorithm for diskUsage completes in O(n) time! The weaker bound was pessimistic because it assumed a worst-case number of entries for each directory. While it is possible that some directories contain a number of entries proportional to n, they cannot all contain that many. To prove the stronger claim, we choose to consider the overall number of iterations of the for loop across all recursive calls. We claim there are precisely n−1 such iterations of that loop overall. We base this claim on the fact that each iteration of that loop makes a recursive call to diskUsage, and yet we have already concluded that there are a total of n calls to diskUsage (including the original call). We therefore conclude that there are O(n) recursive calls, each of which uses O(1) time outside the loop, and that the overall number of operations due to the loop is O(n). Summing all of these bounds, the overall number of operations is O(n). The argument we have made is more advanced than with the earlier examples of recursion. The idea that we can sometimes get a tighter bound on a series of operations by considering the cumulative effect, rather than assuming that each achieves a worst case is a technique called amortization; we will see another example of such analysis in Section 7.2.3. Furthermore, a ﬁle system is an implicit example of a data structure known as a tree, and our disk usage algorithm is really a manifestation of a more general algorithm known as a tree traversal. Trees will be the focus of Chapter 8, and our argument about the O(n) running time of the disk usage algorithm will be generalized for tree traversals in Section 8.4.

Chapter 5. Recursion 5.3 Further Examples of Recursion In this section, we provide additional examples of the use of recursion. We organize our presentation by considering the maximum number of recursive calls that may be started from within the body of a single activation. • If a recursive call starts at most one other, we call this a linear recursion. • If a recursive call may start two others, we call this a binary recursion. • If a recursive call may start three or more others, this is multiple recursion. 5.3.1 Linear Recursion If a recursive method is designed so that each invocation of the body makes at most one new recursive call, this is know as linear recursion. Of the recursions we have seen so far, the implementation of the factorial method (Section 5.1.1) is a clear example of linear recursion. More interestingly, the binary search algorithm (Section 5.1.3) is also an example of linear recursion, despite the term “binary” in the name. The code for binary search (Code Fragment 5.3) includes a case analysis, with two branches that lead to a further recursive call, but only one branch is followed during a particular execution of the body. A consequence of the deﬁnition of linear recursion is that any recursion trace will appear as a single sequence of calls, as we originally portrayed for the factorial method in Figure 5.1 of Section 5.1.1. Note that the linear recursion terminology reﬂects the structure of the recursion trace, not the asymptotic analysis of the running time; for example, we have seen that binary search runs in O(logn) time. Summing the Elements of an Array Recursively Linear recursion can be a useful tool for processing a sequence, such as a Java array. Suppose, for example, that we want to compute the sum of an array of n integers. We can solve this summation problem using linear recursion by observing that if n = 0 the sum is trivially 0, and otherwise it is the sum of the ﬁrst n−1 integers in the array plus the last value in the array. (See Figure 5.9.) 10 11 12 13 14 15 Figure 5.9: Computing the sum of a sequence recursively, by adding the last number to the sum of the ﬁrst n−1.

5.3. Further Examples of Recursion A recursive algorithm for computing the sum of an array of integers based on this intuition is implemented in Code Fragment 5.6. /∗∗Returns the sum of the ﬁrst n integers of the given array. ∗/ public static int linearSum(int[ ] data, int n) { if (n == 0) return 0; else return linearSum(data, n−1) + data[n−1]; } Code Fragment 5.6: Summing an array of integers using linear recursion. A recursion trace of the linearSum method for a small example is given in Figure 5.10. For an input of size n, the linearSum algorithm makes n+ 1 method calls. Hence, it will take O(n) time, because it spends a constant amount of time performing the nonrecursive part of each call. Moreover, we can also see that the memory space used by the algorithm (in addition to the array) is also O(n), as we use a constant amount of memory space for each of the n+1 frames in the trace at the time we make the ﬁnal recursive call (with n = 0). return 15 + data[4] = 15 + 8 = 23 linearSum(data, 4) linearSum(data, 3) linearSum(data, 2) linearSum(data, 1) linearSum(data, 0) linearSum(data, 5) return 0 return 0 + data[0] = 0 + 4 = 4 return 4 + data[1] = 4 + 3 = 7 return 7 + data[2] = 7 + 6 = 13 return 13 + data[3] = 13 + 2 = 15 Figure 5.10: Recursion trace for an execution of linearSum(data, 5) with input parameter data = 4, 3, 6, 2, 8.

Chapter 5. Recursion Reversing a Sequence with Recursion Next, let us consider the problem of reversing the n elements of an array, so that the ﬁrst element becomes the last, the second element becomes second to the last, and so on. We can solve this problem using linear recursion, by observing that the reversal of a sequence can be achieved by swapping the ﬁrst and last elements and then recursively reversing the remaining elements. We present an implementation of this algorithm in Code Fragment 5.7, using the convention that the ﬁrst time we call this algorithm we do so as reverseArray(data, 0, n−1). /∗∗Reverses the contents of subarray data[low] through data[high] inclusive. ∗/ public static void reverseArray(int[ ] data, int low, int high) { if (low < high) { // if at least two elements in subarray int temp = data[low]; // swap data[low] and data[high] data[low] = data[high]; data[high] = temp; reverseArray(data, low + 1, high −1); // recur on the rest } } Code Fragment 5.7: Reversing the elements of an array using linear recursion. We note that whenever a recursive call is made, there will be two fewer elements in the relevant portion of the array. (See Figure 5.11.) Eventually a base case is reached when the condition low < high fails, either because low == high in the case that n is odd, or because low == high + 1 in the case that n is even. The above argument implies that the recursive algorithm of Code Fragment 5.7 is guaranteed to terminate after a total of 1+  n  recursive calls. Because each call involves a constant amount of work, the entire process runs in O(n) time. Figure 5.11: A trace of the recursion for reversing a sequence. The highlighted portion has yet to be reversed.

5.3. Further Examples of Recursion Recursive Algorithms for Computing Powers As another interesting example of the use of linear recursion, we consider the problem of raising a number x to an arbitrary nonnegative integer n. That is, we wish to compute the power function, deﬁned as power(x,n) = xn. (We use the name “power” for this discussion, to differentiate from the pow method of the Math class, which provides such functionality.) We will consider two different recursive formulations for the problem that lead to algorithms with very different performance. A trivial recursive deﬁnition follows from the fact that xn = x·xn−1 for n > 0. power(x,n) =  1 if n = 0 x· power(x,n−1) otherwise. This deﬁnition leads to a recursive algorithm shown in Code Fragment 5.8. /∗∗Computes the value of x raised to the nth power, for nonnegative integer n. ∗/ public static double power(double x, int n) { if (n == 0) return 1; else return x ∗power(x, n−1); } Code Fragment 5.8: Computing the power function using trivial recursion. A recursive call to this version of power(x,n) runs in O(n) time. Its recursion trace has structure very similar to that of the factorial function from Figure 5.1, with the parameter decreasing by one with each call, and constant work performed at each of n+1 levels. However, there is a much faster way to compute the power function using an alternative deﬁnition that employs a squaring technique. Let k =  n  denote the ﬂoor of the integer division (equivalent to n/2 in Java when n is an int). We consider the expression  xk2. When n is even,  n  = n 2 and therefore  xk2 =  x n 2 = xn. When n is odd,  n  = n−1 and  xk2 = xn−1, and therefore xn =  xk2 ·x, just as 213 =  26 ·26 ·2. This analysis leads to the following recursive deﬁnition: power(x,n) =      if n = 0  power  x,  n 2 ·x if n > 0 is odd  power  x,  n 2 if n > 0 is even If we were to implement this recursion making two recursive calls to compute power(x,  n  ) · power(x,  n  ), a trace of the recursion would demonstrate O(n) calls. We can perform signiﬁcantly fewer operations by computing power(x,  n  ) and storing it in a variable as a partial result, and then multiplying it by itself. An implementation based on this recursive deﬁnition is given in Code Fragment 5.9.

Chapter 5. Recursion /∗∗Computes the value of x raised to the nth power, for nonnegative integer n. ∗/ public static double power(double x, int n) { if (n == 0) return 1; else { double partial = power(x, n/2); // rely on truncated division of n double result = partial ∗partial; if (n % 2 == 1) // if n odd, include extra factor of x result ∗= x; return result; } } Code Fragment 5.9: Computing the power function using repeated squaring. To illustrate the execution of our improved algorithm, Figure 5.12 provides a recursion trace of the computation power(2, 13). return 64 ∗64 ∗2 = 8192 power(2, 13) power(2, 6) power(2, 3) power(2, 1) power(2, 0) return 1 return 1 ∗1 ∗2 = 2 return 2 ∗2 ∗2 = 8 return 8 ∗8 = 64 Figure 5.12: Recursion trace for an execution of power(2, 13). To analyze the running time of the revised algorithm, we observe that the exponent in each recursive call of method power(x,n) is at most half of the preceding exponent. As we saw with the analysis of binary search, the number of times that we can divide n by two before getting to one or less is O(logn). Therefore, our new formulation of power results in O(logn) recursive calls. Each individual activation of the method uses O(1) operations (excluding the recursive call), and so the total number of operations for computing power(x,n) is O(logn). This is a signiﬁcant improvement over the original O(n)-time algorithm. The improved version also provides signiﬁcant saving in reducing the memory usage. The ﬁrst version has a recursive depth of O(n), and therefore, O(n) frames are simultaneously stored in memory. Because the recursive depth of the improved version is O(logn), its memory usage is O(logn) as well.

5.3. Further Examples of Recursion 5.3.2 Binary Recursion When a method makes two recursive calls, we say that it uses binary recursion. We have already seen an example of binary recursion when drawing the English ruler (Section 5.1.2). As another application of binary recursion, let us revisit the problem of summing the n integers of an array. Computing the sum of one or zero values is trivial. With two or more values, we can recursively compute the sum of the ﬁrst half, and the sum of the second half, and add those sums together. Our implementation of such an algorithm, in Code Fragment 5.10, is initially invoked as binarySum(data, 0, n−1). /∗∗Returns the sum of subarray data[low] through data[high] inclusive. ∗/ public static int binarySum(int[ ] data, int low, int high) { if (low > high) // zero elements in subarray return 0; else if (low == high) // one element in subarray return data[low]; else { int mid = (low + high) / 2; return binarySum(data, low, mid) + binarySum(data, mid+1, high); } } Code Fragment 5.10: Summing the elements of a sequence using binary recursion. To analyze algorithm binarySum, we consider, for simplicity, the case where n is a power of two. Figure 5.13 shows the recursion trace of an execution of binarySum(data, 0, 7). We label each box with the values of parameters low and high for that call. The size of the range is divided in half at each recursive call, and so the depth of the recursion is 1+log2 n. Therefore, binarySum uses O(logn) amount of additional space, which is a big improvement over the O(n) space used by the linearSum method of Code Fragment 5.6. However, the running time of binarySum is O(n), as there are 2n−1 method calls, each requiring constant time. 0,0 1,1 2,2 4,4 6,6 7,7 3,3 5,5 0,1 4,5 6,7 2,3 0,3 4,7 0,7 Figure 5.13: Recursion trace for the execution of binarySum(data, 0, 7).

Chapter 5. Recursion 5.3.3 Multiple Recursion Generalizing from binary recursion, we deﬁne multiple recursion as a process in which a method may make more than two recursive calls. Our recursion for analyzing the disk space usage of a ﬁle system (see Section 5.1.4) is an example of multiple recursion, because the number of recursive calls made during one invocation was equal to the number of entries within a given directory of the ﬁle system. Another common application of multiple recursion is when we want to enumerate various conﬁgurations in order to solve a combinatorial puzzle. For example, the following are all instances of what are known as summation puzzles: pot + pan = bib dog + cat = pig boy+ girl = baby To solve such a puzzle, we need to assign a unique digit (that is, 0,1,...,9) to each letter in the equation, in order to make the equation true. Typically, we solve such a puzzle by using our human observations of the particular puzzle we are trying to solve to eliminate conﬁgurations (that is, possible partial assignments of digits to letters) until we can work through the feasible conﬁgurations that remain, testing for the correctness of each one. If the number of possible conﬁgurations is not too large, however, we can use a computer to simply enumerate all the possibilities and test each one, without employing any human observations. Such an algorithm can use multiple recursion to work through the conﬁgurations in a systematic way. To keep the description general enough to be used with other puzzles, we consider an algorithm that enumerates and tests all k-length sequences, without repetitions, chosen from a given universe U. We show pseudocode for such an algorithm in Code Fragment 5.11, building the sequence of k elements with the following steps: 1. Recursively generating the sequences of k −1 elements 2. Appending to each such sequence an element not already contained in it. Throughout the execution of the algorithm, we use a set U to keep track of the elements not contained in the current sequence, so that an element e has not been used yet if and only if e is in U. Another way to look at the algorithm of Code Fragment 5.11 is that it enumerates every possible size-k ordered subset of U, and tests each subset for being a possible solution to our puzzle. For summation puzzles, U = {0,1,2,3,4,5,6,7,8,9} and each position in the sequence corresponds to a given letter. For example, the ﬁrst position could stand for b, the second for o, the third for y, and so on.

5.3. Further Examples of Recursion Algorithm PuzzleSolve(k, S, U): Input: An integer k, sequence S, and set U Output: An enumeration of all k-length extensions to S using elements in U without repetitions for each e in U do Add e to the end of S Remove e from U {e is now being used} if k == 1 then Test whether S is a conﬁguration that solves the puzzle if S solves the puzzle then add S to output {a solution} else PuzzleSolve(k −1, S, U) {a recursive call} Remove e from the end of S Add e back to U {e is now considered as unused} Code Fragment 5.11: Solving a combinatorial puzzle by enumerating and testing all possible conﬁgurations. In Figure 5.14, we show a recursion trace of a call to PuzzleSolve(3, S, U), where S is empty and U = {a,b,c}. During the execution, all the permutations of the three characters are generated and tested. Note that the initial call makes three recursive calls, each of which in turn makes two more. If we had executed PuzzleSolve(3, S, U) on a set U consisting of four elements, the initial call would have made four recursive calls, each of which would have a trace looking like the one in Figure 5.14. initial call PuzzleSolve(3, (), {a,b,c}) PuzzleSolve(2, b, {a,c}) PuzzleSolve(2, c, {a,b}) PuzzleSolve(1, ca, {b}) PuzzleSolve(2, a, {b,c}) PuzzleSolve(1, ab, {c}) PuzzleSolve(1, ba, {c}) PuzzleSolve(1, bc, {a}) PuzzleSolve(1, ac, {b}) PuzzleSolve(1, cb, {a}) acb abc bac cab bca cba Figure 5.14: Recursion trace for an execution of PuzzleSolve(3, S, U), where S is empty and U = {a,b,c}. This execution generates and tests all permutations of a, b, and c. We show the permutations generated directly below their respective boxes.

Chapter 5. Recursion 5.4 Designing Recursive Algorithms An algorithm that uses recursion typically has the following form: • Test for base cases. We begin by testing for a set of base cases (there should be at least one). These base cases should be deﬁned so that every possible chain of recursive calls will eventually reach a base case, and the handling of each base case should not use recursion. • Recur. If not a base case, we perform one or more recursive calls. This recursive step may involve a test that decides which of several possible recursive calls to make. We should deﬁne each possible recursive call so that it makes progress towards a base case. Parameterizing a Recursion To design a recursive algorithm for a given problem, it is useful to think of the different ways we might deﬁne subproblems that have the same general structure as the original problem. If one has difﬁculty ﬁnding the repetitive structure needed to design a recursive algorithm, it is sometimes useful to work out the problem on a few concrete examples to see how the subproblems should be deﬁned. A successful recursive design sometimes requires that we redeﬁne the original problem to facilitate similar-looking subproblems. Often, this involved reparameterizing the signature of the method. For example, when performing a binary search in an array, a natural method signature for a caller would appear as binarySearch(data, target). However, in Section 5.1.3, we deﬁned our method with calling signature binarySearch(data, target, low, high), using the additional parameters to demarcate subarrays as the recursion proceeds. This change in parameterization is critical for binary search. Several other examples in this chapter (e.g., reverseArray, linearSum, binarySum) also demonstrated the use of additional parameters in deﬁning recursive subproblems. If we wish to provide a cleaner public interface to an algorithm without exposing the user to the recursive parameterization, a standard technique is to make the recursive version private, and to introduce a cleaner public method (that calls the private one with appropriate parameters). For example, we might offer the following simpler version of binarySearch for public use: /∗∗Returns true if the target value is found in the data array. ∗/ public static boolean binarySearch(int[ ] data, int target) { return binarySearch(data, target, 0, data.length −1); // use parameterized version }

5.5. Recursion Run Amok 5.5 Recursion Run Amok Although recursion is a very powerful tool, it can easily be misused in various ways. In this section, we examine several cases in which a poorly implemented recursion causes drastic inefﬁciency, and we discuss some strategies for recognizing and avoid such pitfalls. We begin by revisiting the element uniqueness problem, deﬁned on page 174 of Section 4.3.3. We can use the following recursive formulation to determine if all n elements of a sequence are unique. As a base case, when n = 1, the elements are trivially unique. For n ≥2, the elements are unique if and only if the ﬁrst n−1 elements are unique, the last n−1 items are unique, and the ﬁrst and last elements are different (as that is the only pair that was not already checked as a subcase). A recursive implementation based on this idea is given in Code Fragment 5.12, named unique3 (to differentiate it from unique1 and unique2 from Chapter 4). /∗∗Returns true if there are no duplicate values from data[low] through data[high].∗/ public static boolean unique3(int[ ] data, int low, int high) { if (low >= high) return true; // at most one item else if (!unique3(data, low, high−1)) return false; // duplicate in ﬁrst n−1 else if (!unique3(data, low+1, high)) return false; // duplicate in last n−1 else return (data[low] != data[high]); // do ﬁrst and last diﬀer? } Code Fragment 5.12: Recursive unique3 for testing element uniqueness. Unfortunately, this is a terribly inefﬁcient use of recursion. The nonrecursive part of each call uses O(1) time, so the overall running time will be proportional to the total number of recursive invocations. To analyze the problem, we let n denote the number of entries under consideration, that is, let n = 1 + high −low. If n = 1, then the running time of unique3 is O(1), since there are no recursive calls for this case. In the general case, the important observation is that a single call to unique3 for a problem of size n may result in two recursive calls on problems of size n −1. Those two calls with size n −1 could in turn result in four calls (two each) with a range of size n −2, and thus eight calls with size n −3 and so on. Thus, in the worst case, the total number of method calls is given by the geometric summation 1+2+4+···+2n−1, which is equal to 2n −1 by Proposition 4.5. Thus, the running time of method unique3 is O(2n). This is an incredibly inefﬁcient method for solving the element uniqueness problem. Its inefﬁciency comes not from the fact that it uses recursion—it comes from the fact that it uses recursion poorly, which is something we address in Exercise C-5.12.

Chapter 5. Recursion An Ineﬃcient Recursion for Computing Fibonacci Numbers In Section 2.2.3, we introduced a process for generating the progression of Fibonacci numbers, which can be deﬁned recursively as follows: F0 = F1 = Fn = Fn−2 +Fn−1 for n > 1. Ironically, a recursive implementation based directly on this deﬁnition results in the method ﬁbonacciBad shown in Code Fragment 5.13, which computes a Fibonacci number by making two recursive calls in each non-base case. /∗∗Returns the nth Fibonacci number (ineﬃciently). ∗/ public static long ﬁbonacciBad(int n) { if (n <= 1) return n; else return ﬁbonacciBad(n−2) + ﬁbonacciBad(n−1); } Code Fragment 5.13: Computing the nth Fibonacci number using binary recursion. Unfortunately, such a direct implementation of the Fibonacci formula results in a terribly inefﬁcient method. Computing the nth Fibonacci number in this way requires an exponential number of calls to the method. Speciﬁcally, let cn denote the number of calls performed in the execution of ﬁbonacciBad(n). Then, we have the following values for the cn’s: c0 = c1 = c2 = 1+c0 +c1 = 1+1+1 = 3 c3 = 1+c1 +c2 = 1+1+3 = 5 c4 = 1+c2 +c3 = 1+3+5 = 9 c5 = 1+c3 +c4 = 1+5+9 = 15 c6 = 1+c4 +c5 = 1+9+15 = 25 c7 = 1+c5 +c6 = 1+15+25 = 41 c8 = 1+c6 +c7 = 1+25+41 = 67 If we follow the pattern forward, we see that the number of calls more than doubles for each two consecutive indices. That is, c4 is more than twice c2, c5 is more than twice c3, c6 is more than twice c4, and so on. Thus, cn > 2n/2, which means that ﬁbonacciBad(n) makes a number of calls that is exponential in n.

5.5. Recursion Run Amok An Eﬃcient Recursion for Computing Fibonacci Numbers We were tempted into using the bad recursive formulation because of the way the nth Fibonacci number, Fn, depends on the two previous values, Fn−2 and Fn−1. But notice that after computing Fn−2, the call to compute Fn−1 requires its own recursive call to compute Fn−2, as it does not have knowledge of the value of Fn−2 that was computed at the earlier level of recursion. That is duplicative work. Worse yet, both of those calls will need to (re)compute the value of Fn−3, as will the computation of Fn−1. This snowballing effect is what leads to the exponential running time of ﬁbonacciBad. We can compute Fn much more efﬁciently using a recursion in which each invocation makes only one recursive call. To do so, we need to redeﬁne the expectations of the method. Rather than having the method return a single value, which is the nth Fibonacci number, we deﬁne a recursive method that returns an array with two consecutive Fibonacci numbers {Fn,Fn−1}, using the convention F−1 = 0. Although it seems to be a greater burden to report two consecutive Fibonacci numbers instead of one, passing this extra information from one level of the recursion to the next makes it much easier to continue the process. (It allows us to avoid having to recompute the second value that was already known within the recursion.) An implementation based on this strategy is given in Code Fragment 5.14. /∗∗Returns array containing the pair of Fibonacci numbers, F(n) and F(n−1). ∗/ public static long[ ] ﬁbonacciGood(int n) { if (n <= 1) { long[ ] answer = {n, 0}; return answer; } else { long[ ] temp = ﬁbonacciGood(n −1); // returns {Fn−1, Fn−2} long[ ] answer = {temp[0] + temp[1], temp[0]}; // we want {Fn, Fn−1} return answer; } } Code Fragment 5.14: Computing the nth Fibonacci number using linear recursion. In terms of efﬁciency, the difference between the bad and good recursions for this problem is like night and day. The ﬁbonacciBad method uses exponential time. We claim that the execution of method ﬁbonacciGood(n) runs in O(n) time. Each recursive call to ﬁbonacciGood decreases the argument n by 1; therefore, a recursion trace includes a series of n method calls. Because the nonrecursive work for each call uses constant time, the overall computation executes in O(n) time.

Chapter 5. Recursion 5.5.1 Maximum Recursive Depth in Java Another danger in the misuse of recursion is known as inﬁnite recursion. If each recursive call makes another recursive call, without ever reaching a base case, then we have an inﬁnite series of such calls. This is a fatal error. An inﬁnite recursion can quickly swamp computing resources, not only due to rapid use of the CPU, but because each successive call creates a frame requiring additional memory. A blatant example of an ill-formed recursion is the following: /∗∗Don't call this (inﬁnite) version. ∗/ public static int ﬁbonacci(int n) { return ﬁbonacci(n); // After all Fn does equal Fn } However, there are far more subtle errors that can lead to an inﬁnite recursion. Revisiting our implementation of binary search (Code Fragment 5.3), when we make a recursive call on the right portion of the sequence (line 15), we specify the subarray from index mid+1 to high. Had that line instead been written as return binarySearch(data, target, mid, high); // sending mid, not mid+1 this could result in an inﬁnite recursion. In particular, when searching a range of two elements, it becomes possible to make a recursive call on the identical range. A programmer should ensure that each recursive call is in some way progressing toward a base case (for example, by having a parameter value that decreases with each call). To combat against inﬁnite recursions, the designers of Java made an intentional decision to limit the overall space used to store activation frames for simultaneously active method calls. If this limit is reached, the Java Virtual Machine throws a StackOverﬂowError. (We will further discuss the “stack” data structure in Section 6.1.) The precise value of this limit depends upon the Java installation, but a typical value might allow upward of 1000 simultaneous calls. For many applications of recursion, allowing up to 1000 nested calls sufﬁces. For example, our binarySearch method (Section 5.1.3) has O(logn) recursive depth, and so for the default recursive limit to be reached, there would need to be 21000 elements (far, far more than the estimated number of atoms in the universe). However, we have seen several linear recursions that have recursive depth proportional to n. Java’s limit on the recursive depth might disrupt such computations. It is possible to reconﬁgure the Java Virtual Machine so that it allows for greater space to be devoted to nested method calls. This is done by setting the -Xss runtime option when starting Java, either as a command-line option or through the settings of an IDE. But it often possible to rely upon the intuition of a recursive algorithm, yet to reimplement it more directly using traditional loops rather than method calls to express the necessary repetition. We discuss just such an approach to conclude the chapter.

5.6. Eliminating Tail Recursion 5.6 Eliminating Tail Recursion The main beneﬁt of a recursive approach to algorithm design is that it allows us to succinctly take advantage of a repetitive structure present in many problems. By making our algorithm description exploit the repetitive structure in a recursive way, we can often avoid complex case analyses and nested loops. This approach can lead to more readable algorithm descriptions, while still being quite efﬁcient. However, the usefulness of recursion comes at a modest cost. In particular, the Java Virtual Machine must maintain frames that keep track of the state of each nested call. When computer memory is at a premium, it can be beneﬁcial to derive nonrecursive implementations of recursive algorithms. In general, we can use the stack data structure, which we will introduce in Section 6.1, to convert a recursive algorithm into a nonrecursive algorithm by managing the nesting of the recursive structure ourselves, rather than relying on the interpreter to do so. Although this only shifts the memory usage from the interpreter to our stack, we may be able to further reduce the memory usage by storing the minimal information necessary. Even better, some forms of recursion can be eliminated without any use of auxiliary memory. One such form is known as tail recursion. A recursion is a tail recursion if any recursive call that is made from one context is the very last operation in that context, with the return value of the recursive call (if any) immediately returned by the enclosing recursion. By necessity, a tail recursion must be a linear recursion (since there is no way to make a second recursive call if you must immediately return the result of the ﬁrst). Of the recursive methods demonstrated in this chapter, the binarySearch method of Code Fragment 5.3 and the reverseArray method of Code Fragment 5.7 are examples of tail recursion. Several others of our linear recursions are almost like tail recursion, but not technically so. For example, our factorial method of Code Fragment 5.1 is not a tail recursion. It concludes with the command: return n ∗factorial(n−1); This is not a tail recursion because an additional multiplication is performed after the recursive call is completed, and the result returned is not the same. For similar reasons, the linearSum method of Code Fragment 5.6, both power methods from Code Fragments 5.8 and 5.9, and the ﬁbonacciGood method of Code Fragment 5.13 fail to be tail recursions. Tail recursions are special, as they can be automatically reimplemented nonrecursively by enclosing the body in a loop for repetition, and replacing a recursive call with new parameters by a reassignment of the existing parameters to those values. In fact, many programming language implementations may convert tail recursions in this way as an optimization.

Chapter 5. Recursion /∗∗Returns true if the target value is found in the data array. ∗/ public static boolean binarySearchIterative(int[ ] data, int target) { int low = 0; int high = data.length −1; while (low <= high) { int mid = (low + high) / 2; if (target == data[mid]) // found a match return true; else if (target < data[mid]) high = mid −1; // only consider values left of mid else low = mid + 1; // only consider values right of mid } return false; // loop ended without success } Code Fragment 5.15: A nonrecursive implementation of binary search. As a tangible example, our binarySearch method can be reimplemented as shown in Code Fragment 5.15. We initialize variables low and high to represent the full extent of the array just prior to our while loop. Then, during each pass of the loop, we either ﬁnd the target, or we narrow the range of the candidate subarray. Where we made the recursive call binarySearch(data, target, low, mid −1) in the original version, we simply replace high = mid −1 in our new version and then continue to the next iteration of the loop. Our original base case condition of low > high has simply been replaced by the opposite loop condition, while low <= high. In our new implementation, we return false to designate a failed search if the while loop ends without having ever returned true from within. Most other linear recursions can be expressed quite efﬁciently with iteration, even if they were not formally tail recursions. For example, there are trivial nonrecursive implementations for computing factorials, computing Fibonacci numbers, summing elements of an array, or reversing the contents of an array. For example, Code Fragment 5.16 provides a nonrecursive method to reverse the contents of an array (as compared to the earlier recursive method from Code Fragment 5.7). /∗∗Reverses the contents of the given array. ∗/ public static void reverseIterative(int[ ] data) { int low = 0, high = data.length −1; while (low < high) { // swap data[low] and data[high] int temp = data[low]; data[low++] = data[high]; // post-increment of low data[high−−] = temp; // post-decrement of high } } Code Fragment 5.16: Reversing the elements of a sequence using iteration.

Chapter 6. Stacks, Queues, and Deques 6.1 Stacks A stack is a collection of objects that are inserted and removed according to the last-in, ﬁrst-out (LIFO) principle. A user may insert objects into a stack at any time, but may only access or remove the most recently inserted object that remains (at the so-called “top” of the stack). The name “stack” is derived from the metaphor of a stack of plates in a spring-loaded, cafeteria plate dispenser. In this case, the fundamental operations involve the “pushing” and “popping” of plates on the stack. When we need a new plate from the dispenser, we “pop” the top plate off the stack, and when we add a plate, we “push” it down on the stack to become the new top plate. Perhaps an even more amusing example is a PEZ ® candy dispenser, which stores mint candies in a spring-loaded container that “pops” out the topmost candy in the stack when the top of the dispenser is lifted (see Figure 6.1). Figure 6.1: A schematic drawing of a PEZ ® dispenser; a physical implementation of the stack ADT. (PEZ ® is a registered trademark of PEZ Candy, Inc.) Stacks are a fundamental data structure. They are used in many applications, including the following. Example 6.1: Internet Web browsers store the addresses of recently visited sites on a stack. Each time a user visits a new site, that site’s address is “pushed” onto the stack of addresses. The browser then allows the user to “pop” back to previously visited sites using the “back” button. Example 6.2: Text editors usually provide an “undo” mechanism that cancels recent editing operations and reverts to former states of a document. This undo operation can be accomplished by keeping text changes in a stack.

6.1. Stacks 6.1.1 The Stack Abstract Data Type Stacks are the simplest of all data structures, yet they are also among the most important, as they are used in a host of different applications, and as a tool for many more sophisticated data structures and algorithms. Formally, a stack is an abstract data type (ADT) that supports the following two update methods: push(e): Adds element e to the top of the stack. pop(): Removes and returns the top element from the stack (or null if the stack is empty). Additionally, a stack supports the following accessor methods for convenience: top(): Returns the top element of the stack, without removing it (or null if the stack is empty). size(): Returns the number of elements in the stack. isEmpty(): Returns a boolean indicating whether the stack is empty. By convention, we assume that elements added to the stack can have arbitrary type and that a newly created stack is empty. Example 6.3: The following table shows a series of stack operations and their effects on an initially empty stack S of integers. Method Return Value Stack Contents push(5) – (5) push(3) – (5, 3) size() (5, 3) pop() (5) isEmpty() false (5) pop() () isEmpty() true () pop() null () push(7) – (7) push(9) – (7, 9) top() (7, 9) push(4) – (7, 9, 4) size() (7, 9, 4) pop() (7, 9) push(6) – (7, 9, 6) push(8) – (7, 9, 6, 8) pop() (7, 9, 6)

Chapter 6. Stacks, Queues, and Deques A Stack Interface in Java In order to formalize our abstraction of a stack, we deﬁne what is known as its application programming interface (API) in the form of a Java interface, which describes the names of the methods that the ADT supports and how they are to be declared and used. This interface is deﬁned in Code Fragment 6.1. We rely on Java’s generics framework (described in Section 2.5.2), allowing the elements stored in the stack to belong to any object type <E>. For example, a variable representing a stack of integers could be declared with type Stack<Integer>. The formal type parameter is used as the parameter type for the push method, and the return type for both pop and top. Recall, from the discussion of Java interfaces in Section 2.3.1, that the interface serves as a type deﬁnition but that it cannot be directly instantiated. For the ADT to be of any use, we must provide one or more concrete classes that implement the methods of the interface associated with that ADT. In the following subsections, we will give two such implementations of the Stack interface: one that uses an array for storage and another that uses a linked list. The java.util.Stack Class Because of the importance of the stack ADT, Java has included, since its original version, a concrete class named java.util.Stack that implements the LIFO semantics of a stack. However, Java’s Stack class remains only for historic reasons, and its interface is not consistent with most other data structures in the Java library. In fact, the current documentation for the Stack class recommends that it not be used, as LIFO functionality (and more) is provided by a more general data structure known as a double-ended queue (which we describe in Section 6.3). For the sake of comparison, Table 6.1 provides a side-by-side comparison of the interface for our stack ADT and the java.util.Stack class. In addition to some differences in method names, we note that methods pop and peek of the java.util.Stack class throw a custom EmptyStackException if called when the stack is empty (whereas null is returned in our abstraction). Our Stack ADT Class java.util.Stack size() size() ⇐ isEmpty() empty() push(e) push(e) pop() pop() ⇐ top() peek() Table 6.1: Methods of our stack ADT and corresponding methods of the class java.util.Stack, with differences highlighted in the right margin.

6.1. Stacks /∗∗ ∗A collection of objects that are inserted and removed according to the last-in ∗ﬁrst-out principle. Although similar in purpose, this interface diﬀers from ∗java.util.Stack. ∗ ∗@author Michael T. Goodrich ∗@author Roberto Tamassia ∗@author Michael H. Goldwasser ∗/ public interface Stack<E> { /∗∗ ∗Returns the number of elements in the stack. ∗@return number of elements in the stack ∗/ int size(); /∗∗ ∗Tests whether the stack is empty. ∗@return true if the stack is empty, false otherwise ∗/ boolean isEmpty(); /∗∗ ∗Inserts an element at the top of the stack. ∗@param e the element to be inserted ∗/ void push(E e); /∗∗ ∗Returns, but does not remove, the element at the top of the stack. ∗@return top element in the stack (or null if empty) ∗/ E top(); /∗∗ ∗Removes and returns the top element from the stack. ∗@return element removed (or null if empty) ∗/ E pop(); } Code Fragment 6.1: Interface Stack documented with comments in Javadoc style (Section 1.9.4). Note also the use of the generic parameterized type, E, which allows a stack to contain elements of any speciﬁed (reference) type.

Chapter 6. Stacks, Queues, and Deques 6.1.2 A Simple Array-Based Stack Implementation As our ﬁrst implementation of the stack ADT, we store elements in an array, named data, with capacity N for some ﬁxed N. We oriented the stack so that the bottom element of the stack is always stored in cell data[0], and the top element of the stack in cell data[t] for index t that is equal to one less than the current size of the stack. (See Figure 6.2.) B D E F G K L M A C t N−1 data: Figure 6.2: Representing a stack with an array; the top element is in cell data[t]. Recalling that arrays start at index 0 in Java, when the stack holds elements from data[0] to data[t] inclusive, it has size t +1. By convention, when the stack is empty it will have t equal to −1 (and thus has size t + 1, which is 0). A complete Java implementation based on this strategy is given in Code Fragment 6.2 (with Javadoc comments omitted due to space considerations). public class ArrayStack<E> implements Stack<E> { public static ﬁnal int CAPACITY=1000; // default array capacity private E[ ] data; // generic array used for storage private int t = −1; // index of the top element in stack public ArrayStack() { this(CAPACITY); } // constructs stack with default capacity public ArrayStack(int capacity) { // constructs stack with given capacity data = (E[ ]) new Object[capacity]; // safe cast; compiler may give warning } public int size() { return (t + 1); } public boolean isEmpty() { return (t == −1); } public void push(E e) throws IllegalStateException { if (size() == data.length) throw new IllegalStateException("Stack is full"); data[++t] = e; // increment t before storing new item } public E top() { if (isEmpty()) return null; return data[t]; } public E pop() { if (isEmpty()) return null; E answer = data[t]; data[t] = null; // dereference to help garbage collection t−−; return answer; } } Code Fragment 6.2: Array-based implementation of the Stack interface.

6.1. Stacks A Drawback of This Array-Based Stack Implementation The array implementation of a stack is simple and efﬁcient. Nevertheless, this implementation has one negative aspect—it relies on a ﬁxed-capacity array, which limits the ultimate size of the stack. For convenience, we allow the user of a stack to specify the capacity as a parameter to the constructor (and offer a default constructor that uses capacity of 1,000). In cases where a user has a good estimate on the number of items needing to go in the stack, the array-based implementation is hard to beat. However, if the estimate is wrong, there can be grave consequences. If the application needs much less space than the reserved capacity, memory is wasted. Worse yet, if an attempt is made to push an item onto a stack that has already reached its maximum capacity, the implementation of Code Fragment 6.2 throws an IllegalStateException, refusing to store the new element. Thus, even with its simplicity and efﬁciency, the array-based stack implementation is not necessarily ideal. Fortunately, we will later demonstrate two approaches for implementing a stack without such a size limitation and with space always proportional to the actual number of elements stored in the stack. One approach, given in the next subsection uses a singly linked list for storage; in Section 7.2.1, we will provide a more advanced array-based approach that overcomes the limit of a ﬁxed capacity. Analyzing the Array-Based Stack Implementation The correctness of the methods in the array-based implementation follows from our deﬁnition of index t. Note well that when pushing an element, t is incremented before placing the new element, so that it uses the ﬁrst available cell. Table 6.2 shows the running times for methods of this array-based stack implementation. Each method executes a constant number of statements involving arithmetic operations, comparisons, and assignments, or calls to size and isEmpty, which both run in constant time. Thus, in this implementation of the stack ADT, each method runs in constant time, that is, they each run in O(1) time. Method Running Time size O(1) isEmpty O(1) top O(1) push O(1) pop O(1) Table 6.2: Performance of a stack realized by an array. The space usage is O(N), where N is the size of the array, determined at the time the stack is instantiated, and independent from the number n ≤N of elements that are actually in the stack.

Chapter 6. Stacks, Queues, and Deques Garbage Collection in Java We wish to draw attention to one interesting aspect involving the implementation of the pop method in Code Fragment 6.2. We set a local variable, answer, to reference the element that is being popped, and then we intentionally reset data[t] to null at line 22, before decrementing t. The assignment to null was not technically required, as our stack would still operate correctly without it. Our reason for returning the cell to a null reference is to assist Java’s garbage collection mechanism, which searches memory for objects that are no longer actively referenced and reclaims their space for future use. (For more details, see Section 15.1.3.) If we continued to store a reference to the popped element in our array, the stack class would ignore it (eventually overwriting the reference if more elements get added to the stack). But, if there were no other active references to the element in the user’s application, that spurious reference in the stack’s array would stop Java’s garbage collector from reclaiming the element. Sample Usage We conclude this section by providing a demonstration of code that creates and uses an instance of the ArrayStack class. In this example, we declare the parameterized type of the stack as the Integer wrapper class. This causes the signature of the push method to accept an Integer instance as a parameter, and for the return type of both top and pop to be an Integer. Of course, with Java’s autoboxing and unboxing (see Section 1.3), a primitive int can be sent as a parameter to push. Stack<Integer> S = new ArrayStack<>(); // contents: () S.push(5); // contents: (5) S.push(3); // contents: (5, 3) System.out.println(S.size()); // contents: (5, 3) outputs 2 System.out.println(S.pop()); // contents: (5) outputs 3 System.out.println(S.isEmpty()); // contents: (5) outputs false System.out.println(S.pop()); // contents: () outputs 5 System.out.println(S.isEmpty()); // contents: () outputs true System.out.println(S.pop()); // contents: () outputs null S.push(7); // contents: (7) S.push(9); // contents: (7, 9) System.out.println(S.top()); // contents: (7, 9) outputs 9 S.push(4); // contents: (7, 9, 4) System.out.println(S.size()); // contents: (7, 9, 4) outputs 3 System.out.println(S.pop()); // contents: (7, 9) outputs 4 S.push(6); // contents: (7, 9, 6) S.push(8); // contents: (7, 9, 6, 8) System.out.println(S.pop()); // contents: (7, 9, 6) outputs 8 Code Fragment 6.3: Sample usage of our ArrayStack class.

6.1. Stacks 6.1.3 Implementing a Stack with a Singly Linked List In this section, we demonstrate how the Stack interface can be easily implemented using a singly linked list for storage. Unlike our array-based implementation, the linked-list approach has memory usage that is always proportional to the number of actual elements currently in the stack, and without an arbitrary capacity limit. In designing such an implementation, we need to decide if the top of the stack is at the front or back of the list. There is clearly a best choice here, however, since we can insert and delete elements in constant time only at the front. With the top of the stack stored at the front of the list, all methods execute in constant time. The Adapter Pattern The adapter design pattern applies to any context where we effectively want to modify an existing class so that its methods match those of a related, but different, class or interface. One general way to apply the adapter pattern is to deﬁne a new class in such a way that it contains an instance of the existing class as a hidden ﬁeld, and then to implement each method of the new class using methods of this hidden instance variable. By applying the adapter pattern in this way, we have created a new class that performs some of the same functions as an existing class, but repackaged in a more convenient way. In the context of the stack ADT, we can adapt our SinglyLinkedList class of Section 3.2.1 to deﬁne a new LinkedStack class, shown in Code Fragment 6.4. This class declares a SinglyLinkedList named list as a private ﬁeld, and uses the following correspondences: Stack Method Singly Linked List Method size() list.size() isEmpty() list.isEmpty() push(e) list.addFirst(e) pop() list.removeFirst() top() list.ﬁrst() public class LinkedStack<E> implements Stack<E> { private SinglyLinkedList<E> list = new SinglyLinkedList<>(); // an empty list public LinkedStack() { } // new stack relies on the initially empty list public int size() { return list.size(); } public boolean isEmpty() { return list.isEmpty(); } public void push(E element) { list.addFirst(element); } public E top() { return list.ﬁrst(); } public E pop() { return list.removeFirst(); } } Code Fragment 6.4: Implementation of a Stack using a SinglyLinkedList as storage.

Chapter 6. Stacks, Queues, and Deques 6.1.4 Reversing an Array Using a Stack As a consequence of the LIFO protocol, a stack can be used as a general toll to reverse a data sequence. For example, if the values 1, 2, and 3 are pushed onto a stack in that order, they will be popped from the stack in the order 3, 2, and then 1. We demonstrate this concept by revisiting the problem of reversing the elements of an array. (We provided a recursive algorithm for this task in Section 5.3.1.) We create an empty stack for auxiliary storage, push all of the array elements onto the stack, and then pop those elements off of the stack while overwriting the cells of the array from beginning to end. In Code Fragment 6.5, we give a Java implementation of this algorithm. We show an example use of this method in Code Fragment 6.6. /∗∗A generic method for reversing an array. ∗/ public static <E> void reverse(E[ ] a) { Stack<E> buﬀer = new ArrayStack<>(a.length); for (int i=0; i < a.length; i++) buﬀer.push(a[i]); for (int i=0; i < a.length; i++) a[i] = buﬀer.pop(); } Code Fragment 6.5: A generic method that reverses the elements in an array with objects of type E, using a stack declared with the interface Stack<E> as its type. /∗∗Tester routine for reversing arrays ∗/ public static void main(String args[ ]) { Integer[ ] a = {4, 8, 15, 16, 23, 42}; // autoboxing allows this String[ ] s = {"Jack", "Kate", "Hurley", "Jin", "Michael"}; System.out.println("a = " + Arrays.toString(a)); System.out.println("s = " + Arrays.toString(s)); System.out.println("Reversing..."); reverse(a); reverse(s); System.out.println("a = " + Arrays.toString(a)); System.out.println("s = " + Arrays.toString(s)); } The output from this method is the following: a = [4, 8, 15, 16, 23, 42] s = [Jack, Kate, Hurley, Jin, Michael] Reversing... a = [42, 23, 16, 15, 8, 4] s = [Michael, Jin, Hurley, Kate, Jack] Code Fragment 6.6: A test of the reverse method using two arrays.

6.1. Stacks 6.1.5 Matching Parentheses and HTML Tags In this subsection, we explore two related applications of stacks, both of which involve testing for pairs of matching delimiters. In our ﬁrst application, we consider arithmetic expressions that may contain various pairs of grouping symbols, such as • Parentheses: “(” and “)” • Braces: “{” and “}” • Brackets: “[” and “]” Each opening symbol must match its corresponding closing symbol. For example, a left bracket, “[,” must match a corresponding right bracket, “],” as in the following expression [(5+x)−(y+z)]. The following examples further illustrate this concept: • Correct: ()(()){([()])} • Correct: ((()(()){([()])})) • Incorrect: )(()){([()])} • Incorrect: ({[])} • Incorrect: ( We leave the precise deﬁnition of a matching group of symbols to Exercise R-6.6. An Algorithm for Matching Delimiters An important task when processing arithmetic expressions is to make sure their delimiting symbols match up correctly. We can use a stack to perform this task with a single left-to-right scan of the original string. Each time we encounter an opening symbol, we push that symbol onto the stack, and each time we encounter a closing symbol, we pop a symbol from the stack (assuming it is not empty) and check that these two symbols form a valid pair. If we reach the end of the expression and the stack is empty, then the original expression was properly matched. Otherwise, there must be an opening delimiter on the stack without a matching symbol. If the length of the original expression is n, the algorithm will make at most n calls to push and n calls to pop. Code Fragment 6.7 presents a Java implementation of such an algorithm. It speciﬁcally checks for delimiter pairs ( ), { }, and [ ], but could easily be changed to accommodate further symbols. Speciﬁcally, we deﬁne two ﬁxed strings, "({[" and ")}]", that are intentionally coordinated to reﬂect the symbol pairs. When examining a character of the expression string, we call the indexOf method of the String class on these special strings to determine if the character matches a delimiter and, if so, which one. Method indexOf returns the the index at which a given character is ﬁrst found in a string (or −1 if the character is not found).

Chapter 6. Stacks, Queues, and Deques /∗∗Tests if delimiters in the given expression are properly matched. ∗/ public static boolean isMatched(String expression) { ﬁnal String opening = "({["; // opening delimiters ﬁnal String closing = ")}]"; // respective closing delimiters Stack<Character> buﬀer = new LinkedStack<>(); for (char c : expression.toCharArray()) { if (opening.indexOf(c) != −1) // this is a left delimiter buﬀer.push(c); else if (closing.indexOf(c) != −1) { // this is a right delimiter if (buﬀer.isEmpty()) // nothing to match with return false; if (closing.indexOf(c) != opening.indexOf(buﬀer.pop())) return false; // mismatched delimiter } } return buﬀer.isEmpty(); // were all opening delimiters matched? } Code Fragment 6.7: Method for matching delimiters in an arithmetic expression. Matching Tags in a Markup Language Another application of matching delimiters is in the validation of markup languages such as HTML or XML. HTML is the standard format for hyperlinked documents on the Internet and XML is an extensible markup language used for a variety of structured data sets. We show a sample HTML document in Figure 6.3. <body> <center> <h1> The Little Boat </h1> </center> <p> The storm tossed the little boat like a cheap sneaker in an old washing machine. The three drunken fishermen were used to such treatment, of course, but not the tree salesman, who even as a stowaway now felt that he had overpaid for the voyage. </p> <ol> <li> Will the salesman die? </li> <li> What color is the boat? </li> <li> And what about Naomi? </li> </ol> </body> The Little Boat The storm tossed the little boat like a cheap sneaker in an old washing machine. The three drunken ﬁshermen were used to such treatment, of course, but not the tree salesman, who even as a stowaway now felt that he had overpaid for the voyage. 1. Will the salesman die? 2. What color is the boat? 3. And what about Naomi? (a) (b) Figure 6.3: Illustrating (a) an HTML document and (b) its rendering.

6.1. Stacks In an HTML document, portions of text are delimited by HTML tags. A simple opening HTML tag has the form “<name>” and the corresponding closing tag has the form “</name>”. For example, we see the <body> tag on the ﬁrst line of Figure 6.3a, and the matching </body> tag at the close of that document. Other commonly used HTML tags that are used in this example include: • <body>: document body • <h1>: section header • <center>: center justify • <p>: paragraph • <ol>: numbered (ordered) list • <li>: list item Ideally, an HTML document should have matching tags, although most browsers tolerate a certain number of mismatching tags. In Code Fragment 6.8, we give a Java method that matches tags in a string representing an HTML document. We make a left-to-right pass through the raw string, using index j to track our progress. The indexOf method of the String class, which optionally accepts a starting index as a second parameter, locates the '<' and '>' characters that deﬁne the tags. Method substring, also of the String class, returns the substring starting at a given index and optionally ending right before another given index. Opening tags are pushed onto the stack, and matched against closing tags as they are popped from the stack, just as we did when matching delimiters in Code Fragment 6.7. /∗∗Tests if every opening tag has a matching closing tag in HTML string. ∗/ public static boolean isHTMLMatched(String html) { Stack<String> buﬀer = new LinkedStack<>(); int j = html.indexOf('<'); // ﬁnd ﬁrst ’<’ character (if any) while (j != −1) { int k = html.indexOf('>', j+1); // ﬁnd next ’>’ character if (k == −1) return false; // invalid tag String tag = html.substring(j+1, k); // strip away < > if (!tag.startsWith("/")) // this is an opening tag buﬀer.push(tag); else { // this is a closing tag if (buﬀer.isEmpty()) return false; // no tag to match if (!tag.substring(1).equals(buﬀer.pop())) return false; // mismatched tag } j = html.indexOf('<', k+1); // ﬁnd next ’<’ character (if any) } return buﬀer.isEmpty(); // were all opening tags matched? } Code Fragment 6.8: Method for testing if an HTML document has matching tags.

Chapter 6. Stacks, Queues, and Deques 6.2 Queues Another fundamental data structure is the queue. It is a close “cousin” of the stack, but a queue is a collection of objects that are inserted and removed according to the ﬁrst-in, ﬁrst-out (FIFO) principle. That is, elements can be inserted at any time, but only the element that has been in the queue the longest can be next removed. We usually say that elements enter a queue at the back and are removed from the front. A metaphor for this terminology is a line of people waiting to get on an amusement park ride. People waiting for such a ride enter at the back of the line and get on the ride from the front of the line. There are many other applications of queues (see Figure 6.4). Stores, theaters, reservation centers, and other similar services typically process customer requests according to the FIFO principle. A queue would therefore be a logical choice for a data structure to handle calls to a customer service center, or a wait-list at a restaurant. FIFO queues are also used by many computing devices, such as a networked printer, or a Web server responding to requests. Tickets (a) Call Center Call Queue (b) Figure 6.4: Real-world examples of a ﬁrst-in, ﬁrst-out queue. (a) People waiting in line to purchase tickets; (b) phone calls being routed to a customer service center.

6.2. Queues 6.2.1 The Queue Abstract Data Type Formally, the queue abstract data type deﬁnes a collection that keeps objects in a sequence, where element access and deletion are restricted to the ﬁrst element in the queue, and element insertion is restricted to the back of the sequence. This restriction enforces the rule that items are inserted and deleted in a queue according to the ﬁrst-in, ﬁrst-out (FIFO) principle. The queue abstract data type (ADT) supports the following two update methods: enqueue(e): Adds element e to the back of queue. dequeue(): Removes and returns the ﬁrst element from the queue (or null if the queue is empty). The queue ADT also includes the following accessor methods (with ﬁrst being analogous to the stack’s top method): ﬁrst(): Returns the ﬁrst element of the queue, without removing it (or null if the queue is empty). size(): Returns the number of elements in the queue. isEmpty(): Returns a boolean indicating whether the queue is empty. By convention, we assume that elements added to the queue can have arbitrary type and that a newly created queue is empty. We formalize the queue ADT with the Java interface shown in Code Fragment 6.9. public interface Queue<E> { /∗∗Returns the number of elements in the queue. ∗/ int size(); /∗∗Tests whether the queue is empty. ∗/ boolean isEmpty(); /∗∗Inserts an element at the rear of the queue. ∗/ void enqueue(E e); /∗∗Returns, but does not remove, the ﬁrst element of the queue (null if empty). ∗/ E ﬁrst(); /∗∗Removes and returns the ﬁrst element of the queue (null if empty). ∗/ E dequeue(); } Code Fragment 6.9: A Queue interface deﬁning the queue ADT, with a standard FIFO protocol for insertions and removals.

Chapter 6. Stacks, Queues, and Deques Example 6.4: The following table shows a series of queue operations and their effects on an initially empty queue Q of integers. Method Return Value ﬁrst ←Q ←last enqueue(5) – (5) enqueue(3) – (5, 3) size() (5, 3) dequeue() (3) isEmpty() false (3) dequeue() () isEmpty() true () dequeue() null () enqueue(7) – (7) enqueue(9) – (7, 9) ﬁrst() (7, 9) enqueue(4) – (7, 9, 4) The java.util.Queue Interface in Java Java provides a type of queue interface, java.util.Queue, which has functionality similar to the traditional queue ADT, given above, but the documentation for the java.util.Queue interface does not insist that it support only the FIFO principle. When supporting the FIFO principle, the methods of the java.util.Queue interface have the equivalences with the queue ADT shown in Table 6.3. The java.util.Queue interface supports two styles for most operations, which vary in the way that they treat exceptional cases. When a queue is empty, the remove() and element() methods throw a NoSuchElementException, while the corresponding methods poll() and peek() return null. For implementations with a bounded capacity, the add method will throw an IllegalStateException when full, while the oﬀer method ignores the new element and returns false to signal that the element was not accepted. Our Queue ADT Interface java.util.Queue throws exceptions returns special value enqueue(e) add(e) oﬀer(e) dequeue() remove() poll() ﬁrst() element() peek() size() size() isEmpty() isEmpty() Table 6.3: Methods of the queue ADT and corresponding methods of the interface java.util.Queue, when supporting the FIFO principle.

6.2. Queues 6.2.2 Array-Based Queue Implementation In Section 6.1.2, we implemented the LIFO semantics of the Stack ADT using an array (albeit, with a ﬁxed capacity), such that every operation executes in constant time. In this section, we will consider how to use an array to efﬁciently support the FIFO semantics of the Queue ADT. Let’s assume that as elements are inserted into a queue, we store them in an array such that the ﬁrst element is at index 0, the second element at index 1, and so on. (See Figure 6.5.) A C D E F G K L M B N−1 data: Figure 6.5: Using an array to store elements of a queue, such that the ﬁrst element inserted, “A”, is at cell 0, the second element inserted, “B”, at cell 1, and so on. With such a convention, the question is how we should implement the dequeue operation. The element to be removed is stored at index 0 of the array. One strategy is to execute a loop to shift all other elements of the queue one cell to the left, so that the front of the queue is again aligned with cell 0 of the array. Unfortunately, the use of such a loop would result in an O(n) running time for the dequeue method. We can improve on the above strategy by avoiding the loop entirely. We will replace a dequeued element in the array with a null reference, and maintain an explicit variable f to represent the index of the element that is currently at the front of the queue. Such an algorithm for dequeue would run in O(1) time. After several dequeue operations, this approach might lead to the conﬁguration portrayed in Figure 6.6. K L M F G f N−1 data: Figure 6.6: Allowing the front of the queue to drift away from index 0. In this representation, index f denotes the location of the front of the queue. However, there remains a challenge with the revised approach. With an array of capacity N, we should be able to store up to N elements before reaching any exceptional case. If we repeatedly let the front of the queue drift rightward over time, the back of the queue would reach the end of the underlying array even when there are fewer than N elements currently in the queue. We must decide how to store additional elements in such a conﬁguration.

Chapter 6. Stacks, Queues, and Deques Using an Array Circularly In developing a robust queue implementation, we allow both the front and back of the queue to drift rightward, with the contents of the queue “wrapping around” the end of an array, as necessary. Assuming that the array has ﬁxed length N, new elements are enqueued toward the “end” of the current queue, progressing from the front to index N −1 and continuing at index 0, then 1. Figure 6.7 illustrates such a queue with ﬁrst element F and last element R. M N O P Q R F G L K N−1 f data: Figure 6.7: Modeling a queue with a circular array that wraps around the end. Implementing such a circular view is relatively easy with the modulo operator, denoted with the symbol % in Java. Recall that the modulo operator is computed by taking the remainder after an integral division. For example, 14 divided by 3 has a quotient of 4 with remainder 2, that is, 14 3 = 42 3. So in Java, 14 / 3 evaluates to the quotient 4, while 14 % 3 evaluates to the remainder 2. The modulo operator is ideal for treating an array circularly. When we dequeue an element and want to “advance” the front index, we use the arithmetic f = ( f +1) % N. As a concrete example, if we have an array of length 10, and a front index 7, we can advance the front by formally computing (7+1) % 10, which is simply 8, as 8 divided by 10 is 0 with a remainder of 8. Similarly, advancing index 8 results in index 9. But when we advance from index 9 (the last one in the array), we compute (9+1) % 10, which evaluates to index 0 (as 10 divided by 10 has a remainder of zero). A Java Queue Implementation A complete implementation of a queue ADT using an array in circular fashion is presented in Code Fragment 6.10. Internally, the queue class maintains the following three instance variables: data: a reference to the underlying array. f: an integer that represents the index, within array data, of the ﬁrst element of the queue (assuming the queue is not empty). sz: an integer representing the current number of elements stored in the queue (not to be confused with the length of the array). We allow the user to specify the capacity of the queue as an optional parameter to the constructor. The implementations of methods size and isEmpty are trivial, given the sz ﬁeld, and the implementation of ﬁrst is simple, given index f. A discussion of update methods enqueue and dequeue follows the presentation of the code.

6.2. Queues /∗∗Implementation of the queue ADT using a ﬁxed-length array. ∗/ public class ArrayQueue<E> implements Queue<E> { // instance variables private E[ ] data; // generic array used for storage private int f = 0; // index of the front element private int sz = 0; // current number of elements // constructors public ArrayQueue() {this(CAPACITY);} // constructs queue with default capacity public ArrayQueue(int capacity) { // constructs queue with given capacity data = (E[ ]) new Object[capacity]; // safe cast; compiler may give warning } // methods /∗∗Returns the number of elements in the queue. ∗/ public int size() { return sz; } /∗∗Tests whether the queue is empty. ∗/ public boolean isEmpty() { return (sz == 0); } /∗∗Inserts an element at the rear of the queue. ∗/ public void enqueue(E e) throws IllegalStateException { if (sz == data.length) throw new IllegalStateException("Queue is full"); int avail = (f + sz) % data.length; // use modular arithmetic data[avail] = e; sz++; } /∗∗Returns, but does not remove, the ﬁrst element of the queue (null if empty). ∗/ public E ﬁrst() { if (isEmpty()) return null; return data[f]; } /∗∗Removes and returns the ﬁrst element of the queue (null if empty). ∗/ public E dequeue() { if (isEmpty()) return null; E answer = data[f]; data[f] = null; // dereference to help garbage collection f = (f + 1) % data.length; sz−−; return answer; } Code Fragment 6.10: Array-based implementation of a queue.

Chapter 6. Stacks, Queues, and Deques Adding and Removing Elements The goal of the enqueue method is to add a new element to the back of the queue. We need to determine the proper index at which to place the new element. Although we do not explicitly maintain an instance variable for the back of the queue, we compute the index of the next opening based on the formula: avail = (f + sz) % data.length; Note that we are using the size of the queue as it exists prior to the addition of the new element. As a sanity check, for a queue with capacity 10, current size 3, and ﬁrst element at index 5, its three elements are stored at indices 5, 6, and 7, and the next element should be added at index 8, computed as (5+3) % 10. As a case with wraparound, if the queue has capacity 10, current size 3, and ﬁrst element at index 8, its three elements are stored at indices 8, 9, and 0, and the next element should be added at index 1, computed as (8+3) % 10. When the dequeue method is called, the current value of f designates the index of the value that is to be removed and returned. We keep a local reference to the element that will be returned, before setting its cell of the array back to null, to aid the garbage collector. Then the index f is updated to reﬂect the removal of the ﬁrst element, and the presumed promotion of the second element to become the new ﬁrst. In most cases, we simply want to increment the index by one, but because of the possibility of a wraparound conﬁguration, we rely on modular arithmetic, computing f = (f+1) % data.length, as originally described on page 242. Analyzing the Eﬃciency of an Array-Based Queue Table 6.4 shows the running times of methods in a realization of a queue by an array. As with our array-based stack implementation, each of the queue methods in the array realization executes a constant number of statements involving arithmetic operations, comparisons, and assignments. Thus, each method in this implementation runs in O(1) time. Method Running Time size O(1) isEmpty O(1) ﬁrst O(1) enqueue O(1) dequeue O(1) Table 6.4: Performance of a queue realized by an array. The space usage is O(N), where N is the size of the array, determined at the time the queue is created, and independent from the number n < N of elements that are actually in the queue.

6.2. Queues 6.2.3 Implementing a Queue with a Singly Linked List As we did for the stack ADT, we can easily adapt a singly linked list to implement the queue ADT while supporting worst-case O(1)-time for all operations, and without any artiﬁcial limit on the capacity. The natural orientation for a queue is to align the front of the queue with the front of the list, and the back of the queue with the tail of the list, because the only update operation that singly linked lists support at the back end is an insertion. Our Java implementation of a LinkedQueue class is given in Code 6.11. /∗∗Realization of a FIFO queue as an adaptation of a SinglyLinkedList. ∗/ public class LinkedQueue<E> implements Queue<E> { private SinglyLinkedList<E> list = new SinglyLinkedList<>(); // an empty list public LinkedQueue() { } // new queue relies on the initially empty list public int size() { return list.size(); } public boolean isEmpty() { return list.isEmpty(); } public void enqueue(E element) { list.addLast(element); } public E ﬁrst() { return list.ﬁrst(); } public E dequeue() { return list.removeFirst(); } } Code Fragment 6.11: Implementation of a Queue using a SinglyLinkedList. Analyzing the Eﬃciency of a Linked Queue Although we had not yet introduced asymptotic analysis when we presented our SinglyLinkedList implementation in Chapter 3, it is clear upon reexamination that each method of that class runs in O(1) worst-case time. Therefore, each method of our LinkedQueue adaptation also runs in O(1) worst-case time. We also avoid the need to specify a maximum size for the queue, as was done in the array-based queue implementation. However, this beneﬁt comes with some expense. Because each node stores a next reference, in addition to the element reference, a linked list uses more space per element than a properly sized array of references. Also, although all methods execute in constant time for both implementations, it seems clear that the operations involving linked lists have a large number of primitive operations per call. For example, adding an element to an array-based queue consists primarily of calculating an index with modular arithmetic, storing the element in the array cell, and incrementing the size counter. For a linked list, an insertion includes the instantiation and initialization of a new node, relinking an existing node to the new node, and incrementing the size counter. In practice, this makes the linked-list method more expensive than the array-based method.

Chapter 6. Stacks, Queues, and Deques 6.2.4 A Circular Queue In Section 3.3, we implemented a circularly linked list class that supports all behaviors of a singly linked list, and an additional rotate() method that efﬁciently moves the ﬁrst element to the end of the list. We can generalize the Queue interface to deﬁne a new CircularQueue interface with such a behavior, as shown in Code Fragment 6.12. public interface CircularQueue<E> extends Queue<E> { /∗∗ ∗Rotates the front element of the queue to the back of the queue. ∗This does nothing if the queue is empty. ∗/ void rotate(); } Code Fragment 6.12: A Java interface, CircularQueue, that extends the Queue ADT with a new rotate() method. This interface can easily be implemented by adapting the CircularlyLinkedList class of Section 3.3 to produce a new LinkedCircularQueue class. This class has an advantage over the traditional LinkedQueue, because a call to Q.rotate() is implemented more efﬁciently than the combination of calls, Q.enqueue(Q.dequeue()), because no nodes are created, destroyed, or relinked by the implementation of a rotate operation on a circularly linked list. A circular queue is an excellent abstraction for applications in which elements are cyclically arranged, such as for multiplayer, turn-based games, or round-robin scheduling of computing processes. In the remainder of this section, we provide a demonstration of the use of a circular queue. The Josephus Problem In the children’s game “hot potato,” a group of n children sit in a circle passing an object, called the “potato,” around the circle. The potato begins with a starting child in the circle, and the children continue passing the potato until a leader rings a bell, at which point the child holding the potato must leave the game after handing the potato to the next child in the circle. After the selected child leaves, the other children close up the circle. This process is then continued until there is only one child remaining, who is declared the winner. If the leader always uses the strategy of ringing the bell so that every k th person is removed from the circle, for some ﬁxed value k, then determining the winner for a given list of children is known as the Josephus problem (named after an ancient story with far more severe consequences than in the children’s game).

6.2. Queues Solving the Josephus Problem Using a Queue We can solve the Josephus problem for a collection of n elements using a circular queue, by associating the potato with the element at the front of the queue and storing elements in the queue according to their order around the circle. Thus, passing the potato is equivalent to rotating the ﬁrst element to the back of the queue. After this process has been performed k −1 times, we remove the front element by dequeuing it from the queue and discarding it. We show a complete Java program for solving the Josephus problem using this approach in Code Fragment 6.13, which describes a solution that runs in O(nk) time. (We can solve this problem faster using techniques beyond the scope of this book.) public class Josephus { /∗∗Computes the winner of the Josephus problem using a circular queue. ∗/ public static <E> E Josephus(CircularQueue<E> queue, int k) { if (queue.isEmpty()) return null; while (queue.size() > 1) { for (int i=0; i < k−1; i++) // skip past k-1 elements queue.rotate(); E e = queue.dequeue(); // remove the front element from the collection System.out.println(" " + e + " is out"); } return queue.dequeue(); // the winner } /∗∗Builds a circular queue from an array of objects. ∗/ public static <E> CircularQueue<E> buildQueue(E a[ ]) { CircularQueue<E> queue = new LinkedCircularQueue<>(); for (int i=0; i<a.length; i++) queue.enqueue(a[i]); return queue; } /∗∗Tester method ∗/ public static void main(String[ ] args) { String[ ] a1 = {"Alice", "Bob", "Cindy", "Doug", "Ed", "Fred"}; String[ ] a2 = {"Gene", "Hope", "Irene", "Jack", "Kim", "Lance"}; String[ ] a3 = {"Mike", "Roberto"}; System.out.println("First winner is " + Josephus(buildQueue(a1), 3)); System.out.println("Second winner is " + Josephus(buildQueue(a2), 10)); System.out.println("Third winner is " + Josephus(buildQueue(a3), 7)); } } Code Fragment 6.13: A complete Java program for solving the Josephus problem using a circular queue.

Chapter 6. Stacks, Queues, and Deques 6.3 Double-Ended Queues We next consider a queue-like data structure that supports insertion and deletion at both the front and the back of the queue. Such a structure is called a doubleended queue, or deque, which is usually pronounced “deck” to avoid confusion with the dequeue method of the regular queue ADT, which is pronounced like the abbreviation “D.Q.” The deque abstract data type is more general than both the stack and the queue ADTs. The extra generality can be useful in some applications. For example, we described a restaurant using a queue to maintain a waitlist. Occasionally, the ﬁrst person might be removed from the queue only to ﬁnd that a table was not available; typically, the restaurant will reinsert the person at the ﬁrst position in the queue. It may also be that a customer at the end of the queue may grow impatient and leave the restaurant. (We will need an even more general data structure if we want to model customers leaving the queue from other positions.) 6.3.1 The Deque Abstract Data Type The deque abstract data type is richer than both the stack and the queue ADTs. To provide a symmetrical abstraction, the deque ADT is deﬁned to support the following update methods: addFirst(e): Insert a new element e at the front of the deque. addLast(e): Insert a new element e at the back of the deque. removeFirst(): Remove and return the ﬁrst element of the deque (or null if the deque is empty). removeLast(): Remove and return the last element of the deque (or null if the deque is empty). Additionally, the deque ADT will include the following accessors: ﬁrst(): Returns the ﬁrst element of the deque, without removing it (or null if the deque is empty). last(): Returns the last element of the deque, without removing it (or null if the deque is empty). size(): Returns the number of elements in the deque. isEmpty(): Returns a boolean indicating whether the deque is empty. We formalize the deque ADT with the Java interface shown in Code Fragment 6.14.

6.3. Double-Ended Queues /∗∗ ∗Interface for a double-ended queue: a collection of elements that can be inserted ∗and removed at both ends; this interface is a simpliﬁed version of java.util.Deque. ∗/ public interface Deque<E> { /∗∗Returns the number of elements in the deque. ∗/ int size(); /∗∗Tests whether the deque is empty. ∗/ boolean isEmpty(); /∗∗Returns, but does not remove, the ﬁrst element of the deque (null if empty). ∗/ E ﬁrst(); /∗∗Returns, but does not remove, the last element of the deque (null if empty). ∗/ E last(); /∗∗Inserts an element at the front of the deque. ∗/ void addFirst(E e); /∗∗Inserts an element at the back of the deque. ∗/ void addLast(E e); /∗∗Removes and returns the ﬁrst element of the deque (null if empty). ∗/ E removeFirst(); /∗∗Removes and returns the last element of the deque (null if empty). ∗/ E removeLast(); } Code Fragment 6.14: A Java interface, Deque, describing the double-ended queue ADT. Note the use of the generic parameterized type, E, allowing a deque to contain elements of any speciﬁed class. Example 6.5: The following table shows a series of operations and their effects on an initially empty deque D of integers. Method Return Value D addLast(5) – (5) addFirst(3) – (3, 5) addFirst(7) – (7, 3, 5) ﬁrst() (7, 3, 5) removeLast() (7, 3) size() (7, 3) removeLast() (7) removeFirst() () addFirst(6) – (6) last() (6) addFirst(8) – (8, 6) isEmpty() false (8, 6) last() (8, 6)

Chapter 6. Stacks, Queues, and Deques 6.3.2 Implementing a Deque We can implement the deque ADT efﬁciently using either an array or a linked list for storing elements. Implementing a Deque with a Circular Array If using an array, we recommend a representation similar to the ArrayQueue class, treating the array in circular fashion and storing the index of the ﬁrst element and the current size of the deque as ﬁelds; the index of the last element can be calculated, as needed, using modular arithmetic. One extra concern is avoiding use of negative values with the modulo operator. When removing the ﬁrst element, the front index is advanced in circular fashion, with the assignment f = (f+1) % N. But when an element is inserted at the front, the ﬁrst index must effectively be decremented in circular fashion and it is a mistake to assign f = (f−1) % N. The problem is that when f is 0, the goal should be to “decrement” it to the other end of the array, and thus to index N−1. However, a calculation such as −1 % 10 in Java results in the value −1. A standard way to decrement an index circularly is instead to assign f = (f−1+N) % N. Adding the additional term of N before the modulus is calculated assures that the result is a positive value. We leave details of this approach to Exercise P-6.40. Implementing a Deque with a Doubly Linked List Because the deque requires insertion and removal at both ends, a doubly linked list is most appropriate for implementing all operations efﬁciently. In fact, the DoublyLinkedList class from Section 3.4.1 already implements the entire Deque interface; we simply need to add the declaration “implements Deque<E>” to that class deﬁnition in order to use it as a deque. Performance of the Deque Operations Table 6.5 shows the running times of methods for a deque implemented with a doubly linked list. Note that every method runs in O(1) time. Method Running Time size, isEmpty O(1) ﬁrst, last O(1) addFirst, addLast O(1) removeFirst, removeLast O(1) Table 6.5: Performance of a deque realized by either a circular array or a doubly linked list. The space usage for the array-based implementation is O(N), where N is the size of the array, while the space usage of the doubly linked list is O(n) where n < N is the actual number of elements in the deque.

6.3. Double-Ended Queues 6.3.3 Deques in the Java Collections Framework The Java Collections Framework includes its own deﬁnition of a deque, as the java.util.Deque interface, as well as several implementations of the interface including one based on use of a circular array (java.util.ArrayDeque) and one based on use of a doubly linked list (java.util.LinkedList). So, if we need to use a deque and would rather not implement one from scratch, we can simply use one of those built-in classes. As is the case with the java.util.Queue class (see page 240), the java.util.Deque provides duplicative methods that use different techniques to signal exceptional cases. A summary of those methods is given in Table 6.6. Our Deque ADT Interface java.util.Deque throws exceptions returns special value ﬁrst() getFirst() peekFirst() last() getLast() peekLast() addFirst(e) addFirst(e) oﬀerFirst(e) addLast(e) addLast(e) oﬀerLast(e) removeFirst() removeFirst() pollFirst() removeLast() removeLast() pollLast() size() size() isEmpty() isEmpty() Table 6.6: Methods of our deque ADT and the corresponding methods of the java.util.Deque interface. When attempting to access or remove the ﬁrst or last element of an empty deque, the methods in the middle column of Table 6.6—that is, getFirst(), getLast(), removeFirst(), and removeLast()—throw a NoSuchElementException. The methods in the rightmost column—that is, peekFirst(), peekLast(), pollFirst(), and pollLast()—simply return the null reference when a deque is empty. In similar manner, when attempting to add an element to an end of a deque with a capacity limit, the addFirst and addLast methods throw an exception, while the oﬀerFirst and oﬀerLast methods return false. The methods that handle bad situations more gracefully (i.e., without throwing exceptions) are useful in applications, known as producer-consumer scenarios, in which it is common for one component of software to look for an element that may have been placed in a queue by another program, or in which it is common to try to insert an item into a ﬁxed-sized buffer that might be full. However, having methods return null when empty are not appropriate for applications in which null might serve as an actual element of a queue.

Chapter 7. List and Iterator ADTs 7.1 The List ADT In Chapter 6, we introduced the stack, queue, and deque abstract data types, and discussed how either an array or a linked list could be used for storage in an efﬁcient concrete implementation of each. Each of those ADTs represents a linearly ordered sequence of elements. The deque is the most general of the three, yet even so, it only allows insertions and deletions at the front or back of a sequence. In this chapter, we explore several abstract data types that represent a linear sequence of elements, but with more general support for adding or removing elements at arbitrary positions. However, designing a single abstraction that is well suited for efﬁcient implementation with either an array or a linked list is challenging, given the very different nature of these two fundamental data structures. Locations within an array are easily described with an integer index. Recall that an index of an element e in a sequence is equal to the number of elements before e in that sequence. By this deﬁnition, the ﬁrst element of a sequence has index 0, and the last has index n−1, assuming that n denotes the total number of elements. The notion of an element’s index is well deﬁned for a linked list as well, although we will see that it is not as convenient of a notion, as there is no way to efﬁciently access an element at a given index without traversing a portion of the linked list that depends upon the magnitude of the index. With that said, Java deﬁnes a general interface, java.util.List, that includes the following index-based methods (and more): size(): Returns the number of elements in the list. isEmpty(): Returns a boolean indicating whether the list is empty. get(i): Returns the element of the list having index i; an error condition occurs if i is not in range [0,size() −1]. set(i, e): Replaces the element at index i with e, and returns the old element that was replaced; an error condition occurs if i is not in range [0,size()−1]. add(i, e): Inserts a new element e into the list so that it has index i, moving all subsequent elements one index later in the list; an error condition occurs if i is not in range [0,size()]. remove(i): Removes and returns the element at index i, moving all subsequent elements one index earlier in the list; an error condition occurs if i is not in range [0,size() −1]. We note that the index of an existing element may change over time, as other elements are added or removed in front of it. We also draw attention to the fact that the range of valid indices for the add method includes the current size of the list, in which case the new element becomes the last.

7.1. The List ADT Example 7.1 demonstrates a series of operations on a list instance, and Code Fragment 7.1 below provides a formal deﬁnition of our simpliﬁed version of the List interface; we use an IndexOutOfBoundsException to signal an invalid index argument. Example 7.1: We demonstrate operations on an initially empty list of characters. Method Return Value List Contents add(0, A) – (A) add(0, B) – (B, A) get(1) A (B, A) set(2, C) “error” (B, A) add(2, C) – (B, A, C) add(4, D) “error” (B, A, C) remove(1) A (B, C) add(1, D) – (B, D, C) add(1, E) – (B, E, D, C) get(4) “error” (B, E, D, C) add(4, F) – (B, E, D, C, F) set(2, G) D (B, E, G, C, F) get(2) G (B, E, G, C, F) /∗∗A simpliﬁed version of the java.util.List interface. ∗/ public interface List<E> { /∗∗Returns the number of elements in this list. ∗/ int size(); /∗∗Returns whether the list is empty. ∗/ boolean isEmpty(); /∗∗Returns (but does not remove) the element at index i. ∗/ E get(int i) throws IndexOutOfBoundsException; /∗∗Replaces the element at index i with e, and returns the replaced element. ∗/ E set(int i, E e) throws IndexOutOfBoundsException; /∗∗Inserts element e to be at index i, shifting all subsequent elements later. ∗/ void add(int i, E e) throws IndexOutOfBoundsException; /∗∗Removes/returns the element at index i, shifting subsequent elements earlier. ∗/ E remove(int i) throws IndexOutOfBoundsException; } Code Fragment 7.1: A simple version of the List interface.

Chapter 7. List and Iterator ADTs 7.2 Array Lists An obvious choice for implementing the list ADT is to use an array A, where A[i] stores (a reference to) the element with index i. We will begin by assuming that we have a ﬁxed-capacity array, but in Section 7.2.1 describe a more advanced technique that effectively allows an array-based list to have unbounded capacity. Such an unbounded list is known as an array list in Java (or a vector in C++ and in the earliest versions of Java). With a representation based on an array A, the get(i) and set(i, e) methods are easy to implement by accessing A[i] (assuming i is a legitimate index). Methods add(i, e) and remove(i) are more time consuming, as they require shifting elements up or down to maintain our rule of always storing an element whose list index is i at index i of the array. (See Figure 7.1.) Our initial implementation of the ArrayList class follows in Code Fragments 7.2 and 7.3. N −1 i n−1 (a) n−1 i N −1 (b) Figure 7.1: Array-based implementation of an array list that is storing n elements: (a) shifting up for an insertion at index i; (b) shifting down for a removal at index i. public class ArrayList<E> implements List<E> { // instance variables public static ﬁnal int CAPACITY=16; // default array capacity private E[ ] data; // generic array used for storage private int size = 0; // current number of elements // constructors public ArrayList() { this(CAPACITY); } // constructs list with default capacity public ArrayList(int capacity) { // constructs list with given capacity data = (E[ ]) new Object[capacity]; // safe cast; compiler may give warning } Code Fragment 7.2: An implementation of a simple ArrayList class with bounded capacity. (Continues in Code Fragment 7.3.)

7.2. Array Lists // public methods /∗∗Returns the number of elements in the array list. ∗/ public int size() { return size; } /∗∗Returns whether the array list is empty. ∗/ public boolean isEmpty() { return size == 0; } /∗∗Returns (but does not remove) the element at index i. ∗/ public E get(int i) throws IndexOutOfBoundsException { checkIndex(i, size); return data[i]; } /∗∗Replaces the element at index i with e, and returns the replaced element. ∗/ public E set(int i, E e) throws IndexOutOfBoundsException { checkIndex(i, size); E temp = data[i]; data[i] = e; return temp; } /∗∗Inserts element e to be at index i, shifting all subsequent elements later. ∗/ public void add(int i, E e) throws IndexOutOfBoundsException, IllegalStateException { checkIndex(i, size + 1); if (size == data.length) // not enough capacity throw new IllegalStateException("Array is full"); for (int k=size−1; k >= i; k−−) // start by shifting rightmost data[k+1] = data[k]; data[i] = e; // ready to place the new element size++; } /∗∗Removes/returns the element at index i, shifting subsequent elements earlier. ∗/ public E remove(int i) throws IndexOutOfBoundsException { checkIndex(i, size); E temp = data[i]; for (int k=i; k < size−1; k++) // shift elements to ﬁll hole data[k] = data[k+1]; data[size−1] = null; // help garbage collection size−−; return temp; } // utility method /∗∗Checks whether the given index is in the range [0, n−1]. ∗/ protected void checkIndex(int i, int n) throws IndexOutOfBoundsException { if (i < 0 || i >= n) throw new IndexOutOfBoundsException("Illegal index: " + i); } } Code Fragment 7.3: An implementation of a simple ArrayList class with bounded capacity. (Continued from Code Fragment 7.2.)

Chapter 7. List and Iterator ADTs The Performance of a Simple Array-Based Implementation Table 7.1 shows the worst-case running times of the methods of an array list with n elements realized by means of an array. Methods isEmpty, size, get and set clearly run in O(1) time, but the insertion and removal methods can take much longer than this. In particular, add(i, e) runs in time O(n). Indeed, the worst case for this operation occurs when i is 0, since all the existing n elements have to be shifted forward. A similar argument applies to method remove(i), which runs in O(n) time, because we have to shift backward n−1 elements in the worst case, when i is 0. In fact, assuming that each possible index is equally likely to be passed as an argument to these operations, their average running time is O(n), for we will have to shift n/2 elements on average. Method Running Time size() O(1) isEmpty() O(1) get(i) O(1) set(i, e) O(1) add(i, e) O(n) remove(i) O(n) Table 7.1: Performance of an array list with n elements realized by a ﬁxed-capacity array. Looking more closely at add(i, e) and remove(i), we note that they each run in time O(n−i + 1), for only those elements at index i and higher have to be shifted up or down. Thus, inserting or removing an item at the end of an array list, using the methods add(n, e) and remove(n −1) respectively, takes O(1) time each. Moreover, this observation has an interesting consequence for the adaptation of the array list ADT to the deque ADT from Section 6.3.1. If we do the “obvious” thing and store elements of a deque so that the ﬁrst element is at index 0 and the last element at index n −1, then methods addLast and removeLast of the deque each run in O(1) time. However, methods addFirst and removeFirst of the deque each run in O(n) time. Actually, with a little effort, we can produce an array-based implementation of the array list ADT that achieves O(1) time for insertions and removals at index 0, as well as insertions and removals at the end of the array list. Achieving this requires that we give up on our rule that an element at index i is stored in the array at index i, however, as we would have to use a circular array approach like the one we used in Section 6.2 to implement a queue. We leave the details of this implementation as Exercise C-7.25.

7.2. Array Lists 7.2.1 Dynamic Arrays The ArrayList implementation in Code Fragments 7.2 and 7.3 (as well as those for a stack, queue, and deque from Chapter 6) has a serious limitation; it requires that a ﬁxed maximum capacity be declared, throwing an exception if attempting to add an element once full. This is a major weakness, because if a user is unsure of the maximum size that will be reached for a collection, there is risk that either too large of an array will be requested, causing an inefﬁcient waste of memory, or that too small of an array will be requested, causing a fatal error when exhausting that capacity. Java’s ArrayList class provides a more robust abstraction, allowing a user to add elements to the list, with no apparent limit on the overall capacity. To provide this abstraction, Java relies on an algorithmic sleight of hand that is known as a dynamic array. In reality, elements of an ArrayList are stored in a traditional array, and the precise size of that traditional array must be internally declared in order for the system to properly allocate a consecutive piece of memory for its storage. For example, Figure 7.2 displays an array with 12 cells that might be stored in memory locations 2146 through 2157 on a computer system. Figure 7.2: An array of 12 cells, allocated in memory locations 2146 through 2157. Because the system may allocate neighboring memory locations to store other data, the capacity of an array cannot be increased by expanding into subsequent cells. The ﬁrst key to providing the semantics of an unbounded array is that an array list instance maintains an internal array that often has greater capacity than the current length of the list. For example, while a user may have created a list with ﬁve elements, the system may have reserved an underlying array capable of storing eight object references (rather than only ﬁve). This extra capacity makes it easy to add a new element to the end of the list by using the next available cell of the array. If a user continues to add elements to a list, all reserved capacity in the underlying array will eventually be exhausted. In that case, the class requests a new, larger array from the system, and copies all references from the smaller array into the beginning of the new array. At that point in time, the old array is no longer needed, so it can be reclaimed by the system. Intuitively, this strategy is much like that of the hermit crab, which moves into a larger shell when it outgrows its previous one.

Chapter 7. List and Iterator ADTs 7.2.2 Implementing a Dynamic Array We now demonstrate how our original version of the ArrayList, from Code Fragments 7.2 and 7.3, can be transformed to a dynamic-array implementation, having unbounded capacity. We rely on the same internal representation, with a traditional array A, that is initialized either to a default capacity or to one speciﬁed as a parameter to the constructor. The key is to provide means to “grow” the array A, when more space is needed. Of course, we cannot actually grow that array, as its capacity is ﬁxed. Instead, when a call to add a new element risks overﬂowing the current array, we perform the following additional steps: 1. Allocate a new array B with larger capacity. 2. Set B[k] = A[k], for k = 0,...,n−1, where n denotes current number of items. 3. Set A = B, that is, we henceforth use the new array to support the list. 4. Insert the new element in the new array. An illustration of this process is shown in Figure 7.3. B A A B A (a) (b) (c) Figure 7.3: An illustration of “growing” a dynamic array: (a) create new array B; (b) store elements of A in B; (c) reassign reference A to the new array. Not shown is the future garbage collection of the old array, or the insertion of a new element. Code Fragment 7.4 provides a concrete implementation of a resize method, which should be included as a protected method within the original ArrayList class. The instance variable data corresponds to array A in the above discussion, and local variable temp corresponds to array B. /∗∗Resizes internal array to have given capacity >= size. ∗/ protected void resize(int capacity) { E[ ] temp = (E[ ]) new Object[capacity]; // safe cast; compiler may give warning for (int k=0; k < size; k++) temp[k] = data[k]; data = temp; // start using the new array } Code Fragment 7.4: An implementation of the ArrayList.resize method.

7.2. Array Lists The remaining issue to consider is how large of a new array to create. A commonly used rule is for the new array to have twice the capacity of the existing array that has been ﬁlled. In Section 7.2.3, we will provide a mathematical analysis to justify such a choice. To complete the revision to our original ArrayList implementation, we redesign the add method so that it calls the new resize utility when detecting that the current array is ﬁlled (rather than throwing an exception). The revised version appears in Code Fragment 7.5. /∗∗Inserts element e to be at index i, shifting all subsequent elements later. ∗/ public void add(int i, E e) throws IndexOutOfBoundsException { checkIndex(i, size + 1); if (size == data.length) // not enough capacity resize(2 ∗data.length); // so double the current capacity ... // rest of method unchanged... Code Fragment 7.5: A revision to the ArrayList.add method, originally from Code Fragment 7.3, which calls the resize method of Code Fragment 7.4 when more capacity is needed. Finally, we note that our original implementation of the ArrayList class includes two constructors: a default constructor that uses an initial capacity of 16, and a parameterized constructor that allows the caller to specify a capacity value. With the use of dynamic arrays, that capacity is no longer a ﬁxed limit. Still, greater efﬁciency is achieved when a user selects an initial capacity that matches the actual size of a data set, as this can avoid time spent on intermediate array reallocations and potential space that is wasted by having too large of an array. 7.2.3 Amortized Analysis of Dynamic Arrays In this section, we will perform a detailed analysis of the running time of operations on dynamic arrays. As a shorthand notation, let us refer to the insertion of an element to be the last element in an array list as a push operation. The strategy of replacing an array with a new, larger array might at ﬁrst seem slow, because a single push operation may require Ω(n) time to perform, where n is the current number of elements in the array. (Recall, from Section 4.3.1, that big-Omega notation, describes an asymptotic lower bound on the running time of an algorithm.) However, by doubling the capacity during an array replacement, our new array allows us to add n further elements before the array must be replaced again. In this way, there are many simple push operations for each expensive one (see Figure 7.4). This fact allows us to show that a series of push operations on an initially empty dynamic array is efﬁcient in terms of its total running time.

Chapter 7. List and Iterator ADTs primitive operations for a push current number of elements 12 13 14 15 16 9 10 Figure 7.4: Running times of a series of push operations on a dynamic array. Using an algorithmic design pattern called amortization, we show that performing a sequence of push operations on a dynamic array is actually quite efﬁcient. To perform an amortized analysis, we use an accounting technique where we view the computer as a coin-operated appliance that requires the payment of one cyberdollar for a constant amount of computing time. When an operation is executed, we should have enough cyber-dollars available in our current “bank account” to pay for that operation’s running time. Thus, the total amount of cyber-dollars spent for any computation will be proportional to the total time spent on that computation. The beauty of using this analysis method is that we can overcharge some operations in order to save up cyber-dollars to pay for others. Proposition 7.2: Let L be an initially empty array list with capacity one, implemented by means of a dynamic array that doubles in size when full. The total time to perform a series of n push operations in L is O(n). Justiﬁcation: Let us assume that one cyber-dollar is enough to pay for the execution of each push operation in L, excluding the time spent for growing the array. Also, let us assume that growing the array from size k to size 2k requires k cyberdollars for the time spent initializing the new array. We shall charge each push operation three cyber-dollars. Thus, we overcharge each push operation that does not cause an overﬂow by two cyber-dollars. Think of the two cyber-dollars proﬁted in an insertion that does not grow the array as being “stored” with the cell in which the element was inserted. An overﬂow occurs when the array L has 2i elements, for some integer i ≥0, and the size of the array used by the array representing L is 2i. Thus, doubling the size of the array will require 2i cyber-dollars. Fortunately, these cyber-dollars can be found stored in cells 2i−1 through 2i −1. (See Figure 7.5.)

7.2. Array Lists (a) $ $ $ $ $ $ $ $ (b) $ $ Figure 7.5: Illustration of a series of push operations on a dynamic array: (a) an 8-cell array is full, with two cyber-dollars “stored” at cells 4 through 7; (b) a push operation causes an overﬂow and a doubling of capacity. Copying the eight old elements to the new array is paid for by the cyber-dollars already stored in the table. Inserting the new element is paid for by one of the cyber-dollars charged to the current push operation, and the two cyber-dollars proﬁted are stored at cell 8. Note that the previous overﬂow occurred when the number of elements became larger than 2i−1 for the ﬁrst time, and thus the cyber-dollars stored in cells 2i−1 through 2i −1 have not yet been spent. Therefore, we have a valid amortization scheme in which each operation is charged three cyber-dollars and all the computing time is paid for. That is, we can pay for the execution of n push operations using 3n cyber-dollars. In other words, the amortized running time of each push operation is O(1); hence, the total running time of n push operations is O(n). Geometric Increase in Capacity Although the proof of Proposition 7.2 relies on the array being doubled each time it is expanded, the O(1) amortized bound per operation can be proven for any geometrically increasing progression of array sizes. (See Section 2.2.3 for discussion of geometric progressions.) When choosing the geometric base, there exists a tradeoff between runtime efﬁciency and memory usage. If the last insertion causes a resize event, with a base of 2 (i.e., doubling the array), the array essentially ends up twice as large as it needs to be. If we instead increase the array by only 25% of its current size (i.e., a geometric base of 1.25), we do not risk wasting as much memory in the end, but there will be more intermediate resize events along the way. Still it is possible to prove an O(1) amortized bound, using a constant factor greater than the 3 cyber-dollars per operation used in the proof of Proposition 7.2 (see Exercise R-7.7). The key to the performance is that the amount of additional space is proportional to the current size of the array.

Chapter 7. List and Iterator ADTs Beware of Arithmetic Progression To avoid reserving too much space at once, it might be tempting to implement a dynamic array with a strategy in which a constant number of additional cells are reserved each time an array is resized. Unfortunately, the overall performance of such a strategy is signiﬁcantly worse. At an extreme, an increase of only one cell causes each push operation to resize the array, leading to a familiar 1+2+3+···+ n summation and Ω(n2) overall cost. Using increases of 2 or 3 at a time is slightly better, as portrayed in Figure 7.4, but the overall cost remains quadratic. primitive operations for a push current number of elements 12 13 14 15 16 9 10 primitive operations for a push current number of elements 14 15 16 (a) (b) Figure 7.6: Running times of a series of push operations on a dynamic array using arithmetic progression of sizes. Part (a) assumes an increase of 2 in the size of the array, while part (b) assumes an increase of 3. Using a ﬁxed increment for each resize, and thus an arithmetic progression of intermediate array sizes, results in an overall time that is quadratic in the number of operations, as shown in the following proposition. In essence, even an increase in 10,000 cells per resize will become insigniﬁcant for large data sets. Proposition 7.3: Performing a series of n push operations on an initially empty dynamic array using a ﬁxed increment with each resize takes Ω(n2) time. Justiﬁcation: Let c > 0 represent the ﬁxed increment in capacity that is used for each resize event. During the series of n push operations, time will have been spent initializing arrays of size c, 2c, 3c, ..., mc for m = ⌈n/c⌉, and therefore, the overall time is proportional to c+2c+3c+···+mc. By Proposition 4.3, this sum is m ∑ i=1 ci = c· m ∑ i=1 i = cm(m+1) ≥c n c(n c +1) ≥1 2c ·n2. Therefore, performing the n push operations takes Ω(n2) time.

7.2. Array Lists Memory Usage and Shrinking an Array Another consequence of the rule of a geometric increase in capacity when adding to a dynamic array is that the ﬁnal array size is guaranteed to be proportional to the overall number of elements. That is, the data structure uses O(n) memory. This is a very desirable property for a data structure. If a container, such as an array list, provides operations that cause the removal of one or more elements, greater care must be taken to ensure that a dynamic array guarantees O(n) memory usage. The risk is that repeated insertions may cause the underlying array to grow arbitrarily large, and that there will no longer be a proportional relationship between the actual number of elements and the array capacity after many elements are removed. A robust implementation of such a data structure will shrink the underlying array, on occasion, while maintaining the O(1) amortized bound on individual operations. However, care must be taken to ensure that the structure cannot rapidly oscillate between growing and shrinking the underlying array, in which case the amortized bound would not be achieved. In Exercise C-7.29, we explore a strategy in which the array capacity is halved whenever the number of actual element falls below one-fourth of that capacity, thereby guaranteeing that the array capacity is at most four times the number of elements; we explore the amortized analysis of such a strategy in Exercises C-7.30 and C-7.31. 7.2.4 Java’s StringBuilder class Near the beginning of Chapter 4, we described an experiment in which we compared two algorithms for composing a long string (Code Fragment 4.2). The ﬁrst of those relied on repeated concatenation using the String class, and the second relied on use of Java’s StringBuilder class. We observed the StringBuilder was signiﬁcantly faster, with empirical evidence that suggested a quadratic running time for the algorithm with repeated concatenations, and a linear running time for the algorithm with the StringBuilder. We are now able to explain the theoretical underpinning for those observations. The StringBuilder class represents a mutable string by storing characters in a dynamic array. With analysis similar to Proposition 7.2, it guarantees that a series of append operations resulting in a string of length n execute in a combined time of O(n). (Insertions at positions other than the end of a string builder do not carry this guarantee, just as they do not for an ArrayList.) In contrast, the repeated use of string concatenation requires quadratic time. We originally analyzed that algorithm on page 172 of Chapter 4. In effect, that approach is akin to a dynamic array with an arithmetic progression of size one, repeatedly copying all characters from one array to a new array with size one greater than before.

Chapter 7. List and Iterator ADTs 7.3 Positional Lists When working with array-based sequences, integer indices provide an excellent means for describing the location of an element, or the location at which an insertion or deletion should take place. However, numeric indices are not a good choice for describing positions within a linked list because, knowing only an element’s index, the only way to reach it is to traverse the list incrementally from its beginning or end, counting elements along the way. Furthermore, indices are not a good abstraction for describing a more local view of a position in a sequence, because the index of an entry changes over time due to insertions or deletions that happen earlier in the sequence. For example, it may not be convenient to describe the location of a person waiting in line based on the index, as that requires knowledge of precisely how far away that person is from the front of the line. We prefer an abstraction, as characterized in Figure 7.7, in which there is some other means for describing a position. Tickets me Figure 7.7: We wish to be able to identify the position of an element in a sequence without the use of an integer index. The label “me” represents some abstraction that identiﬁes the position. Our goal is to design an abstract data type that provides a user a way to refer to elements anywhere in a sequence, and to perform arbitrary insertions and deletions. This would allow us to efﬁciently describe actions such as a person deciding to leave the line before reaching the front, or allowing a friend to “cut” into line right behind him or her. As another example, a text document can be viewed as a long sequence of characters. A word processor uses the abstraction of a cursor to describe a position within the document without explicit use of an integer index, allowing operations such as “delete the character at the cursor” or “insert a new character just after the cursor.” Furthermore, we may be able to refer to an inherent position within a document, such as the beginning of a particular chapter, without relying on a character index (or even a chapter number) that may change as the document evolves.

7.3. Positional Lists For these reasons, we temporarily forego the index-based methods of Java’s formal List interface, and instead develop our own abstract data type that we denote as a positional list. Although a positional list is an abstraction, and need not rely on a linked list for its implementation, we certainly have a linked list in mind as we design the ADT, ensuring that it takes best advantage of particular capabilities of a linked list, such as O(1)-time insertions and deletions at arbitrary positions (something that is not possible with an array-based sequence). We face an immediate challenge in designing the ADT; to achieve constant time insertions and deletions at arbitrary locations, we effectively need a reference to the node at which an element is stored. It is therefore very tempting to develop an ADT in which a node reference serves as the mechanism for describing a position. In fact, our DoublyLinkedList class of Section 3.4.1 has methods addBetween and remove that accept node references as parameters; however, we intentionally declared those methods as private. Unfortunately, the public use of nodes in the ADT would violate the objectoriented design principles of abstraction and encapsulation, which were introduced in Chapter 2. There are several reasons to prefer that we encapsulate the nodes of a linked list, for both our sake and for the beneﬁt of users of our abstraction: • It will be simpler for users of our data structure if they are not bothered with unnecessary details of our implementation, such as low-level manipulation of nodes, or our reliance on the use of sentinel nodes. Notice that to use the addBetween method of our DoublyLinkedList class to add a node at the beginning of a sequence, the header sentinel must be sent as a parameter. • We can provide a more robust data structure if we do not permit users to directly access or manipulate the nodes. We can then ensure that users do not invalidate the consistency of a list by mismanaging the linking of nodes. A more subtle problem arises if a user were allowed to call the addBetween or remove method of our DoublyLinkedList class, sending a node that does not belong to the given list as a parameter. (Go back and look at that code and see why it causes a problem!) • By better encapsulating the internal details of our implementation, we have greater ﬂexibility to redesign the data structure and improve its performance. In fact, with a well-designed abstraction, we can provide a notion of a nonnumeric position, even if using an array-based sequence. (See Exercise C-7.43.) Therefore, in deﬁning the positional list ADT, we also introduce the concept of a position, which formalizes the intuitive notion of the “location” of an element relative to others in the list. (When we do use a linked list for the implementation, we will later see how we can privately use node references as natural manifestations of positions.)

Chapter 7. List and Iterator ADTs 7.3.1 Positions To provide a general abstraction for the location of an element within a structure, we deﬁne a simple position abstract data type. A position supports the following single method: getElement(): Returns the element stored at this position. A position acts as a marker or token within a broader positional list. A position p, which is associated with some element e in a list L, does not change, even if the index of e changes in L due to insertions or deletions elsewhere in the list. Nor does position p change if we replace the element e stored at p with another element. The only way in which a position becomes invalid is if that position (and its element) are explicitly removed from the list. Having a formal deﬁnition of a position type allows positions to serve as parameters to some methods and return values from other methods of the positional list ADT, which we next describe. 7.3.2 The Positional List Abstract Data Type We now view a positional list as a collection of positions, each of which stores an element. The accessor methods provided by the positional list ADT include the following, for a list L: ﬁrst(): Returns the position of the ﬁrst element of L (or null if empty). last(): Returns the position of the last element of L (or null if empty). before(p): Returns the position of L immediately before position p (or null if p is the ﬁrst position). after(p): Returns the position of L immediately after position p (or null if p is the last position). isEmpty(): Returns true if list L does not contain any elements. size(): Returns the number of elements in list L. An error occurs if a position p, sent as a parameter to a method, is not a valid position for the list. Note well that the ﬁrst() and last() methods of the positional list ADT return the associated positions, not the elements. (This is in contrast to the corresponding ﬁrst and last methods of the deque ADT.) The ﬁrst element of a positional list can be determined by subsequently invoking the getElement method on that position, as ﬁrst().getElement. The advantage of receiving a position as a return value is that we can subsequently use that position to traverse the list.

7.3. Positional Lists As a demonstration of a typical traversal of a positional list, Code Fragment 7.6 traverses a list, named guests, that stores string elements, and prints each element while traversing from the beginning of the list to the end. Position<String> cursor = guests.ﬁrst(); while (cursor != null) { System.out.println(cursor.getElement()); cursor = guests.after(cursor); // advance to the next position (if any) } Code Fragment 7.6: A traversal of a positional list. This code relies on the convention that the null reference is returned when the after method is called upon the last position. (That return value is clearly distinguishable from any legitimate position.) The positional list ADT similarly indicates that the null value is returned when the before method is invoked at the front of the list, or when ﬁrst or last methods are called upon an empty list. Therefore, the above code fragment works correctly even if the guests list is empty. Updated Methods of a Positional List The positional list ADT also includes the following update methods: addFirst(e): Inserts a new element e at the front of the list, returning the position of the new element. addLast(e): Inserts a new element e at the back of the list, returning the position of the new element. addBefore(p, e): Inserts a new element e in the list, just before position p, returning the position of the new element. addAfter(p, e): Inserts a new element e in the list, just after position p, returning the position of the new element. set(p, e): Replaces the element at position p with element e, returning the element formerly at position p. remove(p): Removes and returns the element at position p in the list, invalidating the position. There may at ﬁrst seem to be redundancy in the above repertoire of operations for the positional list ADT, since we can perform operation addFirst(e) with addBefore(ﬁrst(), e), and operation addLast(e) with addAfter(last(), e). But these substitutions can only be done for a nonempty list.

Chapter 7. List and Iterator ADTs Example 7.4: The following table shows a series of operations on an initially empty positional list storing integers. To identify position instances, we use variables such as p and q. For ease of exposition, when displaying the list contents, we use subscript notation to denote the position storing an element. Method Return Value List Contents addLast(8) p (8p) ﬁrst() p (8p) addAfter(p, 5) q (8p, 5q) before(q) p (8p, 5q) addBefore(q, 3) r (8p, 3r, 5q) r.getElement() (8p, 3r, 5q) after(p) r (8p, 3r, 5q) before(p) null (8p, 3r, 5q) addFirst(9) s (9s, 8p, 3r, 5q) remove(last()) (9s, 8p, 3r) set(p, 7) (9s, 7p, 3r) remove(q) “error” (9s, 7p, 3r) Java Interface Deﬁnitions We are now ready to formalize the position ADT and positional list ADT. A Java Position interface, representing the position ADT, is given in Code Fragment 7.7. Following that, Code Fragment 7.8 presents a Java deﬁnition for our PositionalList interface. If the getElement() method is called on a Position instance that has previously been removed from its list, an IllegalStateException is thrown. If an invalid Position instance is sent as a parameter to a method of a PositionalList, an IllegalArgumentException is thrown. (Both of those exception types are deﬁned in the standard Java hierarchy.) public interface Position<E> { /∗∗ ∗Returns the element stored at this position. ∗ ∗@return the stored element ∗@throws IllegalStateException if position no longer valid ∗/ E getElement() throws IllegalStateException; } Code Fragment 7.7: The Position interface.

7.3. Positional Lists /∗∗An interface for positional lists. ∗/ public interface PositionalList<E> { /∗∗Returns the number of elements in the list. ∗/ int size(); /∗∗Tests whether the list is empty. ∗/ boolean isEmpty(); /∗∗Returns the ﬁrst Position in the list (or null, if empty). ∗/ Position<E> ﬁrst(); /∗∗Returns the last Position in the list (or null, if empty). ∗/ Position<E> last(); /∗∗Returns the Position immediately before Position p (or null, if p is ﬁrst). ∗/ Position<E> before(Position<E> p) throws IllegalArgumentException; /∗∗Returns the Position immediately after Position p (or null, if p is last). ∗/ Position<E> after(Position<E> p) throws IllegalArgumentException; /∗∗Inserts element e at the front of the list and returns its new Position. ∗/ Position<E> addFirst(E e); /∗∗Inserts element e at the back of the list and returns its new Position. ∗/ Position<E> addLast(E e); /∗∗Inserts element e immediately before Position p and returns its new Position. ∗/ Position<E> addBefore(Position<E> p, E e) throws IllegalArgumentException; /∗∗Inserts element e immediately after Position p and returns its new Position. ∗/ Position<E> addAfter(Position<E> p, E e) throws IllegalArgumentException; /∗∗Replaces the element stored at Position p and returns the replaced element. ∗/ E set(Position<E> p, E e) throws IllegalArgumentException; /∗∗Removes the element stored at Position p and returns it (invalidating p). ∗/ E remove(Position<E> p) throws IllegalArgumentException; } Code Fragment 7.8: The PositionalList interface.

Chapter 7. List and Iterator ADTs 7.3.3 Doubly Linked List Implementation Not surprisingly, our preferred implementation of the PositionalList interface relies on a doubly linked list. Although we implemented a DoublyLinkedList class in Chapter 3, that class does not adhere to the PositionalList interface. In this section, we develop a concrete implementation of the PositionalList interface using a doubly linked list. The low-level details of our new linked-list representation, such as the use of header and trailer sentinels, will be identical to our earlier version; we refer the reader to Section 3.4 for a discussion of the doubly linked list operations. What differs in this section is our management of the positional abstraction. The obvious way to identify locations within a linked list are node references. Therefore, we declare the nested Node class of our linked list so as to implement the Position interface, supporting the required getElement method. So the nodes are the positions. Yet, the Node class is declared as private, to maintain proper encapsulation. All of the public methods of the positional list rely on the Position type, so although we know we are sending and receiving nodes, these are only known to be positions from the outside; as a result, users of our class cannot call any method other than getElement(). In Code Fragments 7.9–7.12, we deﬁne a LinkedPositionalList class, which implements the positional list ADT. We provide the following guide to that code: • Code Fragment 7.9 contains the deﬁnition of the nested Node<E> class, which implements the Position<E> interface. Following that are the declaration of the instance variables of the outer LinkedPositionalList class and its constructor. • Code Fragment 7.10 begins with two important utility methods that help us robustly cast between the Position and Node types. The validate(p) method is called anytime the user sends a Position instance as a parameter. It throws an exception if it determines that the position is invalid, and otherwise returns that instance, implicitly cast as a Node, so that methods of the Node class can subsequently be called. The private position(node) method is used when about to return a Position to the user. Its primary purpose is to make sure that we do not expose either sentinel node to a caller, returning a null reference in such a case. We rely on both of these private utility methods in the public accessor methods that follow. • Code Fragment 7.11 provides most of the public update methods, relying on a private addBetween method to unify the implementations of the various insertion operations. • Code Fragment 7.12 provides the public remove method. Note that it sets all ﬁelds of the removed node back to null—a condition we can later detect to recognize a defunct position.

7.3. Positional Lists /∗∗Implementation of a positional list stored as a doubly linked list. ∗/ public class LinkedPositionalList<E> implements PositionalList<E> { //---------------- nested Node class ---------------- private static class Node<E> implements Position<E> { private E element; // reference to the element stored at this node private Node<E> prev; // reference to the previous node in the list private Node<E> next; // reference to the subsequent node in the list public Node(E e, Node<E> p, Node<E> n) { element = e; prev = p; next = n; } public E getElement() throws IllegalStateException { if (next == null) // convention for defunct node throw new IllegalStateException("Position no longer valid"); return element; } public Node<E> getPrev() { return prev; } public Node<E> getNext() { return next; } public void setElement(E e) { element = e; } public void setPrev(Node<E> p) { prev = p; } public void setNext(Node<E> n) { next = n; } } //----------- end of nested Node class ----------- // instance variables of the LinkedPositionalList private Node<E> header; // header sentinel private Node<E> trailer; // trailer sentinel private int size = 0; // number of elements in the list /∗∗Constructs a new empty list. ∗/ public LinkedPositionalList() { header = new Node<>(null, null, null); // create header trailer = new Node<>(null, header, null); // trailer is preceded by header header.setNext(trailer); // header is followed by trailer } Code Fragment 7.9: An implementation of the LinkedPositionalList class. (Continues in Code Fragments 7.10–7.12.)

Chapter 7. List and Iterator ADTs // private utilities /∗∗Validates the position and returns it as a node. ∗/ private Node<E> validate(Position<E> p) throws IllegalArgumentException { if (!(p instanceof Node)) throw new IllegalArgumentException("Invalid p"); Node<E> node = (Node<E>) p; // safe cast if (node.getNext() == null) // convention for defunct node throw new IllegalArgumentException("p is no longer in the list"); return node; } /∗∗Returns the given node as a Position (or null, if it is a sentinel). ∗/ private Position<E> position(Node<E> node) { if (node == header || node == trailer) return null; // do not expose user to the sentinels return node; } // public accessor methods /∗∗Returns the number of elements in the linked list. ∗/ public int size() { return size; } /∗∗Tests whether the linked list is empty. ∗/ public boolean isEmpty() { return size == 0; } /∗∗Returns the ﬁrst Position in the linked list (or null, if empty). ∗/ public Position<E> ﬁrst() { return position(header.getNext()); } /∗∗Returns the last Position in the linked list (or null, if empty). ∗/ public Position<E> last() { return position(trailer.getPrev()); } /∗∗Returns the Position immediately before Position p (or null, if p is ﬁrst). ∗/ public Position<E> before(Position<E> p) throws IllegalArgumentException { Node<E> node = validate(p); return position(node.getPrev()); } /∗∗Returns the Position immediately after Position p (or null, if p is last). ∗/ public Position<E> after(Position<E> p) throws IllegalArgumentException { Node<E> node = validate(p); return position(node.getNext()); } Code Fragment 7.10: An implementation of the LinkedPositionalList class. (Continued from Code Fragment 7.9; continues in Code Fragments 7.11 and 7.12.)

7.3. Positional Lists // private utilities /∗∗Adds element e to the linked list between the given nodes. ∗/ private Position<E> addBetween(E e, Node<E> pred, Node<E> succ) { Node<E> newest = new Node<>(e, pred, succ); // create and link a new node pred.setNext(newest); succ.setPrev(newest); size++; return newest; } // public update methods /∗∗Inserts element e at the front of the linked list and returns its new Position. ∗/ public Position<E> addFirst(E e) { return addBetween(e, header, header.getNext()); // just after the header } /∗∗Inserts element e at the back of the linked list and returns its new Position. ∗/ public Position<E> addLast(E e) { return addBetween(e, trailer.getPrev(), trailer); // just before the trailer } /∗∗Inserts element e immediately before Position p, and returns its new Position.∗/ public Position<E> addBefore(Position<E> p, E e) throws IllegalArgumentException { Node<E> node = validate(p); return addBetween(e, node.getPrev(), node); } /∗∗Inserts element e immediately after Position p, and returns its new Position. ∗/ public Position<E> addAfter(Position<E> p, E e) throws IllegalArgumentException { Node<E> node = validate(p); return addBetween(e, node, node.getNext()); } /∗∗Replaces the element stored at Position p and returns the replaced element. ∗/ public E set(Position<E> p, E e) throws IllegalArgumentException { Node<E> node = validate(p); E answer = node.getElement(); node.setElement(e); return answer; } Code Fragment 7.11: An implementation of the LinkedPositionalList class. (Continued from Code Fragments 7.9 and 7.10; continues in Code Fragment 7.12.)

Chapter 7. List and Iterator ADTs /∗∗Removes the element stored at Position p and returns it (invalidating p). ∗/ public E remove(Position<E> p) throws IllegalArgumentException { Node<E> node = validate(p); Node<E> predecessor = node.getPrev(); Node<E> successor = node.getNext(); predecessor.setNext(successor); successor.setPrev(predecessor); size−−; E answer = node.getElement(); node.setElement(null); // help with garbage collection node.setNext(null); // and convention for defunct node node.setPrev(null); return answer; } } Code Fragment 7.12: An implementation of the LinkedPositionalList class. (Continued from Code Fragments 7.9–7.11.) The Performance of a Linked Positional List The positional list ADT is ideally suited for implementation with a doubly linked list, as all operations run in worst-case constant time, as shown in Table 7.2. This is in stark contrast to the ArrayList structure (analyzed in Table 7.1), which requires linear time for insertions or deletions at arbitrary positions, due to the need for a loop to shift other elements. Of course, our positional list does not support the index-based methods of the ofﬁcial List interface of Section 7.1. It is possible to add support for those methods by traversing the list while counting nodes (see Exercise C-7.38), but that requires time proportional to the sublist that is traversed. Method Running Time size() O(1) isEmpty() O(1) ﬁrst(), last() O(1) before(p), after(p) O(1) addFirst(e), addLast(e) O(1) addBefore(p, e), addAfter(p, e) O(1) set(p, e) O(1) remove(p) O(1) Table 7.2: Performance of a positional list with n elements realized by a doubly linked list. The space usage is O(n).

7.3. Positional Lists Implementing a Positional List with an Array We can implement a positional list L using an array A for storage, but some care is necessary in designing objects that will serve as positions. At ﬁrst glance, it would seem that a position p need only store the index i at which its associated element is stored within the array. We can then implement method getElement(p) simply by returning A[i]. The problem with this approach is that the index of an element e changes when other insertions or deletions occur before it. If we have already returned a position p associated with element e that stores an outdated index i to a user, the wrong array cell would be accessed when the position was used. (Remember that positions in a positional list should always be deﬁned relative to their neighboring positions, not their indices.) Hence, if we are going to implement a positional list with an array, we need a different approach. We recommend the following representation. Instead of storing the elements of L directly in array A, we store a new kind of position object in each cell of A. A position p stores the element e as well as the current index i of that element within the list. Such a data structure is illustrated in Figure 7.8. N−1 (3,SFO) (2,PVD) (0,JFK) (1,BWI) Figure 7.8: An array-based representation of a positional list. With this representation, we can determine the index currently associated with a position, and we can determine the position currently associated with a speciﬁc index. We can therefore implement an accessor, such as before(p), by ﬁnding the index of the given position and using the array to ﬁnd the neighboring position. When an element is inserted or deleted somewhere in the list, we can loop through the array to update the index variable stored in all later positions in the list that are shifted during the update. Eﬃciency Trade-Oﬀs with an Array-Based Sequence In this array implementation of a sequence, the addFirst, addBefore, addAfter, and remove methods take O(n) time, because we have to shift position objects to make room for the new position or to ﬁll in the hole created by the removal of the old position (just as in the insert and remove methods based on index). All the other position-based methods take O(1) time.

Chapter 7. List and Iterator ADTs 7.4 Iterators An iterator is a software design pattern that abstracts the process of scanning through a sequence of elements, one element at a time. The underlying elements might be stored in a container class, streaming through a network, or generated by a series of computations. In order to unify the treatment and syntax for iterating objects in a way that is independent from a speciﬁc organization, Java deﬁnes the java.util.Iterator interface with the following two methods: hasNext(): Returns true if there is at least one additional element in the sequence, and false otherwise. next(): Returns the next element in the sequence. The interface uses Java’s generic framework, with the next() method returning a parameterized element type. For example, the Scanner class (described in Section 1.6) formally implements the Iterator<String> interface, with its next() method returning a String instance. If the next() method of an iterator is called when no further elements are available, a NoSuchElementException is thrown. Of course, the hasNext() method can be used to detect that condition before calling next(). The combination of these two methods allows a general loop construct for processing elements of the iterator. For example, if we let variable, iter, denote an instance of the Iterator<String> type, then we can write the following: while (iter.hasNext()) { String value = iter.next(); System.out.println(value); } The java.util.Iterator interface contains a third method, which is optionally supported by some iterators: remove(): Removes from the collection the element returned by the most recent call to next(). Throws an IllegalStateException if next has not yet been called, or if remove was already called since the most recent call to next. This method can be used to ﬁlter a collection of elements, for example to discard all negative numbers from a data set. For the sake of simplicity, we will not implement the remove method for most data structures in this book, but we will give two tangible examples later in this section. If removal is not supported, an UnsupportedOperationException is conventionally thrown.

7.4. Iterators 7.4.1 The Iterable Interface and Java’s For-Each Loop A single iterator instance supports only one pass through a collection; calls to next can be made until all elements have been reported, but there is no way to “reset” the iterator back to the beginning of the sequence. However, a data structure that wishes to allow repeated iterations can support a method that returns a new iterator, each time it is called. To provide greater standardization, Java deﬁnes another parameterized interface, named Iterable, that includes the following single method: iterator(): Returns an iterator of the elements in the collection. An instance of a typical collection class in Java, such as an ArrayList, is iterable (but not itself an iterator); it produces an iterator for its collection as the return value of the iterator() method. Each call to iterator() returns a new iterator instance, thereby allowing multiple (even simultaneous) traversals of a collection. Java’s Iterable class also plays a fundamental role in support of the “for-each” loop syntax (described in Section 1.5.2). The loop syntax, for (ElementType variable : collection) { loopBody // may refer to ”variable” } is supported for any instance, collection, of an iterable class. ElementType must be the type of object returned by its iterator, and variable will take on element values within the loopBody. Essentially, this syntax is shorthand for the following: Iterator<ElementType> iter = collection.iterator(); while (iter.hasNext()) { ElementType variable = iter.next(); loopBody // may refer to ”variable” } We note that the iterator’s remove method cannot be invoked when using the for-each loop syntax. Instead, we must explicitly use an iterator. As an example, the following loop can be used to remove all negative numbers from an ArrayList of ﬂoating-point values. ArrayList<Double> data; // populate with random numbers (not shown) Iterator<Double> walk = data.iterator(); while (walk.hasNext()) if (walk.next() < 0.0) walk.remove();

Chapter 7. List and Iterator ADTs 7.4.2 Implementing Iterators There are two general styles for implementing iterators that differ in terms of what work is done when the iterator instance is ﬁrst created, and what work is done each time the iterator is advanced with a call to next(). A snapshot iterator maintains its own private copy of the sequence of elements, which is constructed at the time the iterator object is created. It effectively records a “snapshot” of the sequence of elements at the time the iterator is created, and is therefore unaffected by any subsequent changes to the primary collection that may occur. Implementing snapshot iterators tends to be very easy, as it requires a simple traversal of the primary structure. The downside of this style of iterator is that it requires O(n) time and O(n) auxiliary space, upon construction, to copy and store a collection of n elements. A lazy iterator is one that does not make an upfront copy, instead performing a piecewise traversal of the primary structure only when the next() method is called to request another element. The advantage of this style of iterator is that it can typically be implemented so the iterator requires only O(1) space and O(1) construction time. One downside (or feature) of a lazy iterator is that its behavior is affected if the primary structure is modiﬁed (by means other than by the iterator’s own remove method) before the iteration completes. Many of the iterators in Java’s libraries implement a “fail-fast” behavior that immediately invalidates such an iterator if its underlying collection is modiﬁed unexpectedly. We will demonstrate how to implement iterators for both the ArrayList and LinkedPositionalList classes as examples. We implement lazy iterators for both, including support for the remove operation (but without any fail-fast guarantee). Iterations with the ArrayList class We begin by discussing iteration for the ArrayList<E> class. We will have it implement the Iterable<E> interface. (In fact, that requirement is already part of Java’s List interface.) Therefore, we must add an iterator() method to that class deﬁnition, which returns an instance of an object that implements the Iterator<E> interface. For this purpose, we deﬁne a new class, ArrayIterator, as a nonstatic nested class of ArrayList (i.e., an inner class, as described in Section 2.6). The advantage of having the iterator as an inner class is that it can access private ﬁelds (such as the array A) that are members of the containing list. Our implementation is given in Code Fragment 7.13. The iterator() method of ArrayList returns a new instance of the inner ArrayIterator class. Each iterator maintains a ﬁeld j that represents the index of the next element to be returned. It is initialized to 0, and when j reaches the size of the list, there are no more elements to return. In order to support element removal through the iterator, we also maintain a boolean variable that denotes whether a call to remove is currently permissible.

7.4. Iterators //---------------- nested ArrayIterator class ---------------- /∗∗ ∗A (nonstatic) inner class. Note well that each instance contains an implicit ∗reference to the containing list, allowing it to access the list's members. ∗/ private class ArrayIterator implements Iterator<E> { private int j = 0; // index of the next element to report private boolean removable = false; // can remove be called at this time? /∗∗ ∗Tests whether the iterator has a next object. ∗@return true if there are further objects, false otherwise ∗/ public boolean hasNext() { return j < size; } // size is ﬁeld of outer instance /∗∗ ∗Returns the next object in the iterator. ∗ ∗@return next object ∗@throws NoSuchElementException if there are no further elements ∗/ public E next() throws NoSuchElementException { if (j == size) throw new NoSuchElementException("No next element"); removable = true; // this element can subsequently be removed return data[j++]; // post-increment j, so it is ready for future call to next } /∗∗ ∗Removes the element returned by most recent call to next. ∗@throws IllegalStateException if next has not yet been called ∗@throws IllegalStateException if remove was already called since recent next ∗/ public void remove() throws IllegalStateException { if (!removable) throw new IllegalStateException("nothing to remove"); ArrayList.this.remove(j−1); // that was the last one returned j−−; // next element has shifted one cell to the left removable = false; // do not allow remove again until next is called } } //------------ end of nested ArrayIterator class ------------ /∗∗Returns an iterator of the elements stored in the list. ∗/ public Iterator<E> iterator() { return new ArrayIterator(); // create a new instance of the inner class } Code Fragment 7.13: Code providing support for ArrayList iterators. (This should be nested within the ArrayList class deﬁnition of Code Fragments 7.2 and 7.3.)

Chapter 7. List and Iterator ADTs Iterations with the LinkedPositionalList class In support the concept of iteration with the LinkedPositionalList class, a ﬁrst question is whether to support iteration of the elements of the list or the positions of the list. If we allow a user to iterate through all positions of the list, those positions could be used to access the underlying elements, so support for position iteration is more general. However, it is more standard for a container class to support iteration of the core elements, by default, so that the for-each loop syntax could be used to write code such as the following, for (String guest : waitlist) assuming that variable waitlist has type LinkedPositionalList<String>. For maximum convenience, we will support both forms of iteration. We will have the standard iterator() method return an iterator of the elements of the list, so that our list class formally implements the Iterable interface for the declared element type. For those wishing to iterate through the positions of a list, we will provide a new method, positions(). At ﬁrst glance, it would seem a natural choice for such a method to return an Iterator. However, we prefer for the return type of that method to be an instance that is Iterable (and hence, has its own iterator() method that returns an iterator of positions). Our reason for the extra layer of complexity is that we wish for users of our class to be able to use a for-each loop with a simple syntax such as the following: for (Position<String> p : waitlist.positions()) For this syntax to be legal, the return type of positions() must be Iterable. Code Fragment 7.14 presents our new support for the iteration of positions and elements of a LinkedPositionalList. We deﬁne three new inner classes. The ﬁrst of these is PositionIterator, providing the core functionality of our list iterations. Whereas the array list iterator maintained the index of the next element to be returned as a ﬁeld, this class maintains the position of the next element to be returned (as well as the position of the most recently returned element, to support removal). To support our goal of the positions() method returning an Iterable object, we deﬁne a trivial PositionIterable inner class, which simply constructs and returns a new PositionIterator object each time its iterator() method is called. The positions() method of the top-level class returns a new PositionIterable instance. Our framework relies heavily on these being inner classes, not static nested classes. Finally, we wish to have the top-level iterator() method return an iterator of elements (not positions). Rather than reinvent the wheel, we trivially adapt the PositionIterator class to deﬁne a new ElementIterator class, which lazily manages a position iterator instance, while returning the element stored at each position when next() is called.

7.4. Iterators //---------------- nested PositionIterator class ---------------- private class PositionIterator implements Iterator<Position<E>> { private Position<E> cursor = ﬁrst(); // position of the next element to report private Position<E> recent = null; // position of last reported element /∗∗Tests whether the iterator has a next object. ∗/ public boolean hasNext() { return (cursor != null); } /∗∗Returns the next position in the iterator. ∗/ public Position<E> next() throws NoSuchElementException { if (cursor == null) throw new NoSuchElementException("nothing left"); recent = cursor; // element at this position might later be removed cursor = after(cursor); return recent; } /∗∗Removes the element returned by most recent call to next. ∗/ public void remove() throws IllegalStateException { if (recent == null) throw new IllegalStateException("nothing to remove"); LinkedPositionalList.this.remove(recent); // remove from outer list recent = null; // do not allow remove again until next is called } } //------------ end of nested PositionIterator class ------------ //---------------- nested PositionIterable class ---------------- private class PositionIterable implements Iterable<Position<E>> { public Iterator<Position<E>> iterator() { return new PositionIterator(); } } //------------ end of nested PositionIterable class ------------ /∗∗Returns an iterable representation of the list's positions. ∗/ public Iterable<Position<E>> positions() { return new PositionIterable(); // create a new instance of the inner class } //---------------- nested ElementIterator class ---------------- /∗This class adapts the iteration produced by positions() to return elements. ∗/ private class ElementIterator implements Iterator<E> { Iterator<Position<E>> posIterator = new PositionIterator(); public boolean hasNext() { return posIterator.hasNext(); } public E next() { return posIterator.next().getElement(); } // return element! public void remove() { posIterator.remove(); } } /∗∗Returns an iterator of the elements stored in the list. ∗/ public Iterator<E> iterator() { return new ElementIterator(); } Code Fragment 7.14: Support for providing iterations of positions and elements of a LinkedPositionalList. (This should be nested within the LinkedPositionalList class deﬁnition of Code Fragments 7.9–7.12.)

Chapter 7. List and Iterator ADTs 7.5 The Java Collections Framework Java provides many data structure interfaces and classes, which together form the Java Collections Framework. This framework, which is part of the java.util package, includes versions of several of the data structures discussed in this book, some of which we have already discussed and others of which we will discuss later in this book. The root interface in the Java collections framework is named Collection. This is a general interface for any data structure, such as a list, that represents a collection of elements. The Collection interface includes many methods, including some we have already seen (e.g., size(), isEmpty(), iterator()). It is a superinterface for other interfaces in the Java Collections Framework that can hold elements, including the java.util interfaces Deque, List, and Queue, and other subinterfaces discussed later in this book, including Set (Section 10.5.1) and Map (Section 10.1). The Java Collections Framework also includes concrete classes implementing various interfaces with a combination of properties and underlying representations. We summarize but a few of those classes in Table 7.3. For each, we denote which of the Queue, Deque, or List interfaces are implemented (possibly several). We also discuss several behavioral properties. Some classes enforce, or allow, a ﬁxed capacity limit. Robust classes provide support for concurrency, allowing multiple processes to share use of a data structure in a thread-safe manner. If the structure is designated as blocking, a call to retrieve an element from an empty collection waits until some other process inserts an element. Similarly, a call to insert into a full blocking structure must wait until room becomes available. Interfaces Properties Storage Class Queue Deque List Capacity Limit Thread-Safe Blocking Array Linked List ArrayBlockingQueue ✓ ✓ ✓ ✓ ✓ LinkedBlockingQueue ✓ ✓ ✓ ✓ ✓ ConcurrentLinkedQueue ✓ ✓ ✓ ArrayDeque ✓ ✓ ✓ LinkedBlockingDeque ✓ ✓ ✓ ✓ ✓ ✓ ConcurrentLinkedDeque ✓ ✓ ✓ ✓ ArrayList ✓ ✓ LinkedList ✓ ✓ ✓ ✓ Table 7.3: Several classes in the Java Collections Framework.

7.5. The Java Collections Framework 7.5.1 List Iterators in Java The java.util.LinkedList class does not expose a position concept to users in its API, as we do in our positional list ADT. Instead, the preferred way to access and update a LinkedList object in Java, without using indices, is to use a ListIterator that is returned by the list’s listIterator() method. Such an iterator provides forward and backward traversal methods as well as local update methods. It views its current position as being before the ﬁrst element, between two elements, or after the last element. That is, it uses a list cursor, much like a screen cursor is viewed as being located between two characters on a screen. Speciﬁcally, the java.util.ListIterator interface includes the following methods: add(e): Adds the element e at the current position of the iterator. hasNext(): Returns true if there is an element after the current position of the iterator. hasPrevious(): Returns true if there is an element before the current position of the iterator. previous(): Returns the element e before the current position and sets the current position to be before e. next(): Returns the element e after the current position and sets the current position to be after e. nextIndex(): Returns the index of the next element. previousIndex(): Returns the index of the previous element. remove(): Removes the element returned by the most recent next or previous operation. set(e): Replaces the element returned by the most recent call to the next or previous operation with e. It is risky to use multiple iterators over the same list while modifying its contents. If insertions, deletions, or replacements are required at multiple “places” in a list, it is safer to use positions to specify these locations. But the java.util.LinkedList class does not expose its position objects to the user. So, to avoid the risks of modifying a list that has created multiple iterators, the iterators have a “fail-fast” feature that invalidates such an iterator if its underlying collection is modiﬁed unexpectedly. For example, if a java.util.LinkedList object L has returned ﬁve different iterators and one of them modiﬁes L, a ConcurrentModiﬁcationException is thrown if any of the other four is subsequently used. That is, Java allows many list iterators to be traversing a linked list L at the same time, but if one of them modiﬁes L (using an add, set, or remove method), then all the other iterators for L become invalid. Likewise, if L is modiﬁed by one of its own update methods, then all existing iterators for L immediately become invalid.

Chapter 7. List and Iterator ADTs 7.5.2 Comparison to Our Positional List ADT Java provides functionality similar to our array list and positional lists ADT in the java.util.List interface, which is implemented with an array in java.util.ArrayList and with a linked list in java.util.LinkedList. Moreover, Java uses iterators to achieve a functionality similar to what our positional list ADT derives from positions. Table 7.4 shows corresponding methods between our (array and positional) list ADTs and the java.util interfaces List and ListIterator interfaces, with notes about their implementations in the java.util classes ArrayList and LinkedList. Positional List java.util.List ListIterator Notes ADT Method Method Method size() size() O(1) time isEmpty() isEmpty() O(1) time get(i) A is O(1), L is O(min{i,n−i}) ﬁrst() listIterator() ﬁrst element is next last() listIterator(size()) last element is previous before(p) previous() O(1) time after(p) next() O(1) time set(p, e) set(e) O(1) time set(i, e) A is O(1), L is O(min{i,n−i}) add(i, e) O(n) time addFirst(e) add(0, e) A is O(n), L is O(1) addFirst(e) addFirst(e) only exists in L, O(1) addLast(e) add(e) O(1) time addLast(e) addLast(e) only exists in L, O(1) addAfter(p, e) add(e) insertion is at cursor; A is O(n), L is O(1) addBefore(p, e) add(e) insertion is at cursor; A is O(n), L is O(1) remove(p) remove() deletion is at cursor; A is O(n), L is O(1) remove(i) A is O(1), L is O(min{i,n−i}) Table 7.4: Correspondences between methods in our positional list ADT and the java.util interfaces List and ListIterator. We use A and L as abbreviations for java.util.ArrayList and java.util.LinkedList (or their running times).

7.5. The Java Collections Framework 7.5.3 List-Based Algorithms in the Java Collections Framework In addition to the classes that are provided in the Java Collections Framework, there are a number of simple algorithms that it provides as well. These algorithms are implemented as static methods in the java.util.Collections class (not to be confused with the java.util.Collection interface) and they include the following methods: copy(Ldest, Lsrc): Copies all elements of the Lsrc list into corresponding indices of the Ldest list. disjoint(C, D): Returns a boolean value indicating whether the collections C and D are disjoint. ﬁll(L, e): Replaces each element of the list L with element e. frequency(C, e): Returns the number of elements in the collection C that are equal to e. max(C): Returns the maximum element in the collection C, based on the natural ordering of its elements. min(C): Returns the minimum element in the collection C, based on the natural ordering of its elements. replaceAll(L, e, f): Replaces each element in L that is equal to e with element f. reverse(L): Reverses the ordering of elements in the list L. rotate(L, d): Rotates the elements in the list L by the distance d (which can be negative), in a circular fashion. shuﬄe(L): Pseudorandomly permutes the ordering of the elements in the list L. sort(L): Sorts the list L, using the natural ordering of its elements. swap(L, i, j): Swap the elements at indices i and j of list L.

Chapter 7. List and Iterator ADTs Converting Lists into Arrays Lists are a beautiful concept and they can be applied in a number of different contexts, but there are some instances where it would be useful if we could treat a list like an array. Fortunately, the java.util.Collection interface includes the following helpful methods for generating an array that has the same elements as the given collection: toArray(): Returns an array of elements of type Object containing all the elements in this collection. toArray(A): Returns an array of elements of the same element type as A containing all the elements in this collection. If the collection is a list, then the returned array will have its elements stored in the same order as that of the original list. Thus, if we have a useful array-based method that we want to use on a list or other type of collection, then we can do so by simply using that collection’s toArray() method to produce an array representation of that collection. Converting Arrays into Lists In a similar vein, it is often useful to be able to convert an array into an equivalent list. Fortunately, the java.util.Arrays class includes the following method: asList(A): Returns a list representation of the array A, with the same element type as the elements of A. The list returned by this method uses the array A as its internal representation for the list. So this list is guaranteed to be an array-based list and any changes made to it will automatically be reﬂected in A. Because of these types of side effects, use of the asList method should always be done with caution, so as to avoid unintended consequences. But, used with care, this method can often save us a lot of work. For instance, the following code fragment could be used to randomly shufﬂe an array of Integer objects, arr: Integer[ ] arr = {1, 2, 3, 4, 5, 6, 7, 8}; // allowed by autoboxing List<Integer> listArr = Arrays.asList(arr); Collections.shuﬄe(listArr); // this has side eﬀect of shuﬄing arr It is worth noting that the array A sent to the asList method should be a reference type (hence, our use of Integer rather than int in the above example). This is because the List interface is generic, and requires that the element type be an object.

7.6. Sorting a Positional List 7.6 Sorting a Positional List In Section 3.1.2, we introduced the insertion-sort algorithm in the context of an array-based sequence. In this section, we develop an implementation that operates on a PositionalList, relying on the same high-level algorithm in which each element is placed relative to a growing collection of previously sorted elements. We maintain a variable named marker that represents the rightmost position of the currently sorted portion of a list. During each pass, we consider the position just past the marker as the pivot and consider where the pivot’s element belongs relative to the sorted portion; we use another variable, named walk, to move leftward from the marker, as long as there remains a preceding element with value larger than the pivot’s. A typical conﬁguration of these variables is diagrammed in Figure 7.9. A Java implementation of this strategy is given in Code 7.15. marker pivot walk Figure 7.9: Overview of one step of our insertion-sort algorithm. The shaded elements, those up to and including marker, have already been sorted. In this step, the pivot’s element should be relocated immediately before the walk position. /∗∗Insertion-sort of a positional list of integers into nondecreasing order ∗/ public static void insertionSort(PositionalList<Integer> list) { Position<Integer> marker = list.ﬁrst(); // last position known to be sorted while (marker != list.last()) { Position<Integer> pivot = list.after(marker); int value = pivot.getElement(); // number to be placed if (value > marker.getElement()) // pivot is already sorted marker = pivot; else { // must relocate pivot Position<Integer> walk = marker; // ﬁnd leftmost item greater than value while (walk != list.ﬁrst() && list.before(walk).getElement() > value) walk = list.before(walk); list.remove(pivot); // remove pivot entry and list.addBefore(walk, value); // reinsert value in front of walk } } } Code Fragment 7.15: Java code for performing insertion-sort on a positional list.

Chapter 7. List and Iterator ADTs 7.7 Case Study: Maintaining Access Frequencies The positional list ADT is useful in a number of settings. For example, a program that simulates a game of cards could model each person’s hand as a positional list (Exercise P-7.60). Since most people keep cards of the same suit together, inserting and removing cards from a person’s hand could be implemented using the methods of the positional list ADT, with the positions being determined by a natural order of the suits. Likewise, a simple text editor embeds the notion of positional insertion and deletion, since such editors typically perform all updates relative to a cursor, which represents the current position in the list of characters of text being edited. In this section, we will consider maintaining a collection of elements while keeping track of the number of times each element is accessed. Keeping such access counts allows us to know which elements are among the most popular. Examples of such scenarios include a Web browser that keeps track of a user’s most accessed pages, or a music collection that maintains a list of the most frequently played songs for a user. We will model this with a new favorites list ADT that supports the size and isEmpty methods as well as the following: access(e): Accesses the element e, adding it to the favorites list if it is not already present, and increments its access count. remove(e): Removes element e from the favorites list, if present. getFavorites(k): Returns an iterable collection of the k most accessed elements. 7.7.1 Using a Sorted List Our ﬁrst approach for managing a list of favorites is to store elements in a linked list, keeping them in nonincreasing order of access counts. We access or remove an element by searching the list from the most frequently accessed to the least frequently accessed. Reporting the k most accessed elements is easy, as they are the ﬁrst k entries of the list. To maintain the invariant that elements are stored in nonincreasing order of access counts, we must consider how a single access operation may affect the order. The accessed element’s count increases by one, and so it may become larger than one or more of its preceding neighbors in the list, thereby violating the invariant. Fortunately, we can reestablish the sorted invariant using a technique similar to a single pass of the insertion-sort algorithm, introduced in the previous section. We can perform a backward traversal of the list, starting at the position of the element whose access count has increased, until we locate a valid position after which the element can be relocated.

7.7. Case Study: Maintaining Access Frequencies Using the Composition Pattern We wish to implement a favorites list by making use of a PositionalList for storage. If elements of the positional list were simply elements of the favorites list, we would be challenged to maintain access counts and to keep the proper count with the associated element as the contents of the list are reordered. We use a general objectoriented design pattern, the composition pattern, in which we deﬁne a single object that is composed of two or more other objects. (See, for example, Section 2.5.2.) Speciﬁcally, we deﬁne a nonpublic nested class, Item, that stores the element and its access count as a single instance. We then maintain our favorites list as a PositionalList of item instances, so that the access count for a user’s element is embedded alongside it in our representation. (An Item is never exposed to a user of a FavoritesList.) /∗∗Maintains a list of elements ordered according to access frequency. ∗/ public class FavoritesList<E> { // ---------------- nested Item class ---------------- protected static class Item<E> { private E value; private int count = 0; /∗∗Constructs new item with initial count of zero. ∗/ public Item(E val) { value = val; } public int getCount() { return count; } public E getValue() { return value; } public void increment() { count++; } } //----------- end of nested Item class ----------- PositionalList<Item<E>> list = new LinkedPositionalList<>(); // list of Items public FavoritesList() { } // constructs initially empty favorites list // nonpublic utilities /∗∗Provides shorthand notation to retrieve user's element stored at Position p. ∗/ protected E value(Position<Item<E>> p) { return p.getElement().getValue(); } /∗∗Provides shorthand notation to retrieve count of item stored at Position p. ∗/ protected int count(Position<Item<E>> p) {return p.getElement().getCount();} /∗∗Returns Position having element equal to e (or null if not found). ∗/ protected Position<Item<E>> ﬁndPosition(E e) { Position<Item<E>> walk = list.ﬁrst(); while (walk != null && !e.equals(value(walk))) walk = list.after(walk); return walk; } Code Fragment 7.16: Class FavoritesList. (Continues in Code Fragment 7.17.)

Chapter 7. List and Iterator ADTs /∗∗Moves item at Position p earlier in the list based on access count. ∗/ protected void moveUp(Position<Item<E>> p) { int cnt = count(p); // revised count of accessed item Position<Item<E>> walk = p; while (walk != list.ﬁrst() && count(list.before(walk)) < cnt) walk = list.before(walk); // found smaller count ahead of item if (walk != p) list.addBefore(walk, list.remove(p)); // remove/reinsert item } // public methods /∗∗Returns the number of items in the favorites list. ∗/ public int size() { return list.size(); } /∗∗Returns true if the favorites list is empty. ∗/ public boolean isEmpty() { return list.isEmpty(); } /∗∗Accesses element e (possibly new), increasing its access count. ∗/ public void access(E e) { Position<Item<E>> p = ﬁndPosition(e); // try to locate existing element if (p == null) p = list.addLast(new Item<E>(e)); // if new, place at end p.getElement().increment(); // always increment count moveUp(p); // consider moving forward } /∗∗Removes element equal to e from the list of favorites (if found). ∗/ public void remove(E e) { Position<Item<E>> p = ﬁndPosition(e); // try to locate existing element if (p != null) list.remove(p); } /∗∗Returns an iterable collection of the k most frequently accessed elements. ∗/ public Iterable<E> getFavorites(int k) throws IllegalArgumentException { if (k < 0 || k > size()) throw new IllegalArgumentException("Invalid k"); PositionalList<E> result = new LinkedPositionalList<>(); Iterator<Item<E>> iter = list.iterator(); for (int j=0; j < k; j++) result.addLast(iter.next().getValue()); return result; } } Code Fragment 7.17: Class FavoritesList. (Continued from Code Fragment 7.16.)

7.7. Case Study: Maintaining Access Frequencies 7.7.2 Using a List with the Move-to-Front Heuristic The previous implementation of a favorites list performs the access(e) method in time proportional to the index of e in the favorites list. That is, if e is the k th most popular element in the favorites list, then accessing it takes O(k) time. In many real-life access sequences (e.g., Web pages visited by a user), once an element is accessed it is more likely to be accessed again in the near future. Such scenarios are said to possess locality of reference. A heuristic, or rule of thumb, that attempts to take advantage of the locality of reference that is present in an access sequence is the move-to-front heuristic. To apply this heuristic, each time we access an element we move it all the way to the front of the list. Our hope, of course, is that this element will be accessed again in the near future. Consider, for example, a scenario in which we have n elements and the following series of n2 accesses: • element 1 is accessed n times. • element 2 is accessed n times. • ··· • element n is accessed n times. If we store the elements sorted by their access counts, inserting each element the ﬁrst time it is accessed, then • each access to element 1 runs in O(1) time. • each access to element 2 runs in O(2) time. • ··· • each access to element n runs in O(n) time. Thus, the total time for performing the series of accesses is proportional to n+2n+3n+···+n·n = n(1+2+3+···+n) = n· n(n+1) , which is O(n3). On the other hand, if we use the move-to-front heuristic, inserting each element the ﬁrst time it is accessed, then • each subsequent access to element 1 takes O(1) time. • each subsequent access to element 2 takes O(1) time. • ··· • each subsequent access to element n runs in O(1) time. So the running time for performing all the accesses in this case is O(n2). Thus, the move-to-front implementation has faster access times for this scenario. Still, the move-to-front approach is just a heuristic, for there are access sequences where using the move-to-front approach is slower than simply keeping the favorites list ordered by access counts.

Chapter 7. List and Iterator ADTs The Trade-Oﬀs with the Move-to-Front Heuristic If we no longer maintain the elements of the favorites list ordered by their access counts, when we are asked to ﬁnd the k most accessed elements, we need to search for them. We will implement the getFavorites(k) method as follows: 1. We copy all entries of our favorites list into another list, named temp. 2. We scan the temp list k times. In each scan, we ﬁnd the entry with the largest access count, remove this entry from temp, and add it to the results. This implementation of method getFavorites(k) takes O(kn) time. Thus, when k is a constant, method getFavorites(k) runs in O(n) time. This occurs, for example, when we want to get the “top ten” list. However, if k is proportional to n, then the method getFavorites(k) runs in O(n2) time. This occurs, for example, when we want a “top 25%” list. In Chapter 9 we will introduce a data structure that will allow us to implement getFavorites in O(n+klogn) time (see Exercise P-9.51), and more advanced techniques could be used to perform getFavorites in O(n+klogk) time. We could easily achieve O(nlogn) time if we use a standard sorting algorithm to reorder the temporary list before reporting the top k (see Chapter 12); this approach would be preferred to the original in the case that k is Ω(logn). (Recall the big-Omega notation introduced in Section 4.3.1 to give an asymptotic lower bound on the running time of an algorithm.) There is a specialized sorting algorithm (see Section 12.3.2) that can take advantage of the fact that access counts are integers in order to achieve O(n) time for getFavorites, for any value of k. Implementing the Move-to-Front Heuristic in Java We give an implementation of a favorites list using the move-to-front heuristic in Code Fragment 7.18. The new FavoritesListMTF class inherits most of its functionality from the original FavoritesList as a base class. By our original design, the access method of the original class relies on a protected utility named moveUp to enact the potential shifting of an element forward in the list, after its access count had been incremented. Therefore, we implement the move-to-front heuristic by simply overriding the moveUp method so that each accessed element is moved directly to the front of the list (if not already there). This action is easily implemented by means of the positional list ADT. The more complex portion of our FavoritesListMTF class is the new deﬁnition for the getFavorites method. We rely on the ﬁrst of the approaches outlined above, inserting copies of the items into a temporary list and then repeatedly ﬁnding, reporting, and removing an element that has the largest access count of those remaining.

7.7. Case Study: Maintaining Access Frequencies /∗∗Maintains a list of elements ordered with move-to-front heuristic. ∗/ public class FavoritesListMTF<E> extends FavoritesList<E> { /∗∗Moves accessed item at Position p to the front of the list. ∗/ protected void moveUp(Position<Item<E>> p) { if (p != list.ﬁrst()) list.addFirst(list.remove(p)); // remove/reinsert item } /∗∗Returns an iterable collection of the k most frequently accessed elements. ∗/ public Iterable<E> getFavorites(int k) throws IllegalArgumentException { if (k < 0 || k > size()) throw new IllegalArgumentException("Invalid k"); // we begin by making a copy of the original list PositionalList<Item<E>> temp = new LinkedPositionalList<>(); for (Item<E> item : list) temp.addLast(item); // we repeated ﬁnd, report, and remove element with largest count PositionalList<E> result = new LinkedPositionalList<>(); for (int j=0; j < k; j++) { Position<Item<E>> highPos = temp.ﬁrst(); Position<Item<E>> walk = temp.after(highPos); while (walk != null) { if (count(walk) > count(highPos)) highPos = walk; walk = temp.after(walk); } // we have now found element with highest count result.addLast(value(highPos)); temp.remove(highPos); } return result; } } Code Fragment 7.18: Class FavoritesListMTF implementing the move-to-front heuristic. This class extends FavoritesList (Code Fragments 7.16 and 7.17) and overrides methods moveUp and getFavorites.

Chapter 8. Trees 8.1 General Trees Productivity experts say that breakthroughs come by thinking “nonlinearly.” In this chapter, we will discuss one of the most important nonlinear data structures in computing—trees. Tree structures are indeed a breakthrough in data organization, for they allow us to implement a host of algorithms much faster than when using linear data structures, such as arrays or linked lists. Trees also provide a natural organization for data, and consequently have become ubiquitous structures in ﬁle systems, graphical user interfaces, databases, websites, and many other computer systems. It is not always clear what productivity experts mean by “nonlinear” thinking, but when we say that trees are “nonlinear,” we are referring to an organizational relationship that is richer than the simple “before” and “after” relationships between objects in sequences. The relationships in a tree are hierarchical, with some objects being “above” and some “below” others. Actually, the main terminology for tree data structures comes from family trees, with the terms “parent,” “child,” “ancestor,” and “descendant” being the most common words used to describe relationships. We show an example of a family tree in Figure 8.1. Eldaah Nebaioth Kedar Adbeel Mibsam Mishma Dumah Massa Hadad Tema Jetur Naphish Kedemah Ishmael Gad Naphtali Dan Judah Levi Simeon Asher Issachar Zebulun Dinah Joseph Benjamin Eliphaz Reuel Jeush Jalam Reuben Korah Jacob (Israel) Esau Isaac Zimran Jokshan Medan Midian Ishbak Shuah Abraham Sheba Dedan Ephah Epher Hanoch Abida Figure 8.1: A family tree showing some descendants of Abraham, as recorded in Genesis, chapters 25–36.

8.1. General Trees 8.1.1 Tree Deﬁnitions and Properties A tree is an abstract data type that stores elements hierarchically. With the exception of the top element, each element in a tree has a parent element and zero or more children elements. A tree is usually visualized by placing elements inside ovals or rectangles, and by drawing the connections between parents and children with straight lines. (See Figure 8.2.) We typically call the top element the root of the tree, but it is drawn as the highest element, with the other elements being connected below (just the opposite of a botanical tree). Europe Asia Africa Australia Canada Overseas S. America Domestic International TV CD Tuner Sales Purchasing Manufacturing R&D Electronics R’Us Figure 8.2: A tree with 17 nodes representing the organization of a ﬁctitious corporation. The root stores Electronics R’Us. The children of the root store R&D, Sales, Purchasing, and Manufacturing. The internal nodes store Sales, International, Overseas, Electronics R’Us, and Manufacturing. Formal Tree Deﬁnition Formally, we deﬁne a tree T as a set of nodes storing elements such that the nodes have a parent-child relationship that satisﬁes the following properties: • If T is nonempty, it has a special node, called the root of T, that has no parent. • Each node v of T different from the root has a unique parent node w; every node with parent w is a child of w. Note that according to our deﬁnition, a tree can be empty, meaning that it does not have any nodes. This convention also allows us to deﬁne a tree recursively such that a tree T is either empty or consists of a node r, called the root of T, and a (possibly empty) set of subtrees whose roots are the children of r.

Chapter 8. Trees Other Node Relationships Two nodes that are children of the same parent are siblings. A node v is external if v has no children. A node v is internal if it has one or more children. External nodes are also known as leaves. Example 8.1: In Section 5.1.4, we discussed the hierarchical relationship between ﬁles and directories in a computer’s ﬁle system, although at the time we did not emphasize the nomenclature of a ﬁle system as a tree. In Figure 8.3, we revisit an earlier example. We see that the internal nodes of the tree are associated with directories and the leaves are associated with regular ﬁles. In the Unix and Linux operating systems, the root of the tree is appropriately called the “root directory,” and is represented by the symbol “/.” /user/rt/courses/ cs016/ cs252/ programs/ homeworks/ projects/ papers/ demos/ hw1 hw2 hw3 pr1 pr2 pr3 grades market buylow sellhigh grades Figure 8.3: Tree representing a portion of a ﬁle system. A node u is an ancestor of a node v if u = v or u is an ancestor of the parent of v. Conversely, we say that a node v is a descendant of a node u if u is an ancestor of v. For example, in Figure 8.3, cs252/ is an ancestor of papers/, and pr3 is a descendant of cs016/. The subtree of T rooted at a node v is the tree consisting of all the descendants of v in T (including v itself). In Figure 8.3, the subtree rooted at cs016/ consists of the nodes cs016/, grades, homeworks/, programs/, hw1, hw2, hw3, pr1, pr2, and pr3. Edges and Paths in Trees An edge of tree T is a pair of nodes (u,v) such that u is the parent of v, or vice versa. A path of T is a sequence of nodes such that any two consecutive nodes in the sequence form an edge. For example, the tree in Figure 8.3 contains the path (cs252/, projects/, demos/, market).

8.1. General Trees Ordered Trees A tree is ordered if there is a meaningful linear order among the children of each node; that is, we purposefully identify the children of a node as being the ﬁrst, second, third, and so on. Such an order is usually visualized by arranging siblings left to right, according to their order. Example 8.2: The components of a structured document, such as a book, are hierarchically organized as a tree whose internal nodes are parts, chapters, and sections, and whose leaves are paragraphs, tables, ﬁgures, and so on. (See Figure 8.4.) The root of the tree corresponds to the book itself. We could, in fact, consider expanding the tree further to show paragraphs consisting of sentences, sentences consisting of words, and words consisting of characters. Such a tree is an example of an ordered tree, because there is a well-deﬁned order among the children of each node. ... ... ¶ ¶ ... ¶ ¶ Book Part A Part B References Preface ... ... ... ... Ch. 1 Ch. 5 Ch. 6 Ch. 9 ¶ ¶ ¶ ¶ ... ... ... ... § 1.4 § 1.1 § 5.7 § 5.1 § 9.6 § 9.1 § 6.5 § 6.1 Figure 8.4: An ordered tree associated with a book. Let’s look back at the other examples of trees that we have described thus far, and consider whether the order of children is signiﬁcant. A family tree that describes generational relationships, as in Figure 8.1, is often modeled as an ordered tree, with siblings ordered according to their birth. In contrast, an organizational chart for a company, as in Figure 8.2, is typically considered an unordered tree. Likewise, when using a tree to describe an inheritance hierarchy, as in Figure 2.7, there is no particular signiﬁcance to the order among the subclasses of a parent class. Finally, we consider the use of a tree in modeling a computer’s ﬁle system, as in Figure 8.3. Although an operating system often displays entries of a directory in a particular order (e.g., alphabetical, chronological), such an order is not typically inherent to the ﬁle system’s representation.

Chapter 8. Trees 8.1.2 The Tree Abstract Data Type As we did with positional lists in Section 7.3, we deﬁne a tree ADT using the concept of a position as an abstraction for a node of a tree. An element is stored at each position, and positions satisfy parent-child relationships that deﬁne the tree structure. A position object for a tree supports the method: getElement(): Returns the element stored at this position. The tree ADT then supports the following accessor methods, allowing a user to navigate the various positions of a tree T: root(): Returns the position of the root of the tree (or null if empty). parent(p): Returns the position of the parent of position p (or null if p is the root). children(p): Returns an iterable collection containing the children of position p (if any). numChildren(p): Returns the number of children of position p. If a tree T is ordered, then children(p) reports the children of p in order. In addition to the above fundamental accessor methods, a tree supports the following query methods: isInternal(p): Returns true if position p has at least one child. isExternal(p): Returns true if position p does not have any children. isRoot(p): Returns true if position p is the root of the tree. These methods make programming with trees easier and more readable, since we can use them in the conditionals of if statements and while loops. Trees support a number of more general methods, unrelated to the speciﬁc structure of the tree. These incude: size(): Returns the number of positions (and hence elements) that are contained in the tree. isEmpty(): Returns true if the tree does not contain any positions (and thus no elements). iterator(): Returns an iterator for all elements in the tree (so that the tree itself is Iterable). positions(): Returns an iterable collection of all positions of the tree. If an invalid position is sent as a parameter to any method of a tree, then an IllegalArgumentException is thrown. We do not deﬁne any methods for creating or modifying trees at this point. We prefer to describe different tree update methods in conjunction with speciﬁc implementations of the tree interface, and speciﬁc applications of trees.

8.1. General Trees A Tree Interface in Java In Code Fragment 8.1, we formalize the Tree ADT by deﬁning the Tree interface in Java. We rely upon the same deﬁnition of the Position interface as introduced for positional lists in Section 7.3.2. Note well that we declare the Tree interface to formally extend Java’s Iterable interface (and we include a declaration of the required iterator() method). /∗∗An interface for a tree where nodes can have an arbitrary number of children. ∗/ public interface Tree<E> extends Iterable<E> { Position<E> root(); Position<E> parent(Position<E> p) throws IllegalArgumentException; Iterable<Position<E>> children(Position<E> p) throws IllegalArgumentException; int numChildren(Position<E> p) throws IllegalArgumentException; boolean isInternal(Position<E> p) throws IllegalArgumentException; boolean isExternal(Position<E> p) throws IllegalArgumentException; boolean isRoot(Position<E> p) throws IllegalArgumentException; int size(); boolean isEmpty(); Iterator<E> iterator(); Iterable<Position<E>> positions(); } Code Fragment 8.1: Deﬁnition of the Tree interface. An AbstractTree Base Class in Java In Section 2.3, we discussed the role of interfaces and abstract classes in Java. While an interface is a type deﬁnition that includes public declarations of various methods, an interface cannot include deﬁnitions for any of those methods. In contrast, an abstract class may deﬁne concrete implementations for some of its methods, while leaving other abstract methods without deﬁnition. An abstract class is designed to serve as a base class, through inheritance, for one or more concrete implementations of an interface. When some of the functionality of an interface is implemented in an abstract class, less work remains to complete a concrete implementation. The standard Java libraries include many such abstract classes, including several within the Java Collections Framework. To make their purpose clear, those classes are conventionally named beginning with the word Abstract. For example, there is an AbstractCollection class that implements some of the functionality of the Collection interface, an AbstractQueue class that implements some of the functionality of the Queue interface, and an AbstractList class that implements some of the functionality of the List interface.

Chapter 8. Trees In the case of our Tree interface, we will deﬁne an AbstractTree base class, demonstrating how many tree-based algorithms can be described independently of the low-level representation of a tree data structure. In fact, if a concrete implementation provides three fundamental methods—root(), parent(p), and children(p)— all other behaviors of the Tree interface can be derived within the AbstractTree base class. Code Fragment 8.2 presents an initial implementation of an AbstractTree base class that provides the most trivial methods of the Tree interface. We will defer until Section 8.4 a discussion of general tree-traversal algorithms that can be used to produced the positions() iteration within the AbstractTree class. As with our positional list ADT in Chapter 7, the iteration of positions of the tree can easily be adapted to produce an iteration of the elements of a tree, or even to determine the size of a tree (although our concrete tree implementations will provide more direct means for reporting the size). /∗∗An abstract base class providing some functionality of the Tree interface. ∗/ public abstract class AbstractTree<E> implements Tree<E> { public boolean isInternal(Position<E> p) { return numChildren(p) > 0; } public boolean isExternal(Position<E> p) { return numChildren(p) == 0; } public boolean isRoot(Position<E> p) { return p == root(); } public boolean isEmpty() { return size() == 0; } } Code Fragment 8.2: An initial implementation of the AbstractTree base class. (We add additional functionality to this class as the chapter continues.) 8.1.3 Computing Depth and Height Let p be a position within tree T. The depth of p is the number of ancestors of p, other than p itself. For example, in the tree of Figure 8.2, the node storing International has depth 2. Note that this deﬁnition implies that the depth of the root of T is 0. The depth of p can also be recursively deﬁned as follows: • If p is the root, then the depth of p is 0. • Otherwise, the depth of p is one plus the depth of the parent of p. Based on this deﬁnition, we present a simple recursive algorithm, depth, in Code Fragment 8.3, for computing the depth of a position p in tree T. This method calls itself recursively on the parent of p, and adds 1 to the value returned. The running time of depth(p) for position p is O(dp + 1), where dp denotes the depth of p in the tree, because the algorithm performs a constant-time recursive step for each ancestor of p. Thus, algorithm depth(p) runs in O(n) worst-case time, where n is the total number of positions of T, because a position of T may have depth n −1 if all nodes form a single branch. Although such a running time is a function of the input size, it is more informative to characterize the running time in terms of the parameter dp, as this parameter may be much smaller than n.

8.1. General Trees /∗∗Returns the number of levels separating Position p from the root. ∗/ public int depth(Position<E> p) { if (isRoot(p)) return 0; else return 1 + depth(parent(p)); } Code Fragment 8.3: Method depth, as implemented within the AbstractTree class. Height We next deﬁne the height of a tree to be equal to the maximum of the depths of its positions (or zero, if the tree is empty). For example, the tree of Figure 8.2 has height 4, as the node storing Africa (and its siblings) has depth 4. It is easy to see that the position with maximum depth must be a leaf. In Code Fragment 8.4, we present a method that computes the height of a tree based on this deﬁnition. Unfortunately, such an approach is not very efﬁcient, and so name the algorithm heightBad and declare it as a private method of the AbstractTree class (so that it cannot be used by others). /∗∗Returns the height of the tree. ∗/ private int heightBad() { // works, but quadratic worst-case time int h = 0; for (Position<E> p : positions()) if (isExternal(p)) // only consider leaf positions h = Math.max(h, depth(p)); return h; } Code Fragment 8.4: Method heightBad of the AbstractTree class. Note that this method calls the depth method from Code Fragment 8.3. Although we have not yet deﬁned the positions() method, we will see that it can be implemented such that the entire iteration runs in O(n) time, where n is the number of positions of T. Because heightBad calls algorithm depth(p) on each leaf of T, its running time is O(n+ ∑p∈L(dp + 1)), where L is the set of leaf positions of T. In the worst case, the sum ∑p∈L(dp +1) is proportional to n2. (See Exercise C-8.31.) Thus, algorithm heightBad runs in O(n2) worst-case time. We can compute the height of a tree more efﬁciently, in O(n) worst-case time, by considering a recursive deﬁnition. To do this, we will parameterize a function based on a position within the tree, and calculate the height of the subtree rooted at that position. Formally, we deﬁne the height of a position p in a tree T as follows: • If p is a leaf, then the height of p is 0. • Otherwise, the height of p is one more than the maximum of the heights of p’s children.

Chapter 8. Trees The following proposition relates our original deﬁnition of the height of a tree to the height of the root position using this recursive formula. Proposition 8.3: The height of the root of a nonempty tree T, according to the recursive deﬁnition, equals the maximum depth among all leaves of tree T. We leave the justiﬁcation of this proposition as Exercise R-8.3. An implementation of a recursive algorithm to compute the height of a subtree rooted at a given position p is presented in Code Fragment 8.5. The overall height of a nonempty tree can be computed by sending the root of the tree as a parameter. /∗∗Returns the height of the subtree rooted at Position p. ∗/ public int height(Position<E> p) { int h = 0; // base case if p is external for (Position<E> c : children(p)) h = Math.max(h, 1 + height(c)); return h; } Code Fragment 8.5: Method height for computing the height of a subtree rooted at a position p of an AbstractTree. It is important to understand why method height is more efﬁcient than method heightBad. The algorithm is recursive, and it progresses in a top-down fashion. If the method is initially called on the root of T, it will eventually be called once for each position of T. This is because the root eventually invokes the recursion on each of its children, which in turn invokes the recursion on each of their children, and so on. We can determine the running time of the recursive height algorithm by summing, over all the positions, the amount of time spent on the nonrecursive part of each call. (Review Section 5.2 for analyses of recursive processes.) In our implementation, there is a constant amount of work per position, plus the overhead of computing the maximum over the iteration of children. Although we do not yet have a concrete implementation of children(p), we assume that such an iteration is executed in O(cp + 1) time, where cp denotes the number of children of p. Algorithm height(p) spends O(cp+1) time at each position p to compute the maximum, and its overall running time is O(∑p(cp +1)) = O(n+∑p cp). In order to complete the analysis, we make use of the following property. Proposition 8.4: Let T be a tree with n positions, and let cp denote the number of children of a position p of T. Then, summing over the positions of T, ∑p cp = n−1. Justiﬁcation: Each position of T, with the exception of the root, is a child of another position, and thus contributes one unit to the above sum. By Proposition 8.4, the running time of algorithm height, when called on the root of T, is O(n), where n is the number of positions of T.

8.2. Binary Trees 8.2 Binary Trees A binary tree is an ordered tree with the following properties: 1. Every node has at most two children. 2. Each child node is labeled as being either a left child or a right child. 3. A left child precedes a right child in the order of children of a node. The subtree rooted at a left or right child of an internal node v is called a left subtree or right subtree, respectively, of v. A binary tree is proper if each node has either zero or two children. Some people also refer to such trees as being full binary trees. Thus, in a proper binary tree, every internal node has exactly two children. A binary tree that is not proper is improper. Example 8.5: An important class of binary trees arises in contexts where we wish to represent a number of different outcomes that can result from answering a series of yes-or-no questions. Each internal node is associated with a question. Starting at the root, we go to the left or right child of the current node, depending on whether the answer to the question is “Yes” or “No.” With each decision, we follow an edge from a parent to a child, eventually tracing a path in the tree from the root to a leaf. Such binary trees are known as decision trees, because a leaf position p in such a tree represents a decision of what to do if the questions associated with p’s ancestors are answered in a way that leads to p. A decision tree is a proper binary tree. Figure 8.5 illustrates a decision tree that provides recommendations to a prospective investor. Yes Yes Yes No No No Are you nervous? Will you need to access most of the money within the next 5 years? Are you willing to accept risks in exchange for higher expected returns? Money market fund. Stock portfolio. Savings account. Diversiﬁed portfolio with stocks, bonds, and short-term instruments. Figure 8.5: A decision tree providing investment advice.

Chapter 8. Trees Example 8.6: An arithmetic expression can be represented by a binary tree whose leaves are associated with variables or constants, and whose internal nodes are associated with one of the operators +, −, ∗, and /, as demonstrated in Figure 8.6. Each node in such a tree has a value associated with it. • If a node is leaf, then its value is that of its variable or constant. • If a node is internal, then its value is deﬁned by applying its operation to the values of its children. A typical arithmetic expression tree is a proper binary tree, since each operator +, −, ∗, and / takes exactly two operands. Of course, if we were to allow unary operators, like negation (−), as in “−x,” then we could have an improper binary tree. ∗ + − + + − − / ∗ Figure 8.6: A binary tree representing an arithmetic expression. This tree represents the expression ((((3+1)∗3)/((9−5)+2))−((3∗(7−4))+6)). The value associated with the internal node labeled “/” is 2. A Recursive Binary Tree Deﬁnition Incidentally, we can also deﬁne a binary tree in a recursive way. In that case, a binary tree is either: • An empty tree. • A nonempty tree having a root node r, which stores an element, and two binary trees that are respectively the left and right subtrees of r. We note that one or both of those subtrees can be empty by this deﬁnition.

8.2. Binary Trees 8.2.1 The Binary Tree Abstract Data Type As an abstract data type, a binary tree is a specialization of a tree that supports three additional accessor methods: left(p): Returns the position of the left child of p (or null if p has no left child). right(p): Returns the position of the right child of p (or null if p has no right child). sibling(p): Returns the position of the sibling of p (or null if p has no sibling). Just as in Section 8.1.2 for the tree ADT, we do not deﬁne specialized update methods for binary trees here. Instead, we will consider some possible update methods when we describe speciﬁc implementations and applications of binary trees. Deﬁning a BinaryTree Interface Code Fragment 8.6 formalizes the binary tree ADT by deﬁning a BinaryTree interface in Java. This interface extends the Tree interface that was given in Section 8.1.2 to add the three new behaviors. In this way, a binary tree is expected to support all the functionality that was deﬁned for general trees (e.g., root, isExternal, parent), and the new behaviors left, right, and sibling. /∗∗An interface for a binary tree, in which each node has at most two children. ∗/ public interface BinaryTree<E> extends Tree<E> { /∗∗Returns the Position of p's left child (or null if no child exists). ∗/ Position<E> left(Position<E> p) throws IllegalArgumentException; /∗∗Returns the Position of p's right child (or null if no child exists). ∗/ Position<E> right(Position<E> p) throws IllegalArgumentException; /∗∗Returns the Position of p's sibling (or null if no sibling exists). ∗/ Position<E> sibling(Position<E> p) throws IllegalArgumentException; } Code Fragment 8.6: A BinaryTree interface that extends the Tree interface from Code Fragment 8.1. Deﬁning an AbstractBinaryTree Base Class We continue our use of abstract base classes to promote greater reusability within our code. The AbstractBinaryTree class, presented in Code Fragment 8.7, inherits from the AbstractTree class from Section 8.1.2. It provides additional concrete methods that can be derived from the newly declared left and right methods (which remain abstract).

Chapter 8. Trees The new sibling method is derived from a combination of left, right, and parent. Typically, we identify the sibling of a position p as the “other” child of p’s parent. However, p does not have a sibling if it is the root, or if it is the only child of its parent. We can also use the presumed left and right methods to provide concrete implementations of the numChildren and children methods, which are part of the original Tree interface. Using the terminology of Section 7.4, the implementation of the children method relies on producing a snapshot. We create an empty java.util.ArrayList, which qualiﬁes as being an iterable container, and then add any children that exist, ordered so that a left child is reported before a right child. /∗∗An abstract base class providing some functionality of the BinaryTree interface.∗/ public abstract class AbstractBinaryTree<E> extends AbstractTree<E> implements BinaryTree<E> { /∗∗Returns the Position of p's sibling (or null if no sibling exists). ∗/ public Position<E> sibling(Position<E> p) { Position<E> parent = parent(p); if (parent == null) return null; // p must be the root if (p == left(parent)) // p is a left child return right(parent); // (right child might be null) else // p is a right child return left(parent); // (left child might be null) } /∗∗Returns the number of children of Position p. ∗/ public int numChildren(Position<E> p) { int count=0; if (left(p) != null) count++; if (right(p) != null) count++; return count; } /∗∗Returns an iterable collection of the Positions representing p's children. ∗/ public Iterable<Position<E>> children(Position<E> p) { List<Position<E>> snapshot = new ArrayList<>(2); // max capacity of 2 if (left(p) != null) snapshot.add(left(p)); if (right(p) != null) snapshot.add(right(p)); return snapshot; } } Code Fragment 8.7: An AbstractBinaryTree class that extends the AbstractTree class of Code Fragment 8.2 and implements the BinaryTree interface of Code Fragment 8.6.

8.2. Binary Trees 8.2.2 Properties of Binary Trees Binary trees have several interesting properties dealing with relationships between their heights and number of nodes. We denote the set of all nodes of a tree T at the same depth d as level d of T. In a binary tree, level 0 has at most one node (the root), level 1 has at most two nodes (the children of the root), level 2 has at most four nodes, and so on. (See Figure 8.7.) In general, level d has at most 2d nodes. ... ... ... ... Level Nodes Figure 8.7: Maximum number of nodes in the levels of a binary tree. We can see that the maximum number of nodes on the levels of a binary tree grows exponentially as we go down the tree. From this simple observation, we can derive the following properties relating the height of a binary tree T with its number of nodes. A detailed justiﬁcation of these properties is left as Exercise R-8.8. Proposition 8.7: Let T be a nonempty binary tree, and let n, nE, nI, and h denote the number of nodes, number of external nodes, number of internal nodes, and height of T, respectively. Then T has the following properties: 1. h+1 ≤n ≤2h+1 −1 2. 1 ≤nE ≤2h 3. h ≤nI ≤2h −1 4. log(n+1)−1 ≤h ≤n−1 Also, if T is proper, then T has the following properties: 1. 2h+1 ≤n ≤2h+1 −1 2. h+1 ≤nE ≤2h 3. h ≤nI ≤2h −1 4. log(n+1)−1 ≤h ≤(n−1)/2

Chapter 8. Trees Relating Internal Nodes to External Nodes in a Proper Binary Tree In addition to the earlier binary tree properties, the following relationship exists between the number of internal nodes and external nodes in a proper binary tree. Proposition 8.8: In a nonempty proper binary tree T, with nE external nodes and nI internal nodes, we have nE = nI +1. Justiﬁcation: We justify this proposition by removing nodes from T and dividing them up into two “piles,” an internal-node pile and an external-node pile, until T becomes empty. The piles are initially empty. By the end, we will show that the external-node pile has one more node than the internal-node pile. We consider two cases: Case 1: If T has only one node v, we remove v and place it on the external-node pile. Thus, the external-node pile has one node and the internal-node pile is empty. Case 2: Otherwise (T has more than one node), we remove from T an (arbitrary) external node w and its parent v, which is an internal node. We place w on the external-node pile and v on the internal-node pile. If v has a parent u, then we reconnect u with the former sibling z of w, as shown in Figure 8.8. This operation, removes one internal node and one external node, and leaves the tree being a proper binary tree. Repeating this operation, we eventually are left with a ﬁnal tree consisting of a single node. Note that the same number of external and internal nodes have been removed and placed on their respective piles by the sequence of operations leading to this ﬁnal tree. Now, we remove the node of the ﬁnal tree and we place it on the external-node pile. Thus, the external-node pile has one more node than the internal-node pile. v u w z u z u z (a) (b) (c) Figure 8.8: Operation that removes an external node and its parent node, used in the justiﬁcation of Proposition 8.8. Note that the above relationship does not hold, in general, for improper binary trees and nonbinary trees, although there are other interesting relationships that do hold. (See Exercises C-8.30 through C-8.32.)

8.3. Implementing Trees 8.3 Implementing Trees The AbstractTree and AbstractBinaryTree classes that we have deﬁned thus far in this chapter are both abstract base classes. Although they provide a great deal of support, neither of them can be directly instantiated. We have not yet deﬁned key implementation details for how a tree will be represented internally, and how we can effectively navigate between parents and children. There are several choices for the internal representation of trees. We describe the most common representations in this section. We begin with the case of a binary tree, since its shape is more strictly deﬁned. 8.3.1 Linked Structure for Binary Trees A natural way to realize a binary tree T is to use a linked structure, with a node (see Figure 8.9a) that maintains references to the element stored at a position p and to the nodes associated with the children and parent of p. If p is the root of T, then the parent ﬁeld of p is null. Likewise, if p does not have a left child (respectively, right child), the associated ﬁeld is null. The tree itself maintains an instance variable storing a reference to the root node (if any), and a variable, called size, that represents the overall number of nodes of T. We show such a linked structure representation of a binary tree in Figure 8.9b. parent element right left root ∅ ∅ ∅ ∅ ∅ ∅ ∅ Baltimore Chicago New York Providence Seattle size (a) (b) Figure 8.9: A linked structure for representing: (a) a single node; (b) a binary tree.

Chapter 8. Trees Operations for Updating a Linked Binary Tree The Tree and BinaryTree interfaces deﬁne a variety of methods for inspecting an existing tree, yet they do not declare any update methods. Presuming that a newly constructed tree is empty, we would like to have means for changing the structure of content of a tree. Although the principle of encapsulation suggests that the outward behaviors of an abstract data type need not depend on the internal representation, the efﬁciency of the operations depends greatly upon the representation. We therefore prefer to have each concrete implementation of a tree class support the most suitable behaviors for updating a tree. In the case of a linked binary tree, we suggest that the following update methods be supported: addRoot(e): Creates a root for an empty tree, storing e as the element, and returns the position of that root; an error occurs if the tree is not empty. addLeft(p, e): Creates a left child of position p, storing element e, and returns the position of the new node; an error occurs if p already has a left child. addRight(p, e): Creates a right child of position p, storing element e, and returns the position of the new node; an error occurs if p already has a right child. set(p, e): Replaces the element stored at position p with element e, and returns the previously stored element. attach(p, T1, T2): Attaches the internal structure of trees T1 and T2 as the respective left and right subtrees of leaf position p and resets T1 and T2 to empty trees; an error condition occurs if p is not a leaf. remove(p): Removes the node at position p, replacing it with its child (if any), and returns the element that had been stored at p; an error occurs if p has two children. We have speciﬁcally chosen this collection of operations because each can be implemented in O(1) worst-case time with our linked representation. The most complex of these are attach and remove, due to the case analyses involving the various parent-child relationships and boundary conditions, yet there remains only a constant number of operations to perform. (The implementation of both methods could be greatly simpliﬁed if we used a tree representation with a sentinel node, akin to our treatment of positional lists; see Exercise C-8.38.)

8.3. Implementing Trees Java Implementation of a Linked Binary Tree Structure We now present a concrete implementation of a LinkedBinaryTree class that implements the binary tree ADT, and supports the update methods described on the previous page. The new class formally extends the AbstractBinaryTree base class, inheriting several concrete implementations of methods from that class (as well as the formal designation that it implements the BinaryTree interface). The low-level details of our linked tree implementation are reminiscent of techniques used when implementing the LinkedPositionalList class in Section 7.3.3. We deﬁne a nonpublic nested Node class to represent a node, and to serve as a Position for the public interface. As was portrayed in Figure 8.9, a node maintains a reference to an element, as well as references to its parent, its left child, and its right child (any of which might be null). The tree instance maintains a reference to the root node (possibly null), and a count of the number of nodes in the tree. We also provide a validate utility that is called anytime a Position is received as a parameter, to ensure that it is a valid node. In the case of a linked tree, we adopt a convention in which we set a node’s parent pointer to itself when it is removed from a tree, so that we can later recognize it as an invalid position. The entire LinkedBinaryTree class is presented in Code Fragments 8.8–8.11. We provide the following guide to that code: • Code Fragment 8.8 contains the deﬁnition of the nested Node class, which implements the Position interface. It also deﬁnes a method, createNode, that returns a new node instance. Such a design uses what is known as the factory method pattern, allowing us to later subclass our tree in order to use a specialized node type. (See Section 11.2.1.) Code Fragment 8.8 concludes with the declaration of the instance variables of the outer LinkedBinaryTree class and its constructor. • Code Fragment 8.9 includes the protected validate(p) method, followed by the accessors size, root, left, and right. We note that all other methods of the Tree and BinaryTree interfaces are derived from these four concrete methods, via the AbstractTree and AbstractBinaryTree base classes. • Code Fragments 8.10 and 8.11 provide the six update methods for a linked binary tree, as described on the preceding page. We note that the three methods—addRoot, addLeft, and addRight—each rely on use of the factory method, createNode, to produce a new node instance. The remove method, given at the end of Code Fragment 8.11, intentionally sets the parent ﬁeld of a deleted node to refer to itself, in accordance with our conventional representation of a defunct node (as detected within the validate method). It resets all other ﬁelds to null, to aid in garbage collection.

Chapter 8. Trees /∗∗Concrete implementation of a binary tree using a node-based, linked structure. ∗/ public class LinkedBinaryTree<E> extends AbstractBinaryTree<E> { //---------------- nested Node class ---------------- protected static class Node<E> implements Position<E> { private E element; // an element stored at this node private Node<E> parent; // a reference to the parent node (if any) private Node<E> left; // a reference to the left child (if any) private Node<E> right; // a reference to the right child (if any) /∗∗Constructs a node with the given element and neighbors. ∗/ public Node(E e, Node<E> above, Node<E> leftChild, Node<E> rightChild) { element = e; parent = above; left = leftChild; right = rightChild; } // accessor methods public E getElement() { return element; } public Node<E> getParent() { return parent; } public Node<E> getLeft() { return left; } public Node<E> getRight() { return right; } // update methods public void setElement(E e) { element = e; } public void setParent(Node<E> parentNode) { parent = parentNode; } public void setLeft(Node<E> leftChild) { left = leftChild; } public void setRight(Node<E> rightChild) { right = rightChild; } } //----------- end of nested Node class ----------- /∗∗Factory function to create a new node storing element e. ∗/ protected Node<E> createNode(E e, Node<E> parent, Node<E> left, Node<E> right) { return new Node<E>(e, parent, left, right); } // LinkedBinaryTree instance variables protected Node<E> root = null; // root of the tree private int size = 0; // number of nodes in the tree // constructor public LinkedBinaryTree() { } // constructs an empty binary tree Code Fragment 8.8: An implementation of the LinkedBinaryTree class. (Continues in Code Fragments 8.9–8.11.)

8.3. Implementing Trees // nonpublic utility /∗∗Validates the position and returns it as a node. ∗/ protected Node<E> validate(Position<E> p) throws IllegalArgumentException { if (!(p instanceof Node)) throw new IllegalArgumentException("Not valid position type"); Node<E> node = (Node<E>) p; // safe cast if (node.getParent() == node) // our convention for defunct node throw new IllegalArgumentException("p is no longer in the tree"); return node; } // accessor methods (not already implemented in AbstractBinaryTree) /∗∗Returns the number of nodes in the tree. ∗/ public int size() { return size; } /∗∗Returns the root Position of the tree (or null if tree is empty). ∗/ public Position<E> root() { return root; } /∗∗Returns the Position of p's parent (or null if p is root). ∗/ public Position<E> parent(Position<E> p) throws IllegalArgumentException { Node<E> node = validate(p); return node.getParent(); } /∗∗Returns the Position of p's left child (or null if no child exists). ∗/ public Position<E> left(Position<E> p) throws IllegalArgumentException { Node<E> node = validate(p); return node.getLeft(); } /∗∗Returns the Position of p's right child (or null if no child exists). ∗/ public Position<E> right(Position<E> p) throws IllegalArgumentException { Node<E> node = validate(p); return node.getRight(); } Code Fragment 8.9: An implementation of the LinkedBinaryTree class. (Continued from Code Fragment 8.8; continues in Code Fragments 8.10 and 8.11.)

Chapter 8. Trees // update methods supported by this class /∗∗Places element e at the root of an empty tree and returns its new Position. ∗/ public Position<E> addRoot(E e) throws IllegalStateException { if (!isEmpty()) throw new IllegalStateException("Tree is not empty"); root = createNode(e, null, null, null); size = 1; return root; } /∗∗Creates a new left child of Position p storing element e; returns its Position. ∗/ public Position<E> addLeft(Position<E> p, E e) throws IllegalArgumentException { Node<E> parent = validate(p); if (parent.getLeft() != null) throw new IllegalArgumentException("p already has a left child"); Node<E> child = createNode(e, parent, null, null); parent.setLeft(child); size++; return child; } /∗∗Creates a new right child of Position p storing element e; returns its Position. ∗/ public Position<E> addRight(Position<E> p, E e) throws IllegalArgumentException { Node<E> parent = validate(p); if (parent.getRight() != null) throw new IllegalArgumentException("p already has a right child"); Node<E> child = createNode(e, parent, null, null); parent.setRight(child); size++; return child; } /∗∗Replaces the element at Position p with e and returns the replaced element. ∗/ public E set(Position<E> p, E e) throws IllegalArgumentException { Node<E> node = validate(p); E temp = node.getElement(); node.setElement(e); return temp; } Code Fragment 8.10: An implementation of the LinkedBinaryTree class. (Continued from Code Fragments 8.8 and 8.9; continues in Code Fragment 8.11.)

8.3. Implementing Trees /∗∗Attaches trees t1 and t2 as left and right subtrees of external p. ∗/ public void attach(Position<E> p, LinkedBinaryTree<E> t1, LinkedBinaryTree<E> t2) throws IllegalArgumentException { Node<E> node = validate(p); if (isInternal(p)) throw new IllegalArgumentException("p must be a leaf"); size += t1.size() + t2.size(); if (!t1.isEmpty()) { // attach t1 as left subtree of node t1.root.setParent(node); node.setLeft(t1.root); t1.root = null; t1.size = 0; } if (!t2.isEmpty()) { // attach t2 as right subtree of node t2.root.setParent(node); node.setRight(t2.root); t2.root = null; t2.size = 0; } } /∗∗Removes the node at Position p and replaces it with its child, if any. ∗/ public E remove(Position<E> p) throws IllegalArgumentException { Node<E> node = validate(p); if (numChildren(p) == 2) throw new IllegalArgumentException("p has two children"); Node<E> child = (node.getLeft() != null ? node.getLeft() : node.getRight() ); if (child != null) child.setParent(node.getParent()); // child’s grandparent becomes its parent if (node == root) root = child; // child becomes root else { Node<E> parent = node.getParent(); if (node == parent.getLeft()) parent.setLeft(child); else parent.setRight(child); } size−−; E temp = node.getElement(); node.setElement(null); // help garbage collection node.setLeft(null); node.setRight(null); node.setParent(node); // our convention for defunct node return temp; } } //----------- end of LinkedBinaryTree class ----------- Code Fragment 8.11: An implementation of the LinkedBinaryTree class. (Continued from Code Fragments 8.8–8.10.)

Chapter 8. Trees Performance of the Linked Binary Tree Implementation To summarize the efﬁciencies of the linked structure representation, we analyze the running times of the LinkedBinaryTree methods, including derived methods that are inherited from the AbstractTree and AbstractBinaryTree classes: • The size method, implemented in LinkedBinaryTree, uses an instance variable storing the number of nodes of a tree and therefore takes O(1) time. Method isEmpty, inherited from AbstractTree, relies on a single call to size and thus takes O(1) time. • The accessor methods root, left, right, and parent are implemented directly in LinkedBinaryTree and take O(1) time each. The sibling, children, and numChildren methods are derived in AbstractBinaryTree using on a constant number of calls to these other accessors, so they run in O(1) time as well. • The isInternal and isExternal methods, inherited from the AbstractTree class, rely on a call to numChildren, and thus run in O(1) time as well. The isRoot method, also implemented in AbstractTree, relies on a comparison to the result of the root method and runs in O(1) time. • The update method, set, clearly runs in O(1) time. More signiﬁcantly, all of the methods addRoot, addLeft, addRight, attach, and remove run in O(1) time, as each involves relinking only a constant number of parent-child relationships per operation. • Methods depth and height were each analyzed in Section 8.1.3. The depth method at position p runs in O(dp +1) time where dp is its depth; the height method on the root of the tree runs in O(n) time. The overall space requirement of this data structure is O(n), for a tree with n nodes, as there is an instance of the Node class for every node, in addition to the top-level size and root ﬁelds. Table 8.1 summarizes the performance of the linked structure implementation of a binary tree. Method Running Time size, isEmpty O(1) root, parent, left, right, sibling, children, numChildren O(1) isInternal, isExternal, isRoot O(1) addRoot, addLeft, addRight, set, attach, remove O(1) depth(p) O(dp +1) height O(n) Table 8.1: Running times for the methods of an n-node binary tree implemented with a linked structure. The space usage is O(n).

8.3. Implementing Trees 8.3.2 Array-Based Representation of a Binary Tree An alternative representation of a binary tree T is based on a way of numbering the positions of T. For every position p of T, let f(p) be the integer deﬁned as follows. • If p is the root of T, then f(p) = 0. • If p is the left child of position q, then f(p) = 2f(q)+1. • If p is the right child of position q, then f(p) = 2f(q)+2. The numbering function f is known as a level numbering of the positions in a binary tree T, for it numbers the positions on each level of T in increasing order from left to right. (See Figure 8.10.) Note well that the level numbering is based on potential positions within a tree, not the actual shape of a speciﬁc tree, so they are not necessarily consecutive. For example, in Figure 8.10(b), there are no nodes with level numbering 13 or 14, because the node with level numbering 6 has no children. (a) ... ... (b) + − + ∗ + − ∗ − / Figure 8.10: Binary tree level numbering: (a) general scheme; (b) an example.

Chapter 8. Trees The level numbering function f suggests a representation of a binary tree T by means of an array-based structure A, with the element at position p of T stored at index f(p) of the array. We show an example of an array-based representation of a binary tree in Figure 8.11. / + ∗ − + ∗ + + − / Figure 8.11: Representation of a binary tree by means of an array. One advantage of an array-based representation of a binary tree is that a position p can be represented by the single integer f(p), and that position-based methods such as root, parent, left, and right can be implemented using simple arithmetic operations on the number f(p). Based on our formula for the level numbering, the left child of p has index 2f(p) + 1, the right child of p has index 2f(p) + 2, and the parent of p has index ⌊( f(p)−1)/2⌋. We leave the details of a complete arraybased implementation as Exercise R-8.16. The space usage of an array-based representation depends greatly on the shape of the tree. Let n be the number of nodes of T, and let fM be the maximum value of f(p) over all the nodes of T. The array A requires length N = 1 + fM, since elements range from A[0] to A[ fM]. Note that A may have a number of empty cells that do not refer to existing positions of T. In fact, in the worst case, N = 2n −1, the justiﬁcation of which is left as an exercise (R-8.14). In Section 9.3, we will see a class of binary trees, called “heaps” for which N = n. Thus, in spite of the worst-case space usage, there are applications for which the array representation of a binary tree is space efﬁcient. Still, for general binary trees, the exponential worst-case space requirement of this representation is prohibitive. Another drawback of an array representation is that many update operations for trees cannot be efﬁciently supported. For example, removing a node and promoting its child takes O(n) time because it is not just the child that moves locations within the array, but all descendants of that child.

8.3. Implementing Trees 8.3.3 Linked Structure for General Trees When representing a binary tree with a linked structure, each node explicitly maintains ﬁelds left and right as references to individual children. For a general tree, there is no a priori limit on the number of children that a node may have. A natural way to realize a general tree T as a linked structure is to have each node store a single container of references to its children. For example, a children ﬁeld of a node can be an array or list of references to the children of the node (if any). Such a linked representation is schematically illustrated in Figure 8.12. element parent children Baltimore Chicago New York Providence Seattle (a) (b) Figure 8.12: The linked structure for a general tree: (a) the structure of a node; (b) a larger portion of the data structure associated with a node and its children. Table 8.2 summarizes the performance of the implementation of a general tree using a linked structure. The analysis is left as an exercise (R-8.13), but we note that, by using a collection to store the children of each position p, we can implement children(p) by simply iterating that collection. Method Running Time size, isEmpty O(1) root, parent, isRoot, isInternal, isExternal O(1) numChildren(p) O(1) children(p) O(cp +1) depth(p) O(dp +1) height O(n) Table 8.2: Running times of the accessor methods of an n-node general tree implemented with a linked structure. We let cp denote the number of children of a position p, and dp its depth. The space usage is O(n).

Chapter 8. Trees 8.4 Tree Traversal Algorithms A traversal of a tree T is a systematic way of accessing, or “visiting,” all the positions of T. The speciﬁc action associated with the “visit” of a position p depends on the application of this traversal, and could involve anything from incrementing a counter to performing some complex computation for p. In this section, we describe several common traversal schemes for trees, implement them in the context of our various tree classes, and discuss several common applications of tree traversals. 8.4.1 Preorder and Postorder Traversals of General Trees In a preorder traversal of a tree T, the root of T is visited ﬁrst and then the subtrees rooted at its children are traversed recursively. If the tree is ordered, then the subtrees are traversed according to the order of the children. The pseudocode for the preorder traversal of the subtree rooted at a position p is shown in Code Fragment 8.12. Algorithm preorder(p): perform the “visit” action for position p { this happens before any recursion } for each child c in children(p) do preorder(c) { recursively traverse the subtree rooted at c } Code Fragment 8.12: Algorithm preorder for performing the preorder traversal of a subtree rooted at position p of a tree. Figure 8.13 portrays the order in which positions of a sample tree are visited during an application of the preorder traversal algorithm. Paper Title Abstract § 1 References § 2 § 3 § 1.1 § 1.2 § 2.1 § 2.2 § 2.3 § 3.1 § 3.2 Figure 8.13: Preorder traversal of an ordered tree, where the children of each position are ordered from left to right.

8.4. Tree Traversal Algorithms Postorder Traversal Another important tree traversal algorithm is the postorder traversal. In some sense, this algorithm can be viewed as the opposite of the preorder traversal, because it recursively traverses the subtrees rooted at the children of the root ﬁrst, and then visits the root (hence, the name “postorder”). Pseudocode for the postorder traversal is given in Code Fragment 8.13, and an example of a postorder traversal is portrayed in Figure 8.14. Algorithm postorder(p): for each child c in children(p) do postorder(c) { recursively traverse the subtree rooted at c } perform the “visit” action for position p { this happens after any recursion } Code Fragment 8.13: Algorithm postorder for performing the postorder traversal of a subtree rooted at position p of a tree. Paper Title Abstract § 1 References § 2 § 3 § 1.1 § 1.2 § 2.1 § 2.2 § 2.3 § 3.1 § 3.2 Figure 8.14: Postorder traversal of the ordered tree of Figure 8.13. Running-Time Analysis Both preorder and postorder traversal algorithms are efﬁcient ways to access all the positions of a tree. The analysis of either of these traversal algorithms is similar to that of algorithm height, given in Code Fragment 8.5 of Section 8.1.3. At each position p, the nonrecursive part of the traversal algorithm requires time O(cp +1), where cp is the number of children of p, under the assumption that the “visit” itself takes O(1) time. By Proposition 8.4, the overall running time for the traversal of tree T is O(n), where n is the number of positions in the tree. This running time is asymptotically optimal since the traversal must visit all n positions of the tree.

Chapter 8. Trees 8.4.2 Breadth-First Tree Traversal Although the preorder and postorder traversals are common ways of visiting the positions of a tree, another approach is to traverse a tree so that we visit all the positions at depth d before we visit the positions at depth d +1. Such an algorithm is known as a breadth-ﬁrst traversal. A breadth-ﬁrst traversal is a common approach used in software for playing games. A game tree represents the possible choices of moves that might be made by a player (or computer) during a game, with the root of the tree being the initial conﬁguration for the game. For example, Figure 8.15 displays a partial game tree for Tic-Tac-Toe. X X X O X O X O X O O X X O X O X O X O X O O X O X X Figure 8.15: Partial game tree for Tic-Tac-Toe when ignoring symmetries; annotations denote the order in which positions are visited in a breadth-ﬁrst tree traversal. A breadth-ﬁrst traversal of such a game tree is often performed because a computer may be unable to explore a complete game tree in a limited amount of time. So the computer will consider all moves, then responses to those moves, going as deep as computational time allows. Pseudocode for a breadth-ﬁrst traversal is given in Code Fragment 8.14. The process is not recursive, since we are not traversing entire subtrees at once. We use a queue to produce a FIFO (i.e., ﬁrst-in ﬁrst-out) semantics for the order in which we visit nodes. The overall running time is O(n), due to the n calls to enqueue and n calls to dequeue. Algorithm breadthﬁrst(): Initialize queue Q to contain root() while Q not empty do p = Q.dequeue() { p is the oldest entry in the queue } perform the “visit” action for position p for each child c in children(p) do Q.enqueue(c) { add p’s children to the end of the queue for later visits } Code Fragment 8.14: Algorithm for performing a breadth-ﬁrst traversal of a tree.

8.4. Tree Traversal Algorithms 8.4.3 Inorder Traversal of a Binary Tree The standard preorder, postorder, and breadth-ﬁrst traversals that were introduced for general trees can be directly applied to binary trees. In this section, we will introduce another common traversal algorithm speciﬁcally for a binary tree. During an inorder traversal, we visit a position between the recursive traversals of its left and right subtrees. The inorder traversal of a binary tree T can be informally viewed as visiting the nodes of T “from left to right.” Indeed, for every position p, the inorder traversal visits p after all the positions in the left subtree of p and before all the positions in the right subtree of p. Pseudocode for the inorder traversal algorithm is given in Code Fragment 8.15, and an example of an inorder traversal is portrayed in Figure 8.16. Algorithm inorder(p): if p has a left child lc then inorder(lc) { recursively traverse the left subtree of p } perform the “visit” action for position p if p has a right child rc then inorder(rc) { recursively traverse the right subtree of p } Code Fragment 8.15: Algorithm inorder for performing an inorder traversal of a subtree rooted at position p of a binary tree. + − − × + × / + − Figure 8.16: Inorder traversal of a binary tree. The inorder traversal algorithm has several important applications. When using a binary tree to represent an arithmetic expression, as in Figure 8.16, the inorder traversal visits positions in a consistent order with the standard representation of the expression, as in 3+1×3/9−5+2... (albeit without parentheses).

Chapter 8. Trees Binary Search Trees An important application of the inorder traversal algorithm arises when we store an ordered sequence of elements in a binary tree, deﬁning a structure we call a binary search tree. Let S be a set whose unique elements have an order relation. For example, S could be a set of integers. A binary search tree for S is a proper binary tree T such that, for each internal position p of T: • Position p stores an element of S, denoted as e(p). • Elements stored in the left subtree of p (if any) are less than e(p). • Elements stored in the right subtree of p (if any) are greater than e(p). An example of a binary search tree is shown in Figure 8.17. The above properties assure that an inorder traversal of a binary search tree T visits the elements in nondecreasing order. Figure 8.17: A binary search tree storing integers. The solid path is traversed when searching (successfully) for 42. The dashed path is traversed when searching (unsuccessfully) for 70. We can use a binary search tree T for set S to ﬁnd whether a given search value v is in S, by traversing a path down the tree T, starting at the root. At each internal position p encountered, we compare our search value v with the element e(p) stored at p. If v < e(p), then the search continues in the left subtree of p. If v = e(p), then the search terminates successfully. If v > e(p), then the search continues in the right subtree of p. Finally, if we reach a leaf, the search terminates unsuccessfully. In other words, a binary search tree can be viewed as a binary decision tree (recall Example 8.5), where the question asked at each internal node is whether the element at that node is less than, equal to, or larger than the element being searched for. We illustrate several examples of the search operation in Figure 8.17. Note that the running time of searching in a binary search tree T is proportional to the height of T. Recall from Proposition 8.7 that the height of a binary tree with n nodes can be as small as log(n+1)−1 or as large as n−1. Thus, binary search trees are most efﬁcient when they have small height. Chapter 11 is devoted to the study of search trees.

8.4. Tree Traversal Algorithms 8.4.4 Implementing Tree Traversals in Java When ﬁrst deﬁning the tree ADT in Section 8.1.2, we stated that tree T must include the following supporting methods: iterator(): Returns an iterator for all elements in the tree. positions(): Returns an iterable collection of all positions of the tree. At that time, we did not make any assumption about the order in which these iterations report their results. In this section, we will demonstrate how any of the tree traversal algorithms we have introduced can be used to produce these iterations as concrete implementations within the AbstractTree or AbstractBinaryTree base classes. First, we note that an iteration of all elements of a tree can easily be produced if we have an iteration of all positions of that tree. Code Fragment 8.16 provides an implementation of the iterator() method by adapting an iteration produced by the positions() method. In fact, this is the identical approach we used in Code Fragment 7.14 of Section 7.4.2 for the LinkedPositionalList class. //---------------- nested ElementIterator class ---------------- /∗This class adapts the iteration produced by positions() to return elements. ∗/ private class ElementIterator implements Iterator<E> { Iterator<Position<E>> posIterator = positions().iterator(); public boolean hasNext() { return posIterator.hasNext(); } public E next() { return posIterator.next().getElement(); } // return element! public void remove() { posIterator.remove(); } } /∗∗Returns an iterator of the elements stored in the tree. ∗/ public Iterator<E> iterator() { return new ElementIterator(); } Code Fragment 8.16: Iterating all elements of an AbstractTree instance, based upon an iteration of the positions of the tree. To implement the positions() method, we have a choice of tree traversal algorithms. Given that there are advantages to each of those traversal orders, we provide public implementations of each strategy that can be called directly by a user of our class. We can then trivially adapt one of those as a default order for the positions method of the AbstractTree class. For example, on the following page we will deﬁne a public method, preorder(), that returns an iteration of the positions of a tree in preorder; Code Fragment 8.17 demonstrates how the positions() method can be trivially deﬁned to rely on that order. public Iterable<Position<E>> positions() { return preorder(); } Code Fragment 8.17: Deﬁning preorder as the default traversal algorithm for the public positions method of an abstract tree.

Chapter 8. Trees Preorder Traversals We begin by considering the preorder traversal algorithm. Our goal is to provide a public method preorder(), as part of the AbstractTree class, which returns an iterable container of the positions of the tree in preorder. For ease of implementation, we choose to produce a snapshot iterator, as deﬁned in Section 7.4.2, returning a list of all positions. (Exercise C-8.47 explores the goal of implementing a lazy iterator that reports positions in preorder.) We begin by deﬁning a private utility method, preorderSubtree, given in Code Fragment 8.18, which allows us to parameterize the recursive process with a speciﬁc position of the tree that serves as the root of a subtree to traverse. (We also pass a list as a parameter that serves as a buffer to which “visited” positions are added.) /∗∗Adds positions of the subtree rooted at Position p to the given snapshot. ∗/ private void preorderSubtree(Position<E> p, List<Position<E>> snapshot) { snapshot.add(p); // for preorder, we add position p before exploring subtrees for (Position<E> c : children(p)) preorderSubtree(c, snapshot); } Code Fragment 8.18: A recursive subroutine for performing a preorder traversal of the subtree rooted at position p of a tree. This code should be included within the body of the AbstractTree class. The preorderSubtree method follows the high-level algorithm originally described as pseudocode in Code Fragment 8.12. It has an implicit base case, as the for loop body never executes if a position has no children. The public preorder method, shown in Code Fragment 8.19, has the responsibility of creating an empty list for the snapshot buffer, and invoking the recursive method at the root of the tree (assuming the tree is nonempty). We rely on a java.util.ArrayList instance as an Iterable instance for the snapshot buffer. /∗∗Returns an iterable collection of positions of the tree, reported in preorder. ∗/ public Iterable<Position<E>> preorder() { List<Position<E>> snapshot = new ArrayList<>(); if (!isEmpty()) preorderSubtree(root(), snapshot); // ﬁll the snapshot recursively return snapshot; } Code Fragment 8.19: A public method that performs a preorder traversal of an entire tree. This code should be included within the body of the AbstractTree class.

8.4. Tree Traversal Algorithms Postorder Traversal We implement a postorder traversal using a similar design as we used for a preorder traversal. The only difference is that a “visited” position is not added to a postorder snapshot until after all of its subtrees have been traversed. Both the recursive utility and the top-level public method are given in Code Fragment 8.20. /∗∗Adds positions of the subtree rooted at Position p to the given snapshot. ∗/ private void postorderSubtree(Position<E> p, List<Position<E>> snapshot) { for (Position<E> c : children(p)) postorderSubtree(c, snapshot); snapshot.add(p); // for postorder, we add position p after exploring subtrees } /∗∗Returns an iterable collection of positions of the tree, reported in postorder. ∗/ public Iterable<Position<E>> postorder() { List<Position<E>> snapshot = new ArrayList<>(); if (!isEmpty()) postorderSubtree(root(), snapshot); // ﬁll the snapshot recursively return snapshot; } Code Fragment 8.20: Support for performing a postorder traversal of a tree. This code should be included within the body of the AbstractTree class. Breadth-First Traversal On the following page, we will provide an implementation of the breadth-ﬁrst traversal algorithm in the context of our AbstractTree class (Code Fragment 8.21). Recall that the breadth-ﬁrst traversal algorithm is not recursive; it relies on a queue of positions to manage the traversal process. We will use the LinkedQueue class from Section 6.2.3, although any implementation of the queue ADT would sufﬁce. Inorder Traversal for Binary Trees The preorder, postorder, and breadth-ﬁrst traversal algorithms are applicable to all trees. The inorder traversal algorithm, because it explicitly relies on the notion of a left and right child of a node, only applies to binary trees. We therefore include its deﬁnition within the body of the AbstractBinaryTree class. We use a similar design to our preorder and postorder traversals, with a private recursive utility for traversing subtrees. (See Code Fragment 8.22.) For many applications of binary trees (for example, see Chapter 11), an inorder traversal is the most natural order. Therefore, Code Fragment 8.22 makes it the default for the AbstractBinaryTree class by overriding the positions method that was inherited from the AbstractTree class. Because the iterator() method relies on positions(), it will also use inorder when reporting the elements of a binary tree.

Chapter 8. Trees /∗∗Returns an iterable collection of positions of the tree in breadth-ﬁrst order. ∗/ public Iterable<Position<E>> breadthﬁrst() { List<Position<E>> snapshot = new ArrayList<>(); if (!isEmpty()) { Queue<Position<E>> fringe = new LinkedQueue<>(); fringe.enqueue(root()); // start with the root while (!fringe.isEmpty()) { Position<E> p = fringe.dequeue(); // remove from front of the queue snapshot.add(p); // report this position for (Position<E> c : children(p)) fringe.enqueue(c); // add children to back of queue } } return snapshot; } Code Fragment 8.21: An implementation of a breadth-ﬁrst traversal of a tree. This code should be included within the body of the AbstractTree class. /∗∗Adds positions of the subtree rooted at Position p to the given snapshot. ∗/ private void inorderSubtree(Position<E> p, List<Position<E>> snapshot) { if (left(p) != null) inorderSubtree(left(p), snapshot); snapshot.add(p); if (right(p) != null) inorderSubtree(right(p), snapshot); } /∗∗Returns an iterable collection of positions of the tree, reported in inorder. ∗/ public Iterable<Position<E>> inorder() { List<Position<E>> snapshot = new ArrayList<>(); if (!isEmpty()) inorderSubtree(root(), snapshot); // ﬁll the snapshot recursively return snapshot; } /∗∗Overrides positions to make inorder the default order for binary trees. ∗/ public Iterable<Position<E>> positions() { return inorder(); } Code Fragment 8.22: Support for performing an inorder traversal of a binary tree, and for making that order the default traversal for binary trees. This code should be included within the body of the AbstractBinaryTree class.

8.4. Tree Traversal Algorithms 8.4.5 Applications of Tree Traversals In this section, we demonstrate several representative applications of tree traversals, including some customizations of the standard traversal algorithms. Table of Contents When using a tree to represent the hierarchical structure of a document, a preorder traversal of the tree can be used to produce a table of contents for the document. For example, the table of contents associated with the tree from Figure 8.13 is displayed in Figure 8.18. Part (a) of that ﬁgure gives a simple presentation with one element per line; part (b) shows a more attractive presentation, produced by indenting each element based on its depth within the tree. Paper Paper Title Title Abstract Abstract §1 §1 §1.1 §1.1 §1.2 §1.2 §2 §2 §2.1 §2.1 ... ... (a) (b) Figure 8.18: Table of contents for a document represented by the tree in Figure 8.13: (a) without indentation; (b) with indentation based on depth within the tree. The unindented version of the table of contents can be produced with the following code, given a tree T supporting the preorder() method: for (Position<E> p : T.preorder()) System.out.println(p.getElement()); To produce the presentation of Figure 8.18(b), we indent each element with a number of spaces equal to twice the element’s depth in the tree (hence, the root element was unindented). If we assume that method, spaces(n), produces a string of n spaces, we could replace the body of the above loop with the statement System.out.println(spaces(2∗T.depth(p)) + p.getElement()). Unfortunately, although the work to produce the preorder traversal runs in O(n) time, based on the analysis of Section 8.4.1, the calls to depth incur a hidden cost. Making a call to depth from every position of the tree results in O(n2) worst-case time, as noted when analyzing the algorithm heightBad in Section 8.1.3.

Chapter 8. Trees A preferred approach to producing an indented table of contents is to redesign a top-down recursion that includes the current depth as an additional parameter. Such an implementation is provided in Code Fragment 8.23. This implementation runs in worst-case O(n) time (except, technically, the time it takes to print strings of increasing lengths). /∗∗Prints preorder representation of subtree of T rooted at p having depth d. ∗/ public static <E> void printPreorderIndent(Tree<E> T, Position<E> p, int d) { System.out.println(spaces(2∗d) + p.getElement()); // indent based on d for (Position<E> c : T.children(p)) printPreorderIndent(T, c, d+1); // child depth is d+1 } Code Fragment 8.23: Efﬁcient recursion for printing indented version of a preorder traversal. To print an entire tree T, the recursion should be started with form printPreorderIndent(T, T.root(), 0). In the example of Figure 8.18, we were fortunate in that the numbering was embedded within the elements of the tree. More generally, we might be interested in using a preorder traversal to display the structure of a tree, with indentation and also explicit numbering that was not present in the tree. For example, we might display the tree from Figure 8.2 beginning as: Electronics R’Us 1 R&D 2 Sales 2.1 Domestic 2.2 International 2.2.1 Canada 2.2.2 S. America This is more challenging, because the numbers used as labels are implicit in the structure of the tree. A label depends on the path from the root to the current position. To accomplish our goal, we add an additional parameter to the recursive signature. We send a list of integers representing the labels leading to a particular position. For example, when visiting the node Domestic above, we will send the list of values {2,1} that comprise its label. At the implementation level, we wish to avoid the inefﬁciency of duplicating such lists when sending a new parameter from one level of the recursion to the next. A standard solution is to pass the same list instance throughout the recursion. At one level of the recursion, a new entry is temporarily added to the end of the list before making further recursive calls. In order to “leave no trace,” the extraneous entry must later be removed from the list by the same recursive call that added it. An implementation based on this approach is given in Code Fragment 8.24.

8.4. Tree Traversal Algorithms /∗∗Prints labeled representation of subtree of T rooted at p having depth d. ∗/ public static <E> void printPreorderLabeled(Tree<E> T, Position<E> p, ArrayList<Integer> path) { int d = path.size(); // depth equals the length of the path System.out.print(spaces(2∗d)); // print indentation, then label for (int j=0; j < d; j++) System.out.print(path.get(j) + (j == d−1 ? " " : ".")); System.out.println(p.getElement()); path.add(1); // add path entry for ﬁrst child for (Position<E> c : T.children(p)) { printPreorderLabeled(T, c, path); path.set(d, 1 + path.get(d)); // increment last entry of path } path.remove(d); // restore path to its incoming state } Code Fragment 8.24: Efﬁcient recursion for printing an indented and labeled presentation of a preorder traversal. Computing Disk Space In Example 8.1, we considered the use of a tree as a model for a ﬁle-system structure, with internal positions representing directories and leaves representing ﬁles. In fact, when introducing the use of recursion back in Chapter 5, we speciﬁcally examined the topic of ﬁle systems (see Section 5.1.4). Although we did not explicitly model it as a tree at that time, we gave an implementation of an algorithm for computing the disk usage (Code Fragment 5.5). The recursive computation of disk space is emblematic of a postorder traversal, as we cannot effectively compute the total space used by a directory until after we know the space that is used by its children directories. Unfortunately, the formal implementation of postorder, as given in Code Fragment 8.20, does not sufﬁce for this purpose. We would like to have a mechanism for children to return information to the parent as part of the traversal process. A custom solution to the disk space problem, with each level of recursion providing a return value to the (parent) caller, is provided in Code Fragment 8.25. /∗∗Returns total disk space for subtree of T rooted at p. ∗/ public static int diskSpace(Tree<Integer> T, Position<Integer> p) { int subtotal = p.getElement(); // we assume element represents space usage for (Position<Integer> c : T.children(p)) subtotal += diskSpace(T, c); return subtotal; } Code Fragment 8.25: Recursive computation of disk space for a tree. We assume that each tree element reports the local space used at that position.

Chapter 8. Trees Parenthetic Representations of a Tree It is not possible to reconstruct a general tree, given only the preorder sequence of elements, as in Figure 8.18a. Some additional context is necessary for the structure of the tree to be well deﬁned. The use of indentation or numbered labels provides such context, with a very human-friendly presentation. However, there are more concise string representations of trees that are computer-friendly. In this section, we explore one such representation. The parenthetic string representation P(T) of tree T is recursively deﬁned. If T consists of a single position p, then P(T) = p.getElement(). Otherwise, it is deﬁned recursively as, P(T) = p.getElement() +"("+P(T1)+", "+ ··· +", "+P(Tk)+")" where p is the root of T and T1,T2,...,Tk are the subtrees rooted at the children of p, which are given in order if T is an ordered tree. We are using “+” here to denote string concatenation. As an example, the parenthetic representation of the tree of Figure 8.2 would appear as follows (line breaks are cosmetic): Electronics R’Us (R&D, Sales (Domestic, International (Canada, S. America, Overseas (Africa, Europe, Asia, Australia))), Purchasing, Manufacturing (TV, CD, Tuner)) Although the parenthetic representation is essentially a preorder traversal, we cannot easily produce the additional punctuation using the formal implementation of preorder. The opening parenthesis must be produced just before the loop over a position’s children, the separating commas between children, and the closing parenthesis just after the loop completes. The Java method parenthesize, shown in Code Fragment 8.26, is a custom traversal that prints such a parenthetic string representation of a tree T. /∗∗Prints parenthesized representation of subtree of T rooted at p. ∗/ public static <E> void parenthesize(Tree<E> T, Position<E> p) { System.out.print(p.getElement()); if (T.isInternal(p)) { boolean ﬁrstTime = true; for (Position<E> c : T.children(p)) { System.out.print( (ﬁrstTime ? " (" : ", ") ); // determine proper punctuation ﬁrstTime = false; // any future passes will get comma parenthesize(T, c); // recur on child } System.out.print(")"); } } Code Fragment 8.26: Method that prints parenthetic string representation of a tree.

8.4. Tree Traversal Algorithms Using Inorder Traversal for Tree Drawing An inorder traversal can be applied to the problem of computing a graphical layout of a binary tree, as shown in Figure 8.19. We assume the convention, common to computer graphics, that x-coordinates increase left to right and y-coordinates increase top to bottom, so that the origin is in the upper left corner of the drawing. Figure 8.19: An inorder drawing of a binary tree. The geometry is determined by an algorithm that assigns x- and y-coordinates to each position p of a binary tree T using the following two rules: • x(p) is the number of positions visited before p in an inorder traversal of T. • y(p) is the depth of p in T. Code Fragment 8.27 provides an implementation of a recursive method that assigns x- and y-coordinates to positions of a tree in this manner. Depth information is passed from one level of the recursion to another, as done in our earlier example for indentation. To maintain an accurate value for the x-coordinate as the traversal proceeds, the method must be provided with the value of x that should be assigned to the leftmost node of the current subtree, and it must return to its parent a revised value of x that is appropriate for the ﬁrst node drawn to the right of the subtree. public static <E> int layout(BinaryTree<E> T, Position<E> p, int d, int x) { if (T.left(p) != null) x = layout(T, T.left(p), d+1, x); // resulting x will be increased p.getElement().setX(x++); // post-increment x p.getElement().setY(d); if (T.right(p) != null) x = layout(T, T.right(p), d+1, x); // resulting x will be increased return x; } Code Fragment 8.27: Recursive method for computing coordinates at which to draw positions of a binary tree. We assume that the element type for the tree supports setX and setY methods. The initial call should be layout(T, T.root(), 0, 0).

Chapter 8. Trees 8.4.6 Euler Tours The various applications described in Section 8.4.5 demonstrate the great power of recursive tree traversals, but they also show that not every application strictly ﬁts the mold of a preorder, postorder, or inorder traversal. We can unify the tree-traversal algorithms into a single framework known as an Euler tour traversal. The Euler tour traversal of a tree T can be informally deﬁned as a “walk” around T, where we start by going from the root toward its leftmost child, viewing the edges of T as being “walls” that we always keep to our left. (See Figure 8.20.) + − − × + × / + − Figure 8.20: Euler tour traversal of a tree. The complexity of the walk is O(n), for a tree with n nodes, because it progresses exactly two times along each of the n −1 edges of the tree—once going downward along the edge, and later going upward along the edge. To unify the concept of preorder and postorder traversals, we can view there being two notable “visits” to each position p: • A “pre visit” occurs when ﬁrst reaching the position, that is, when the walk passes immediately left of the node in our visualization. • A “post visit” occurs when the walk later proceeds upward from that position, that is, when the walk passes to the right of the node in our visualization. The process of an Euler tour can be naturally viewed as recursive. In between the “pre visit” and “post visit” of a given position will be a recursive tour of each of its subtrees. Looking at Figure 8.20 as an example, there is a contiguous portion of the entire tour that is itself an Euler tour of the subtree of the node with element “/”. That tour contains two contiguous subtours, one traversing that position’s left subtree and another traversing the right subtree. In the special case of a binary tree, we can designate the time when the walk passes immediately below a node as an “in visit” event. This will be just after the tour of its left subtree (if any), but before the tour of its right subtree (if any).

8.4. Tree Traversal Algorithms The pseudocode for an Euler tour traversal of a subtree rooted at a position p is shown in Code Fragment 8.28. Algorithm eulerTour(T, p): perform the “pre visit” action for position p for each child c in T.children(p) do eulerTour(T, c) { recursively tour the subtree rooted at c } perform the “post visit” action for position p Code Fragment 8.28: Algorithm eulerTour for performing an Euler tour traversal of a subtree rooted at position p of a tree. The Euler tour traversal extends the preorder and postorder traversals, but it can also perform other kinds of traversals. For example, suppose we wish to compute the number of descendants of each position p in an n-node binary tree. We start an Euler tour by initializing a counter to 0, and then increment the counter during the “pre visit” for each position. To determine the number of descendants of a position p, we compute the difference between the values of the counter from when the pre-visit occurs and when the post-visit occurs, and add 1 (for p). This simple rule gives us the number of descendants of p, because each node in the subtree rooted at p is counted between p’s visit on the left and p’s visit on the right. Therefore, we have an O(n)-time method for computing the number of descendants of each node. For the case of a binary tree, we can customize the algorithm to include an explicit “in visit” action, as shown in Code Fragment 8.29. Algorithm eulerTourBinary(T, p): perform the “pre visit” action for position p if p has a left child lc then eulerTourBinary(T, lc) { recursively tour the left subtree of p } perform the “in visit” action for position p if p has a right child rc then eulerTourBinary(T, rc) { recursively tour the right subtree of p } perform the “post visit” action for position p Code Fragment 8.29: Algorithm eulerTourBinary for performing an Euler tour traversal of a subtree rooted at position p of a binary tree. For example, a binary Euler tour can produce a traditional parenthesized arithmetic expression, such as "((((3+1)x3)/((9-5)+2))-((3x(7-4))+6))" for the tree in Figure 8.20, as follows: • “Pre visit” action: if the position is internal, print “(”. • “In visit” action: print the value or operator stored at the position. • “Post visit” action: if the position is internal, print “)”.

Chapter 9. Priority Queues 9.1 The Priority Queue Abstract Data Type 9.1.1 Priorities In Chapter 6, we introduced the queue ADT as a collection of objects that are added and removed according to the ﬁrst-in, ﬁrst-out (FIFO) principle. A company’s customer call center embodies such a model in which waiting customers are told “calls will be answered in the order that they were received.” In that setting, a new call is added to the back of the queue, and each time a customer service representative becomes available, he or she is connected with the call that is removed from the front of the call queue. In practice, there are many applications in which a queue-like structure is used to manage objects that must be processed in some way, but for which the ﬁrst-in, ﬁrst-out policy does not sufﬁce. Consider, for example, an air-trafﬁc control center that has to decide which ﬂight to clear for landing from among many approaching the airport. This choice may be inﬂuenced by factors such as each plane’s distance from the runway, time spent waiting in a holding pattern, or amount of remaining fuel. It is unlikely that the landing decisions are based purely on a FIFO policy. There are other situations in which a “ﬁrst come, ﬁrst serve” policy might seem reasonable, yet for which other priorities come into play. To use another airline analogy, suppose a certain ﬂight is fully booked an hour prior to departure. Because of the possibility of cancellations, the airline maintains a queue of standby passengers hoping to get a seat. Although the priority of a standby passenger is inﬂuenced by the check-in time of that passenger, other considerations include the fare paid and frequent-ﬂyer status. So it may be that an available seat is given to a passenger who has arrived later than another, if such a passenger is assigned a better priority by the airline agent. In this chapter, we introduce a new abstract data type known as a priority queue. This is a collection of prioritized elements that allows arbitrary element insertion, and allows the removal of the element that has ﬁrst priority. When an element is added to a priority queue, the user designates its priority by providing an associated key. The element with the minimal key will be the next to be removed from the queue (thus, an element with key 1 will be given priority over an element with key 2). Although it is quite common for priorities to be expressed numerically, any Java object may be used as a key, as long as there exists means to compare any two instances a and b, in a way that deﬁnes a natural order of the keys. With such generality, applications may develop their own notion of priority for each element. For example, different ﬁnancial analysts may assign different ratings (i.e., priorities) to a particular asset, such as a share of stock.

9.1. The Priority Queue Abstract Data Type 9.1.2 The Priority Queue ADT We model an element and its priority as a key-value composite known as an entry. (However, we defer until Section 9.2.1 the technical deﬁnition of the Entry type.) We deﬁne the priority queue ADT to support the following methods: insert(k, v): Creates an entry with key k and value v in the priority queue. min(): Returns (but does not remove) a priority queue entry (k,v) having minimal key; returns null if the priority queue is empty. removeMin(): Removes and returns an entry (k,v) having minimal key from the priority queue; returns null if the priority queue is empty. size(): Returns the number of entries in the priority queue. isEmpty(): Returns a boolean indicating whether the priority queue is empty. A priority queue may have multiple entries with equivalent keys, in which case methods min and removeMin may report an arbitrary choice among those entry having minimal key. Values may be any type of object. In our initial model for a priority queue, we assume that an element’s key remains ﬁxed once it has been added to a priority queue. In Section 9.5, we consider an extension that allows a user to update an element’s key within the priority queue. Example 9.1: The following table shows a series of operations and their effects on an initially empty priority queue. The “Priority Queue Contents” column is somewhat deceiving since it shows the entries sorted by key. Such an internal representation is not required of a priority queue. Method Return Value Priority Queue Contents insert(5,A) { (5,A) } insert(9,C) { (5,A), (9,C) } insert(3,B) { (3,B), (5,A), (9,C) } min() (3,B) { (3,B), (5,A), (9,C) } removeMin() (3,B) { (5,A), (9,C) } insert(7,D) { (5,A), (7,D), (9,C) } removeMin() (5,A) { (7,D), (9,C) } removeMin() (7,D) { (9,C) } removeMin() (9,C) { } removeMin() null { } isEmpty() true { }

Chapter 9. Priority Queues 9.2 Implementing a Priority Queue In this section, we discuss several technical issues involving the implementation of the priority queue ADT in Java, and we deﬁne an abstract base class that provides functionality that is shared by all priority queue implementations in this chapter. We then provide two concrete priority queue implementations using a positional list L (see Section 7.3) for storage. They differ in whether or not entries are maintained in sorted order according to their keys. 9.2.1 The Entry Composite One challenge in implementing a priority queue is that we must keep track of both an element and its key, even as entries are relocated within a data structure. This is reminiscent of a case study from Section 7.7 in which we maintain a list of elements with access frequencies. In that setting, we introduced the composition design pattern, deﬁning an Item class that paired each element with its associated count in our primary data structure. For priority queues, we use composition to pair a key k and a value v as a single object. To formalize this, we deﬁne the public interface, Entry, shown in Code Fragment 9.1. /∗∗Interface for a key-value pair. ∗/ public interface Entry<K,V> { K getKey(); // returns the key stored in this entry V getValue(); // returns the value stored in this entry } Code Fragment 9.1: Java interface for an entry storing a key-value pair. We then use the Entry type in the formal interface for the priority queue, shown in Code Fragment 9.2. This allows us to return both a key and value as a single object from methods such as min and removeMin. We also deﬁne the insert method to return an entry; in a more advanced adaptable priority queue (see Section 9.5), that entry can be subsequently updated or removed. /∗∗Interface for the priority queue ADT. ∗/ public interface PriorityQueue<K,V> { int size(); boolean isEmpty(); Entry<K,V> insert(K key, V value) throws IllegalArgumentException; Entry<K,V> min(); Entry<K,V> removeMin(); } Code Fragment 9.2: Java interface for the priority queue ADT.

9.2. Implementing a Priority Queue 9.2.2 Comparing Keys with Total Orders In deﬁning the priority queue ADT, we can allow any type of object to serve as a key, but we must be able to compare keys to each other in a meaningful way. More so, the results of the comparisons must not be contradictory. For a comparison rule, which we denote by ≤, to be self-consistent, it must deﬁne a total order relation, which is to say that it satisﬁes the following properties for any keys k1, k2, and k3: • Comparability property: k1 ≤k2 or k2 ≤k1. • Antisymmetric property: if k1 ≤k2 and k2 ≤k1, then k1 = k2. • Transitive property: if k1 ≤k2 and k2 ≤k3, then k1 ≤k3. The comparability property states that comparison rule is deﬁned for every pair of keys. Note that this property implies the following one: • Reﬂexive property: k ≤k. A comparison rule, ≤, that deﬁnes a total order relation will never lead to a contradiction. Such a rule deﬁnes a linear ordering among a set of keys; hence, if a (ﬁnite) set of elements has a total order deﬁned for it, then the notion of a minimal key, kmin, is well deﬁned, as a key in which kmin ≤k, for any other key k in our set. The Comparable Interface Java provides two means for deﬁning comparisons between object types. The ﬁrst of these is that a class may deﬁne what is known as the natural ordering of its instances by formally implementing the java.lang.Comparable interface, which includes a single method, compareTo. The syntax a.compareTo(b) must return an integer i with the following meaning: • i < 0 designates that a < b. • i = 0 designates that a = b. • i > 0 designates that a > b. For example, the compareTo method of the String class deﬁnes the natural ordering of strings to be lexicographic, which is a case-sensitive extension of the alphabetic ordering to Unicode. The Comparator Interface In some applications, we may want to compare objects according to some notion other than their natural ordering. For example, we might be interested in which of two strings is the shortest, or in deﬁning our own complex rules for judging which of two stocks is more promising. To support generality, Java deﬁnes the java.util.Comparator interface. A comparator is an object that is external to the class of the keys it compares. It provides a method with the signature compare(a, b) that returns an integer with similar meaning to the compareTo method described above.

Chapter 9. Priority Queues As a concrete example, Code Fragment 9.3 deﬁnes a comparator that evaluates strings based on their length (rather than their natural lexicographic order). public class StringLengthComparator implements Comparator<String> { /∗∗Compares two strings according to their lengths. ∗/ public int compare(String a, String b) { if (a.length() < b.length()) return −1; else if (a.length() == b.length()) return 0; else return 1; } } Code Fragment 9.3: A comparator that evaluates strings based on their lengths. Comparators and the Priority Queue ADT For a general and reusable form of a priority queue, we allow a user to choose any key type and to send an appropriate comparator instance as a parameter to the priority queue constructor. The priority queue will use that comparator anytime it needs to compare two keys to each other. For convenience, we also allow a default priority queue to instead rely on the natural ordering for the given keys (assuming those keys come from a comparable class). In that case, we build our own instance of a DefaultComparator class, shown in Code Fragment 9.4. public class DefaultComparator<E> implements Comparator<E> { public int compare(E a, E b) throws ClassCastException { return ((Comparable<E>) a).compareTo(b); } } Code Fragment 9.4: A DefaultComparator class that implements a comparator based upon the natural ordering of its element type. 9.2.3 The AbstractPriorityQueue Base Class To manage technical issues common to all our priority queue implementations, we deﬁne an abstract base class named AbstractPriorityQueue in Code Fragment 9.5. (See Section 2.3.3 for a discussion of abstract base classes.) This includes a nested PQEntry class that implements the public Entry interface. Our abstract class also declares and initializes an instance variable, comp, that stores the comparator being used for the priority queue. We then provide a protected method, compare, that invokes the comparator on the keys of two given entries.

9.2. Implementing a Priority Queue /∗∗An abstract base class to assist implementations of the PriorityQueue interface.∗/ public abstract class AbstractPriorityQueue<K,V> implements PriorityQueue<K,V> { //---------------- nested PQEntry class ---------------- protected static class PQEntry<K,V> implements Entry<K,V> { private K k; // key private V v; // value public PQEntry(K key, V value) { k = key; v = value; } // methods of the Entry interface public K getKey() { return k; } public V getValue() { return v; } // utilities not exposed as part of the Entry interface protected void setKey(K key) { k = key; } protected void setValue(V value) { v = value; } } //----------- end of nested PQEntry class ----------- // instance variable for an AbstractPriorityQueue /∗∗The comparator deﬁning the ordering of keys in the priority queue. ∗/ private Comparator<K> comp; /∗∗Creates an empty priority queue using the given comparator to order keys. ∗/ protected AbstractPriorityQueue(Comparator<K> c) { comp = c; } /∗∗Creates an empty priority queue based on the natural ordering of its keys. ∗/ protected AbstractPriorityQueue() { this(new DefaultComparator<K>()); } /∗∗Method for comparing two entries according to key ∗/ protected int compare(Entry<K,V> a, Entry<K,V> b) { return comp.compare(a.getKey(), b.getKey()); } /∗∗Determines whether a key is valid. ∗/ protected boolean checkKey(K key) throws IllegalArgumentException { try { return (comp.compare(key,key) == 0); // see if key can be compared to itself } catch (ClassCastException e) { throw new IllegalArgumentException("Incompatible key"); } } /∗∗Tests whether the priority queue is empty. ∗/ public boolean isEmpty() { return size() == 0; } } Code Fragment 9.5: The AbstractPriorityQueue class. This provides a nested PQEntry class that composes a key and a value into a single object, and support for managing a comparator. For convenience, we also provide an implementation of isEmpty based on a presumed size method.

Chapter 9. Priority Queues 9.2.4 Implementing a Priority Queue with an Unsorted List In our ﬁrst concrete implementation of a priority queue, we store entries within an unsorted linked list. Code Fragment 9.6 presents our UnsortedPriorityQueue class as a subclass of the AbstractPriorityQueue class (from Code Fragment 9.5). For internal storage, key-value pairs are represented as composites, using instances of the inherited PQEntry class. These entries are stored within a PositionalList that is an instance variable. We assume that the positional list is implemented with a doubly linked list, as in Section 7.3, so that all operations of that ADT execute in O(1) time. We begin with an empty list when a new priority queue is constructed. At all times, the size of the list equals the number of key-value pairs currently stored in the priority queue. For this reason, our priority queue size method simply returns the length of the internal list. By the design of our AbstractPriorityQueue class, we inherit a concrete implementation of the isEmpty method that relies on a call to our size method. Each time a key-value pair is added to the priority queue, via the insert method, we create a new PQEntry composite for the given key and value, and add that entry to the end of the list. Such an implementation takes O(1) time. The remaining challenge is that when min or removeMin is called, we must locate the entry with minimal key. Because the entries are not sorted, we must inspect all entries to ﬁnd one with a minimal key. For convenience, we deﬁne a private ﬁndMin utility that returns the position of an entry with minimal key. Knowledge of the position allows the removeMin method to invoke the remove method on the positional list. The min method simply uses the position to retrieve the entry when preparing a key-value tuple to return. Due to the loop for ﬁnding the minimal key, both min and removeMin methods run in O(n) time, where n is the number of entries in the priority queue. A summary of the running times for the UnsortedPriorityQueue class is given in Table 9.1. Method Running Time size O(1) isEmpty O(1) insert O(1) min O(n) removeMin O(n) Table 9.1: Worst-case running times of the methods of a priority queue of size n, realized by means of an unsorted, doubly linked list. The space requirement is O(n).

9.2. Implementing a Priority Queue /∗∗An implementation of a priority queue with an unsorted list. ∗/ public class UnsortedPriorityQueue<K,V> extends AbstractPriorityQueue<K,V> { /∗∗primary collection of priority queue entries ∗/ private PositionalList<Entry<K,V>> list = new LinkedPositionalList<>(); /∗∗Creates an empty priority queue based on the natural ordering of its keys. ∗/ public UnsortedPriorityQueue() { super(); } /∗∗Creates an empty priority queue using the given comparator to order keys. ∗/ public UnsortedPriorityQueue(Comparator<K> comp) { super(comp); } /∗∗Returns the Position of an entry having minimal key. ∗/ private Position<Entry<K,V>> ﬁndMin() { // only called when nonempty Position<Entry<K,V>> small = list.ﬁrst(); for (Position<Entry<K,V>> walk : list.positions()) if (compare(walk.getElement(), small.getElement()) < 0) small = walk; // found an even smaller key return small; } /∗∗Inserts a key-value pair and returns the entry created. ∗/ public Entry<K,V> insert(K key, V value) throws IllegalArgumentException { checkKey(key); // auxiliary key-checking method (could throw exception) Entry<K,V> newest = new PQEntry<>(key, value); list.addLast(newest); return newest; } /∗∗Returns (but does not remove) an entry with minimal key. ∗/ public Entry<K,V> min() { if (list.isEmpty()) return null; return ﬁndMin().getElement(); } /∗∗Removes and returns an entry with minimal key. ∗/ public Entry<K,V> removeMin() { if (list.isEmpty()) return null; return list.remove(ﬁndMin()); } /∗∗Returns the number of items in the priority queue. ∗/ public int size() { return list.size(); } } Code Fragment 9.6: An implementation of a priority queue using an unsorted list. The parent class AbstractPriorityQueue is given in Code Fragment 9.5, and the LinkedPositionalList class is from Section 7.3.

Chapter 9. Priority Queues 9.2.5 Implementing a Priority Queue with a Sorted List Our next implementation of a priority queue also uses a positional list, yet maintains entries sorted by nondecreasing keys. This ensures that the ﬁrst element of the list is an entry with the smallest key. Our SortedPriorityQueue class is given in Code Fragment 9.7. The implementation of min and removeMin are rather straightforward given knowledge that the ﬁrst element of a list has a minimal key. We rely on the ﬁrst method of the positional list to ﬁnd the position of the ﬁrst entry, and the remove method to remove the entry from the list. Assuming that the list is implemented with a doubly linked list, operations min and removeMin take O(1) time. This beneﬁt comes at a cost, however, for method insert now requires that we scan the list to ﬁnd the appropriate position to insert the new entry. Our implementation starts at the end of the list, walking backward until the new key is smaller than that of an existing entry; in the worst case, it progresses until reaching the front of the list. Therefore, the insert method takes O(n) worst-case time, where n is the number of entries in the priority queue at the time the method is executed. In summary, when using a sorted list to implement a priority queue, insertion runs in linear time, whereas ﬁnding and removing the minimum can be done in constant time. Comparing the Two List-Based Implementations Table 9.2 compares the running times of the methods of a priority queue realized by means of a sorted and unsorted list, respectively. We see an interesting tradeoff when we use a list to implement the priority queue ADT. An unsorted list supports fast insertions but slow queries and deletions, whereas a sorted list allows fast queries and deletions, but slow insertions. Method Unsorted List Sorted List size O(1) O(1) isEmpty O(1) O(1) insert O(1) O(n) min O(n) O(1) removeMin O(n) O(1) Table 9.2: Worst-case running times of the methods of a priority queue of size n, realized by means of an unsorted or sorted list, respectively. We assume that the list is implemented by a doubly linked list. The space requirement is O(n).

9.2. Implementing a Priority Queue /∗∗An implementation of a priority queue with a sorted list. ∗/ public class SortedPriorityQueue<K,V> extends AbstractPriorityQueue<K,V> { /∗∗primary collection of priority queue entries ∗/ private PositionalList<Entry<K,V>> list = new LinkedPositionalList<>(); /∗∗Creates an empty priority queue based on the natural ordering of its keys. ∗/ public SortedPriorityQueue() { super(); } /∗∗Creates an empty priority queue using the given comparator to order keys. ∗/ public SortedPriorityQueue(Comparator<K> comp) { super(comp); } /∗∗Inserts a key-value pair and returns the entry created. ∗/ public Entry<K,V> insert(K key, V value) throws IllegalArgumentException { checkKey(key); // auxiliary key-checking method (could throw exception) Entry<K,V> newest = new PQEntry<>(key, value); Position<Entry<K,V>> walk = list.last(); // walk backward, looking for smaller key while (walk != null && compare(newest, walk.getElement()) < 0) walk = list.before(walk); if (walk == null) list.addFirst(newest); // new key is smallest else list.addAfter(walk, newest); // newest goes after walk return newest; } /∗∗Returns (but does not remove) an entry with minimal key. ∗/ public Entry<K,V> min() { if (list.isEmpty()) return null; return list.ﬁrst().getElement(); } /∗∗Removes and returns an entry with minimal key. ∗/ public Entry<K,V> removeMin() { if (list.isEmpty()) return null; return list.remove(list.ﬁrst()); } /∗∗Returns the number of items in the priority queue. ∗/ public int size() { return list.size(); } } Code Fragment 9.7: An implementation of a priority queue using a sorted list. The parent class AbstractPriorityQueue is given in Code Fragment 9.5, and the LinkedPositionalList class is from Section 7.3.

Chapter 9. Priority Queues 9.3 Heaps The two strategies for implementing a priority queue ADT in the previous section demonstrate an interesting trade-off. When using an unsorted list to store entries, we can perform insertions in O(1) time, but ﬁnding or removing an element with minimal key requires an O(n)-time loop through the entire collection. In contrast, if using a sorted list, we can trivially ﬁnd or remove the minimal element in O(1) time, but adding a new element to the queue may require O(n) time to restore the sorted order. In this section, we provide a more efﬁcient realization of a priority queue using a data structure called a binary heap. This data structure allows us to perform both insertions and removals in logarithmic time, which is a signiﬁcant improvement over the list-based implementations discussed in Section 9.2. The fundamental way the heap achieves this improvement is to use the structure of a binary tree to ﬁnd a compromise between elements being entirely unsorted and perfectly sorted. 9.3.1 The Heap Data Structure A heap (see Figure 9.1) is a binary tree T that stores entries at its positions, and that satisﬁes two additional properties: a relational property deﬁned in terms of the way keys are stored in T and a structural property deﬁned in terms of the shape of T itself. The relational property is the following: Heap-Order Property: In a heap T, for every position p other than the root, the key stored at p is greater than or equal to the key stored at p’s parent. As a consequence of the heap-order property, the keys encountered on a path from the root to a leaf of T are in nondecreasing order. Also, a minimal key is always stored at the root of T. This makes it easy to locate such an entry when min or removeMin is called, as it is informally said to be “at the top of the heap” (hence, the name “heap” for the data structure). By the way, the heap data structure deﬁned here has nothing to do with the memory heap (Section 15.1.2) used in the runtime environment supporting a programming language like Java. For the sake of efﬁciency, as will become clear later, we want the heap T to have as small a height as possible. We enforce this requirement by insisting that the heap T satisfy an additional structural property; it must be what we term complete. Complete Binary Tree Property: A heap T with height h is a complete binary tree if levels 0,1,2,...,h −1 of T have the maximal number of nodes possible (namely, level i has 2i nodes, for 0 ≤i ≤h−1) and the remaining nodes at level h reside in the leftmost possible positions at that level.

9.3. Heaps (14,E) (5,A) (6,Z) (20,B) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (13,W) (12,H) (4,C) Figure 9.1: Example of a heap storing 13 entries with integer keys. The last position is the one storing entry (13,W). The tree in Figure 9.1 is complete because levels 0, 1, and 2 are full, and the six nodes in level 3 are in the six leftmost possible positions at that level. In formalizing what we mean by the leftmost possible positions, we refer to the discussion of level numbering from Section 8.3.2, in the context of an array-based representation of a binary tree. (In fact, in Section 9.3.2 we will discuss the use of an array to represent a heap.) A complete binary tree with n elements is one that has positions with level numbering 0 through n −1. For example, in an array-based representation of the above tree, its 13 entries would be stored consecutively from A[0] to A[12]. The Height of a Heap Let h denote the height of T. Insisting that T be complete also has an important consequence, as shown in Proposition 9.2. Proposition 9.2: A heap T storing n entries has height h = ⌊logn⌋. Justiﬁcation: From the fact that T is complete, we know that the number of nodes in levels 0 through h−1 of T is precisely 1+2+4+···+2h−1 = 2h −1, and that the number of nodes in level h is at least 1 and at most 2h. Therefore n ≥2h −1+1 = 2h and n ≤2h −1+2h = 2h+1 −1. By taking the logarithm of both sides of inequality n ≥2h, we see that height h ≤logn. By rearranging terms and taking the logarithm of both sides of inequality n ≤2h+1 −1, we see that h ≥log(n + 1) −1. Since h is an integer, these two inequalities imply that h = ⌊logn⌋.

Chapter 9. Priority Queues 9.3.2 Implementing a Priority Queue with a Heap Proposition 9.2 has an important consequence, for it implies that if we can perform update operations on a heap in time proportional to its height, then those operations will run in logarithmic time. Let us therefore turn to the problem of how to efﬁciently perform various priority queue methods using a heap. We will use the composition pattern from Section 9.2.1 to store key-value pairs as entries in the heap. The size and isEmpty methods can be implemented based on examination of the tree, and the min operation is equally trivial because the heap property assures that the element at the root of the tree has a minimal key. The interesting algorithms are those for implementing the insert and removeMin methods. Adding an Entry to the Heap Let us consider how to perform insert(k, v) on a priority queue implemented with a heap T. We store the pair (k,v) as an entry at a new node of the tree. To maintain the complete binary tree property, that new node should be placed at a position p just beyond the rightmost node at the bottom level of the tree, or as the leftmost position of a new level, if the bottom level is already full (or if the heap is empty). Up-Heap Bubbling After an Insertion After this action, the tree T is complete, but it may violate the heap-order property. Hence, unless position p is the root of T (that is, the priority queue was empty before the insertion), we compare the key at position p to that of p’s parent, which we denote as q. If key kp ≥kq, the heap-order property is satisﬁed and the algorithm terminates. If instead kp < kq, then we need to restore the heap-order property, which can be locally achieved by swapping the entries stored at positions p and q. (See Figure 9.2c and d.) This swap causes the new entry to move up one level. Again, the heap-order property may be violated, so we repeat the process, going up in T until no violation of the heap-order property occurs. (See Figure 9.2e and h.) The upward movement of the newly inserted entry by means of swaps is conventionally called up-heap bubbling. A swap either resolves the violation of the heap-order property or propagates it one level up in the heap. In the worst case, upheap bubbling causes the new entry to move all the way up to the root of heap T. Thus, in the worst case, the number of swaps performed in the execution of method insert is equal to the height of T. By Proposition 9.2, that bound is ⌊logn⌋.

9.3. Heaps (14,E) (5,A) (6,Z) (20,B) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (13,W) (12,H) (4,C) (2,T) (5,A) (6,Z) (20,B) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (4,C) (a) (b) (20,B) (5,A) (6,Z) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (2,T) (4,C) (2,T) (5,A) (6,Z) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B) (4,C) (c) (d) (2,T) (5,A) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B) (6,Z) (4,C) (6,Z) (5,A) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B) (2,T) (4,C) (e) (f) (4,C) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B) (6,Z) (2,T) (5,A) (6,Z) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (13,W) (12,H) (14,E) (20,B) (2,T) (4,C) (5,A) (g) (h) Figure 9.2: Insertion of a new entry with key 2 into the heap of Figure 9.1: (a) initial heap; (b) after adding a new node; (c and d) swap to locally restore the partial order property; (e and f) another swap; (g and h) ﬁnal swap.

Chapter 9. Priority Queues Removing the Entry with Minimal Key Let us now turn to method removeMin of the priority queue ADT. We know that an entry with the smallest key is stored at the root r of T (even if there is more than one entry with smallest key). However, in general we cannot simply delete node r, because this would leave two disconnected subtrees. Instead, we ensure that the shape of the heap respects the complete binary tree property by deleting the leaf at the last position p of T, deﬁned as the rightmost position at the bottommost level of the tree. To preserve the entry from the last position p, we copy it to the root r (in place of the entry with minimal key that is being removed by the operation). Figure 9.3a and b illustrates an example of these steps, with minimal entry (4,C) being removed from the root and replaced by entry (13,W) from the last position. The node at the last position is removed from the tree. Down-Heap Bubbling After a Removal We are not yet done, however, for even though T is now complete, it likely violates the heap-order property. If T has only one node (the root), then the heap-order property is trivially satisﬁed and the algorithm terminates. Otherwise, we distinguish two cases, where p initially denotes the root of T: • If p has no right child, let c be the left child of p. • Otherwise (p has both children), let c be a child of p with minimal key. If key kp ≤kc, the heap-order property is satisﬁed and the algorithm terminates. If instead kp > kc, then we need to restore the heap-order property. This can be locally achieved by swapping the entries stored at p and c. (See Figure 9.3c and d.) It is worth noting that when p has two children, we intentionally consider the smaller key of the two children. Not only is the key of c smaller than that of p, it is at least as small as the key at c’s sibling. This ensures that the heap-order property is locally restored when that smaller key is promoted above the key that had been at p and that at c’s sibling. Having restored the heap-order property for node p relative to its children, there may be a violation of this property at c; hence, we may have to continue swapping down T until no violation of the heap-order property occurs. (See Figure 9.3e–h.) This downward swapping process is called down-heap bubbling. A swap either resolves the violation of the heap-order property or propagates it one level down in the heap. In the worst case, an entry moves all the way down to the bottom level. (See Figure 9.3.) Thus, the number of swaps performed in the execution of method removeMin is, in the worst case, equal to the height of heap T, that is, it is ⌊logn⌋ by Proposition 9.2.

9.3. Heaps (13,W) (6,Z) (20,B) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (12,H) (14,E) (4,C) (5,A) (13,W) (14,E) (12,H) (25,J) (16,X) (11,S) (15,K) (9,F) (7,Q) (20,B) (6,Z) (5,A) (a) (b) (13,W) (20,B) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (12,H) (14,E) (5,A) (6,Z) (13,W) (14,E) (12,H) (25,J) (16,X) (11,S) (15,K) (9,F) (7,Q) (20,B) (6,Z) (5,A) (c) (d) (9,F) (20,B) (7,Q) (15,K) (11,S) (16,X) (25,J) (12,H) (14,E) (5,A) (13,W) (6,Z) (13,W) (14,E) (12,H) (25,J) (16,X) (11,S) (15,K) (7,Q) (20,B) (6,Z) (5,A) (9,F) (e) (f) (13,W) (20,B) (7,Q) (15,K) (5,A) (9,F) (11,S) (14,E) (25,J) (16,X) (12,H) (6,Z) (13,W) (20,B) (7,Q) (15,K) (5,A) (9,F) (12,H) (11,S) (14,E) (25,J) (16,X) (6,Z) (g) (h) Figure 9.3: Removal of the entry with the smallest key from a heap: (a and b) deletion of the last node, whose entry gets stored into the root; (c and d) swap to locally restore the heap-order property; (e and f) another swap; (g and h) ﬁnal swap.

Chapter 9. Priority Queues Array-Based Representation of a Complete Binary Tree The array-based representation of a binary tree (Section 8.3.2) is especially suitable for a complete binary tree. We recall that in this implementation, the elements of the tree are stored in an array-based list A such that the element at position p is stored in A with index equal to the level number f(p) of p, deﬁned as follows: • If p is the root, then f(p) = 0. • If p is the left child of position q, then f(p) = 2f(q)+1. • If p is the right child of position q, then f(p) = 2f(q)+2. For a tree with of size n, the elements have contiguous indices in the range [0,n−1] and the last position of is always at index n−1. (See Figure 9.4.) (12,H) (4,C) (5,A) (6,Z) (20,B) (7,Q) (9,F) (15,K) (11,S) (16,X) (25,J) (13,W) (14,E) (5,A) (13,W) (11,S) (12,H) (14,E) (25,J) (16,X) (20,B) (7,Q) (9,F) (15,K) (6,Z) (4,C) Figure 9.4: Array-based representation of a heap. The array-based heap representation avoids some complexities of a linked tree structure. Speciﬁcally, methods insert and removeMin depend on locating the last position of a heap. With the array-based representation of a heap of size n, the last position is simply at index n−1. Locating the last position in a heap implemented with a linked tree structure requires more effort. (See Exercise C-9.33.) If the size of a priority queue is not known in advance, use of an array-based representation does introduce the need to dynamically resize the array on occasion, as is done with a Java ArrayList. The space usage of such an array-based representation of a complete binary tree with n nodes is O(n), and the time bounds of methods for adding or removing elements become amortized. (See Section 7.2.2.) Java Heap Implementation In Code Fragments 9.8 and 9.9, we provide a Java implementation of a heap-based priority queue. Although we think of our heap as a binary tree, we do not formally

9.3. Heaps use the binary tree ADT. We prefer to use the more efﬁcient array-based representation of a tree, maintaining a Java ArrayList of entry composites. To allow us to formalize our algorithms using tree-like terminology of parent, left, and right, the class includes protected utility methods that compute the level numbering of a parent or child of another position (lines 10–14 of Code Fragment 9.8). However, the “positions” in this representation are simply integer indices into the array-list. Our class also has protected utilities swap, upheap, and downheap for the lowlevel movement of entries within the array-list. A new entry is added the end of the array-list, and then repositioned as needed with upheap. To remove the entry with minimal key (which resides at index 0), we move the last entry of the array-list from index n−1 to index 0, and then invoke downheap to reposition it. /∗∗An implementation of a priority queue using an array-based heap. ∗/ public class HeapPriorityQueue<K,V> extends AbstractPriorityQueue<K,V> { /∗∗primary collection of priority queue entries ∗/ protected ArrayList<Entry<K,V>> heap = new ArrayList<>(); /∗∗Creates an empty priority queue based on the natural ordering of its keys. ∗/ public HeapPriorityQueue() { super(); } /∗∗Creates an empty priority queue using the given comparator to order keys. ∗/ public HeapPriorityQueue(Comparator<K> comp) { super(comp); } // protected utilities protected int parent(int j) { return (j−1) / 2; } // truncating division protected int left(int j) { return 2∗j + 1; } protected int right(int j) { return 2∗j + 2; } protected boolean hasLeft(int j) { return left(j) < heap.size(); } protected boolean hasRight(int j) { return right(j) < heap.size(); } /∗∗Exchanges the entries at indices i and j of the array list. ∗/ protected void swap(int i, int j) { Entry<K,V> temp = heap.get(i); heap.set(i, heap.get(j)); heap.set(j, temp); } /∗∗Moves the entry at index j higher, if necessary, to restore the heap property. ∗/ protected void upheap(int j) { while (j > 0) { // continue until reaching root (or break statement) int p = parent(j); if (compare(heap.get(j), heap.get(p)) >= 0) break; // heap property veriﬁed swap(j, p); j = p; // continue from the parent's location } } Code Fragment 9.8: Priority queue that uses an array-based heap and extends AbstractPriorityQueue (Code Fragment 9.5). (Continues in Code Fragment 9.9.)

Chapter 9. Priority Queues /∗∗Moves the entry at index j lower, if necessary, to restore the heap property. ∗/ protected void downheap(int j) { while (hasLeft(j)) { // continue to bottom (or break statement) int leftIndex = left(j); int smallChildIndex = leftIndex; // although right may be smaller if (hasRight(j)) { int rightIndex = right(j); if (compare(heap.get(leftIndex), heap.get(rightIndex)) > 0) smallChildIndex = rightIndex; // right child is smaller } if (compare(heap.get(smallChildIndex), heap.get(j)) >= 0) break; // heap property has been restored swap(j, smallChildIndex); j = smallChildIndex; // continue at position of the child } } // public methods /∗∗Returns the number of items in the priority queue. ∗/ public int size() { return heap.size(); } /∗∗Returns (but does not remove) an entry with minimal key (if any). ∗/ public Entry<K,V> min() { if (heap.isEmpty()) return null; return heap.get(0); } /∗∗Inserts a key-value pair and returns the entry created. ∗/ public Entry<K,V> insert(K key, V value) throws IllegalArgumentException { checkKey(key); // auxiliary key-checking method (could throw exception) Entry<K,V> newest = new PQEntry<>(key, value); heap.add(newest); // add to the end of the list upheap(heap.size() −1); // upheap newly added entry return newest; } /∗∗Removes and returns an entry with minimal key (if any). ∗/ public Entry<K,V> removeMin() { if (heap.isEmpty()) return null; Entry<K,V> answer = heap.get(0); swap(0, heap.size() −1); // put minimum item at the end heap.remove(heap.size() −1); // and remove it from the list; downheap(0); // then ﬁx new root return answer; } } Code Fragment 9.9: Priority queue implemented with an array-based heap (continued from Code Fragment 9.8).

9.3. Heaps 9.3.3 Analysis of a Heap-Based Priority Queue Table 9.3 shows the running time of the priority queue ADT methods for the heap implementation of a priority queue, assuming that two keys can be compared in O(1) time and that the heap T is implemented with an array-based or linked-based tree representation. In short, each of the priority queue ADT methods can be performed in O(1) or in O(logn) time, where n is the number of entries at the time the method is executed. The analysis of the running time of the methods is based on the following: • The heap T has n nodes, each storing a reference to a key-value entry. • The height of heap T is O(logn), since T is complete (Proposition 9.2). • The min operation runs in O(1) because the root of the tree contains such an element. • Locating the last position of a heap, as required for insert and removeMin, can be performed in O(1) time for an array-based representation, or O(logn) time for a linked-tree representation. (See Exercise C-9.33.) • In the worst case, up-heap and down-heap bubbling perform a number of swaps equal to the height of T. Method Running Time size, isEmpty O(1) min O(1) insert O(logn)∗ removeMin O(logn)∗ ∗amortized, if using dynamic array Table 9.3: Performance of a priority queue realized by means of a heap. We let n denote the number of entries in the priority queue at the time an operation is executed. The space requirement is O(n). The running time of operations min and removeMin are amortized for an array-based representation, due to occasional resizing of a dynamic array; those bounds are worst case with a linked tree structure. We conclude that the heap data structure is a very efﬁcient realization of the priority queue ADT, independent of whether the heap is implemented with a linked structure or an array. The heap-based implementation achieves fast running times for both insertion and removal, unlike the implementations that were based on using an unsorted or sorted list.

Chapter 9. Priority Queues 9.3.4 Bottom-Up Heap Construction ⋆ If we start with an initially empty heap, n successive calls to the insert operation will run in O(nlogn) time in the worst case. However, if all n key-value pairs to be stored in the heap are given in advance, such as during the ﬁrst phase of the heap-sort algorithm (introduced in Section 9.4.2), there is an alternative bottom-up construction method that runs in O(n) time. In this section, we describe the bottom-up heap construction, and provide an implementation that can be used by the constructor of a heap-based priority queue. For simplicity of exposition, we describe this bottom-up heap construction assuming the number of keys, n, is an integer such that n = 2h+1 −1. That is, the heap is a complete binary tree with every level being full, so the heap has height h = log(n + 1) −1. Viewed nonrecursively, bottom-up heap construction consists of the following h+1 = log(n+1) steps: 1. In the ﬁrst step (see Figure 9.5b), we construct (n+ 1)/2 elementary heaps storing one entry each. 2. In the second step (see Figure 9.5c–d), we form (n+1)/4 heaps, each storing three entries, by joining pairs of elementary heaps and adding a new entry. The new entry is placed at the root and may have to be swapped with the entry stored at a child to preserve the heap-order property. 3. In the third step (see Figure 9.5e–f), we form (n+ 1)/8 heaps, each storing 7 entries, by joining pairs of 3-entry heaps (constructed in the previous step) and adding a new entry. The new entry is placed initially at the root, but may have to move down with a down-heap bubbling to preserve the heap-order property. ... i. In the generic ith step, 2 ≤i ≤h, we form (n+1)/2i heaps, each storing 2i−1 entries, by joining pairs of heaps storing (2i−1 −1) entries (constructed in the previous step) and adding a new entry. The new entry is placed initially at the root, but may have to move down with a down-heap bubbling to preserve the heap-order property. ... h+1. In the last step (see Figure 9.5g–h), we form the ﬁnal heap, storing all the n entries, by joining two heaps storing (n−1)/2 entries (constructed in the previous step) and adding a new entry. The new entry is placed initially at the root, but may have to move down with a down-heap bubbling to preserve the heap-order property. We illustrate bottom-up heap construction in Figure 9.5 for h = 3.

9.3. Heaps (a) (b) (c) (d) (e) (f) (g) (h) Figure 9.5: Bottom-up construction of a heap with 15 entries: (a and b) we begin by constructing 1-entry heaps on the bottom level; (c and d) we combine these heaps into 3-entry heaps; (e and f) we build 7-entry heaps; (g and h) we create the ﬁnal heap. The paths of the down-heap bubblings are highlighted in (d, f, and h). For simplicity, we only show the key within each node instead of the entire entry.

Chapter 9. Priority Queues Java Implementation of a Bottom-Up Heap Construction Implementing a bottom-up heap construction is quite easy, given the existence of a “down-heap” utility method. The “merging” of two equally sized heaps that are subtrees of a common position p, as described in the opening of this section, can be accomplished simply by down-heaping p’s entry. For example, that is what happened to the key 14 in going from Figure 9.5(f) to (g). With our array-based representation of a heap, if we initially store all n entries in arbitrary order within the array, we can implement the bottom-up heap construction process with a single loop that makes a call to downheap from each position of the tree, as long as those calls are ordered starting with the deepest level and ending with the root of the tree. In fact, that loop can start with the deepest internal position, since there is no effect when down-heap is called at an external position. In Code Fragment 9.10, we augment the original HeapPriorityQueue class from Section 9.3.2 to provide support for the bottom-up construction of an initial collection. We introduce a nonpublic utility method, heapify, that calls downheap on each nonleaf position, beginning with the deepest and concluding with a call at the root of the tree. We introduce an additional constructor for the class that accepts an initial sequence of keys and values, parameterized as two coordinate arrays that are presumed to have the same length. We create new entries, pairing the ﬁrst key with the ﬁrst value, the second key with the second value, and so on. We then call the heapify utility to establish the heap ordering. For brevity, we omit a similar constructor that accepts a nondefault comparator for the priority queue. /∗∗Creates a priority queue initialized with the given key-value pairs. ∗/ public HeapPriorityQueue(K[ ] keys, V[ ] values) { super(); for (int j=0; j < Math.min(keys.length, values.length); j++) heap.add(new PQEntry<>(keys[j], values[j])); heapify(); } /∗∗Performs a bottom-up construction of the heap in linear time. ∗/ protected void heapify() { int startIndex = parent(size()−1); // start at PARENT of last entry for (int j=startIndex; j >= 0; j−−) // loop until processing the root downheap(j); } Code Fragment 9.10: Revision to the HeapPriorityQueue class of Code Fragments 9.8 and 9.9, supporting linear-time construction given an initial collection of key-value pairs.

9.3. Heaps Asymptotic Analysis of Bottom-Up Heap Construction Bottom-up heap construction is asymptotically faster than incrementally inserting n entries into an initially empty heap. Intuitively, we are performing a single downheap operation at each position in the tree, rather than a single up-heap operation from each. Since more nodes are closer to the bottom of a tree than the top, the sum of the downward paths is linear, as shown in the following proposition. Proposition 9.3: Bottom-up construction of a heap with n entries takes O(n) time, assuming two keys can be compared in O(1) time. Justiﬁcation: The primary cost of the construction is due to the down-heap steps performed at each nonleaf position. Let πv denote the path of T from nonleaf node v to its “inorder successor” leaf, that is, the path that starts at v, goes to the right child of v, and then goes down leftward until it reaches a leaf. Although, πv is not necessarily the path followed by the down-heap bubbling step from v, its number of edges ∥πv∥is proportional to the height of the subtree rooted at v, and thus a bound on the complexity of the down-heap operation at v. The total running time of the bottom-up heap construction algorithm is therefore bounded by the sum ∑v ∥πv∥. For intuition, Figure 9.6 illustrates the justiﬁcation “visually,” marking each edge with the label of the nonleaf node v whose path πv contains that edge. We claim that the paths πv for all nonleaf v are edge-disjoint, and thus the sum of the path lengths is bounded by the number of total edges in the tree, hence O(n). To show this, we consider what we term “right-leaning” and “left-leaning” edges (i.e., those going from a parent to a right, respectively left, child). A particular rightleaning edge e can only be part of the path πv for node v that is the parent in the relationship represented by e. Left-leaning edges can be partitioned by considering the leaf that is reached if continuing down leftward until reaching a leaf. Each nonleaf node only uses left-leaning edges in the group leading to that nonleaf node’s inorder successor. Since each nonleaf node must have a different inorder successor, no two such paths can contain the same left-leaning edge. We conclude that the bottom-up construction of heap T takes O(n) time. Figure 9.6: Visual justiﬁcation of the linear running time of bottom-up heap construction. Each edge e is labeled with a node v for which πv contains e (if any).

Chapter 9. Priority Queues 9.3.5 Using the java.util.PriorityQueue Class There is no priority queue interface built into Java, but Java does include a class, java.util.PriorityQueue, which implements the java.util.Queue interface. Instead of adding and removing elements according to the standard FIFO policy used by most queues, the java.util.PriorityQueue class processes its entries according to a priority The “front” of the queue will always be a minimal element, with priorities based either on the natural ordering of the elements, or in accordance with a comparator object sent as a parameter when constructing the priority queue. The most notable difference between the java.util.PriorityQueue class and our own priority queue ADT is the model for managing keys and values. Whereas our public interface distinguishes between keys and values, the java.util.PriorityQueue class relies on a single element type. That element is effectively treated as a key. If a user wishes to insert distinct keys and values, the burden is on the user to deﬁne and insert appropriate composite objects, and to ensure that those objects can be compared based on their keys. (The Java Collections Framework does include its own entry interface, java.util.Map.Entry, and a concrete implementation in the java.util.AbstractMap.SimpleEntry class; we discuss the map ADT in the next chapter.) Table 9.4 shows the correspondance between methods of our priority queue ADT and those of the java.util.PriorityQueue class. The java.util.PriorityQueue class is implemented with a heap, so it guarantees O(logn)-time performance for methods add and remove, and constant-time performance for accessors peek, size, and isEmpty. In addition, it provides a parameterized method, remove(e), that removes a speciﬁc element e from the priority queue. However, that method runs in O(n) time, performing a sequential search to locate the element within the heap. (In Section 9.5, we extend our heap-based priority queue implementation to support a more efﬁcient means for removing an arbitrary entry, or for updating the priority of an existing entry.) Our Priority Queue ADT java.util.PriorityQueue Class insert(k,v) add(new SimpleEntry(k,v)) min() peek() removeMin() remove() size() size() isEmpty() isEmpty() Table 9.4: Methods of our priority queue ADT and the corresponding methods when using the java.util.PriorityQueue class.

9.4. Sorting with a Priority Queue 9.4 Sorting with a Priority Queue One application of priority queues is sorting, where we are given a sequence of elements that can be compared according to a total order relation, and we want to rearrange them in increasing order (or at least in nondecreasing order if there are ties). The algorithm for sorting a sequence S with a priority queue P is quite simple and consists of the following two phases: 1. In the ﬁrst phase, we insert the elements of S as keys into an initially empty priority queue P by means of a series of n insert operations, one for each element. 2. In the second phase, we extract the elements from P in nondecreasing order by means of a series of n removeMin operations, putting them back into S in that order. A Java implementation of this algorithm is given in Code Fragment 9.11, assuming that the sequence is stored as a positional list. (Code for a different type of collection, such as an array or an array list, would be similar.) The algorithm works correctly for any priority queue P, no matter how P is implemented. However, the running time of the algorithm is determined by the running times of operations insert and removeMin, which do depend on how P is implemented. Indeed, pqSort should be considered more a sorting “scheme” than a sorting “algorithm,” because it does not specify how the priority queue P is implemented. The pqSort scheme is the paradigm of several popular sorting algorithms, including selection-sort, insertion-sort, and heap-sort, which we will discuss in this section. /∗∗Sorts sequence S, using initially empty priority queue P to produce the order. ∗/ public static <E> void pqSort(PositionalList<E> S, PriorityQueue<E,?> P) { int n = S.size(); for (int j=0; j < n; j++) { E element = S.remove(S.ﬁrst()); P.insert(element, null); // element is key; null value } for (int j=0; j < n; j++) { E element = P.removeMin().getKey(); S.addLast(element); // the smallest key in P is next placed in S } } Code Fragment 9.11: An implementation of a pqSort method that sorts elements of a positional list using an initially empty priority queue to produce the ordering.

Chapter 9. Priority Queues 9.4.1 Selection-Sort and Insertion-Sort We next demonstrate how the pqSort scheme results in two classic sorting algorithms when using an unsorted or sorted list for a priority queue. Selection-Sort In Phase 1 of the pqSort scheme, we insert all elements into a priority queue P; in Phase 2 we repeatedly remove the minimal element from P using the removeMin method. If we implement P with an unsorted list, then Phase 1 of pqSort takes O(n) time, for we can insert each element in O(1) time. In Phase 2, the running time of each removeMin operation is proportional to the size of P. Thus, the bottleneck computation is the repeated “selection” of the minimum element in Phase 2. For this reason, this algorithm is better known as selection-sort. (See Figure 9.7.) Sequence S Priority Queue P Input (7, 4, 8, 2, 5, 3, 9) () Phase 1 (a) (4, 8, 2, 5, 3, 9) (7) (b) (8, 2, 5, 3, 9) (7, 4) ... ... ... (g) () (7, 4, 8, 2, 5, 3, 9) Phase 2 (a) (2) (7, 4, 8, 5, 3, 9) (b) (2, 3) (7, 4, 8, 5, 9) (c) (2, 3, 4) (7, 8, 5, 9) (d) (2, 3, 4, 5) (7, 8, 9) (e) (2, 3, 4, 5, 7) (8, 9) (f) (2, 3, 4, 5, 7, 8) (9) (g) (2, 3, 4, 5, 7, 8, 9) () Figure 9.7: Execution of selection-sort on sequence S = (7, 4, 8, 2, 5, 3, 9). As noted above, the bottleneck is in Phase 2 where we repeatedly remove an entry with smallest key from the priority queue P. The size of P starts at n and incrementally decreases with each removeMin until it becomes 0. Thus, the ﬁrst removeMin operation takes time O(n), the second one takes time O(n−1), and so on, until the last (nth) operation takes time O(1). Therefore, the total time needed for the second phase is O(n+(n−1)+···+2+1) = O n ∑ i=1 i ! . By Proposition 4.3, we have ∑n i=1 i = n(n+1)/2. Thus, Phase 2 takes time O(n2), as does the entire selection-sort algorithm.

9.4. Sorting with a Priority Queue Insertion-Sort If we implement the priority queue P using a sorted list, then the running time of Phase 2 improves to O(n), for each operation removeMin on P now takes O(1) time. Unfortunately, Phase 1 now becomes the bottleneck for the running time, since, in the worst case, each insert operation takes time proportional to the size of P. This sorting algorithm is therefore better known as insertion-sort (see Figure 9.8), for the bottleneck in this sorting algorithm involves the repeated “insertion” of a new element at the appropriate position in a sorted list. Sequence S Priority Queue P Input (7, 4, 8, 2, 5, 3, 9) () Phase 1 (a) (4, 8, 2, 5, 3, 9) (7) (b) (8, 2, 5, 3, 9) (4, 7) (c) (2, 5, 3, 9) (4, 7, 8) (d) (5, 3, 9) (2, 4, 7, 8) (e) (3, 9) (2, 4, 5, 7, 8) (f) (9) (2, 3, 4, 5, 7, 8) (g) () (2, 3, 4, 5, 7, 8, 9) Phase 2 (a) (2) (3, 4, 5, 7, 8, 9) (b) (2, 3) (4, 5, 7, 8, 9) ... ... ... (g) (2, 3, 4, 5, 7, 8, 9) () Figure 9.8: Execution of insertion-sort on sequence S = (7, 4, 8, 2, 5, 3, 9). In Phase 1, we repeatedly remove the ﬁrst element of S and insert it into P. In Phase 2, we repeatedly perform the removeMin operation on P and add the returned element to the end of S. Analyzing the running time of Phase 1 of insertion-sort, we note that it is O(1+2+...+(n−1)+n) = O n ∑ i=1 i ! . Again, by recalling Proposition 4.3, Phase 1 runs in O(n2) time, and hence, so does the entire insertion-sort algorithm. Alternatively, we could change our deﬁnition of insertion-sort so that we insert elements starting from the end of the priority-queue list in Phase 1, in which case performing insertion-sort on a sequence that is already sorted would run in O(n) time. Indeed, the running time of insertion-sort in this case is O(n+I), where I is the number of inversions in the sequence, that is, the number of pairs of elements that start out in the input sequence in the wrong relative order.

Chapter 9. Priority Queues 9.4.2 Heap-Sort As we have previously observed, realizing a priority queue with a heap has the advantage that all the methods in the priority queue ADT run in logarithmic time or better. Hence, this realization is suitable for applications where fast running times are sought for all the priority queue methods. Therefore, let us again consider the pqSort scheme, this time using a heap-based implementation of the priority queue. During Phase 1, the ith insert operation takes O(logi) time, since the heap has i entries after the operation is performed. Therefore, this phase takes O(nlogn) time. (It could be improved to O(n) with the bottom-up heap construction described in Section 9.3.4.) During the second phase of method pqSort, the j th removeMin operation runs in O(log(n−j + 1)), since the heap has n−j + 1 entries at the time the operation is performed. Summing over all j, this phase takes O(nlogn) time, so the entire priority-queue sorting algorithm runs in O(nlogn) time when we use a heap to implement the priority queue. This sorting algorithm is better known as heap-sort, and its performance is summarized in the following proposition. Proposition 9.4: The heap-sort algorithm sorts a sequence S of n elements in O(nlogn) time, assuming two elements of S can be compared in O(1) time. Let us stress that the O(nlogn) running time of heap-sort is considerably better than the O(n2) running time of selection-sort and insertion-sort. Implementing Heap-Sort In-Place If the sequence S to be sorted is implemented by means of an array-based sequence, such as an ArrayList in Java, we can speed up heap-sort and reduce its space requirement by a constant factor by using a portion of the array itself to store the heap, thus avoiding the use of an auxiliary heap data structure. This is accomplished by modifying the algorithm as follows: 1. We redeﬁne the heap operations to be a maximum-oriented heap, with each position key being at least as large as its children. This can be done by recoding the algorithm, or by providing a new comparator that reverses the outcome of each comparison. At any time during the execution of the algorithm, we use the left portion of S, up to a certain index i −1, to store the entries of the heap, and the right portion of S, from index i to n −1, to store the elements of the sequence. Thus, the ﬁrst i elements of S (at indices 0,...,i−1) provide the array-list representation of the heap. 2. In the ﬁrst phase of the algorithm, we start with an empty heap and move the boundary between the heap and the sequence from left to right, one step at a time. In step i, for i = 1,...,n, we expand the heap by adding the element at index i−1.

9.4. Sorting with a Priority Queue 3. In the second phase of the algorithm, we start with an empty sequence and move the boundary between the heap and the sequence from right to left, one step at a time. At step i, for i = 1,...,n, we remove a maximal element from the heap and store it at index n−i. In general, we say that a sorting algorithm is in-place if it uses only a small amount of memory in addition to the sequence storing the objects to be sorted. The variation of heap-sort above qualiﬁes as in-place; instead of transferring elements out of the sequence and then back in, we simply rearrange them. We illustrate the second phase of in-place heap-sort in Figure 9.9. (e) (c) (f) (b) (a) (d) Figure 9.9: Phase 2 of an in-place heap-sort. The heap portion of each sequence representation is highlighted. The binary tree that each sequence (implicitly) represents is diagrammed with the most recent path of down-heap bubbling highlighted.

Chapter 9. Priority Queues 9.5 Adaptable Priority Queues The methods of the priority queue ADT given in Section 9.1.2 are sufﬁcient for most basic applications of priority queues, such as sorting. However, there are situations in which additional methods would be useful, as shown by the scenarios below involving the standby airline passenger application. • A standby passenger with a pessimistic attitude may become tired of waiting and decide to leave ahead of the boarding time, requesting to be removed from the waiting list. Thus, we would like to remove from the priority queue the entry associated with this passenger. Operation removeMin does not sufﬁce since the passenger leaving does not necessarily have ﬁrst priority. Instead, we want a new operation, remove, that removes an arbitrary entry. • Another standby passenger ﬁnds her gold frequent-ﬂyer card and shows it to the agent. Thus, her priority has to be modiﬁed accordingly. To achieve this change of priority, we would like to have a new operation replaceKey allowing us to replace the key of an existing entry with a new key. • Finally, a third standby passenger notices her name is misspelled on the ticket and asks it to be corrected. To perform this change, we need to update the passenger’s record. Hence, we would like to have a new operation replaceValue, allowing us to replace the value of an existing entry with a new value. The Adaptable Priority Queue ADT The above scenarios motivate the deﬁnition of a new adaptable priority queue ADT that extends the priority queue ADT with additional functionality. We will see another application of adaptable priority queues when implementing certain graph algorithms in Sections 14.6.2 and 14.7.1. In order to implement methods remove, replaceKey, and replaceValue efﬁciently, we need a mechanism for ﬁnding a user’s element within a priority queue, ideally in a way that avoids performing a linear search through the entire collection. In the original deﬁnition of the priority queue ADT, a call to insert(k, v) formally returns an instance of type Entry to the user. In order to be able to update or remove an entry in our new adaptable priority queue ADT, the user must retain that Entry object as a token that can be sent back as a parameter to identify the relevant entry. Formally, the adaptable priority queue ADT includes the following methods (in addition to those of the standard priority queue): remove(e): Removes entry e from the priority queue. replaceKey(e, k): Replaces the key of existing entry e with k. replaceValue(e, v): Replaces the value of existing entry e with v. An error occurs with each of these methods if parameter e is invalid (for example, because it had previously been removed from the priority queue).

9.5. Adaptable Priority Queues 9.5.1 Location-Aware Entries To allow an entry instance to encode a location within a priority queue, we extend the PQEntry class (originally deﬁned with the AbstractPriorityQueue base class), adding a third ﬁeld that designates the current index of an entry within the array-based representation of the heap, as shown in Figure 9.10. (This approach is similar to our recommendation, on page 281, for implementing the positional list abstraction with an array.) token (15,K,3) (16,X,7) (9,F,4) (5,A,1) (20,B,6) (6,Z,2) (7,Q,5) (4,C,0) Figure 9.10: Representing a heap using an array of location-aware entries. The third ﬁeld of each entry instance corresponds to the index of that entry within the array. Identiﬁer token is presumed to be an entry reference in the user’s scope. When we perform priority queue operations on our heap, causing entries to be relocated within our structure, we must make sure to update the third ﬁeld of each affected entry to reﬂect its new index within the array. As an example, Figure 9.11 shows the state of the above heap after a call to removeMin(). The heap operation causes the minimal entry, (4,C), to be removed, and the last entry, (16,X), to be temporarily moved from the last position to the root, followed by a down-heap bubble phase. During the down-heap, element (16,X) is swapped with its left child, (5,A), at index 1 of the list, then swapped with its right child, (9,F), at index 4 of the list. In the ﬁnal conﬁguration, the last ﬁeld for all affected entries has been modiﬁed to reﬂect their new location. token (9,F,1) (16,X,4) (7,Q,5) (15,K,3) (20,B,6) (6,Z,2) (5,A,0) Figure 9.11: The result of a call to removeMin() on the heap originally portrayed in Figure 9.10. Identiﬁer token continues to reference the same entry as in the original conﬁguration, but the placement of that entry in the array has changed, as has the third ﬁeld of the entry.

Chapter 9. Priority Queues 9.5.2 Implementing an Adaptable Priority Queue Code Fragments 9.12 and 9.13 present a Java implementation of an adaptable priority queue, as a subclass of the HeapPriorityQueue class from Section 9.3.2. We begin by deﬁning a nested AdaptablePQEntry class (lines 5–15) that extends the inherited PQEntry class, augmenting it with an additional index ﬁeld. The inherited insert method is overridden, so that we create and initialize an instance of the AdaptablePQEntry class (not the original PQEntry class). An important aspect of our design is that the original HeapPriorityQueue class relies exclusively on a protected swap method for all low-level data movement during up-heap or down-heap operations. The AdaptablePriorityQueue class overrides that utility in order to update the stored indices of our location-aware entries when they are relocated (as discussed on the previous page). When an entry is sent as a parameter to remove, replaceKey, or replaceValue, we rely on the new index ﬁeld of that entry to designate where the element resides in the heap (a fact that is easily validated). When a key of an existing entry is replaced, that new key may violate the heap-order property by being either too big or too small. We provide a new bubble utility that determines whether an upheap or down-heap bubbling step is warranted. When removing an arbitrary entry, we replace it with the last entry in the heap (to maintain the complete binary tree property) and perform the bubbling step, since the displaced element may have a key that is too large or too small for its new location. Performance of Adaptable Priority Queue Implementations The performance of an adaptable priority queue by means of our location-aware heap structure is summarized in Table 9.5. The new class provides the same asymptotic efﬁciency and space usage as the nonadaptive version, and provides logarithmic performance for the new locator-based remove and replaceKey methods, and constant-time performance for the new replaceValuemethod. Method Running Time size, isEmpty, min O(1) insert O(logn) remove O(logn) removeMin O(logn) replaceKey O(logn) replaceValue O(1) Table 9.5: Running times of the methods of an adaptable priority queue with size n, realized by means of our array-based heap representation. The space requirement is O(n).

9.5. Adaptable Priority Queues /∗∗An implementation of an adaptable priority queue using an array-based heap. ∗/ public class HeapAdaptablePriorityQueue<K,V> extends HeapPriorityQueue<K,V> implements AdaptablePriorityQueue<K,V> { //---------------- nested AdaptablePQEntry class ---------------- /∗∗Extension of the PQEntry to include location information. ∗/ protected static class AdaptablePQEntry<K,V> extends PQEntry<K,V> { private int index; // entry’s current index within the heap public AdaptablePQEntry(K key, V value, int j) { super(key, value); // this sets the key and value index = j; // this sets the new ﬁeld } public int getIndex() { return index; } public void setIndex(int j) { index = j; } } //----------- end of nested AdaptablePQEntry class ----------- /∗∗Creates an empty adaptable priority queue using natural ordering of keys. ∗/ public HeapAdaptablePriorityQueue() { super(); } /∗∗Creates an empty adaptable priority queue using the given comparator. ∗/ public HeapAdaptablePriorityQueue(Comparator<K> comp) { super(comp);} // protected utilites /∗∗Validates an entry to ensure it is location-aware. ∗/ protected AdaptablePQEntry<K,V> validate(Entry<K,V> entry) throws IllegalArgumentException { if (!(entry instanceof AdaptablePQEntry)) throw new IllegalArgumentException("Invalid entry"); AdaptablePQEntry<K,V> locator = (AdaptablePQEntry<K,V>) entry; // safe int j = locator.getIndex(); if (j >= heap.size() || heap.get(j) != locator) throw new IllegalArgumentException("Invalid entry"); return locator; } /∗∗Exchanges the entries at indices i and j of the array list. ∗/ protected void swap(int i, int j) { super.swap(i,j); // perform the swap ((AdaptablePQEntry<K,V>) heap.get(i)).setIndex(i); // reset entry's index ((AdaptablePQEntry<K,V>) heap.get(j)).setIndex(j); // reset entry's index } Code Fragment 9.12: An implementation of an adaptable priority queue. (Continues in Code Fragment 9.13.) This extends the HeapPriorityQueue class of Code Fragments 9.8 and 9.9.

Chapter 9. Priority Queues /∗∗Restores the heap property by moving the entry at index j upward/downward.∗/ protected void bubble(int j) { if (j > 0 && compare(heap.get(j), heap.get(parent(j))) < 0) upheap(j); else downheap(j); // although it might not need to move } /∗∗Inserts a key-value pair and returns the entry created. ∗/ public Entry<K,V> insert(K key, V value) throws IllegalArgumentException { checkKey(key); // might throw an exception Entry<K,V> newest = new AdaptablePQEntry<>(key, value, heap.size()); heap.add(newest); // add to the end of the list upheap(heap.size() −1); // upheap newly added entry return newest; } /∗∗Removes the given entry from the priority queue. ∗/ public void remove(Entry<K,V> entry) throws IllegalArgumentException { AdaptablePQEntry<K,V> locator = validate(entry); int j = locator.getIndex(); if (j == heap.size() −1) // entry is at last position heap.remove(heap.size() −1); // so just remove it else { swap(j, heap.size() −1); // swap entry to last position heap.remove(heap.size() −1); // then remove it bubble(j); // and ﬁx entry displaced by the swap } } /∗∗Replaces the key of an entry. ∗/ public void replaceKey(Entry<K,V> entry, K key) throws IllegalArgumentException { AdaptablePQEntry<K,V> locator = validate(entry); checkKey(key); // might throw an exception locator.setKey(key); // method inherited from PQEntry bubble(locator.getIndex()); // with new key, may need to move entry } /∗∗Replaces the value of an entry. ∗/ public void replaceValue(Entry<K,V> entry, V value) throws IllegalArgumentException { AdaptablePQEntry<K,V> locator = validate(entry); locator.setValue(value); // method inherited from PQEntry } Code Fragment 9.13: An implementation of an adaptable priority queue (continued from Code Fragment 9.12).

Chapter 10. Maps, Hash Tables, and Skip Lists 10.1 Maps A map is an abstract data type designed to efﬁciently store and retrieve values based upon a uniquely identifying search key for each. Speciﬁcally, a map stores keyvalue pairs (k,v), which we call entries, where k is the key and v is its corresponding value. Keys are required to be unique, so that the association of keys to values deﬁnes a mapping. Figure 10.1 provides a conceptual illustration of a map using the ﬁle-cabinet metaphor. For a more modern metaphor, think about the web as being a map whose entries are the web pages. The key of a page is its URL (e.g., http://datastructures.net/) and its value is the page content. Figure 10.1: A conceptual illustration of the map ADT. Keys (labels) are assigned to values (folders) by a user. The resulting entries (labeled folders) are inserted into the map (ﬁle cabinet). The keys can be used later to retrieve or remove values. Maps are also known as associative arrays, because the entry’s key serves somewhat like an index into the map, in that it assists the map in efﬁciently locating the associated entry. However, unlike a standard array, a key of a map need not be numeric, and is does not directly designate a position within the structure. Common applications of maps include the following: • A university’s information system relies on some form of a student ID as a key that is mapped to that student’s associated record (such as the student’s name, address, and course grades) serving as the value. • The domain-name system (DNS) maps a host name, such as www.wiley.com, to an Internet-Protocol (IP) address, such as 208.215.179.146. • A social media site typically relies on a (nonnumeric) username as a key that can be efﬁciently mapped to a particular user’s associated information. • A company’s customer base may be stored as a map, with a customer’s account number or unique user ID as a key, and a record with the customer’s information as a value. The map would allow a service representative to quickly access a customer’s record, given the key. • A computer graphics system may map a color name, such as 'turquoise', to the triple of numbers that describes the color’s RGB (red-green-blue) representation, such as (64, 224, 208).

10.1. Maps 10.1.1 The Map ADT Since a map stores a collection of objects, it should be viewed as a collection of key-value pairs. As an ADT, a map M supports the following methods: size(): Returns the number of entries in M. isEmpty(): Returns a boolean indicating whether M is empty. get(k): Returns the value v associated with key k, if such an entry exists; otherwise returns null. put(k, v): If M does not have an entry with key equal to k, then adds entry (k,v) to M and returns null; else, replaces with v the existing value of the entry with key equal to k and returns the old value. remove(k): Removes from M the entry with key equal to k, and returns its value; if M has no such entry, then returns null. keySet(): Returns an iterable collection containing all the keys stored in M. values(): Returns an iterable collection containing all the values of entries stored in M (with repetition if multiple keys map to the same value). entrySet(): Returns an iterable collection containing all the key-value entries in M. Maps in the java.util Package Our deﬁnition of the map ADT is a simpliﬁed version of the java.util.Map interface. For the elements of the iteration returned by entrySet, we will rely on the composite Entry interface introduced in Section 9.2.1 (the java.util.Map relies on the nested java.util.Map.Entry interface). Notice that each of the operations get(k), put(k, v), and remove(k) returns the existing value associated with key k, if the map has such an entry, and otherwise returns null. This introduces ambiguity in an application for which null is allowed as a natural value associated with a key k. That is, if an entry (k, null) exists in a map, then the operation get(k) will return null, not because it couldn’t ﬁnd the key, but because it found the key and is returning its associated value. Some implementations of the java.util.Map interface explicitly forbid use of a null value (and null keys, for that matter). However, to resolve the ambiguity when null is allowable, the interface contains a boolean method, containsKey(k) to deﬁnitively check whether k exists as a key. (We leave implementation of such a method as an exercise.)

Chapter 10. Maps, Hash Tables, and Skip Lists Example 10.1: In the following, we show the effect of a series of operations on an initially empty map storing entries with integer keys and single-character values. Method Return Value Map isEmpty() true {} put(5,A) null {(5,A)} put(7,B) null {(5,A),(7,B)} put(2,C) null {(5,A),(7,B),(2,C)} put(8,D) null {(5,A),(7,B),(2,C),(8,D)} put(2,E) C {(5,A),(7,B),(2,E),(8,D)} get(7) B {(5,A),(7,B),(2,E),(8,D)} get(4) null {(5,A),(7,B),(2,E),(8,D)} get(2) E {(5,A),(7,B),(2,E),(8,D)} size() {(5,A),(7,B),(2,E),(8,D)} remove(5) A {(7,B),(2,E),(8,D)} remove(2) E {(7,B),(8,D)} get(2) null {(7,B),(8,D)} remove(2) null {(7,B),(8,D)} isEmpty() false {(7,B),(8,D)} entrySet() {(7,B),(8,D)} {(7,B),(8,D)} keySet() {7,8} {(7,B),(8,D)} values() {B,D} {(7,B),(8,D)} A Java Interface for the Map ADT A formal deﬁnition of a Java interface for our version of the map ADT is given in Code Fragment 10.1. It uses the generics framework (Section 2.5.2), with K designating the key type and V designating the value type. public interface Map<K,V> { int size(); boolean isEmpty(); V get(K key); V put(K key, V value); V remove(K key); Iterable<K> keySet(); Iterable<V> values(); Iterable<Entry<K,V>> entrySet(); } Code Fragment 10.1: Java interface for our simpliﬁed version of the map ADT.

10.1. Maps 10.1.2 Application: Counting Word Frequencies As a case study for using a map, consider the problem of counting the number of occurrences of words in a document. This is a standard task when performing a statistical analysis of a document, for example, when categorizing an email or news article. A map is an ideal data structure to use here, for we can use words as keys and word counts as values. We show such an application in Code Fragment 10.2. We begin with an empty map, mapping words to their integer frequencies. (We rely on the ChainHashMap class that will be introduced in Section 10.2.4.) We ﬁrst scan through the input, considering adjacent alphabetic characters to be words, which we then convert to lowercase. For each word found, we attempt to retrieve its current frequency from the map using the get method, with a yet unseen word having frequency zero. We then (re)set its frequency to be one more to reﬂect the current occurrence of the word. After processing the entire input, we loop through the entrySet() of the map to determine which word has the most occurrences. /∗∗A program that counts words in a document, printing the most frequent. ∗/ public class WordCount { public static void main(String[ ] args) { Map<String,Integer> freq = new ChainHashMap<>(); // or any concrete map // scan input for words, using all nonletters as delimiters Scanner doc = new Scanner(System.in).useDelimiter("[^a-zA-Z]+"); while (doc.hasNext()) { String word = doc.next().toLowerCase(); // convert next word to lowercase Integer count = freq.get(word); // get the previous count for this word if (count == null) count = 0; // if not in map, previous count is zero freq.put(word, 1 + count); // (re)assign new count for this word } int maxCount = 0; String maxWord = "no word"; for (Entry<String,Integer> ent : freq.entrySet()) // ﬁnd max-count word if (ent.getValue() > maxCount) { maxWord = ent.getKey(); maxCount = ent.getValue(); } System.out.print("The most frequent word is '" + maxWord); System.out.println("' with " + maxCount + " occurrences."); } } Code Fragment 10.2: A program for counting word frequencies in a document, printing the most frequent word. The document is parsed using the Scanner class, for which we change the delimiter for separating tokens from whitespace to any nonletter. We also convert words to lowercase.

Chapter 10. Maps, Hash Tables, and Skip Lists 10.1.3 An AbstractMap Base Class In the remainder of this chapter (and the next), we will be providing many different implementations of the map ADT using a variety of data structures, each with its own trade-off of advantages and disadvantages. As we have done in earlier chapters, we rely on a combination of abstract and concrete classes in the interest of greater code reuse. Figure 10.2 provides a preview of those classes. (Section 10.1.1) SortedTableMap (Chapter 11) TreeMap (Section 10.2.4) AbstractHashMap (Section 10.2.4) ChainHashMap (Section 10.2.4) ProbeHashMap (Section 10.1.4) UnsortedTableMap (Section 10.3) AbstractSortedMap (Section 10.1.3) AbstractMap Map (Section 10.3.1) ≪interface≫ SortedMap (Section 10.3) ≪interface≫ (additional subclasses) Figure 10.2: Our hierarchy of map types (with references to where they are deﬁned). We begin, in this section, by designing an AbstractMap base class that provides functionality that is shared by all of our map implementations. More speciﬁcally, the base class (given in Code Fragment 10.3) provides the following support: • An implementation of the isEmpty method, based upon the presumed implementation of the size method. • A nested MapEntry class that implements the public Entry interface, while providing a composite for storing key-value entries in a map data structure. • Concrete implementations of the keySet and values methods, based upon an adaption to the entrySet method. In this way, concrete map classes need only implement the entrySet method to provide all three forms of iteration. We implement the iterations using the technique introduced in Section 7.4.2 (at that time providing an iteration of all elements of a positional list given an iteration of all positions of the list).

10.1. Maps public abstract class AbstractMap<K,V> implements Map<K,V> { public boolean isEmpty() { return size() == 0; } //---------------- nested MapEntry class ---------------- protected static class MapEntry<K,V> implements Entry<K,V> { private K k; // key private V v; // value public MapEntry(K key, V value) { k = key; v = value; } // public methods of the Entry interface public K getKey() { return k; } public V getValue() { return v; } // utilities not exposed as part of the Entry interface protected void setKey(K key) { k = key; } protected V setValue(V value) { V old = v; v = value; return old; } } //----------- end of nested MapEntry class ----------- // Support for public keySet method... private class KeyIterator implements Iterator<K> { private Iterator<Entry<K,V>> entries = entrySet().iterator(); // reuse entrySet public boolean hasNext() { return entries.hasNext(); } public K next() { return entries.next().getKey(); } // return key! public void remove() { throw new UnsupportedOperationException(); } } private class KeyIterable implements Iterable<K> { public Iterator<K> iterator() { return new KeyIterator(); } } public Iterable<K> keySet() { return new KeyIterable(); } // Support for public values method... private class ValueIterator implements Iterator<V> { private Iterator<Entry<K,V>> entries = entrySet().iterator(); // reuse entrySet public boolean hasNext() { return entries.hasNext(); } public V next() { return entries.next().getValue(); } // return value! public void remove() { throw new UnsupportedOperationException(); } } private class ValueIterable implements Iterable<V> { public Iterator<V> iterator() { return new ValueIterator(); } } public Iterable<V> values() { return new ValueIterable(); } } Code Fragment 10.3: Implementation of the AbstractMap base class.

Chapter 10. Maps, Hash Tables, and Skip Lists 10.1.4 A Simple Unsorted Map Implementation We demonstrate the use of the AbstractMap class with a very simple concrete implementation of the map ADT that relies on storing key-value pairs in arbitrary order within a Java ArrayList. The presentation of such an UnsortedTableMap class is given in Code Fragments 10.4 and 10.5. Each of the fundamental methods get(k), put(k, v), and remove(k) requires an initial scan of the array to determine whether an entry with key equal to k exists. For this reason, we provide a nonpublic utility, ﬁndIndex(key), that returns the index at which such an entry is found, or −1 if no such entry is found. (See Code Fragment 10.4.) The rest of the implementation is rather simple. One subtlety worth mentioning is the way in which we remove an entry from the array list. Although we could use the remove method of the ArrayList class, that would result in an unnecessary loop to shift all subsequent entries to the left. Because the map is unordered, we prefer to ﬁll the vacated cell of the array by relocating the last entry to that location. Such an update step runs in constant time. Unfortunately, the UnsortedTableMap class on the whole is not very efﬁcient. On a map with n entries, each of the fundamental methods takes O(n) time in the worst case because of the need to scan through the entire list when searching for an existing entry. Fortunately, as we discuss in the next section, there is a much faster strategy for implementing the map ADT. public class UnsortedTableMap<K,V> extends AbstractMap<K,V> { /∗∗Underlying storage for the map of entries. ∗/ private ArrayList<MapEntry<K,V>> table = new ArrayList<>(); /∗∗Constructs an initially empty map. ∗/ public UnsortedTableMap() { } // private utility /∗∗Returns the index of an entry with equal key, or −1 if none found. ∗/ private int ﬁndIndex(K key) { int n = table.size(); for (int j=0; j < n; j++) if (table.get(j).getKey().equals(key)) return j; return −1; // special value denotes that key was not found } Code Fragment 10.4: An implementation of a map using a Java ArrayList as an unsorted table. (Continues in Code Fragment 10.5.) The parent class AbstractMap is given in Code Fragment 10.3.

10.1. Maps /∗∗Returns the number of entries in the map. ∗/ public int size() { return table.size(); } /∗∗Returns the value associated with the speciﬁed key (or else null). ∗/ public V get(K key) { int j = ﬁndIndex(key); if (j == −1) return null; // not found return table.get(j).getValue(); } /∗∗Associates given value with given key, replacing a previous value (if any). ∗/ public V put(K key, V value) { int j = ﬁndIndex(key); if (j == −1) { table.add(new MapEntry<>(key, value)); // add new entry return null; } else // key already exists return table.get(j).setValue(value); // replaced value is returned } /∗∗Removes the entry with the speciﬁed key (if any) and returns its value. ∗/ public V remove(K key) { int j = ﬁndIndex(key); int n = size(); if (j == −1) return null; // not found V answer = table.get(j).getValue(); if (j != n −1) table.set(j, table.get(n−1)); // relocate last entry to ’hole’ created by removal table.remove(n−1); // remove last entry of table return answer; } // Support for public entrySet method... private class EntryIterator implements Iterator<Entry<K,V>> { private int j=0; public boolean hasNext() { return j < table.size(); } public Entry<K,V> next() { if (j == table.size()) throw new NoSuchElementException(); return table.get(j++); } public void remove() { throw new UnsupportedOperationException(); } } private class EntryIterable implements Iterable<Entry<K,V>> { public Iterator<Entry<K,V>> iterator() { return new EntryIterator(); } } /∗∗Returns an iterable collection of all key-value entries of the map. ∗/ public Iterable<Entry<K,V>> entrySet() { return new EntryIterable(); } } Code Fragment 10.5: An implementation of a map using a Java ArrayList as an unsorted table (continued from Code Fragment 10.4).

Chapter 10. Maps, Hash Tables, and Skip Lists 10.2 Hash Tables In this section, we introduce one of the most efﬁcient data structures for implementing a map, and the one that is used most in practice. This structure is known as a hash table. Intuitively, a map M supports the abstraction of using keys as “addresses” that help locate an entry. As a mental warm-up, consider a restricted setting in which a map with n entries uses keys that are known to be integers in a range from 0 to N −1 for some N ≥n. In this case, we can represent the map using a lookup table of length N, as diagrammed in Figure 10.3. D Z C Q Figure 10.3: A lookup table with length 11 for a map containing entries (1,D), (3,Z), (6,C), and (7,Q). In this representation, we store the value associated with key k at index k of the table (presuming that we have a distinct way to represent an empty slot). Basic map operations get, put, and remove can be implemented in O(1) worst-case time. There are two challenges in extending this framework to the more general setting of a map. First, we may not wish to devote an array of length N if it is the case that N ≫n. Second, we do not in general require that a map’s keys be integers. The novel concept for a hash table is the use of a hash function to map general keys to corresponding indices in a table. Ideally, keys will be well distributed in the range from 0 to N −1 by a hash function, but in practice there may be two or more distinct keys that get mapped to the same index. As a result, we will conceptualize our table as a bucket array, as shown in Figure 10.4, in which each bucket may manage a collection of entries that are sent to a speciﬁc index by the hash function. (To save space, an empty bucket may be replaced by a null reference.) (1,D) (25,C) (3,F) (14,Z) (39,C) (6,A) (7,Q) Figure 10.4: A bucket array of capacity 11 with entries (1,D), (25,C), (3,F), (14,Z), (6,A), (39,C), and (7,Q), using a simple hash function.

10.2. Hash Tables 10.2.1 Hash Functions The goal of a hash function, h, is to map each key k to an integer in the range [0,N −1], where N is the capacity of the bucket array for a hash table. Equipped with such a hash function, h, the main idea of this approach is to use the hash function value, h(k), as an index into our bucket array, A, instead of the key k (which may not be appropriate for direct use as an index). That is, we store the entry (k,v) in the bucket A[h(k)]. If there are two or more keys with the same hash value, then two different entries will be mapped to the same bucket in A. In this case, we say that a collision has occurred. To be sure, there are ways of dealing with collisions, which we will discuss later, but the best strategy is to try to avoid them in the ﬁrst place. We say that a hash function is “good” if it maps the keys in our map so as to sufﬁciently minimize collisions. For practical reasons, we also would like a hash function to be fast and easy to compute. It is common to view the evaluation of a hash function, h(k), as consisting of two portions—a hash code that maps a key k to an integer, and a compression function that maps the hash code to an integer within a range of indices, [0,N −1], for a bucket array. (See Figure 10.5.) −2 hash code compression function Arbitrary Objects N −1 ··· −1 ··· ··· Figure 10.5: Two parts of a hash function: a hash code and a compression function. The advantage of separating the hash function into two such components is that the hash code portion of that computation is independent of a speciﬁc hash table size. This allows the development of a general hash code for each object that can be used for a hash table of any size; only the compression function depends upon the table size. This is particularly convenient, because the underlying bucket array for a hash table may be dynamically resized, depending on the number of entries currently stored in the map. (See Section 10.2.3.)

Chapter 10. Maps, Hash Tables, and Skip Lists Hash Codes The ﬁrst action that a hash function performs is to take an arbitrary key k in our map and compute an integer that is called the hash code for k; this integer need not be in the range [0,N −1], and may even be negative. We desire that the set of hash codes assigned to our keys should avoid collisions as much as possible. For if the hash codes of our keys cause collisions, then there is no hope for our compression function to avoid them. In this subsection, we begin by discussing the theory of hash codes. Following that, we discuss practical implementations of hash codes in Java. Treating the Bit Representation as an Integer To begin, we note that, for any data type X that is represented using at most as many bits as our integer hash codes, we can simply take as a hash code for X an integer interpretation of its bits. Java relies on 32-bit hash codes, so for base types byte, short, int, and char, we can achieve a good hash code simply by casting a value to int. Likewise, for a variable x of base type ﬂoat, we can convert x to an integer using a call to Float.ﬂoatToIntBits(x), and then use this integer as x’s hash code. For a type whose bit representation is longer than a desired hash code (such as Java’s long and double types), the above scheme is not immediately applicable. One possibility is to use only the high-order 32 bits (or the low-order 32 bits). This hash code, of course, ignores half of the information present in the original key, and if many of the keys in our map only differ in these bits, then they will collide using this simple hash code. A better approach is to combine in some way the high-order and low-order portions of a 64-bit key to form a 32-bit hash code, which takes all the original bits into consideration. A simple implementation is to add the two components as 32bit numbers (ignoring overﬂow), or to take the exclusive-or of the two components. These approaches of combining components can be extended to any object x whose binary representation can be viewed as an n-tuple (x0,x1,...,xn−1) of 32-bit integers, for example, by forming a hash code for x as ∑n−1 i=0 xi, or as x0 ⊕x1⊕···⊕xn−1, where the ⊕symbol represents the bitwise exclusive-or operation (which is the ˆ operator in Java). Polynomial Hash Codes The summation and exclusive-or hash codes, described above, are not good choices for character strings or other variable-length objects that can be viewed as tuples of the form (x0,x1,...,xn−1), where the order of the xi’s is signiﬁcant. For example, consider a 16-bit hash code for a character string s that sums the Unicode values of the characters in s. This hash code unfortunately produces lots of unwanted

10.2. Hash Tables collisions for common groups of strings. In particular, "temp01" and "temp10" collide using this function, as do "stop", "tops", "pots", and "spot". A better hash code should somehow take into consideration the positions of the xi’s. An alternative hash code, which does exactly this, is to choose a nonzero constant, a ̸= 1, and use as a hash code the value x0an−1 +x1an−2 +···+xn−2a+xn−1. Mathematically speaking, this is simply a polynomial in a that takes the components (x0,x1,...,xn−1) of an object x as its coefﬁcients. This hash code is therefore called a polynomial hash code. By Horner’s rule (see Exercise C-4.54), this polynomial can be computed as xn−1 +a(xn−2 +a(xn−3 +···+a(x2 +a(x1 +ax0))···)). Intuitively, a polynomial hash code uses multiplication by different powers as a way to spread out the inﬂuence of each component across the resulting hash code. Of course, on a typical computer, evaluating a polynomial will be done using the ﬁnite bit representation for a hash code; hence, the value will periodically overﬂow the bits used for an integer. Since we are more interested in a good spread of the object x with respect to other keys, we simply ignore such overﬂows. Still, we should be mindful that such overﬂows are occurring and choose the constant a so that it has some nonzero, low-order bits, which will serve to preserve some of the information content even as we are in an overﬂow situation. We have done some experimental studies that suggest that 33, 37, 39, and 41 are particularly good choices for a when working with character strings that are English words. In fact, in a list of over 50,000 English words formed as the union of the word lists provided in two variants of Unix, we found that taking a to be 33, 37, 39, or 41 produced fewer than 7 collisions in each case! Cyclic-Shift Hash Codes A variant of the polynomial hash code replaces multiplication by a with a cyclic shift of a partial sum by a certain number of bits. For example, a 5-bit cyclic shift of the 32-bit value 00111101100101101010100010101000 is achieved by taking the leftmost ﬁve bits and placing those on the rightmost side of the representation, resulting in 10110010110101010001010100000111. While this operation has little natural meaning in terms of arithmetic, it accomplishes the goal of varying the bits of the calculation. In Java, a cyclic shift of bits can be accomplished through careful use of the bitwise shift operators.

Chapter 10. Maps, Hash Tables, and Skip Lists An implementation of a cyclic-shift hash code computation for a character string in Java appears as follows: static int hashCode(String s) { int h=0; for (int i=0; i<s.length(); i++) { h = (h << 5) | (h >>> 27); // 5-bit cyclic shift of the running sum h += (int) s.charAt(i); // add in next character } return h; } As with the traditional polynomial hash code, ﬁne-tuning is required when using a cyclic-shift hash code, as we must wisely choose the amount to shift by for each new character. Our choice of a 5-bit shift is justiﬁed by experiments run on a list of just over 230,000 English words, comparing the number of collisions for various shift amounts (see Table 10.1). Collisions Shift Total Max Table 10.1: Comparison of collision behavior for the cyclic-shift hash code as applied to a list of 230,000 English words. The “Total” column records the total number of words that collide with at least one other, and the “Max” column records the maximum number of words colliding at any one hash code. Note that with a cyclic shift of 0, this hash code reverts to the one that simply sums all the characters.

10.2. Hash Tables Hash Codes in Java The notion of hash codes are an integral part of the Java language. The Object class, which serves as an ancestor of all object types, includes a default hashCode() method that returns a 32-bit integer of type int, which serves as an object’s hash code. The default version of hashCode() provided by the Object class is often just an integer representation derived from the object’s memory address. However, we must be careful if relying on the default version of hashCode() when authoring a class. For hashing schemes to be reliable, it is imperative that any two objects that are viewed as “equal” to each other have the same hash code. This is important because if an entry is inserted into a map, and a later search is performed on a key that is considered equivalent to that entry’s key, the map must recognize this as a match. (See, for example, the UnsortedTableMap.ﬁndIndex method in Code Fragment 10.4.) Therefore, when using a hash table to implement a map, we want equivalent keys to have the same hash code so that they are guaranteed to map to the same bucket. More formally, if a class deﬁnes equivalence through the equals method (see Section 3.5), then that class should also provide a consistent implementation of the hashCode method, such that if x.equals(y) then x.hashCode() == y.hashCode(). As an example, Java’s String class deﬁnes the equals method so that two instances are equivalent if they have precisely the same sequence of characters. That class also overrides the hashCode method to provide consistent behavior. In fact, the implementation of hash codes for the String class is excellent. If we repeat the experiment from the previous page using Java’s implementation of hash codes, there are only 12 collisions among more than 230,000 words. Java’s primitive wrapper classes also deﬁne hashCode, using techniques described on page 412. As an example of how to properly implement hashCode for a user-deﬁned class, we will revisit the SinglyLinkedList class from Chapter 3. We deﬁned the equals method for that class, in Section 3.5.2, so that two lists are equivalent if they represent equal-length sequences of elements that are pairwise equivalent. We can compute a robust hash code for a list by taking the exclusive-or of its elements’ hash codes, while performing a cyclic shift. (See Code Fragment 10.6.) public int hashCode() { int h = 0; for (Node walk=head; walk != null; walk = walk.getNext()) { h ˆ= walk.getElement().hashCode(); // bitwise exclusive-or with element’s code h = (h << 5) | (h >>> 27); // 5-bit cyclic shift of composite code } return h; } Code Fragment 10.6: A robust implementation of the hashCode method for the SinglyLinkedList class from Chapter 3.

Chapter 10. Maps, Hash Tables, and Skip Lists Compression Functions The hash code for a key k will typically not be suitable for immediate use with a bucket array, because the integer hash code may be negative or may exceed the capacity of the bucket array. Thus, once we have determined an integer hash code for a key object k, there is still the issue of mapping that integer into the range [0,N −1]. This computation, known as a compression function, is the second action performed as part of an overall hash function. A good compression function is one that minimizes the number of collisions for a given set of distinct hash codes. The Division Method A simple compression function is the division method, which maps an integer i to i mod N, where N, the size of the bucket array, is a ﬁxed positive integer. Additionally, if we take N to be a prime number, then this compression function helps “spread out” the distribution of hashed values. Indeed, if N is not prime, then there is greater risk that patterns in the distribution of hash codes will be repeated in the distribution of hash values, thereby causing collisions. For example, if we insert keys with hash codes {200,205,210,215,220,... ,600} into a bucket array of size 100, then each hash code will collide with three others. But if we use a bucket array of size 101, then there will be no collisions. If a hash function is chosen well, it should ensure that the probability of two different keys getting hashed to the same bucket is 1/N. Choosing N to be a prime number is not always enough, however, for if there is a repeated pattern of hash codes of the form pN + q for several different p’s, then there will still be collisions. The MAD Method A more sophisticated compression function, which helps eliminate repeated patterns in a set of integer keys, is the Multiply-Add-and-Divide (or “MAD”) method. This method maps an integer i to [(ai+b) mod p] mod N, where N is the size of the bucket array, p is a prime number larger than N, and a and b are integers chosen at random from the interval [0, p−1], with a > 0. This compression function is chosen in order to eliminate repeated patterns in the set of hash codes and get us closer to having a “good” hash function, that is, one such that the probability any two different keys collide is 1/N. This good behavior would be the same as we would have if these keys were “thrown” into A uniformly at random.

10.2. Hash Tables 10.2.2 Collision-Handling Schemes The main idea of a hash table is to take a bucket array, A, and a hash function, h, and use them to implement a map by storing each entry (k,v) in the “bucket” A[h(k)]. This simple idea is challenged, however, when we have two distinct keys, k1 and k2, such that h(k1) = h(k2). The existence of such collisions prevents us from simply inserting a new entry (k,v) directly into the bucket A[h(k)]. It also complicates our procedure for performing insertion, search, and deletion operations. Separate Chaining A simple and efﬁcient way for dealing with collisions is to have each bucket A[ j] store its own secondary container, holding all entries (k,v) such that h(k) = j. A natural choice for the secondary container is a small map instance implemented using an unordered list, as described in Section 10.1.4. This collision resolution rule is known as separate chaining, and is illustrated in Figure 10.6. A Figure 10.6: A hash table of size 13, storing 10 entries with integer keys, with collisions resolved by separate chaining. The compression function is h(k) = k mod 13. For simplicity, we do not show the values associated with the keys. In the worst case, operations on an individual bucket take time proportional to the size of the bucket. Assuming we use a good hash function to index the n entries of our map in a bucket array of capacity N, the expected size of a bucket is n/N. Therefore, if given a good hash function, the core map operations run in O(⌈n/N⌉). The ratio λ = n/N, called the load factor of the hash table, should be bounded by a small constant, preferably below 1. As long as λ is O(1), the core operations on the hash table run in O(1) expected time.

Chapter 10. Maps, Hash Tables, and Skip Lists Open Addressing The separate chaining rule has many nice properties, such as affording simple implementations of map operations, but it nevertheless has one slight disadvantage: It requires the use of an auxiliary data structure to hold entries with colliding keys. If space is at a premium (for example, if we are writing a program for a small handheld device), then we can use the alternative approach of storing each entry directly in a table slot. This approach saves space because no auxiliary structures are employed, but it requires a bit more complexity to properly handle collisions. There are several variants of this approach, collectively referred to as open addressing schemes, which we discuss next. Open addressing requires that the load factor is always at most 1 and that entries are stored directly in the cells of the bucket array itself. Linear Probing and Its Variants A simple method for collision handling with open addressing is linear probing. With this approach, if we try to insert an entry (k,v) into a bucket A[ j] that is already occupied, where j = h(k), then we next try A[( j +1) mod N]. If A[( j +1) mod N] is also occupied, then we try A[( j + 2) mod N], and so on, until we ﬁnd an empty bucket that can accept the new entry. Once this bucket is located, we simply insert the entry there. Of course, this collision resolution strategy requires that we change the implementation when searching for an existing key—the ﬁrst step of all get, put, or remove operations. In particular, to attempt to locate an entry with key equal to k, we must examine consecutive slots, starting from A[h(k)], until we either ﬁnd an entry with an equal key or we ﬁnd an empty bucket. (See Figure 10.7.) The name “linear probing” comes from the fact that accessing a cell of the bucket array can be viewed as a “probe,” and that consecutive probes occur in neighboring cells (when viewed circularly). New element with key = 15 to be inserted Must probe 4 times before ﬁnding empty slot Figure 10.7: Insertion into a hash table with integer keys using linear probing. The hash function is h(k) = k mod 11. Values associated with keys are not shown.

10.2. Hash Tables To implement a deletion, we cannot simply remove a found entry from its slot in the array. For example, after the insertion of key 15 portrayed in Figure 10.7, if the entry with key 37 were trivially deleted, a subsequent search for 15 would fail because that search would start by probing at index 4, then index 5, and then index 6, at which an empty cell is found. A typical way to get around this difﬁculty is to replace a deleted entry with a special “defunct” sentinel object. With this special marker possibly occupying spaces in our hash table, we modify our search algorithm so that the search for a key k will skip over cells containing the defunct sentinel and continue probing until reaching the desired entry or an empty bucket (or returning back to where we started from). Additionally, our algorithm for put should remember a defunct location encountered during the search for k, since this is a valid place to put a new entry (k,v), if no existing entry is found beyond it. Although use of an open addressing scheme can save space, linear probing suffers from an additional disadvantage. It tends to cluster the entries of a map into contiguous runs, which may even overlap (particularly if more than half of the cells in the hash table are occupied). Such contiguous runs of occupied hash cells cause searches to slow down considerably. Another open addressing strategy, known as quadratic probing, iteratively tries the buckets A[(h(k)+ f(i)) mod N], for i = 0,1,2,..., where f(i) = i2, until ﬁnding an empty bucket. As with linear probing, the quadratic probing strategy complicates the removal operation, but it does avoid the kinds of clustering patterns that occur with linear probing. Nevertheless, it creates its own kind of clustering, called secondary clustering, where the set of ﬁlled array cells still has a nonuniform pattern, even if we assume that the original hash codes are distributed uniformly. When N is prime and the bucket array is less than half full, the quadratic probing strategy is guaranteed to ﬁnd an empty slot. However, this guarantee is not valid once the table becomes at least half full, or if N is not chosen as a prime number; we explore the cause of this type of clustering in an exercise (C-10.42). An open addressing strategy that does not cause clustering of the kind produced by linear probing or the kind produced by quadratic probing is the double hashing strategy. In this approach, we choose a secondary hash function, h′, and if h maps some key k to a bucket A[h(k)] that is already occupied, then we iteratively try the buckets A[(h(k) + f(i)) mod N] next, for i = 1,2,3,..., where f(i) = i · h′(k). In this scheme, the secondary hash function is not allowed to evaluate to zero; a common choice is h′(k) = q−(k mod q), for some prime number q < N. Also, N should be a prime. Another approach to avoid clustering with open addressing is to iteratively try buckets A[(h(k) + f(i)) mod N] where f(i) is based on a pseudorandom number generator, providing a repeatable, but somewhat arbitrary, sequence of subsequent probes that depends upon bits of the original hash code.

Chapter 10. Maps, Hash Tables, and Skip Lists 10.2.3 Load Factors, Rehashing, and Eﬃciency In the hash table schemes described thus far, it is important that the load factor, λ = n/N, be kept below 1. With separate chaining, as λ gets very close to 1, the probability of a collision greatly increases, which adds overhead to our operations, since we must revert to linear-time list-based methods in buckets that have collisions. Experiments and average-case analyses suggest that we should maintain λ < 0.9 for hash tables with separate chaining. (By default, Java’s implementation uses separate chaining with λ < 0.75.) With open addressing, on the other hand, as the load factor λ grows beyond 0.5 and starts approaching 1, clusters of entries in the bucket array start to grow as well. These clusters cause the probing strategies to “bounce around” the bucket array for a considerable amount of time before they ﬁnd an empty slot. In Exercise C10.42, we explore the degradation of quadratic probing when λ ≥0.5. Experiments suggest that we should maintain λ < 0.5 for an open addressing scheme with linear probing, and perhaps only a bit higher for other open addressing schemes. If an insertion causes the load factor of a hash table to go above the speciﬁed threshold, then it is common to resize the table (to regain the speciﬁed load factor) and to reinsert all objects into this new table. Although we need not deﬁne a new hash code for each object, we do need to reapply a new compression function that takes into consideration the size of the new table. Rehashing will generally scatter the entries throughout the new bucket array. When rehashing to a new table, it is a good requirement for the new array’s size to be a prime number approximately double the previous size (see Exercise C-10.32). In that way, the cost of rehashing all the entires in the table can be amortized against the time used to insert them in the ﬁrst place (as with dynamic arrays; see Section 7.2.1). Eﬃciency of Hash Tables Although the details of the average-case analysis of hashing are beyond the scope of this book, its probabilistic basis is quite intuitive. If our hash function is good, then we expect the entries to be uniformly distributed in the N cells of the bucket array. Thus, to store n entries, the expected number of keys in a bucket would be ⌈n/N⌉, which is O(1) if n is O(N). The costs associated with a periodic rehashing (when resizing a table after occasional insertions or deletions) can be accounted for separately, leading to an additional O(1) amortized cost for put and remove. In the worst case, a poor hash function could map every entry to the same bucket. This would result in linear-time performance for the core map operations with separate chaining, or with any open addressing model in which the secondary sequence of probes depends only on the hash code. A summary of these costs is given in Table 10.2.

10.2. Hash Tables Method Unsorted Hash Table List expected worst case get O(n) O(1) O(n) put O(n) O(1) O(n) remove O(n) O(1) O(n) size, isEmpty O(1) O(1) O(1) entrySet, keySet, values O(n) O(n) O(n) Table 10.2: Comparison of the running times of the methods of a map realized by means of an unsorted list (as in Section 10.1.4) or a hash table. We let n denote the number of entries in the map, and we assume that the bucket array supporting the hash table is maintained such that its capacity is proportional to the number of entries in the map. An Anecdote About Hashing and Computer Security In a 2003 academic paper, researchers discuss the possibility of exploiting a hash table’s worst-case performance to cause a denial-of-service (DoS) attack of Internet technologies. Since many published algorithms compute hash codes with a deterministic function, an attacker could precompute a very large number of moderatelength strings that all hash to the identical 32-bit hash code. (Recall that by any of the hashing schemes we describe, other than double hashing, if two keys are mapped to the same hash code, they will be inseparable in the collision resolution.) This concern was brought to the attention of the Java development team, and that of many other programming languages, but deemed an insigniﬁcant risk at the time by most. (Kudos to the Perl team for implementing a ﬁx in 2003.) In late 2011, another team of researchers demonstrated an implementation of just such an attack. Web servers allow a series of key-value parameters to be embedded in a URL using a syntax such as ?key1=val1&key2=val2&key3=val3. Those key-value pairs are strings and a typical Web server immediately stores them in a hash-map. Servers already place a limit on the length and number of such parameters, to avoid overload, but they presume that the total insertion time in the map will be linear in the number of entries, given the expected constant-time operations. However, if all keys were to collide, the insertions into the map will require quadratic time, causing the server to perform an inordinate amount of work. In 2012, the OpenJDK team announced the following resolution: they distributed a security patch that includes an alternative hash function that introduces randomization into the computation of hash codes, making it less tractable to reverse engineer a set of colliding strings. However, to avoid breaking existing code, the new feature is disabled by default in Java SE 7 and, when enabled, is only used for hashing strings and only when a table size grows beyond a certain threshold. Enhanced hashing will be enabled in Java SE 8 for all types and uses.

Chapter 10. Maps, Hash Tables, and Skip Lists 10.2.4 Java Hash Table Implementation In this section, we develop two implementations of a hash table, one using separate chaining and the other using open addressing with linear probing. While these approaches to collision resolution are quite different, there are many higherlevel commonalities to the two hashing algorithms. For that reason, we extend the AbstractMap class (from Code Fragment 10.3) to deﬁne a new AbstractHashMap class (see Code Fragment 10.7), which provides much of the functionality common to our two hash table implementations. We will begin by discussing what this abstract class does not do—it does not provide any concrete representation of a table of “buckets.” With separate chaining, each bucket will be a secondary map. With open addressing, however, there is no tangible container for each bucket; the “buckets” are effectively interleaved due to the probing sequences. In our design, the AbstractHashMap class presumes the following to be abstract methods—to be implemented by each concrete subclass: createTable(): This method should create an initially empty table having size equal to a designated capacity instance variable. bucketGet(h, k): This method should mimic the semantics of the public get method, but for a key k that is known to hash to bucket h. bucketPut(h, k, v): This method should mimic the semantics of the public put method, but for a key k that is known to hash to bucket h. bucketRemove(h, k): This method should mimic the semantics of the public remove method, but for a key k known to hash to bucket h. entrySet(): This standard map method iterates through all entries of the map. We do not delegate this on a per-bucket basis because “buckets” in open addressing are not inherently disjoint. What the AbstractHashMap class does provide is mathematical support in the form of a hash compression function using a randomized Multiply-Add-andDivide (MAD) formula, and support for automatically resizing the underlying hash table when the load factor reaches a certain threshold. The hashValue method relies on an original key’s hash code, as returned by its hashCode() method, followed by MAD compression based on a prime number and the scale and shift parameters that are randomly chosen in the constructor. To manage the load factor, the AbstractHashMap class declares a protected member, n, which should equal the current number of entries in the map; however, it must rely on the subclasses to update this ﬁeld from within methods bucketPut and bucketRemove. If the load factor of the table increases beyond 0.5, we request a bigger table (using the createTable method) and reinsert all entries into the new table. (For simplicity, this implementation uses tables of size 2k + 1, even though these are not generally prime.)

10.2. Hash Tables public abstract class AbstractHashMap<K,V> extends AbstractMap<K,V> { protected int n = 0; // number of entries in the dictionary protected int capacity; // length of the table private int prime; // prime factor private long scale, shift; // the shift and scaling factors public AbstractHashMap(int cap, int p) { prime = p; capacity = cap; Random rand = new Random(); scale = rand.nextInt(prime−1) + 1; shift = rand.nextInt(prime); createTable(); } public AbstractHashMap(int cap) { this(cap, 109345121); } // default prime public AbstractHashMap() { this(17); } // default capacity // public methods public int size() { return n; } public V get(K key) { return bucketGet(hashValue(key), key); } public V remove(K key) { return bucketRemove(hashValue(key), key); } public V put(K key, V value) { V answer = bucketPut(hashValue(key), key, value); if (n > capacity / 2) // keep load factor <= 0.5 resize(2 ∗capacity −1); // (or ﬁnd a nearby prime) return answer; } // private utilities private int hashValue(K key) { return (int) ((Math.abs(key.hashCode()∗scale + shift) % prime) % capacity); } private void resize(int newCap) { ArrayList<Entry<K,V>> buﬀer = new ArrayList<>(n); for (Entry<K,V> e : entrySet()) buﬀer.add(e); capacity = newCap; createTable(); // based on updated capacity n = 0; // will be recomputed while reinserting entries for (Entry<K,V> e : buﬀer) put(e.getKey(), e.getValue()); } // protected abstract methods to be implemented by subclasses protected abstract void createTable(); protected abstract V bucketGet(int h, K k); protected abstract V bucketPut(int h, K k, V v); protected abstract V bucketRemove(int h, K k); } Code Fragment 10.7: A base class for our hash table implementations, extending the AbstractMap class from Code Fragment 10.3.

Chapter 10. Maps, Hash Tables, and Skip Lists Separate Chaining To represent each bucket for separate chaining, we use an instance of the simpler UnsortedTableMap class from Section 10.1.4. This technique, in which we use a simple solution to a problem to create a new, more advanced solution, is known as bootstrapping. The advantage of using a map for each bucket is that it becomes easy to delegate responsibilities for top-level map operations to the appropriate bucket. The entire hash table is then represented as a ﬁxed-capacity array A of the secondary maps. Each cell, A[h], is initially a null reference; we only create a secondary map when an entry is ﬁrst hashed to a particular bucket. As a general rule, we implement bucketGet(h, k) by calling A[h].get(k), we implement bucketPut(h, k, v) by calling A[h].put(k, v), and bucketRemove(h, k) by calling A[h].remove(k). However, care is needed for two reasons. First, because we choose to leave table cells as null until a secondary map is needed, each of these fundamental operations must begin by checking to see if A[h] is null. In the case of bucketGet and bucketRemove, if the bucket does not yet exist, we can simply return null as there can not be any entry matching key k. In the case of bucketPut, a new entry must be inserted, so we instantiate a new UnsortedTableMap for A[h] before continuing. The second issue is that, in our AbstractHashMap framework, the subclass has the responsibility to properly maintain the instance variable n when an entry is newly inserted or deleted. Remember that when put(k, v) is called on a map, the size of the map only increases if key k is new to the map (otherwise, the value of an existing entry is reassigned). Similarly, a call to remove(k) only decreases the size of the map when an entry with key equal to k is found. In our implementation, we determine the change in the overall size of the map, by determining if there is any change in the size of the relevant secondary map before and after an operation. Code Fragment 10.8 provides a complete deﬁnition for our ChainHashMap class, which implements a hash table with separate chaining. If we assume that the hash function performs well, a map with n entries and a table of capacity N will have an expected bucket size of n/N (recall, this is its load factor). So even though the individual buckets, implemented as UnsortedTableMap instances, are not particularly efﬁcient, each bucket has expected O(1) size, provided that n is O(N), as in our implementation. Therefore, the expected running time of operations get, put, and remove for this map is O(1). The entrySet method (and thus the related keySet and values) runs in O(n+N) time, as it loops through the length of the table (with length N) and through all buckets (which have cumulative lengths n).

10.2. Hash Tables public class ChainHashMap<K,V> extends AbstractHashMap<K,V> { // a ﬁxed capacity array of UnsortedTableMap that serve as buckets private UnsortedTableMap<K,V>[ ] table; // initialized within createTable public ChainHashMap() { super(); } public ChainHashMap(int cap) { super(cap); } public ChainHashMap(int cap, int p) { super(cap, p); } /∗∗Creates an empty table having length equal to current capacity. ∗/ protected void createTable() { table = (UnsortedTableMap<K,V>[ ]) new UnsortedTableMap[capacity]; } /∗∗Returns value associated with key k in bucket with hash value h, or else null. ∗/ protected V bucketGet(int h, K k) { UnsortedTableMap<K,V> bucket = table[h]; if (bucket == null) return null; return bucket.get(k); } /∗∗Associates key k with value v in bucket with hash value h; returns old value. ∗/ protected V bucketPut(int h, K k, V v) { UnsortedTableMap<K,V> bucket = table[h]; if (bucket == null) bucket = table[h] = new UnsortedTableMap<>(); int oldSize = bucket.size(); V answer = bucket.put(k,v); n += (bucket.size() −oldSize); // size may have increased return answer; } /∗∗Removes entry having key k from bucket with hash value h (if any). ∗/ protected V bucketRemove(int h, K k) { UnsortedTableMap<K,V> bucket = table[h]; if (bucket == null) return null; int oldSize = bucket.size(); V answer = bucket.remove(k); n −= (oldSize −bucket.size()); // size may have decreased return answer; } /∗∗Returns an iterable collection of all key-value entries of the map. ∗/ public Iterable<Entry<K,V>> entrySet() { ArrayList<Entry<K,V>> buﬀer = new ArrayList<>(); for (int h=0; h < capacity; h++) if (table[h] != null) for (Entry<K,V> entry : table[h].entrySet()) buﬀer.add(entry); return buﬀer; } } Code Fragment 10.8: A concrete hash map implementation using separate chaining.

Chapter 10. Maps, Hash Tables, and Skip Lists Linear Probing Our implementation of a ProbeHashMap class, using open addressing with linear probing, is given in Code Fragments 10.9 and 10.10. In order to support deletions, we use a technique described in Section 10.2.2 in which we place a special marker in a table location at which an entry has been deleted, so that we can distinguish between it and a location that has always been empty. To this end, we create a ﬁxed entry instance, DEFUNCT, as a sentinel (disregarding any key or value stored within), and use references to that instance to mark vacated cells. The most challenging aspect of open addressing is to properly trace the series of probes when collisions occur during a search for an existing entry, or placement of a new entry. To this end, the three primary map operations each rely on a utility, ﬁndSlot, that searches for an entry with key k in “bucket” h (that is, where h is the index returned by the hash function for key k). When attempting to retrieve the value associated with a given key, we must continue probing until we ﬁnd the key, or until we reach a table slot with a null reference. We cannot stop the search upon reaching an DEFUNCT sentinel, because it represents a location that may have been ﬁlled at the time the desired entry was once inserted. When a key-value pair is being placed in the map, we must ﬁrst attempt to ﬁnd an existing entry with the given key, so that we might overwrite its value. Therefore, we must search beyond any occurrences of the DEFUNCT sentinel when inserting. However, if no match is found, we prefer to repurpose the ﬁrst slot marked with DEFUNCT, if any, when placing the new element in the table. The ﬁndSlot method enacts this logic, continuing an unsuccessful search until ﬁnding a truly empty slot, and returning the index of the ﬁrst available slot for an insertion. When deleting an existing entry within bucketRemove, we intentionally set the table entry to the DEFUNCT sentinel in accordance with our strategy. public class ProbeHashMap<K,V> extends AbstractHashMap<K,V> { private MapEntry<K,V>[ ] table; // a ﬁxed array of entries (all initially null) private MapEntry<K,V> DEFUNCT = new MapEntry<>(null, null); //sentinel public ProbeHashMap() { super(); } public ProbeHashMap(int cap) { super(cap); } public ProbeHashMap(int cap, int p) { super(cap, p); } /∗∗Creates an empty table having length equal to current capacity. ∗/ protected void createTable() { table = (MapEntry<K,V>[ ]) new MapEntry[capacity]; // safe cast } /∗∗Returns true if location is either empty or the ”defunct” sentinel. ∗/ private boolean isAvailable(int j) { return (table[j] == null || table[j] == DEFUNCT); } Code Fragment 10.9: Concrete ProbeHashMap class that uses linear probing for collision resolution. (Continues in Code Fragment 10.10.)

10.2. Hash Tables /∗∗Returns index with key k, or −(a+1) such that k could be added at index a. ∗/ private int ﬁndSlot(int h, K k) { int avail = −1; // no slot available (thus far) int j = h; // index while scanning table do { if (isAvailable(j)) { // may be either empty or defunct if (avail == −1) avail = j; // this is the ﬁrst available slot! if (table[j] == null) break; // if empty, search fails immediately } else if (table[j].getKey().equals(k)) return j; // successful match j = (j+1) % capacity; // keep looking (cyclically) } while (j != h); // stop if we return to the start return −(avail + 1); // search has failed } /∗∗Returns value associated with key k in bucket with hash value h, or else null. ∗/ protected V bucketGet(int h, K k) { int j = ﬁndSlot(h, k); if (j < 0) return null; // no match found return table[j].getValue(); } /∗∗Associates key k with value v in bucket with hash value h; returns old value. ∗/ protected V bucketPut(int h, K k, V v) { int j = ﬁndSlot(h, k); if (j >= 0) // this key has an existing entry return table[j].setValue(v); table[−(j+1)] = new MapEntry<>(k, v); // convert to proper index n++; return null; } /∗∗Removes entry having key k from bucket with hash value h (if any). ∗/ protected V bucketRemove(int h, K k) { int j = ﬁndSlot(h, k); if (j < 0) return null; // nothing to remove V answer = table[j].getValue(); table[j] = DEFUNCT; // mark this slot as deactivated n−−; return answer; } /∗∗Returns an iterable collection of all key-value entries of the map. ∗/ public Iterable<Entry<K,V>> entrySet() { ArrayList<Entry<K,V>> buﬀer = new ArrayList<>(); for (int h=0; h < capacity; h++) if (!isAvailable(h)) buﬀer.add(table[h]); return buﬀer; } } Code Fragment 10.10: Concrete ProbeHashMap class that uses linear probing for collision resolution (continued from Code Fragment 10.9).

Chapter 10. Maps, Hash Tables, and Skip Lists 10.3 Sorted Maps The traditional map ADT allows a user to look up the value associated with a given key, but the search for that key is a form known as an exact search. In this section, we will introduce an extension known as the sorted map ADT that includes all behaviors of the standard map, plus the following: ﬁrstEntry(): Returns the entry with smallest key value (or null, if the map is empty). lastEntry(): Returns the entry with largest key value (or null, if the map is empty). ceilingEntry(k): Returns the entry with the least key value greater than or equal to k (or null, if no such entry exists). ﬂoorEntry(k): Returns the entry with the greatest key value less than or equal to k (or null, if no such entry exists). lowerEntry(k): Returns the entry with the greatest key value strictly less than k (or null, if no such entry exists). higherEntry(k): Returns the entry with the least key value strictly greater than k (or null if no such entry exists). subMap(k1, k2): Returns an iteration of all entries with key greater than or equal to k1, but strictly less than k2. We note that the above methods are included within the java.util.NavigableMap interface (which extends the simpler java.util.SortedMap interface). To motivate the use of a sorted map, consider a computer system that maintains information about events that have occurred (such as ﬁnancial transactions), with a time stamp marking the occurrence of each event. If the time stamps were unique for a particular system, we could organize a map with a time stamp serving as a key, and a record about the event that occurred at that time as the value. A particular time stamp could serve as a reference ID for an event, in which case we can quickly retrieve information about that event from the map. However, the (unsorted) map ADT does not provide any way to get a list of all events ordered by the time at which they occur, or to search for which event occurred closest to a particular time. In fact, hash-based implementations of the map ADT intentionally scatter keys that may seem very “near” to each other in the original domain, so that they are more uniformly distributed in a hash table.

10.3. Sorted Maps 10.3.1 Sorted Search Tables Several data structures can efﬁciently support the sorted map ADT, and we will examine some advanced techniques in Section 10.4 and Chapter 11. In this section, we will begin by exploring a simple implementation of a sorted map. We store the map’s entries in an array list A so that they are in increasing order of their keys. (See Figure 10.8.) We refer to this implementation as a sorted search table. 12 14 17 19 22 25 27 28 33 10 11 12 13 14 15 Figure 10.8: Realization of a map by means of a sorted search table. We show only the keys for this map, so as to highlight their ordering. As was the case with the unsorted table map of Section 10.1.4, the sorted search table has a space requirement that is O(n). The primary advantage of this representation, and our reason for insisting that A be array-based, is that it allows us to use the binary search algorithm for a variety of efﬁcient operations. Binary Search and Inexact Searches We originally presented the binary search algorithm in Section 5.1.3, as a means for detecting whether a given target is stored within a sorted sequence. In our original presentation (Code Fragment 5.3 on page 197), a binarySearch method returned true or false to designate whether the desired target was found. The important realization is that, while performing a binary search, we can instead return the index at or near where a target might be found. During a successful search, the standard implementation determines the precise index at which the target is found. During an unsuccessful search, although the target is not found, the algorithm will effectively determine a pair of indices designating elements of the collection that are just less than or just greater than the missing target. In Code Fragments 10.11 and 10.12, we present a complete implementation of a class, SortedTableMap, that supports the sorted map ADT. The most notable feature of our design is the inclusion of a ﬁndIndex utility method. This method uses the recursive binary search algorithm, but returns the index of the leftmost entry in the search range having key greater than or equal to k; if no entry in the search range has such a key, we return the index just beyond the end of the search range. By this convention, if an entry has the target key, the search returns the index of that entry. (Recall that keys are unique in a map.) If the key is absent, the method returns the index at which a new entry with that key would be inserted.

Chapter 10. Maps, Hash Tables, and Skip Lists public class SortedTableMap<K,V> extends AbstractSortedMap<K,V> { private ArrayList<MapEntry<K,V>> table = new ArrayList<>(); public SortedTableMap() { super(); } public SortedTableMap(Comparator<K> comp) { super(comp); } /∗∗Returns the smallest index for range table[low..high] inclusive storing an entry with a key greater than or equal to k (or else index high+1, by convention). ∗/ private int ﬁndIndex(K key, int low, int high) { if (high < low) return high + 1; // no entry qualiﬁes int mid = (low + high) / 2; int comp = compare(key, table.get(mid)); if (comp == 0) return mid; // found exact match else if (comp < 0) return ﬁndIndex(key, low, mid −1); // answer is left of mid (or possibly mid) else return ﬁndIndex(key, mid + 1, high); // answer is right of mid } /∗∗Version of ﬁndIndex that searches the entire table ∗/ private int ﬁndIndex(K key) { return ﬁndIndex(key, 0, table.size() −1); } /∗∗Returns the number of entries in the map. ∗/ public int size() { return table.size(); } /∗∗Returns the value associated with the speciﬁed key (or else null). ∗/ public V get(K key) { int j = ﬁndIndex(key); if (j == size() || compare(key, table.get(j)) != 0) return null; // no match return table.get(j).getValue(); } /∗∗Associates the given value with the given key, returning any overridden value.∗/ public V put(K key, V value) { int j = ﬁndIndex(key); if (j < size() && compare(key, table.get(j)) == 0) // match exists return table.get(j).setValue(value); table.add(j, new MapEntry<K,V>(key,value)); // otherwise new return null; } /∗∗Removes the entry having key k (if any) and returns its associated value. ∗/ public V remove(K key) { int j = ﬁndIndex(key); if (j == size() || compare(key, table.get(j)) != 0) return null; // no match return table.remove(j).getValue(); } Code Fragment 10.11: An implementation of the SortedTableMap class. (Continues in Code Fragment 10.12.) The AbstractSortedMap base class (available online), provides the utility method, compare, based on a given comparator.

10.3. Sorted Maps /∗∗Utility returns the entry at index j, or else null if j is out of bounds. ∗/ private Entry<K,V> safeEntry(int j) { if (j < 0 || j >= table.size()) return null; return table.get(j); } /∗∗Returns the entry having the least key (or null if map is empty). ∗/ public Entry<K,V> ﬁrstEntry() { return safeEntry(0); } /∗∗Returns the entry having the greatest key (or null if map is empty). ∗/ public Entry<K,V> lastEntry() { return safeEntry(table.size()−1); } /∗∗Returns the entry with least key greater than or equal to given key (if any). ∗/ public Entry<K,V> ceilingEntry(K key) { return safeEntry(ﬁndIndex(key)); } /∗∗Returns the entry with greatest key less than or equal to given key (if any). ∗/ public Entry<K,V> ﬂoorEntry(K key) { int j = ﬁndIndex(key); if (j == size() || ! key.equals(table.get(j).getKey())) j−−; // look one earlier (unless we had found a perfect match) return safeEntry(j); } /∗∗Returns the entry with greatest key strictly less than given key (if any). ∗/ public Entry<K,V> lowerEntry(K key) { return safeEntry(ﬁndIndex(key) −1); // go strictly before the ceiling entry } public Entry<K,V> higherEntry(K key) { /∗∗Returns the entry with least key strictly greater than given key (if any). ∗/ int j = ﬁndIndex(key); if (j < size() && key.equals(table.get(j).getKey())) j++; // go past exact match return safeEntry(j); } // support for snapshot iterators for entrySet() and subMap() follow private Iterable<Entry<K,V>> snapshot(int startIndex, K stop) { ArrayList<Entry<K,V>> buﬀer = new ArrayList<>(); int j = startIndex; while (j < table.size() && (stop == null || compare(stop, table.get(j)) > 0)) buﬀer.add(table.get(j++)); return buﬀer; } public Iterable<Entry<K,V>> entrySet() { return snapshot(0, null); } public Iterable<Entry<K,V>> subMap(K fromKey, K toKey) { return snapshot(ﬁndIndex(fromKey), toKey); } } Code Fragment 10.12: An implementation of the SortedTableMap class (continued from Code Fragment 10.11).

Chapter 10. Maps, Hash Tables, and Skip Lists Analysis We conclude by analyzing the performance of our SortedTableMap implementation. A summary of the running times for all methods of the sorted map ADT (including the traditional map operations) is given in Table 10.3. It should be clear that the size, ﬁrstEntry, and lastEntry methods run in O(1) time, and that iterating the keys of the table in either direction can be performed in O(n) time. The analysis for the various forms of search all depend on the fact that a binary search on a table with n entries runs in O(logn) time. This claim was originally shown as Proposition 5.2 in Section 5.2, and that analysis clearly applies to our ﬁndIndex method as well. We therefore claim an O(logn) worst-case running time for methods get, ceilingEntry, ﬂoorEntry, lowerEntry, and higherEntry. Each of these makes a single call to ﬁndIndex, followed by a constant number of additional steps to determine the appropriate answer based on the index. The analysis of subMap is a bit more interesting. It begins with a binary search to ﬁnd the ﬁrst item within the range (if any). After that, it executes a loop that takes O(1) time per iteration to gather subsequent values until reaching the end of the range. If there are s items reported in the range, the total running time is O(s+logn). In contrast to the efﬁcient search operations, update operations for a sorted table may take considerable time. Although binary search can help identify the index at which an update occurs, both insertions and deletions require, in the worst case, that linearly many existing elements be shifted in order to maintain the sorted order of the table. Speciﬁcally, the potential call to table.add from within put and table.remove from within remove lead to O(n) worst-case time. (See the discussion of corresponding operations of the ArrayList class in Section 7.2.) In conclusion, sorted tables are primarily used in situations where we expect many searches but relatively few updates. Method Running Time size O(1) get O(logn) put O(n); O(logn) if map has entry with given key remove O(n) ﬁrstEntry, lastEntry O(1) ceilingEntry, ﬂoorEntry, O(logn) lowerEntry, higherEntry subMap O(s+logn) where s items are reported entrySet, keySet, values O(n) Table 10.3: Performance of a sorted map, as implemented with SortedTableMap. We use n to denote the number of items in the map at the time the operation is performed. The space requirement is O(n).

10.3. Sorted Maps 10.3.2 Two Applications of Sorted Maps In this section, we explore applications in which there is particular advantage to using a sorted map rather than a traditional (unsorted) map. To apply a sorted map, keys must come from a domain that is totally ordered. Furthermore, to take advantage of the inexact or range searches afforded by a sorted map, there should be some reason why nearby keys have relevance to a search. Flight Databases There are several websites on the Internet that allow users to perform queries on ﬂight databases to ﬁnd ﬂights between various cities, typically with the intent to buy a ticket. To make a query, a user speciﬁes origin and destination cities, a departure date, and a departure time. To support such queries, we can model the ﬂight database as a map, where keys are Flight objects that contain ﬁelds corresponding to these four parameters. That is, a key is a tuple k = (origin,destination,date,time). Additional information about a ﬂight, such as the ﬂight number, the number of seats still available in ﬁrst (F) and coach (Y) class, the ﬂight duration, and the fare, can be stored in the value object. Finding a requested ﬂight is not simply a matter of ﬁnding an exact match for a requested query. Although a user typically wants to exactly match the origin and destination cities, he or she may have ﬂexibility for the departure date, and certainly will have some ﬂexibility for the departure time on a speciﬁc day. We can handle such a query by ordering our keys lexicographically. Then, an efﬁcient implementation for a sorted map would be a good way to satisfy users’ queries. For instance, given a user query key k, we could call ceilingEntry(k) to return the ﬁrst ﬂight between the desired cities, having a departure date and time matching the desired query or later. Better yet, with well-constructed keys, we could use subMap(k1, k2) to ﬁnd all ﬂights within a given range of times. For example, if k1 = (ORD, PVD, 05May, 09:30), and k2 = (ORD, PVD, 05May, 20:00), a respective call to subMap(k1, k2) might result in the following sequence of key-value pairs: (ORD, PVD, 05May, 09:53) : (AA 1840, F5, Y15, 02:05, $251), (ORD, PVD, 05May, 13:29) : (AA 600, F2, Y0, 02:16, $713), (ORD, PVD, 05May, 17:39) : (AA 416, F3, Y9, 02:09, $365), (ORD, PVD, 05May, 19:50) : (AA 1828, F9, Y25, 02:13, $186)

Chapter 10. Maps, Hash Tables, and Skip Lists Maxima Sets Life is full of trade-offs. We often have to trade off a desired performance measure against a corresponding cost. Suppose, for the sake of an example, we are interested in maintaining a database rating automobiles by their maximum speeds and their cost. We would like to allow someone with a certain amount of money to query our database to ﬁnd the fastest car they can possibly afford. We can model such a trade-off problem as this by using a key-value pair to model the two parameters that we are trading off, which in this case would be the pair (cost,speed) for each car. Notice that some cars are strictly better than other cars using this measure. For example, a car with cost-speed pair (30000,100) is strictly better than a car with cost-speed pair (40000,90). At the same time, there are some cars that are not strictly dominated by another car. For example, a car with cost-speed pair (30000,100) may be better or worse than a car with cost-speed pair (40000,120), depending on how much money we have to spend. (See Figure 10.9.) Performance Cost d f h a p g b e c Figure 10.9: Illustrating the cost-performance trade-off with pairs represented by points in the plane. Notice that point p is strictly better than points c, d, and e, but may be better or worse than points a, b, f, g, and h, depending on the price we are willing to pay. Thus, if we were to add p to our set, we could remove the points c, d, and e, but not the others. Formally, we say a cost-performance pair (a,b) dominates pair (c,d) ̸= (a,b) if a ≤c and b ≥d, that is, if the ﬁrst pair has no greater cost and at least as good performance. A pair (a,b) is called a maximum pair if it is not dominated by any other pair. We are interested in maintaining the set of maxima of a collection of cost-performance pairs. That is, we would like to add new pairs to this collection (for example, when a new car is introduced), and to query this collection for a given dollar amount, d, to ﬁnd the fastest car that costs no more than d dollars.

10.3. Sorted Maps Maintaining a Maxima Set with a Sorted Map We can store the set of maxima pairs in a sorted map so that the cost is the key ﬁeld and performance (speed) is the value. We can then implement operations add(c, p), which adds a new cost-performance entry (c, p), and best(c), which returns the entry having best performance of those with cost at most c. Code Fragment 10.13 provides an implementation of such a CostPerformanceDatabase class. /∗∗Maintains a database of maximal (cost,performance) pairs. ∗/ public class CostPerformanceDatabase { SortedMap<Integer,Integer> map = new SortedTableMap<>(); /∗∗Constructs an initially empty database. ∗/ public CostPerformanceDatabase() { } /∗∗Returns the (cost,performance) entry with largest cost not exceeding c. ∗(or null if no entry exist with cost c or less). ∗/ public Entry<Integer,Integer> best(int cost) { return map.ﬂoorEntry(cost); } /∗∗Add a new entry with given cost c and performance p. ∗/ public void add(int c, int p) { Entry<Integer,Integer> other = map.ﬂoorEntry(c); // other is at least as cheap if (other != null && other.getValue() >= p) // if its performance is as good, return; // (c,p) is dominated, so ignore map.put(c, p); // else, add (c,p) to database // and now remove any entries that are dominated by the new one other = map.higherEntry(c); // other is more expensive than c while (other != null && other.getValue() <= p) { // if not better performance map.remove(other.getKey()); // remove the other entry other = map.higherEntry(c); } } } Code Fragment 10.13: An implementation of a class maintaining a set of maximal cost-performance entries using a sorted map. Unfortunately, if we implement the sorted map using the SortedTableMap class, the add behavior has O(n) worst-case running time. If, on the other hand, we implement the map using a skip list, which we next describe, we can perform best(c) queries in O(logn) expected time and add(c, p) updates in O((1+r)logn) expected time, where r is the number of points removed.

Chapter 10. Maps, Hash Tables, and Skip Lists 10.4 Skip Lists In Section 10.3.1, we saw that a sorted table will allow O(logn)-time searches via the binary search algorithm. Unfortunately, update operations on a sorted table have O(n) worst-case running time because of the need to shift elements. In Chapter 7 we demonstrated that linked lists support very efﬁcient update operations, as long as the position within the list is identiﬁed. Unfortunately, we cannot perform fast searches on a standard linked list; for example, the binary search algorithm requires an efﬁcient means for direct accessing an element of a sequence by index. An interesting data structure for efﬁciently realizing the sorted map ADT is the skip list. Skip lists provide a clever compromise to efﬁciently support search and update operations; they are implemented as the java.util.ConcurrentSkipListMap class. A skip list S for a map M consists of a series of lists {S0,S1,...,Sh}. Each list Si stores a subset of the entries of M sorted by increasing keys, plus entries with two sentinel keys denoted −∞and +∞, where −∞is smaller than every possible key that can be inserted in M and +∞is larger than every possible key that can be inserted in M. In addition, the lists in S satisfy the following: • List S0 contains every entry of the map M (plus sentinels −∞and +∞). • For i = 1,...,h−1, list Si contains (in addition to −∞and +∞) a randomly generated subset of the entries in list Si−1. • List Sh contains only −∞and +∞. An example of a skip list is shown in Figure 10.10. It is customary to visualize a skip list S with list S0 at the bottom and lists S1,...,Sh above it. Also, we refer to h as the height of skip list S. Intuitively, the lists are set up so that Si+1 contains roughly alternate entries of Si. However, the halving of the number of entries from one list to the next is not enforced as an explicit property of skip lists; instead, randomization is used. As -∞ -∞ -∞ -∞ -∞ -∞ S5 S4 S3 S2 S1 S0 +∞ +∞ +∞ +∞ +∞ +∞ Figure 10.10: Example of a skip list storing 10 entries. For simplicity, we show only the entries’ keys, not their associated values.

10.4. Skip Lists we shall see in the details of the insertion method, the entries in Si+1 are chosen at random from the entries in Si by picking each entry from Si to also be in Si+1 with probability 1/2. That is, in essence, we “ﬂip a coin” for each entry in Si and place that entry in Si+1 if the coin comes up “heads.” Thus, we expect S1 to have about n/2 entries, S2 to have about n/4 entries, and, in general, Si to have about n/2i entries. As a consequence, we expect the height h of S to be about logn. Functions that generate random-like numbers are built into most modern computers, because they are used extensively in computer games, cryptography, and computer simulations. Some functions, called pseudorandom number generators, generate such numbers, starting with an initial seed. (See discussion of the java.util.Random class in Section 3.1.3.) Other methods use hardware devices to extract “true” random numbers from nature. In any case, we will assume that our computer has access to numbers that are sufﬁciently random for our analysis. An advantage of using randomization in data structure and algorithm design is that the structures and methods that result can be simple and efﬁcient. The skip list has the same logarithmic time bounds for searching as is achieved by the binary search algorithm, yet it extends that performance to update methods when inserting or deleting entries. Nevertheless, the bounds are expected for the skip list, while binary search of a sorted table has a worst-case bound. A skip list makes random choices in arranging its structure in such a way that search and update times are O(logn) on average, where n is the number of entries in the map. Interestingly, the notion of average time complexity used here does not depend on the probability distribution of the keys in the input. Instead, it depends on the use of a random-number generator in the implementation of the insertions to help decide where to place the new entry. The running time is averaged over all possible outcomes of the random numbers used when inserting entries. As with the position abstraction used for lists and trees, we view a skip list as a two-dimensional collection of positions arranged horizontally into levels and vertically into towers. Each level is a list Si and each tower contains positions storing the same entry across consecutive lists. The positions in a skip list can be traversed using the following operations: next(p): Returns the position following p on the same level. prev(p): Returns the position preceding p on the same level. above(p): Returns the position above p in the same tower. below(p): Returns the position below p in the same tower. We conventionally assume that these operations return null if the position requested does not exist. Without going into the details, we note that we can easily implement a skip list by means of a linked structure such that the individual traversal methods each take O(1) time, given a skip-list position p. Such a linked structure is essentially a collection of h doubly linked lists aligned at towers, which are also doubly linked lists.

Chapter 10. Maps, Hash Tables, and Skip Lists 10.4.1 Search and Update Operations in a Skip List The skip-list structure affords simple map search and update algorithms. In fact, all of the skip-list search and update algorithms are based on an elegant SkipSearch method that takes a key k and ﬁnds the position p of the entry in list S0 that has the largest key less than or equal to k (which is possibly −∞). Searching in a Skip List Suppose we are given a search key k. We begin the SkipSearch method by setting a position variable p to the topmost, left position in the skip list S, called the start position of S. That is, the start position is the position of Sh storing the special entry with key −∞. We then perform the following steps (see Figure 10.11), where key(p) denotes the key of the entry at position p: 1. If S.below(p) is null, then the search terminates—we are at the bottom and have located the entry in S with the largest key less than or equal to the search key k. Otherwise, we drop down to the next lower level in the present tower by setting p = S.below(p). 2. Starting at position p, we move p forward until it is at the rightmost position on the present level such that key(p) ≤k. We call this the scan forward step. Note that such a position always exists, since each level contains the keys +∞and −∞. It may be that p remains where it started after we perform such a forward scan for this level. 3. Return to step 1. +∞ +∞ +∞ +∞ +∞ S0 S5 S4 S3 S2 S1 -∞ -∞ -∞ -∞ -∞ -∞ +∞ Figure 10.11: Example of a search in a skip list. The positions examined when searching for key 50 are highlighted. We give a pseudocode description of the skip-list search algorithm, SkipSearch, in Code Fragment 10.14. Given this method, we perform the map operation get(k) by computing p = SkipSearch(k) and testing whether or not key(p) = k. If these two keys are equal, we return the associated value; otherwise, we return null.

10.4. Skip Lists Algorithm SkipSearch(k): Input: A search key k Output: Position p in the bottom list S0 with the largest key having key(p) ≤k p = s {begin at start position} while below(p) ̸= null do p = below(p) {drop down} while k ≥key(next(p)) do p = next(p) {scan forward} return p Code Fragment 10.14: Algorithm to search a skip list S for key k. Variable s holds the start position of S. As it turns out, the expected running time of algorithm SkipSearch on a skip list with n entries is O(logn). We postpone the justiﬁcation of this fact, however, until after we discuss the implementation of the update methods for skip lists. Navigation starting at the position identiﬁed by SkipSearch(k) can be easily used to provide the additional forms of searches in the sorted map ADT (e.g., ceilingEntry, subMap). Insertion in a Skip List The execution of the map operation put(k, v) begins with a call to SkipSearch(k). This gives us the position p of the bottom-level entry with the largest key less than or equal to k (note that p may hold the special entry with key −∞). If key(p) = k, the associated value is overwritten with v. Otherwise, we need to create a new tower for entry (k,v). We insert (k,v) immediately after position p within S0. After inserting the new entry at the bottom level, we use randomization to decide the height of the tower for the new entry. We “ﬂip” a coin, and if the ﬂip comes up tails, then we stop here. Else (the ﬂip comes up heads), we backtrack to the previous (next higher) level and insert (k,v) in this level at the appropriate position. We again ﬂip a coin; if it comes up heads, we go to the next higher level and repeat. Thus, we continue to insert the new entry (k,v) in lists until we ﬁnally get a ﬂip that comes up tails. We link together all the references to the new entry (k,v) created in this process to create its tower. A fair coin ﬂip can be simulated with Java’s builtin pseudorandom number generator java.util.Random by calling nextBoolean(), which returns true or false, each with probability 1/2. We give the insertion algorithm for a skip list S in Code Fragment 10.15 and we illustrate it in Figure 10.12. The algorithm uses an insertAfterAbove(p, q, (k,v)) method that inserts a position storing the entry (k,v) after position p (on the same level as p) and above position q, returning the new position r (and setting internal references so that next, prev, above, and below methods will work correctly for p, q, and r). The expected running time of the insertion algorithm on a skip list with n entries is O(logn), as we show in Section 10.4.2.

Chapter 10. Maps, Hash Tables, and Skip Lists Algorithm SkipInsert(k, v): Input: Key k and value v Output: Topmost position of the entry inserted in the skip list p = SkipSearch(k) {position in bottom list with largest key less than k} q = null {current node of new entry’s tower} i = −1 {current height of new entry’s tower} repeat i = i +1 {increase height of new entry’s tower} if i ≥h then h = h +1 {add a new level to the skip list} t = next(s) s = insertAfterAbove(null, s, (−∞,null)) {grow leftmost tower} insertAfterAbove(s, t, (+∞,null)) {grow rightmost tower} q = insertAfterAbove(p, q, (k,v)) {add node to new entry’s tower} while above(p) == null do p = prev(p) {scan backward} p = above(p) {jump up to higher level} until coinFlip() == tails n = n +1 return q {top node of new entry’s tower} Code Fragment 10.15: Insertion in a skip list of entry (k,v) We assume the skip list does not have an entry with key k. Method coinFlip() returns “heads” or “tails”, each with probability 1/2. Instance variables n, h, and s respectively hold the number of entries, the height, and the start node of the skip list. S1 S2 S3 S4 S5 +∞ +∞ +∞ +∞ +∞ +∞ -∞ -∞ -∞ -∞ -∞ -∞ S0 Figure 10.12: Insertion of an entry with key 42 into the skip list of Figure 10.10 using method SkipInsert (Code Fragment 10.15). We assume that the random “coin ﬂips” for the new entry came up heads three times in a row, followed by tails. The positions visited are highlighted in blue. The positions of the tower of the new entry (variable q) are drawn with thick lines, and the positions preceding them (variable p) are ﬂagged.

10.4. Skip Lists Removal in a Skip List Like the search and insertion algorithms, the removal algorithm for a skip list is quite simple. In fact, it is even easier than the insertion algorithm. To perform the map operation remove(k), we will begin by executing method SkipSearch(k). If the returned position p stores an entry with key different from k, we return null. Otherwise, we remove p and all the positions above p, which are easily accessed by using above operations to climb up the tower of this entry in S starting at position p. While removing levels of the tower, we reestablish links between the horizontal neighbors of each removed position. The removal algorithm is illustrated in Figure 10.13 and a detailed description of it is left as an exercise (R-10.24). As we show in the next subsection, the remove operation in a skip list with n entries has O(logn) expected running time. Before we give this analysis, however, there are some minor improvements to the skip-list data structure we would like to discuss. First, we do not actually need to store references to values at the levels of the skip list above the bottom level, because all that is needed at these levels are references to keys. In fact, we can more efﬁciently represent a tower as a single object, storing the key-value pair, and maintaining j previous references and j next references if the tower reaches level Sj. Second, for the horizontal axes, it is possible to keep the list singly linked, storing only the next references. We can perform insertions and removals in strictly a top-down, scan-forward fashion. We explore the details of this optimization in Exercise C-10.55. Neither of these optimizations improve the asymptotic performance of skip lists by more than a constant factor, but these improvements can, nevertheless, be meaningful in practice. In fact, experimental evidence suggests that optimized skip lists are faster in practice than AVL trees and other balanced search trees, which are discussed in Chapter 11. S5 S4 S3 S2 S1 -∞ -∞ -∞ -∞ +∞ +∞ +∞ +∞ +∞ -∞ -∞ +∞ S0 Figure 10.13: Removal of the entry with key 25 from the skip list of Figure 10.12. The positions visited after the search for the position of S0 holding the entry are highlighted in blue. The positions removed are drawn with dashed lines.

Chapter 10. Maps, Hash Tables, and Skip Lists Maintaining the Topmost Level A skip list S must maintain a reference to the start position (the topmost, leftmost position in S) as an instance variable, and must have a policy for any insertion that wishes to continue growing the tower for a new entry past the top level of S. There are two possible courses of action we can take, both of which have their merits. One possibility is to restrict the top level, h, to be kept at some ﬁxed value that is a function of n, the number of entries currently in the map (from the analysis we will see that h = max{10,2⌈logn⌉} is a reasonable choice, and picking h = 3⌈logn⌉ is even safer). Implementing this choice means that we must modify the insertion algorithm to stop inserting a new position once we reach the topmost level (unless ⌈logn⌉< ⌈log(n+1)⌉, in which case we can now go at least one more level, since the bound on the height is increasing). The other possibility is to let an insertion continue growing a tower as long as heads keep getting returned from the random number generator. This is the approach taken by algorithm SkipInsert of Code Fragment 10.15. As we show in the analysis of skip lists, the probability that an insertion will go to a level that is more than O(logn) is very low, so this design choice should also work. Either choice will still result in the expected O(logn) time to perform search, insertion, and removal, as we will show in the next section. 10.4.2 Probabilistic Analysis of Skip Lists ⋆ As we have shown above, skip lists provide a simple implementation of a sorted map. In terms of worst-case performance, however, skip lists are not a superior data structure. In fact, if we do not ofﬁcially prevent an insertion from continuing signiﬁcantly past the current highest level, then the insertion algorithm can go into what is almost an inﬁnite loop (it is not actually an inﬁnite loop, however, since the probability of having a fair coin repeatedly come up heads forever is 0). Moreover, we cannot inﬁnitely add positions to a list without eventually running out of memory. In any case, if we terminate position insertion at the highest level h, then the worst-case running time for performing the get, put, and remove map operations in a skip list S with n entries and height h is O(n+ h). This worst-case performance occurs when the tower of every entry reaches level h−1, where h is the height of S. However, this event has very low probability. Judging from this worst case, we might conclude that the skip-list structure is strictly inferior to the other map implementations discussed earlier in this chapter. But this would not be a fair analysis, for this worst-case behavior is a gross overestimate. ⋆We use a star (⋆) to indicate sections containing material more advanced than the material in the rest of the chapter; this material can be considered optional in a ﬁrst reading.

10.4. Skip Lists Bounding the Height of a Skip List Because the insertion step involves randomization, a more accurate analysis of skip lists involves a bit of probability. At ﬁrst, this might seem like a major undertaking, for a complete and thorough probabilistic analysis could require deep mathematics (and, indeed, there are several such deep analyses that have appeared in data structures research literature). Fortunately, such an analysis is not necessary to understand the expected asymptotic behavior of skip lists. The informal and intuitive probabilistic analysis we give below uses only basic concepts of probability theory. Let us begin by determining the expected value of the height h of a skip list S with n entries (assuming that we do not terminate insertions early). The probability that a given entry has a tower of height i ≥1 is equal to the probability of getting i consecutive heads when ﬂipping a coin, that is, this probability is 1/2i. Hence, the probability Pi that level i has at least one position is at most Pi ≤n 2i , because the probability that any one of n different events occurs is at most the sum of the probabilities that each occurs. The probability that the height h of S is larger than i is equal to the probability that level i has at least one position, that is, it is no more than Pi. This means that h is larger than, say, 3logn with probability at most P3logn ≤ n 23logn = n n3 = 1 n2 . For example, if n = 1000, this probability is a one-in-a-million long shot. More generally, given a constant c > 1, h is larger than clogn with probability at most 1/nc−1. That is, the probability that h is smaller than clogn is at least 1−1/nc−1. Thus, with high probability, the height h of S is O(logn). Analyzing Search Time in a Skip List Next, consider the running time of a search in skip list S, and recall that such a search involves two nested while loops. The inner loop performs a scan forward on a level of S as long as the next key is no greater than the search key k, and the outer loop drops down to the next level and repeats the scan forward iteration. Since the height h of S is O(logn) with high probability, the number of drop-down steps is O(logn) with high probability.

Chapter 10. Maps, Hash Tables, and Skip Lists So we have yet to bound the number of scan-forward steps we make. Let ni be the number of keys examined while scanning forward at level i. Observe that, after the key at the starting position, each additional key examined in a scan-forward at level i cannot also belong to level i + 1. If any of these keys were on the previous level, we would have encountered them in the previous scan-forward step. Thus, the probability that any key is counted in ni is 1/2. Therefore, the expected value of ni is exactly equal to the expected number of times we must ﬂip a fair coin before it comes up heads. This expected value is 2. Hence, the expected amount of time spent scanning forward at any level i is O(1). Since S has O(logn) levels with high probability, a search in S takes expected time O(logn). By a similar analysis, we can show that the expected running time of an insertion or a removal is O(logn). Space Usage in a Skip List Finally, let us turn to the space requirement of a skip list S with n entries. As we observed above, the expected number of positions at level i is n/2i, which means that the expected total number of positions in S is h ∑ i=0 n 2i = n h ∑ i=0 2i . Using Proposition 4.5 on geometric summations, we have h ∑ i=0 2i =   1 h+1 −1 2 −1 = 2·  1− 2h+1  < 2 for all h ≥0. Hence, the expected space requirement of S is O(n). Table 10.4 summarizes the performance of a sorted map realized by a skip list. Method Running Time size, isEmpty O(1) get O(logn) expected put O(logn) expected remove O(logn) expected ﬁrstEntry, lastEntry O(1) ceilingEntry, ﬂoorEntry O(logn) expected lowerEntry, higherEntry subMap O(s+logn) expected, with s entries reported entrySet, keySet, values O(n) Table 10.4: Performance of a sorted map implemented with a skip list. We use n to denote the number of entries in the dictionary at the time the operation is performed. The expected space requirement is O(n).

10.5. Sets, Multisets, and Multimaps 10.5 Sets, Multisets, and Multimaps We conclude this chapter by examining several additional abstractions that are closely related to the map ADT, and that can be implemented using data structures similar to those for a map. • A set is an unordered collection of elements, without duplicates, that typically supports efﬁcient membership tests. In essence, elements of a set are like keys of a map, but without any auxiliary values. • A multiset (also known as a bag) is a set-like container that allows duplicates. • A multimap is similar to a traditional map, in that it associates values with keys; however, in a multimap the same key can be mapped to multiple values. For example, the index of this book (page 714) maps a given term to one or more locations at which the term occurs elsewhere in the book. 10.5.1 The Set ADT The Java Collections Framework deﬁnes the java.util.Set interface, which includes the following fundamental methods: add(e): Adds the element e to S (if not already present). remove(e): Removes the element e from S (if it is present). contains(e): Returns whether e is an element of S. iterator(): Returns an iterator of the elements of S. There is also support for the traditional mathematical set operations of union, intersection, and subtraction of two sets S and T: S∪T = {e: e is in S or e is in T}, S∩T = {e: e is in S and e is in T}, S−T = {e: e is in S and e is not in T}. In the java.util.Set interface, these operations are provided through the following methods, if executed on a set S: addAll(T): Updates S to also include all elements of set T, effectively replacing S by S∪T. retainAll(T): Updates S so that it only keeps those elements that are also elements of set T, effectively replacing S by S∩T. removeAll(T): Updates S by removing any of its elements that also occur in set T, effectively replacing S by S−T.

Chapter 10. Maps, Hash Tables, and Skip Lists The template method pattern can be applied to implement each of the methods addAll, retainAll, and removeAll using only calls to the more fundamental methods add, remove, contains, and iterator. In fact, the java.util.AbstractSet class provides such implementations. To demonstrate the technique, we could implement the addAll method in the context of a set class as follows: public void addAll(Set<E> other) { for (E element : other) // rely on iterator( ) method of other add(element); // duplicates will be ignored by add } The removeAll and retailAll methods can be implemented with similar techniques, although a bit more care is needed for retainAll, to avoid removing elements while iterating over the same set (see Exercise C-10.59). The efﬁciency of these methods for a concrete set implementation will depend on the underlying efﬁciency of the fundamental methods upon which they rely. Sorted Sets For the standard set abstraction, there is no explicit notion of keys being ordered; all that is assumed is that the equals method can detect equivalent elements. If, however, elements come from a Comparable class (or a suitable Comparator object is provided), we can extend the notion of a set to deﬁne the sorted set ADT, including the following additional methods: ﬁrst(): Returns the smallest element in S. last(): Returns the largest element in S. ceiling(e): Returns the smallest element greater than or equal to e. ﬂoor(e): Returns the largest element less than or equal to e. lower(e): Returns the largest element strictly less than e. higher(e): Returns the smallest element strictly greater than e. subSet(e1, e2): Returns an iteration of all elements greater than or equal to e1, but strictly less than e2. pollFirst(): Returns and removes the smallest element in S. pollLast(): Returns and removes the largest element in S. In the Java Collection Framework, the above methods are included in a combination of the java.util.SortedSet and java.util.NavigableSet interfaces.

10.5. Sets, Multisets, and Multimaps Implementing Sets Although a set is a completely different abstraction than a map, the techniques used to implement the two can be quite similar. In effect, a set is simply a map in which (unique) keys do not have associated values. Therefore, any data structure used to implement a map can be modiﬁed to implement the set ADT with similar performance guarantees. As a trivial adaption of a map, each set element can be stored as a key, and the null reference can be stored as an (irrelevant) value. Of course, such an implementation is unnecessarily wasteful; a more efﬁcient set implementation should abandon the Entry composite and store set elements directly in a data structure. The Java Collections Framework includes the following set implementations, mirroring similar data structures used for maps: • java.util.HashSet provides an implementation of the (unordered) set ADT with a hash table. • java.util.concurrent.ConcurrentSkipListSet provides an implementation of the sorted set ADT using a skip list. • java.util.TreeSet provides an implementation of the sorted set ADT using a balanced search tree. (Search trees are the focus of Chapter 11.) 10.5.2 The Multiset ADT Before discussing models for a multiset abstraction, we must carefully consider the notion of “duplicate” elements. Throughout the Java Collections Framework, objects are considered equivalent to each other based on the standard equals method (see Section 3.5). For example, keys of a map must be unique, but the notion of uniqueness allows distinct yet equivalent objects to be matched. This is important for many typical uses of maps. For example, when strings are used as keys, the instance of the string "October" that is used when inserting an entry may not be the same instance of "October" that is used when later retrieving the associated value. The call birthstones.get("October") will succeed in such a scenario because strings are considered equal to each other. In the context of multisets, if we represent a collection that appears through the notion of equivalence as {a,a,a,a,b,c,c}, we must decide if we want a data structure to explicitly maintain each instance of a (because each might be distinct though equivalent), or just that there exist four occurrences. In either case, a multiset can be implemented by directly adapting a map. We can use one element from a group of equivalent occurrences as the key in a map, with the associated value either a secondary container containing all of the equivalent instances, or a count of the number of occurrences. Note that our word-frequency application in Section 10.1.2 uses just such a map, associating strings with counts.

Chapter 10. Maps, Hash Tables, and Skip Lists The Java Collections Framework does not include any form of a multiset. However, implementations exist in several widely used, open source Java collections libraries. The Apache Commons deﬁnes Bag and SortedBag interfaces that correspond respectively to unsorted and sorted multisets. The Google Core Libraries for Java (named Guava) includes Multiset and SortedMultiset interfaces for these abstractions. Both of those libraries take the approach of modeling a multiset as a collection of elements having multiplicities, and both offer several concrete implementations using standard data structures. In formalizing the abstract data type, the Multiset interface of the Guava library includes the following behaviors (and more): add(e): Adds a single occurrences of e to the multiset. contains(e): Returns true if the multiset contains an element equal to e. count(e): Returns the number of occurrences of e in the multiset. remove(e): Removes a single occurrence of e from the multiset. remove(e, n): Removes n occurrences of e from the multiset. size(): Returns the number of elements of the multiset (including duplicates). iterator(): Returns an iteration of all elements of the multiset (repeating those with multiplicity greater than one). The multiset ADT also includes the notion of an immutable Entry that represents an element and its count, and the SortedMultiset interface includes additional methods such as ﬁrstEntry and lastEntry. 10.5.3 The Multimap ADT Like a map, a multimap stores entries that are key-value pairs (k,v), where k is the key and v is the value. Whereas a map insists that entries have unique keys, a multimap allows multiple entries to have the same key, much like an English dictionary, which allows multiple deﬁnitions for the same word. That is, we will allow a multimap to contain entries (k,v) and (k,v′) having the same key. There are two standard approaches for representing a multimap as a variation of a traditional map. One is to redesign the underlying data structure to allow separate entries to be stored for pairs such as (k,v) and (k,v′). The other is to map key k to a secondary container of all values associated with that key (e.g., {v,v′}). Much as it is missing a formal abstraction for a multiset, the Java Collections Framework does not include any multiset interface nor implementations. However, as we will soon demonstrate, it is easy to represent a multiset by adapting other collection classes that are included in the java.util package.

10.5. Sets, Multisets, and Multimaps To formalize the multimap abstract data type, we consider a simpliﬁed version of the Multimap interface included in Google’s Guava library. Among its methods are the following: get(k): Returns a collection of all values associated with key k in the multimap. put(k, v): Adds a new entry to the multimap associating key k with value v, without overwriting any existing mappings for key k. remove(k, v): Removes an entry mapping key k to value v from the multimap (if one exists). removeAll(k): Removes all entries having key equal to k from the multimap. size(): Returns the number of entries of the multiset (including multiple associations). entries(): Returns a collection of all entries in the multimap. keys(): Returns a collection of keys for all entries in the multimap (including duplicates for keys with multiple bindings). keySet(): Returns a nonduplicative collection of keys in the multimap. values(): Returns a collection of values for all entries in the multimap. In Code Fragments 10.16 and 10.17, we provide an implementation of a class, HashMultimap, that uses a java.util.HashMap to map each key to a secondary ArrayList of all values that are associated with the key. For brevity, we omit the formality of deﬁning a Multimap interface, and we provide the entries() method as the only form of iteration. public class HashMultimap<K,V> { Map<K,List<V>> map = new HashMap<>(); // the primary map int total = 0; // total number of entries in the multimap /∗∗Constructs an empty multimap. ∗/ public HashMultimap() { } /∗∗Returns the total number of entries in the multimap. ∗/ public int size() { return total; } /∗∗Returns whether the multimap is empty. ∗/ public boolean isEmpty() { return (total == 0); } /∗∗Returns a (possibly empty) iteration of all values associated with the key. ∗/ Iterable<V> get(K key) { List<V> secondary = map.get(key); if (secondary != null) return secondary; return new ArrayList<>(); // return an empty list of values } Code Fragment 10.16: An implementation of a multimap as an adaptation of classes from the java.util package. (Continues in Code Fragment 10.17.)

Chapter 10. Maps, Hash Tables, and Skip Lists /∗∗Adds a new entry associating key with value. ∗/ void put(K key, V value) { List<V> secondary = map.get(key); if (secondary == null) { secondary = new ArrayList<>(); map.put(key, secondary); // begin using new list as secondary structure } secondary.add(value); total++; } /∗∗Removes the (key,value) entry, if it exists. ∗/ boolean remove(K key, V value) { boolean wasRemoved = false; List<V> secondary = map.get(key); if (secondary != null) { wasRemoved = secondary.remove(value); if (wasRemoved) { total−−; if (secondary.isEmpty()) map.remove(key); // remove secondary structure from primary map } } return wasRemoved; } /∗∗Removes all entries with the given key. ∗/ Iterable<V> removeAll(K key) { List<V> secondary = map.get(key); if (secondary != null) { total −= secondary.size(); map.remove(key); } else secondary = new ArrayList<>(); // return empty list of removed values return secondary; } /∗∗Returns an iteration of all entries in the multimap. ∗/ Iterable<Map.Entry<K,V>> entries() { List<Map.Entry<K,V>> result = new ArrayList<>(); for (Map.Entry<K,List<V>> secondary : map.entrySet()) { K key = secondary.getKey(); for (V value : secondary.getValue()) result.add(new AbstractMap.SimpleEntry<K,V>(key,value)); } return result; } } Code Fragment 10.17: An implementation of a multimap as an adaptation of classes from the java.util package. (Continued from Code Fragment 10.16.)

Chapter 11. Search Trees 11.1 Binary Search Trees In Chapter 8 we introduced the tree data structure and demonstrated a variety of applications. One important use is as a search tree (as described on page 338). In this chapter, we use a search-tree structure to efﬁciently implement a sorted map. The three most fundamental methods of of a map (see Section 10.1.1) are: get(k): Returns the value v associated with key k, if such an entry exists; otherwise returns null. put(k, v): Associates value v with key k, replacing and returning any existing value if the map already contains an entry with key equal to k. remove(k): Removes the entry with key equal to k, if one exists, and returns its value; otherwise returns null. The sorted map ADT includes additional functionality (see Section 10.3), guaranteeing that an iteration reports keys in sorted order, and supporting additional searches such as higherEntry(k) and subMap(k1, k2). Binary trees are an excellent data structure for storing entries of a map, assuming we have an order relation deﬁned on the keys. In this chapter, we deﬁne a binary search tree as a proper binary tree (see Section 8.2) such that each internal position p stores a key-value pair (k,v) such that: • Keys stored in the left subtree of p are less than k. • Keys stored in the right subtree of p are greater than k. An example of such a binary search tree is given in Figure 11.1. Notice that the leaves of the tree serve only as “placeholders.” Their use as sentinels simpliﬁes the presentation of several of our search and update algorithms. With care, they can be represented as null references in practice, thereby reducing the number of nodes in half (since there are more leaves than internal nodes in a proper binary tree). Figure 11.1: A binary search tree with integer keys. We omit the display of associated values in this chapter, since they are not relevant to the order of entries within a search tree.

11.1. Binary Search Trees 11.1.1 Searching Within a Binary Search Tree The most important consequence of the structural property of a binary search tree is its namesake search algorithm. We can attempt to locate a particular key in a binary search tree by viewing it as a decision tree (recall Figure 8.5). In this case, the question asked at each internal position p is whether the desired key k is less than, equal to, or greater than the key stored at position p, which we denote as key(p). If the answer is “less than,” then the search continues in the left subtree. If the answer is “equal,” then the search terminates successfully. If the answer is “greater than,” then the search continues in the right subtree. Finally, if we reach a leaf, then the search terminates unsuccessfully. (See Figure 11.2.) (a) (b) Figure 11.2: (a) A successful search for key 65 in a binary search tree; (b) an unsuccessful search for key 68 that terminates at the leaf to the left of the key 76. We describe this approach in Code Fragment 11.1. If key k occurs in a subtree rooted at p, a call to TreeSearch(p, k) results in the position at which the key is found. For an unsuccessful search, the TreeSearch algorithm returns the ﬁnal leaf explored on the search path (which we will later make use of when determining where to insert a new entry in a search tree). Algorithm TreeSearch(p, k): if p is external then return p {unsuccessful search} else if k == key(p) then return p {successful search} else if k < key(p) then return TreeSearch(left(p), k) {recur on left subtree} else {we know that k > key(p)} return TreeSearch(right(p), k) {recur on right subtree} Code Fragment 11.1: Recursive search in a binary search tree.

Chapter 11. Search Trees Analysis of Binary Tree Searching The analysis of the worst-case running time of searching in a binary search tree T is simple. Algorithm TreeSearch is recursive and executes a constant number of primitive operations for each recursive call. Each recursive call of TreeSearch is made on a child of the previous position. That is, TreeSearch is called on the positions of a path of T that starts at the root and goes down one level at a time. Thus, the number of such positions is bounded by h+1, where h is the height of T. In other words, since we spend O(1) time per position encountered in the search, the overall search runs in O(h) time, where h is the height of the binary search tree T. (See Figure 11.3.) Tree T: Time per level Total time: Height h O(h) O(1) O(1) O(1) Figure 11.3: Illustrating the running time of searching in a binary search tree. The ﬁgure uses a standard visualization shortcut of a binary search tree as a big triangle and a path from the root as a zig-zag line. In the context of the sorted map ADT, the search will be used as a subroutine for implementing the get method, as well as for the put and remove methods, since each of these begins by trying to locate an existing entry with the given key. We will later demonstrate how to implement sorted map operations, such as lowerEntry and higherEntry, by navigating within the tree after performing a standard search. All of these operations will run in worst-case O(h) time for a tree with height h. Admittedly, the height h of T can be as large as the number of entries, n, but we expect that it is usually much smaller. Later in this chapter we will show various strategies to maintain an upper bound of O(logn) on the height of a search tree T.

11.1. Binary Search Trees 11.1.2 Insertions and Deletions Binary search trees allow implementations of the put and remove operations using algorithms that are fairly straightforward, although not trivial. Insertion The map operation put(k, v) begins with a search for an entry with key k. If found, that entry’s existing value is reassigned. Otherwise, the new entry can be inserted into the underlying tree by expanding the leaf that was reached at the end of the failed search into an internal node. The binary search-tree property is sustained by that placement (note that it is placed exactly where a search would expect it). Let us assume a proper binary tree supports the following update operation: expandExternal(p, e): Stores entry e at the external position p, and expands p to be internal, having two new leaves as children. We can then describe the TreeInsert algorithm with the pseudocode given in in Code Fragment 11.2. An example of insertion into a binary search tree is shown in Figure 11.4. Algorithm TreeInsert(k, v): Input: A search key k to be associated with value v p = TreeSearch(root(), k) if k == key(p) then Change p’s value to (v) else expandExternal(p, (k,v)) Code Fragment 11.2: Algorithm for inserting a key-value pair into a map that is represented as a binary search tree. (a) (b) Figure 11.4: Insertion of an entry with key 68 into the search tree of Figure 11.2. Finding the position to insert is shown in (a), and the resulting tree is shown in (b).

Chapter 11. Search Trees Deletion Deleting an entry from a binary search tree is a bit more complex than inserting a new entry because the position of an entry to be deleted might be anywhere in the tree (as opposed to insertions, which always occur at a leaf). To delete an entry with key k, we begin by calling TreeSearch(root(), k) to ﬁnd the position p storing an entry with key equal to k (if any). If the search returns an external node, then there is no entry to remove. Otherwise, we distinguish between two cases (of increasing difﬁculty): • If at most one of the children of position p is internal, the deletion of the entry at position p is easily implemented (see Figure 11.5). Let position r be a child of p that is internal (or an arbitrary child, if both are leaves). We will remove p and the leaf that is r’s sibling, while promoting r upward to take the place of p. We note that all remaining ancestor-descendant relationships that remain in the tree after the operation existed before the operation; therefore, the binary search-tree property is maintained. • If position p has two children, we cannot simply remove the node from the tree since this would create a “hole” and two orphaned children. Instead, we proceed as follows (see Figure 11.6): ◦We locate position r containing the entry having the greatest key that is strictly less than that of position p (its so-called predecessor in the ordering of keys). That predecessor will always be located in the rightmost internal position of the left subtree of position p. ◦We use r’s entry as a replacement for the one being deleted at position p. Because r has the immediately preceding key in the map, any entries in p’s right subtree will have keys greater than r and any other entries in p’s left subtree will have keys less than r. Therefore, the binary search-tree property is satisﬁed after the replacement. ◦Having used r’s entry as a replacement for p, we instead delete the node at position r from the tree. Fortunately, since r was located as the rightmost internal position in a subtree, r does not have an internal right child. Therefore, its deletion can be performed using the ﬁrst (and simpler) approach. As with searching and insertion, this algorithm for a deletion involves the traversal of a single path downward from the root, possibly moving an entry between two positions of this path, and removing a node from that path and promoting its child. Therefore, it executes in time O(h) where h is the height of the tree.

11.1. Binary Search Trees p r r (a) (b) Figure 11.5: Deletion from the binary search tree of Figure 11.4b, where the entry to delete (with key 32) is stored at a position p with one child r: (a) before the deletion; (b) after the deletion. p r r p (a) (b) Figure 11.6: Deletion from the binary search tree of Figure 11.5b, where the entry to delete (with key 88) is stored at a position p with two children, and replaced by its predecessor r: (a) before the deletion; (b) after the deletion.

Chapter 11. Search Trees 11.1.3 Java Implementation In Code Fragments 11.3 through 11.6 we deﬁne a TreeMap class that implements the sorted map ADT while using a binary search tree for storage. The TreeMap class is declared as a child of the AbstractSortedMap base class, thereby inheriting support for performing comparisons based upon a given (or default) Comparator, a nested MapEntry class for storing key-value pairs, and concrete implementations of methods keySet and values based upon the entrySet method, which we will provide. (See Figure 10.2 on page 406 for an overview of our entire map hierarchy.) For representing the tree structure, our TreeMap class maintains an instance of a subclass of the LinkedBinaryTree class from Section 8.3.1. In this implementation, we choose to represent the search tree as a proper binary tree, with explicit leaf nodes in the binary tree as sentinels, and map entries stored only at internal nodes. (We leave the task of a more space-efﬁcient implementation to Exercise P-11.55.) The TreeSearch algorithm of Code Fragment 11.1 is implemented as a private recursive method, treeSearch(p, k). That method either returns a position with an entry equal to key k, or else the last position that is visited on the search path. The method is not only used for all of the primary map operations, get(k), put(k, v), and remove(k), but for most of the sorted map methods, as the ﬁnal internal position visited during an unsuccessful search has either the greatest key less than k or the least key greater than k. Finally, we note that our TreeMap class is designed so that it can be subclassed to implement various forms of balanced search trees. We discuss the balancing framework more thoroughly in Section 11.2, but there are two aspects of the design that impact the code presented in this section. First, our tree member is technically declared as an instance of a BalanceableBinaryTree class, which is a specialization of the LinkedBinaryTree class; however, we rely only on the inherited behaviors in this section. Second, our code is peppered with calls to presumed methods named rebalanceAccess, rebalanceInsert, and rebalanceDelete; these methods do not do anything in this class, but they serve as hooks that can later be customized. We conclude with a brief guide to the organization of our code. Code Fragment 11.3: Beginning of TreeMap class, including constructors, size method, and expandExternal and treeSearch utilities. Code Fragment 11.4: Map operations get(k), put(k, v), and remove(k). Code Fragment 11.5: Sorted map ADT methods lastEntry(), ﬂoorEntry(k), and lowerEntry(k), and protected utility treeMax. Symmetric methods ﬁrstEntry(), ceilingEntry(k), higherEntry(k), and treeMin are provided online. Code Fragment 11.6: Support for producing an iteration of all entries (method entrySet of the map ADT), or of a selected range of entries (method subMap(k1, k2) of the sorted map ADT).

11.1. Binary Search Trees /∗∗An implementation of a sorted map using a binary search tree. ∗/ public class TreeMap<K,V> extends AbstractSortedMap<K,V> { // To represent the underlying tree structure, we use a specialized subclass of the // LinkedBinaryTree class that we name BalanceableBinaryTree (see Section 11.2). protected BalanceableBinaryTree<K,V> tree = new BalanceableBinaryTree<>(); /∗∗Constructs an empty map using the natural ordering of keys. ∗/ public TreeMap() { super(); // the AbstractSortedMap constructor tree.addRoot(null); // create a sentinel leaf as root } /∗∗Constructs an empty map using the given comparator to order keys. ∗/ public TreeMap(Comparator<K> comp) { super(comp); // the AbstractSortedMap constructor tree.addRoot(null); // create a sentinel leaf as root } /∗∗Returns the number of entries in the map. ∗/ public int size() { return (tree.size() −1) / 2; // only internal nodes have entries } /∗∗Utility used when inserting a new entry at a leaf of the tree ∗/ private void expandExternal(Position<Entry<K,V>> p, Entry<K,V> entry) { tree.set(p, entry); // store new entry at p tree.addLeft(p, null); // add new sentinel leaves as children tree.addRight(p, null); } // Omitted from this code fragment, but included in the online version of the code, // are a series of protected methods that provide notational shorthands to wrap // operations on the underlying linked binary tree. For example, we support the // protected syntax root() as shorthand for tree.root() with the following utility: protected Position<Entry<K,V>> root() { return tree.root(); } /∗∗Returns the position in p's subtree having given key (or else the terminal leaf).∗/ private Position<Entry<K,V>> treeSearch(Position<Entry<K,V>> p, K key) { if (isExternal(p)) return p; // key not found; return the ﬁnal leaf int comp = compare(key, p.getElement()); if (comp == 0) return p; // key found; return its position else if (comp < 0) return treeSearch(left(p), key); // search left subtree else return treeSearch(right(p), key); // search right subtree } Code Fragment 11.3: Beginning of a TreeMap class based on a binary search tree.

Chapter 11. Search Trees /∗∗Returns the value associated with the speciﬁed key (or else null). ∗/ public V get(K key) throws IllegalArgumentException { checkKey(key); // may throw IllegalArgumentException Position<Entry<K,V>> p = treeSearch(root(), key); rebalanceAccess(p); // hook for balanced tree subclasses if (isExternal(p)) return null; // unsuccessful search return p.getElement().getValue(); // match found } /∗∗Associates the given value with the given key, returning any overridden value.∗/ public V put(K key, V value) throws IllegalArgumentException { checkKey(key); // may throw IllegalArgumentException Entry<K,V> newEntry = new MapEntry<>(key, value); Position<Entry<K,V>> p = treeSearch(root(), key); if (isExternal(p)) { // key is new expandExternal(p, newEntry); rebalanceInsert(p); // hook for balanced tree subclasses return null; } else { // replacing existing key V old = p.getElement().getValue(); set(p, newEntry); rebalanceAccess(p); // hook for balanced tree subclasses return old; } } /∗∗Removes the entry having key k (if any) and returns its associated value. ∗/ public V remove(K key) throws IllegalArgumentException { checkKey(key); // may throw IllegalArgumentException Position<Entry<K,V>> p = treeSearch(root(), key); if (isExternal(p)) { // key not found rebalanceAccess(p); // hook for balanced tree subclasses return null; } else { V old = p.getElement().getValue(); if (isInternal(left(p)) && isInternal(right(p))) { // both children are internal Position<Entry<K,V>> replacement = treeMax(left(p)); set(p, replacement.getElement()); p = replacement; } // now p has at most one child that is an internal node Position<Entry<K,V>> leaf = (isExternal(left(p)) ? left(p) : right(p)); Position<Entry<K,V>> sib = sibling(leaf); remove(leaf); remove(p); // sib is promoted in p’s place rebalanceDelete(sib); // hook for balanced tree subclasses return old; } } Code Fragment 11.4: Primary map operations for the TreeMap class.

11.1. Binary Search Trees /∗∗Returns the position with the maximum key in subtree rooted at Position p. ∗/ protected Position<Entry<K,V>> treeMax(Position<Entry<K,V>> p) { Position<Entry<K,V>> walk = p; while (isInternal(walk)) walk = right(walk); return parent(walk); // we want the parent of the leaf } /∗∗Returns the entry having the greatest key (or null if map is empty). ∗/ public Entry<K,V> lastEntry() { if (isEmpty()) return null; return treeMax(root()).getElement(); } /∗∗Returns the entry with greatest key less than or equal to given key (if any). ∗/ public Entry<K,V> ﬂoorEntry(K key) throws IllegalArgumentException { checkKey(key); // may throw IllegalArgumentException Position<Entry<K,V>> p = treeSearch(root(), key); if (isInternal(p)) return p.getElement(); // exact match while (!isRoot(p)) { if (p == right(parent(p))) return parent(p).getElement(); // parent has next lesser key else p = parent(p); } return null; // no such ﬂoor exists } /∗∗Returns the entry with greatest key strictly less than given key (if any). ∗/ public Entry<K,V> lowerEntry(K key) throws IllegalArgumentException { checkKey(key); // may throw IllegalArgumentException Position<Entry<K,V>> p = treeSearch(root(), key); if (isInternal(p) && isInternal(left(p))) return treeMax(left(p)).getElement(); // this is the predecessor to p // otherwise, we had failed search, or match with no left child while (!isRoot(p)) { if (p == right(parent(p))) return parent(p).getElement(); // parent has next lesser key else p = parent(p); } return null; // no such lesser key exists } Code Fragment 11.5: A sample of the sorted map operations for the TreeMap class. The symmetrical utility, treeMin, and public methods ﬁrstEntry, ceilingEntry, and higherEntry are available online.

Chapter 11. Search Trees /∗∗Returns an iterable collection of all key-value entries of the map. ∗/ public Iterable<Entry<K,V>> entrySet() { ArrayList<Entry<K,V>> buﬀer = new ArrayList<>(size()); for (Position<Entry<K,V>> p : tree.inorder()) if (isInternal(p)) buﬀer.add(p.getElement()); return buﬀer; } /∗∗Returns an iterable of entries with keys in range [fromKey, toKey). ∗/ public Iterable<Entry<K,V>> subMap(K fromKey, K toKey) { ArrayList<Entry<K,V>> buﬀer = new ArrayList<>(size()); if (compare(fromKey, toKey) < 0) // ensure that fromKey < toKey subMapRecurse(fromKey, toKey, root(), buﬀer); return buﬀer; } private void subMapRecurse(K fromKey, K toKey, Position<Entry<K,V>> p, ArrayList<Entry<K,V>> buﬀer) { if (isInternal(p)) if (compare(p.getElement(), fromKey) < 0) // p's key is less than fromKey, so any relevant entries are to the right subMapRecurse(fromKey, toKey, right(p), buﬀer); else { subMapRecurse(fromKey, toKey, left(p), buﬀer); // ﬁrst consider left subtree if (compare(p.getElement(), toKey) < 0) { // p is within range buﬀer.add(p.getElement()); // so add it to buﬀer, and consider subMapRecurse(fromKey, toKey, right(p), buﬀer); // right subtree as well } } } Code Fragment 11.6: TreeMap operations supporting iteration of the entire map, or a portion of the map with a given key range. 11.1.4 Performance of a Binary Search Tree An analysis of the operations of our TreeMap class is given in Table 11.1. Almost all operations have a worst-case running time that depends on h, where h is the height of the current tree. This is because most operations rely on traversing a path from the root of the tree, and the maximum path length within a tree is proportional to the height of the tree. Most notably, our implementations of map operations get, put, and remove, and most of the sorted map operations, each begins with a call to the treeSearch utility. Similar paths are traced when searching for the minimum or maximum entry in a subtree, a task used when ﬁnding a replacement during a deletion or in ﬁnding the overall ﬁrst or last entry in the map. An iteration of the entire map is accomplished in O(n) time using an inorder traversal of the underlying tree, and the recursive subMap implementation can be shown to run in O(s+h) worst-case bound for a call that reports s results (see Exercise C-11.34).

11.1. Binary Search Trees Method Running Time size, isEmpty O(1) get, put, remove O(h) ﬁrstEntry, lastEntry O(h) ceilingEntry, ﬂoorEntry, lowerEntry, higherEntry O(h) subMap O(s+h) entrySet, keySet, values O(n) Table 11.1: Worst-case running times of the operations for a TreeMap. We denote the current height of the tree with h, and the number of entries reported by subMap as s. The space usage is O(n), where n is the number of entries stored in the map. A binary search tree T is therefore an efﬁcient implementation of a map with n entries only if its height is small. In the best case, T has height h = ⌈log(n+1)⌉−1, which yields logarithmic-time performance for most of the map operations. In the worst case, however, T has height n, in which case it would look and feel like an ordered list implementation of a map. Such a worst-case conﬁguration arises, for example, if we insert entries with keys in increasing or decreasing order. (See Figure 11.7.) Figure 11.7: Example of a binary search tree with linear height, obtained by inserting entries in increasing order of their keys. We can nevertheless take comfort that, on average, a binary search tree with n keys generated from a random series of insertions and removals of keys has expected height O(logn); the justiﬁcation of this statement is beyond the scope of the book, requiring careful mathematical language to precisely deﬁne what we mean by a random series of insertions and removals, and sophisticated probability theory. In applications where one cannot guarantee the random nature of updates, it is better to rely on variations of search trees, presented in the remainder of this chapter, that guarantee a worst-case height of O(logn), and thus O(logn) worstcase time for searches, insertions, and deletions.

Chapter 11. Search Trees 11.2 Balanced Search Trees In the closing of the previous section, we noted that if we could assume a random series of insertions and removals, the standard binary search tree supports O(logn) expected running times for the basic map operations. However, we may only claim O(n) worst-case time, because some sequences of operations may lead to an unbalanced tree with height proportional to n. In the remainder of this chapter, we will explore four search-tree algorithms that provide stronger performance guarantees. Three of the four data structures (AVL trees, splay trees, and red-black trees) are based on augmenting a standard binary search tree with occasional operations to reshape the tree and reduce its height. The primary operation to rebalance a binary search tree is known as a rotation. During a rotation, we “rotate” a child to be above its parent, as diagrammed in Figure 11.8. y x y T1 T2 T3 T1 T2 T3 x Figure 11.8: A rotation operation in a binary search tree. A rotation can be performed to transform the left formation into the right, or the right formation into the left. Note that all keys in subtree T1 have keys less than that of position x, all keys in subtree T2 have keys that are between those of positions x and y, and all keys in subtree T3 have keys that are greater than that of position y. To maintain the binary search-tree property through a rotation, we note that if position x was a left child of position y prior to a rotation (and therefore the key of x is less than the key of y), then y becomes the right child of x after the rotation, and vice versa. Furthermore, we must relink the subtree of entries with keys that lie between the keys of the two positions that are being rotated. For example, in Figure 11.8 the subtree labeled T2 represents entries with keys that are known to be greater than that of position x and less than that of position y. In the ﬁrst conﬁguration of that ﬁgure, T2 is the right subtree of position x; in the second conﬁguration, it is the left subtree of position y. Because a single rotation modiﬁes a constant number of parent-child relationships, it can be implemented in O(1) time with a linked binary tree representation.

11.2. Balanced Search Trees In the context of a tree-balancing algorithm, a rotation allows the shape of a tree to be modiﬁed while maintaining the search-tree property. If used wisely, this operation can be performed to avoid highly unbalanced tree conﬁgurations. For example, a rightward rotation from the ﬁrst formation of Figure 11.8 to the second reduces the depth of each node in subtree T1 by one, while increasing the depth of each node in subtree T3 by one. (Note that the depth of nodes in subtree T2 are unaffected by the rotation.) One or more rotations can be combined to provide broader rebalancing within a tree. One such compound operation we consider is a trinode restructuring. For this manipulation, we consider a position x, its parent y, and its grandparent z. The goal is to restructure the subtree rooted at z in order to reduce the overall path length to x and its subtrees. Pseudocode for a restructure(x) method is given in Code Fragment 11.7 and illustrated in Figure 11.9. In describing a trinode restructuring, we temporarily rename the positions x, y, and z as a, b, and c, so that a precedes b and b precedes c in an inorder traversal of T. There are four possible orientations mapping x, y, and z to a, b, and c, as shown in Figure 11.9, which are uniﬁed into one case by our relabeling. The trinode restructuring replaces z with the node identiﬁed as b, makes the children of this node be a and c, and makes the children of a and c be the four previous children of x, y, and z (other than x and y), while maintaining the inorder relationships of all the nodes in T. Algorithm restructure(x): Input: A position x of a binary search tree T that has both a parent y and a grandparent z Output: Tree T after a trinode restructuring (which corresponds to a single or double rotation) involving positions x, y, and z 1: Let (a, b, c) be a left-to-right (inorder) listing of the positions x, y, and z, and let (T1, T2, T3, T4) be a left-to-right (inorder) listing of the four subtrees of x, y, and z not rooted at x, y, or z. 2: Replace the subtree rooted at z with a new subtree rooted at b. 3: Let a be the left child of b and let T1 and T2 be the left and right subtrees of a, respectively. 4: Let c be the right child of b and let T3 and T4 be the left and right subtrees of c, respectively. Code Fragment 11.7: The trinode restructuring operation in a binary search tree. In practice, the modiﬁcation of a tree T caused by a trinode restructuring operation can be implemented through case analysis either as a single rotation (as in Figure 11.9a and b) or as a double rotation (as in Figure 11.9c and d). The double rotation arises when position x has the middle of the three relevant keys and is ﬁrst rotated above its parent, and then above what was originally its grandparent. In any of the cases, the trinode restructuring is completed with O(1) running time.

Chapter 11. Search Trees single rotation T1 a = z b = y T2 c = x T3 T4 a = z T1 T2 b = y c = x T3 T4 (a) T1 a = x T1 T2 b = y c = z T3 T4 single rotation T4 c = z b = y T3 a = x T2 (b) T3 T1 a = z T1 T2 b = x c = y T3 T4 double rotation a = z T4 c = y b = x T2 (c) T2 T1 T2 b = x c = z T3 T4 double rotation a = y T4 c = z T1 a = y b = x T3 (d) Figure 11.9: Schematic illustration of a trinode restructuring operation: (a and b) require a single rotation; (c and d) require a double rotation.

11.2. Balanced Search Trees 11.2.1 Java Framework for Balancing Search Trees Our TreeMap class (introduced in Section 11.1.3) is a fully functional map implementation. However, the running time for its operations depend on the height of the tree, and in the worst-case, that height may be O(n) for a map with n entries. Therefore, we have intentionally designed the TreeMap class in a way that allows it to be easily extended to provide more advanced tree-balancing strategies. In later sections of this chapter, we will implement subclasses AVLTreeMap, SplayTreeMap, and RBTreeMap. In this section, we describe three important forms of support that the TreeMap class offers these subclasses. Hooks for Rebalancing Operations Our implementation of the basic map operations in Section 11.1.3 includes strategic calls to three nonpublic methods that serve as hooks for rebalancing algorithms: • A call to rebalanceInsert(p) is made from within the put method, after a new node is added to the tree at position p (line 61 of Code Fragment 11.4). • A call to rebalanceDelete(p) is made from within the remove method, after a node is deleted from the tree (line 88 of Code Fragment 11.4); position p identiﬁes the child of the removed node that was promoted in its place. • A call to rebalanceAccess(p) is made by any call to get, put, or remove that does not result in a structural change. Position p, which could be internal or external, represents the deepest node of the tree that was accessed during the operation. This hook is speciﬁcally used by the splay tree structure (see Section 11.4) to restructure a tree so that more frequently accessed nodes are brought closer to the root. Within our TreeMap class, we provide the trivial declarations of these three methods, having bodies that do nothing, as shown in Code Fragment 11.8. A subclass of TreeMap may override any of these methods to implement a nontrivial action to rebalance a tree. This is another example of the template method design pattern, as originally discussed in Section 2.3.3. protected void rebalanceInsert(Position<Entry<K,V>> p) { } protected void rebalanceDelete(Position<Entry<K,V>> p) { } protected void rebalanceAccess(Position<Entry<K,V>> p) { } Code Fragment 11.8: Trivial deﬁnitions of TreeMap methods that serve as hooks for our rebalancing framework. These methods may be overridden by subclasses in order to perform appropriate rebalancing operations.

Chapter 11. Search Trees Protected Methods for Rotating and Restructuring To support common restructuring operations, our TreeMap class relies on storing the tree as an instance of a new nested class, BalanceableBinaryTree (shown in Code Fragments 11.9 and 11.10). That class is a specialization of the original LinkedBinaryTree class from Section 8.3.1. This new class provides protected utility methods rotate and restructure that, respectively, implement a single rotation and a trinode restructuring (described at the beginning of Section 11.2). Although these methods are not invoked by the standard TreeMap operations, their inclusion supports greater code reuse, as they are available to all balanced-tree subclasses. These methods are implemented in Code Fragment 11.10. To simplify the code, we deﬁne an additional relink utility that properly links parent and child nodes to each other. The focus of the rotate method then becomes redeﬁning the relationship between the parent and child, relinking a rotated node directly to its original grandparent, and shifting the “middle” subtree (that labeled as T2 in Figure 11.8) between the rotated nodes. For the trinode restructuring, we determine whether to perform a single or double rotation, as originally described in Figure 11.9. The four cases in that ﬁgure demonstrate a downward path z to y to x that are respectively right-right, left-left, right-left, and left-right. The ﬁrst two patterns, with matching orientation, warrant a single rotation moving y upward, while the last two patterns, with opposite orientations, warrant a double rotation moving x upward. Specialized Nodes with an Auxiliary Data Member Many tree-balancing strategies require that some form of auxiliary “balancing” information be stored at nodes of a tree. To ease the burden on the balanced-tree subclasses, we choose to add an auxiliary integer value to every node within the BalanceableSearchTree class. This is accomplished by deﬁning a new BSTNode class, which itself inherits from the nested LinkedBinaryTree.Node class. The new class declares the auxiliary variable, and provides methods for getting and setting its value. We draw attention to an important subtlety in our design, including that of the original LinkedBinaryTree subclass. Whenever a low-level operation on an underlying linked tree requires a new node, we must ensure that the correct type of node is created. That is, for our balanceable tree, we need each node to be a BTNode, which includes the auxiliary ﬁeld. However, the creation of nodes occurs within low-level operations, such as addLeft and addRight, that reside in the original LinkedBinaryTree class.

11.2. Balanced Search Trees We rely on a technique known as the factory method design pattern. The LinkedBinaryTree class includes a protected method, createNode (originally given at lines 30–33 of Code Fragment 8.8), that is responsible for instantiating a new node of the appropriate type. The rest of the code in that class makes sure to always use the createNode method when a new node is needed. In the LinkedBinaryTree class, the createNode method returns a simple Node instance. In our new BalanceableBinaryTree class, we override the createNode method (see lines 22–27 in Code Fragment 11.9), so that a new instance of the BSTNode class is returned. In this way, we effectively change the behavior of the low-level operations in the LinkedBinaryTree class so that it uses instances of our specialized node class, and therefore, that every node in our balanced trees includes support for the new auxiliary ﬁeld. /∗∗A specialized version of LinkedBinaryTree with support for balancing. ∗/ protected static class BalanceableBinaryTree<K,V> extends LinkedBinaryTree<Entry<K,V>> { //-------------- nested BSTNode class -------------- // this extends the inherited LinkedBinaryTree.Node class protected static class BSTNode<E> extends Node<E> { int aux=0; BSTNode(E e, Node<E> parent, Node<E> leftChild, Node<E> rightChild) { super(e, parent, leftChild, rightChild); } public int getAux() { return aux; } public void setAux(int value) { aux = value; } } //--------- end of nested BSTNode class --------- // positional-based methods related to aux ﬁeld public int getAux(Position<Entry<K,V>> p) { return ((BSTNode<Entry<K,V>>) p).getAux(); } public void setAux(Position<Entry<K,V>> p, int value) { ((BSTNode<Entry<K,V>>) p).setAux(value); } // Override node factory function to produce a BSTNode (rather than a Node) protected Node<Entry<K,V>> createNode(Entry<K,V> e, Node<Entry<K,V>> parent, Node<Entry<K,V>> left, Node<Entry<K,V>> right) { return new BSTNode<>(e, parent, left, right); } Code Fragment 11.9: The BalanceableBinaryTree class, which is nested within the TreeMap class deﬁnition. (Continues in Code Fragment 11.10.)

Chapter 11. Search Trees /∗∗Relinks a parent node with its oriented child node. ∗/ private void relink(Node<Entry<K,V>> parent, Node<Entry<K,V>> child, boolean makeLeftChild) { child.setParent(parent); if (makeLeftChild) parent.setLeft(child); else parent.setRight(child); } /∗∗Rotates Position p above its parent. ∗/ public void rotate(Position<Entry<K,V>> p) { Node<Entry<K,V>> x = validate(p); Node<Entry<K,V>> y = x.getParent(); // we assume this exists Node<Entry<K,V>> z = y.getParent(); // grandparent (possibly null) if (z == null) { root = x; // x becomes root of the tree x.setParent(null); } else relink(z, x, y == z.getLeft()); // x becomes direct child of z // now rotate x and y, including transfer of middle subtree if (x == y.getLeft()) { relink(y, x.getRight(), true); // x’s right child becomes y’s left relink(x, y, false); // y becomes x’s right child } else { relink(y, x.getLeft(), false); // x’s left child becomes y’s right relink(x, y, true); // y becomes left child of x } } /∗∗Performs a trinode restructuring of Position x with its parent/grandparent. ∗/ public Position<Entry<K,V>> restructure(Position<Entry<K,V>> x) { Position<Entry<K,V>> y = parent(x); Position<Entry<K,V>> z = parent(y); if ((x == right(y)) == (y == right(z))) { // matching alignments rotate(y); // single rotation (of y) return y; // y is new subtree root } else { // opposite alignments rotate(x); // double rotation (of x) rotate(x); return x; // x is new subtree root } } } Code Fragment 11.10: The BalanceableBinaryTree class, which is nested within the TreeMap class deﬁnition (continued from Code Fragment 11.9).

11.3. AVL Trees 11.3 AVL Trees The TreeMap class, which uses a standard binary search tree as its data structure, should be an efﬁcient map data structure, but its worst-case performance for the various operations is linear time, because it is possible that a series of operations results in a tree with linear height. In this section, we describe a simple balancing strategy that guarantees worst-case logarithmic running time for all the fundamental map operations. Deﬁnition of an AVL Tree The simple correction is to add a rule to the binary search-tree deﬁnition that will maintain a logarithmic height for the tree. Recall that we deﬁned the height of a subtree rooted at position p of a tree to be the number of edges on the longest path from p to a leaf (see Section 8.1.3). By this deﬁnition, a leaf position has height 0. In this section, we consider the following height-balance property, which characterizes the structure of a binary search tree T in terms of the heights of its nodes. Height-Balance Property: For every internal position p of T, the heights of the children of p differ by at most 1. Any binary search tree T that satisﬁes the height-balance property is said to be an AVL tree, named after the initials of its inventors: Adel’son-Vel’skii and Landis. An example of an AVL tree is shown in Figure 11.10. Figure 11.10: An example of an AVL tree. The keys of the entries are shown inside the nodes, and the heights of the nodes are shown above the nodes (all leaves have height 0).

Chapter 11. Search Trees An immediate consequence of the height-balance property is that a subtree of an AVL tree is itself an AVL tree. The height-balance property also has the important consequence of keeping the height small, as shown in the following proposition. Proposition 11.1: The height of an AVL tree storing n entries is O(logn). Justiﬁcation: Instead of trying to ﬁnd an upper bound on the height of an AVL tree directly, it turns out to be easier to work on the “inverse problem” of ﬁnding a lower bound on the minimum number of internal nodes, denoted as n(h), of an AVL tree with height h. We will show that n(h) grows at least exponentially. From this, it will be an easy step to derive that the height of an AVL tree storing n entries is O(logn). We begin by noting that n(1) = 1 and n(2) = 2, because an AVL tree of height 1 must have exactly one internal node and an AVL tree of height 2 must have at least two internal nodes. Now, an AVL tree with the minimum number of nodes having height h for h ≥3, is such that both its subtrees are AVL trees with the minimum number of nodes: one with height h−1 and the other with height h−2. Taking the root into account, we obtain the following formula that relates n(h) to n(h−1) and n(h−2), for h ≥3: n(h) = 1+n(h−1)+n(h−2). (11.1) At this point, the reader familiar with the properties of Fibonacci progressions (Sections 2.2.3 and 5.5) will already see that n(h) is a function exponential in h. To formalize that observation, we proceed as follows. Formula 11.1 implies that n(h) is a strictly increasing function of h. Thus, we know that n(h−1) > n(h−2). Replacing n(h−1) with n(h−2) in Formula 11.1 and dropping the 1, we get, for h ≥3, n(h) > 2·n(h−2). (11.2) Formula 11.2 indicates that n(h) at least doubles each time h increases by 2, which intuitively means that n(h) grows exponentially. To show this fact in a formal way, we apply Formula 11.2 repeatedly, yielding the following series of inequalities: n(h) > 2·n(h−2) > 4·n(h−4) > 8·n(h−6) ... > 2i ·n(h−2i). (11.3) That is, n(h) > 2i·n(h−2i), for any integer i, such that h−2i ≥1. Since we already know the values of n(1) and n(2), we pick i so that h−2i is equal to either 1 or 2.

11.3. AVL Trees That is, we pick i = h  −1. By substituting the above value of i in Formula 11.3, we obtain, for h ≥3, n(h) > 2⌈h 2⌉−1 ·n  h−2 h  +2  ≥ 2⌈h 2⌉−1n(1) ≥ h 2 −1. (11.4) By taking logarithms of both sides of Formula 11.4, we obtain log(n(h)) > h 2 −1, from which we get h < 2log(n(h))+2, (11.5) which implies that an AVL tree storing n entries has height at most 2logn+2. By Proposition 11.1 and the analysis of binary search trees given in Section 11.1, the operation get, in a map implemented with an AVL tree, runs in time O(logn), where n is the number of entries in the map. Of course, we still have to show how to maintain the height-balance property after an insertion or deletion. 11.3.1 Update Operations Given a binary search tree T, we say that a position is balanced if the absolute value of the difference between the heights of its children is at most 1, and we say that it is unbalanced otherwise. Thus, the height-balance property characterizing AVL trees is equivalent to saying that every position is balanced. The insertion and deletion operations for AVL trees begin similarly to the corresponding operations for (standard) binary search trees, but with post-processing for each operation to restore the balance of any portions of the tree that are adversely affected by the change. Insertion Suppose that tree T satisﬁes the height-balance property, and hence is an AVL tree, prior to the insertion of a new entry. An insertion of a new entry in a binary search tree, as described in Section 11.1.2, results in a leaf position p being expanded to become internal, with two new external children. This action may violate the height-balance property (see, for example, Figure 11.11a), yet the only positions that may become unbalanced are ancestors of p, because those are the only positions whose subtrees have changed. Therefore, let us describe how to restructure T to ﬁx any unbalance that may have occurred.

Chapter 11. Search Trees T4 T2 T1 z y x T3 T1 T2 T4 y x z T3 (a) (b) Figure 11.11: An example insertion of an entry with key 54 in the AVL tree of Figure 11.10: (a) after adding a new node for key 54, the nodes storing keys 78 and 44 become unbalanced; (b) a trinode restructuring restores the height-balance property. We show the heights of nodes above them, and we identify the nodes x, y, and z and subtrees T1, T2, T3, and T4 participating in the trinode restructuring. We restore the balance of the nodes in the binary search tree T by a simple “search-and-repair” strategy. In particular, let z be the ﬁrst position we encounter in going up from p toward the root of T such that z is unbalanced (see Figure 11.11a.) Also, let y denote the child of z with greater height (and note that y must be an ancestor of p). Finally, let x be the child of y with greater height (there cannot be a tie and position x must also be an ancestor of p, possibly p itself). We rebalance the subtree rooted at z by calling the trinode restructuring method, restructure(x), originally described in Section 11.2. An example of such a restructuring in the context of an AVL insertion is portrayed in Figure 11.11. To formally argue the correctness of this process in reestablishing the AVL height-balance property, we consider the implication of z being the nearest ancestor of p that became unbalanced after the insertion of p. It must be that the height of y increased by one due to the insertion and that it is now 2 greater than its sibling. Since y remains balanced, it must be that it formerly had subtrees with equal heights, and that the subtree containing x has increased its height by one. That subtree increased either because x = p, and thus its height changed from 0 to 1, or because x previously had equal-height subtrees and the height of the one containing p has increased by 1. Letting h ≥0 denote the height of the tallest child of x, this scenario might be portrayed as in Figure 11.12. After the trinode restructuring, each of x, y, and z is balanced. Furthermore, the root of the subtree after the restructuring has height h + 2, which is precisely the height that z had before the insertion of the new entry. Therefore, any ancestor of z that became temporarily unbalanced becomes balanced again, and this one restructuring restores the height-balance property globally.

11.3. AVL Trees T3 T2 x h−1 h−1 T4 y T1 z h h+2 h h h+1 (a) h+2 T3 T2 x h−1 h T4 y T1 z h h+3 h h+1 (b) h h+2 T2 T1 h z h−1 h+1 T4 h T3 y h+1 x (c) Figure 11.12: Rebalancing of a subtree during a typical insertion into an AVL tree: (a) before the insertion; (b) after an insertion in subtree T3 causes imbalance at z; (c) after restoring balance with trinode restructuring. Notice that the overall height of the subtree after the insertion is the same as before the insertion.

Chapter 11. Search Trees Deletion Recall that a deletion from a regular binary search tree results in the structural removal of a node having either zero or one internal children. Such a change may violate the height-balance property in an AVL tree. In particular, if position p represents a (possibly external) child of the removed node in tree T, there may be an unbalanced node on the path from p to the root of T. (See Figure 11.13a.) In fact, there can be at most one such unbalanced node. (The justiﬁcation of this fact is left as Exercise C-11.41.) T1 T2 T4 x z T3 y T1 T4 T2 x y T3 z (a) (b) Figure 11.13: Deletion of the entry with key 32 from the AVL tree of Figure 11.11b: (a) after removing the node storing key 32, the root becomes unbalanced; (b) a trinode restructuring of x, y, and z restores the height-balance property. As with insertion, we use trinode restructuring to restore balance in the tree T. In particular, let z be the ﬁrst unbalanced position encountered going up from p toward the root of T, and let y be that child of z with greater height (y will not be an ancestor of p). Furthermore, let x be the child of y deﬁned as follows: if one of the children of y is taller than the other, let x be the taller child of y; else (both children of y have the same height), let x be the child of y on the same side as y (that is, if y is the left child of z, let x be the left child of y, else let x be the right child of y). We then perform a restructure(x) operation. (See Figure 11.13b.) The restructured subtree is rooted at the middle position denoted as b in the description of the trinode restructuring operation. The height-balance property is guaranteed to be locally restored within the subtree of b. (See Exercises R-11.11 and R-11.12.) Unfortunately, this trinode restructuring may reduce the height of the subtree rooted at b by 1, which may cause an ancestor of b to become unbalanced. So, after rebalancing z, we continue walking up T looking for unbalanced positions. If we ﬁnd another, we perform a restructure operation to restore its balance, and continue marching up T looking for more, all the way to the root. Since the height of T is O(logn), where n is the number of entries, by Proposition 11.1, O(logn) trinode restructurings are sufﬁcient to restore the height-balance property.

11.3. AVL Trees Performance of AVL Trees By Proposition 11.1, the height of an AVL tree with n entries is guaranteed to be O(logn). Because the standard binary search-tree operation had running times bounded by the height (see Table 11.1), and because the additional work in maintaining balance factors and restructuring an AVL tree can be bounded by the length of a path in the tree, the traditional map operations run in worst-case logarithmic time with an AVL tree. We summarize these results in Table 11.2, and illustrate this performance in Figure 11.14. Method Running Time size, isEmpty O(1) get, put, remove O(logn) ﬁrstEntry, lastEntry O(logn) ceilingEntry, ﬂoorEntry, lowerEntry, higherEntry O(logn) subMap O(s+logn) entrySet, keySet, values O(n) Table 11.2: Worst-case running times of operations for an n-entry sorted map realized as an AVL tree T, with s denoting the number of entries reported by subMap. Worst-case time: O(logn) Height Time per level AVL Tree T: down phase up phase O(logn) O(1) O(1) O(1) Figure 11.14: Illustrating the running time of searches and updates in an AVL tree. The time performance is O(1) per level, broken into a down phase, which typically involves searching, and an up phase, which typically involves updating height values and performing local trinode restructurings (rotations).

Chapter 11. Search Trees 11.3.2 Java Implementation A complete implementation of an AVLTreeMap class is provided in Code Fragments 11.11 and 11.12. It inherits from the standard TreeMap class and relies on the balancing framework described in Section 11.2.1. We highlight two important aspects of our implementation. First, the AVLTreeMap uses the node’s auxiliary balancing variable to store the height of the subtree rooted at that node, with leaves having a balance factor of 0 by default. We also provide several utilities involving heights of nodes (see Code Fragment 11.11). To implement the core logic of the AVL balancing strategy, we deﬁne a utility, named rebalance, that sufﬁces to restore the height-balance property after an insertion or a deletion (see Code Fragment 11.11). Although the inherited behaviors for insertion and deletion are quite different, the necessary post-processing for an AVL tree can be uniﬁed. In both cases, we trace an upward path from the position p at which the change took place, recalculating the height of each position based on the (updated) heights of its children. We perform a trinode restructuring operation if an imbalanced position is reached. The upward march from p continues until we reach an ancestor with height that was unchanged by the map operation, or with height that was restored to its previous value by a trinode restructuring operation, or until reaching the root of the tree (in which case the overall height of the tree has increased by one). To easily detect the stopping condition, we record the “old” height of a position, as it existed before the insertion or deletion operation begin, and compare that to the newly calculated height after a possible restructuring. /∗∗An implementation of a sorted map using an AVL tree. ∗/ public class AVLTreeMap<K,V> extends TreeMap<K,V> { /∗∗Constructs an empty map using the natural ordering of keys. ∗/ public AVLTreeMap() { super(); } /∗∗Constructs an empty map using the given comparator to order keys. ∗/ public AVLTreeMap(Comparator<K> comp) { super(comp); } /∗∗Returns the height of the given tree position. ∗/ protected int height(Position<Entry<K,V>> p) { return tree.getAux(p); } /∗∗Recomputes the height of the given position based on its children's heights. ∗/ protected void recomputeHeight(Position<Entry<K,V>> p) { tree.setAux(p, 1 + Math.max(height(left(p)), height(right(p)))); } /∗∗Returns whether a position has balance factor between −1 and 1 inclusive. ∗/ protected boolean isBalanced(Position<Entry<K,V>> p) { return Math.abs(height(left(p)) −height(right(p))) <= 1; } Code Fragment 11.11: AVLTreeMap class. (Continues in Code Fragment 11.12.)

11.3. AVL Trees /∗∗Returns a child of p with height no smaller than that of the other child. ∗/ protected Position<Entry<K,V>> tallerChild(Position<Entry<K,V>> p) { if (height(left(p)) > height(right(p))) return left(p); // clear winner if (height(left(p)) < height(right(p))) return right(p); // clear winner // equal height children; break tie while matching parent's orientation if (isRoot(p)) return left(p); // choice is irrelevant if (p == left(parent(p))) return left(p); // return aligned child else return right(p); } /∗∗ ∗Utility used to rebalance after an insert or removal operation. This traverses the ∗path upward from p, performing a trinode restructuring when imbalance is found, ∗continuing until balance is restored. ∗/ protected void rebalance(Position<Entry<K,V>> p) { int oldHeight, newHeight; do { oldHeight = height(p); // not yet recalculated if internal if (!isBalanced(p)) { // imbalance detected // perform trinode restructuring, setting p to resulting root, // and recompute new local heights after the restructuring p = restructure(tallerChild(tallerChild(p))); recomputeHeight(left(p)); recomputeHeight(right(p)); } recomputeHeight(p); newHeight = height(p); p = parent(p); } while (oldHeight != newHeight && p != null); } /∗∗Overrides the TreeMap rebalancing hook that is called after an insertion. ∗/ protected void rebalanceInsert(Position<Entry<K,V>> p) { rebalance(p); } /∗∗Overrides the TreeMap rebalancing hook that is called after a deletion. ∗/ protected void rebalanceDelete(Position<Entry<K,V>> p) { if (!isRoot(p)) rebalance(parent(p)); } } Code Fragment 11.12: AVLTreeMap class (continued from Code Fragment 11.11).

Chapter 11. Search Trees 11.4 Splay Trees The next search-tree structure we study is known as a a splay tree. This structure is conceptually quite different from the other balanced search trees we will discuss in this chapter, for a splay tree does not strictly enforce a logarithmic upper bound on the height of the tree. In fact, no additional height, balance, or other auxiliary data need be stored with the nodes of this tree. The efﬁciency of splay trees is due to a certain move-to-root operation, called splaying, that is performed at the bottommost position p reached during every insertion, deletion, or even a search. (In essence, this is a variant of the move-to-front heuristic that we explored for lists in Section 7.7.2.) Intuitively, a splay operation causes more frequently accessed elements to remain nearer to the root, thereby reducing the typical search times. The surprising thing about splaying is that it allows us to guarantee a logarithmic amortized running time, for insertions, deletions, and searches. 11.4.1 Splaying Given a node x of a binary search tree T, we splay x by moving x to the root of T through a sequence of restructurings. The particular restructurings we perform are important, for it is not sufﬁcient to move x to the root of T by just any sequence of restructurings. The speciﬁc operation we perform to move x up depends upon the relative positions of x, its parent y, and x’s grandparent z (if it exists). There are three cases that we will consider. zig-zig: The node x and its parent y are both left children or both right children. (See Figure 11.15.) We promote x, making y a child of x and z a child of y, while maintaining the inorder relationships of the nodes in T. T1 y T2 T3 T4 z x T4 T3 T2 T1 y z x (a) (b) Figure 11.15: Zig-zig: (a) before; (b) after. There is another symmetric conﬁguration where x and y are left children.

11.4. Splay Trees zig-zag: One of x and y is a left child and the other is a right child. (See Figure 11.16.) In this case, we promote x by making x have y and z as its children, while maintaining the inorder relationships of the nodes in T. x z T4 y T2 T3 T1 T2 y T3 T4 z x T1 (a) (b) Figure 11.16: Zig-zag: (a) before; (b) after. There is another symmetric conﬁguration where x is a right child and y is a left child. zig: x does not have a grandparent. (See Figure 11.17.) In this case, we perform a single rotation to promote x over y, making y a child of x, while maintaining the relative inorder relationships of the nodes in T. T1 T2 T3 y x T1 T2 T3 y x (a) (b) Figure 11.17: Zig: (a) before; (b) after. There is another symmetric conﬁguration where x is originally a left child of y. We perform a zig-zig or a zig-zag when x has a grandparent, and we perform a zig when x has a parent but not a grandparent. A splaying step consists of repeating these restructurings at x until x becomes the root of T. An example of the splaying of a node is shown in Figures 11.18 and 11.19.

Chapter 11. Search Trees (a) (b) (c) Figure 11.18: Example of splaying a node: (a) splaying the node storing 14 starts with a zig-zag; (b) after the zig-zag; (c) the next step will be a zig-zig. (Continues in Figure 11.19.)

11.4. Splay Trees (d) (e) (f) Figure 11.19: Example of splaying a node:(d) after the zig-zig; (e) the next step is again a zig-zig; (f) after the zig-zig. (Continued from Figure 11.18.)

Chapter 11. Search Trees 11.4.2 When to Splay The rules that dictate when splaying is performed are as follows: • When searching for key k, if k is found at position p, we splay p, else we splay the parent of the leaf position at which the search terminates unsuccessfully. For example, the splaying in Figures 11.18 and 11.19 would be performed after searching successfully for key 14 or unsuccessfully for key 15. • When inserting key k, we splay the newly created internal node where k gets inserted. For example, the splaying in Figures 11.18 and 11.19 would be performed if 14 were the newly inserted key. We show a sequence of insertions in a splay tree in Figure 11.20. (a) (b) (c) (d) (e) (f) (g) Figure 11.20: A sequence of insertions in a splay tree: (a) initial tree; (b) after inserting 3, but before a zig step; (c) after splaying; (d) after inserting 2, but before a zig-zag step; (e) after splaying; (f) after inserting 4, but before a zig-zig step; (g) after splaying.

11.4. Splay Trees • When deleting a key k, we splay the position p that is the parent of the removed node; recall that by the removal algorithm for binary search trees, the removed node may be that originally containing k, or a descendant node with a replacement key. An example of splaying following a deletion is shown in Figure 11.21. w p (a) (b) (c) (d) (e) Figure 11.21: Deletion from a splay tree: (a) the deletion of 8 from the root node is performed by moving to the root the key of its inorder predecessor w, deleting w, and splaying the parent p of w; (b) splaying p starts with a zig-zig; (c) after the zig-zig; (d) the next step is a zig; (e) after the zig.

Chapter 11. Search Trees 11.4.3 Java Implementation Although the mathematical analysis of a splay tree’s performance is complex (see Section 11.4.4), the implementation of splay trees is a rather simple adaptation to a standard binary search tree. Code Fragment 11.13 provides a complete implementation of a SplayTreeMap class, based upon the underlying TreeMap class and use of the balancing framework described in Section 11.2.1. Note that the original TreeMap class makes calls to the rebalanceAccess method, not just from within the get method, but also within the put method when modifying the value associated with an existing key, and within a failed remove operation. /∗∗An implementation of a sorted map using a splay tree. ∗/ public class SplayTreeMap<K,V> extends TreeMap<K,V> { /∗∗Constructs an empty map using the natural ordering of keys. ∗/ public SplayTreeMap() { super(); } /∗∗Constructs an empty map using the given comparator to order keys. ∗/ public SplayTreeMap(Comparator<K> comp) { super(comp); } /∗∗Utility used to rebalance after a map operation. ∗/ private void splay(Position<Entry<K,V>> p) { while (!isRoot(p)) { Position<Entry<K,V>> parent = parent(p); Position<Entry<K,V>> grand = parent(parent); if (grand == null) // zig case rotate(p); else if ((parent == left(grand)) == (p == left(parent))) { // zig-zig case rotate(parent); // move PARENT upward rotate(p); // then move p upward } else { // zig-zag case rotate(p); // move p upward rotate(p); // move p upward again } } } // override the various TreeMap rebalancing hooks to perform the appropriate splay protected void rebalanceAccess(Position<Entry<K,V>> p) { if (isExternal(p)) p = parent(p); if (p != null) splay(p); } protected void rebalanceInsert(Position<Entry<K,V>> p) { splay(p); } protected void rebalanceDelete(Position<Entry<K,V>> p) { if (!isRoot(p)) splay(parent(p)); } } Code Fragment 11.13: A complete implementation of the SplayTreeMap class.

11.4. Splay Trees 11.4.4 Amortized Analysis of Splaying ⋆ After a zig-zig or zig-zag, the depth of position p decreases by two, and after a zig the depth of p decreases by one. Thus, if p has depth d, splaying p consists of a sequence of ⌊d/2⌋zig-zigs and/or zig-zags, plus one ﬁnal zig if d is odd. Since a single zig-zig, zig-zag, or zig affects a constant number of nodes, it can be done in O(1) time. Thus, splaying a position p in a binary search tree T takes time O(d), where d is the depth of p in T. In other words, the time for performing a splaying step for a position p is asymptotically the same as the time needed just to reach that position in a top-down search from the root of T. Worst-Case Time In the worst case, the overall running time of a search, insertion, or deletion in a splay tree of height h is O(h), since the position we splay might be the deepest position in the tree. Moreover, it is possible for h to be as large as n, as shown in Figure 11.20. Thus, from a worst-case point of view, a splay tree is not an attractive data structure. In spite of its poor worst-case performance, a splay tree performs well in an amortized sense. That is, in a sequence of intermixed searches, insertions, and deletions, each operation takes on average logarithmic time. We perform the amortized analysis of splay trees using the accounting method. Amortized Performance of Splay Trees For our analysis, we note that the time for performing a search, insertion, or deletion is proportional to the time for the associated splaying. So let us consider only splaying time. Let T be a splay tree with n keys, and let w be a node of T. We deﬁne the size n(w) of w as the number of nodes in the subtree rooted at w. Note that this deﬁnition implies that the size of an internal node is one more than the sum of the sizes of its children. We deﬁne the rank r(w) of a node w as the logarithm in base 2 of the size of w, that is, r(w) = log(n(w)). Clearly, the root of T has the maximum size, n, and the maximum rank, logn, while each leaf has size 1 and rank 0. We use cyber-dollars to pay for the work we perform in splaying a position p in T, and we assume that one cyber-dollar pays for a zig, while two cyber-dollars pay for a zig-zig or a zig-zag. Hence, the cost of splaying a position at depth d is d cyber-dollars. We keep a virtual account storing cyber-dollars at each position of T. Note that this account exists only for the purpose of our amortized analysis, and does not need to be included in a data structure implementing the splay tree T.

Chapter 11. Search Trees An Accounting Analysis of Splaying When we perform a splaying, we pay a certain number of cyber-dollars (the exact value of the payment will be determined at the end of our analysis). We distinguish three cases: • If the payment is equal to the splaying work, then we use it all to pay for the splaying. • If the payment is greater than the splaying work, we deposit the excess in the accounts of several nodes. • If the payment is less than the splaying work, we make withdrawals from the accounts of several nodes to cover the deﬁciency. We show below that a payment of O(logn) cyber-dollars per operation is sufﬁcient to keep the system working, that is, to ensure that each node keeps a nonnegative account balance. An Accounting Invariant for Splaying We use a scheme in which transfers are made between the accounts of the nodes to ensure that there will always be enough cyber-dollars to withdraw for paying for splaying work when needed. In order to use the accounting method to perform our analysis of splaying, we maintain the following invariant: Before and after a splaying, each node w of T has r(w) cyber-dollars in its account. Note that the invariant is “ﬁnancially sound,” since it does not require us to make a preliminary deposit to endow a tree with zero keys. Let r(T) be the sum of the ranks of all the nodes of T. To preserve the invariant after a splaying, we must make a payment equal to the splaying work plus the total change in r(T). We refer to a single zig, zig-zig, or zig-zag operation in a splaying as a splaying substep. Also, we denote the rank of a node w of T before and after a splaying substep with r(w) and r′(w), respectively. The following proposition gives an upper bound on the change of r(T) caused by a single splaying substep. We will repeatedly use this lemma in our analysis of a full splaying of a node to the root.

11.4. Splay Trees Proposition 11.2: Let δ be the variation of r(T) caused by a single splaying substep (a zig, zig-zig, or zig-zag) for a node x in T. We have the following: • δ ≤3(r′(x)−r(x))−2 if the substep is a zig-zig or zig-zag. • δ ≤3(r′(x)−r(x)) if the substep is a zig. Justiﬁcation: We use the fact that, if a > 0, b > 0, and c > a+b, loga+logb < 2logc−2. (11.6) Let us consider the change in r(T) caused by each type of splaying substep. zig-zig: (Recall Figure 11.15.) Since the size of each node is one more than the size of its two children, note that only the ranks of x, y, and z change in a zig-zig operation, where y is the parent of x and z is the parent of y. Also, r′(x) = r(z), r′(y) ≤r′(x), and r(x) ≤r(y). Thus, δ = r′(x)+r′(y)+r′(z)−r(x)−r(y)−r(z) = r′(y)+r′(z)−r(x)−r(y) ≤ r′(x)+r′(z)−2r(x). (11.7) Note that n(x) + n′(z) < n′(x). Thus, r(x) + r′(z) < 2r′(x) −2, as per Formula 11.6; that is, r′(z) < 2r′(x)−r(x)−2. This inequality and Formula 11.7 imply δ ≤ r′(x)+(2r′(x)−r(x)−2)−2r(x) ≤ 3(r′(x)−r(x))−2. zig-zag: (Recall Figure 11.16.) Again, by the deﬁnition of size and rank, only the ranks of x, y, and z change, where y denotes the parent of x and z denotes the parent of y. Also, r(x) < r(y) < r(z) = r′(x). Thus, δ = r′(x)+r′(y)+r′(z)−r(x)−r(y)−r(z) = r′(y)+r′(z)−r(x)−r(y) ≤ r′(y)+r′(z)−2r(x). (11.8) Note that n′(y)+n′(z) < n′(x); hence, r′(y)+r′(z) < 2r′(x)−2, as per Formula 11.6. Thus, δ ≤ 2r′(x)−2−2r(x) = 2(r′(x)−r(x))−2 ≤3(r′(x)−r(x))−2. zig: (Recall Figure 11.17.) In this case, only the ranks of x and y change, where y denotes the parent of x. Also, r′(y) ≤r(y) and r′(x) ≥r(x). Thus, δ = r′(y)+r′(x)−r(y)−r(x) ≤ r′(x)−r(x) ≤ 3(r′(x)−r(x)).

Chapter 11. Search Trees Proposition 11.3: Let T be a splay tree with root t, and let ∆be the total variation of r(T) caused by splaying a node x at depth d. We have ∆≤3(r(t)−r(x))−d +2. Justiﬁcation: Splaying node x consists of c = ⌈d/2⌉splaying substeps, each of which is a zig-zig or a zig-zag, except possibly the last one, which is a zig if d is odd. Let r0(x) = r(x) be the initial rank of x, and for i = 1,...,c, let ri(x) be the rank of x after the ith substep and δi be the variation of r(T) caused by the ith substep. By Proposition 11.2, the total variation ∆of r(T) caused by splaying x is ∆ = c ∑ i=1 δi ≤ 2+ c ∑ i=1 3(ri(x)−ri−1(x))−2 = 3(rc(x)−r0(x))−2c+2 ≤ 3(r(t)−r(x))−d +2. By Proposition 11.3, if we make a payment of 3(r(t)−r(x))+2 cyber-dollars towards the splaying of node x, we have enough cyber-dollars to maintain the invariant, keeping r(w) cyber-dollars at each node w in T, and pay for the entire splaying work, which costs d cyber-dollars. Since the size of the root t is n, its rank r(t) = logn. Given that r(x) ≥0, the payment to be made for splaying is O(logn) cyber-dollars. To complete our analysis, we have to compute the cost for maintaining the invariant when a node is inserted or deleted. When inserting a new node w into a splay tree with n keys, the ranks of all the ancestors of w are increased. Namely, let w0,wi,...,wd be the ancestors of w, where w0 = w, wi is the parent of wi−1, and wd is the root. For i = 1,...,d, let n′(wi) and n(wi) be the size of wi before and after the insertion, respectively, and let r′(wi) and r(wi) be the rank of wi before and after the insertion. We have n′(wi) = n(wi)+1. Also, since n(wi) + 1 ≤n(wi+1), for i = 0,1,...,d −1, we have the following for each i in this range: r′(wi) = log(n′(wi)) = log(n(wi)+1) ≤log(n(wi+1)) = r(wi+1). Thus, the total variation of r(T) caused by the insertion is d ∑ i=1  r′(wi)−r(wi)  ≤ r′(wd)+ d−1 ∑ i=1 (r(wi+1)−r(wi)) = r′(wd)−r(w0) ≤ logn. Therefore, a payment of O(logn) cyber-dollars is sufﬁcient to maintain the invariant when a new node is inserted.

11.4. Splay Trees When deleting a node w from a splay tree with n keys, the ranks of all the ancestors of w are decreased. Thus, the total variation of r(T) caused by the deletion is negative, and we do not need to make any payment to maintain the invariant when a node is deleted. Therefore, we may summarize our amortized analysis in the following proposition (which is sometimes called the “balance proposition” for splay trees): Proposition 11.4: Consider a sequence of m operations on a splay tree, each one a search, insertion, or deletion, starting from a splay tree with zero keys. Also, let ni be the number of keys in the tree after operation i, and n be the total number of insertions. The total running time for performing the sequence of operations is O m+ m ∑ i=1 logni ! , which is O(mlogn). In other words, the amortized running time of performing a search, insertion, or deletion in a splay tree is O(logn), where n is the size of the splay tree at the time. Thus, a splay tree can achieve logarithmic-time amortized performance for implementing a sorted map ADT. This amortized performance matches the worstcase performance of AVL trees, (2,4) trees, and red-black trees, but it does so using a simple binary tree that does not need any extra balance information stored at each of its nodes. In addition, splay trees have a number of other interesting properties that are not shared by these other balanced search trees. We explore one such additional property in the following proposition (which is sometimes called the “Static Optimality” proposition for splay trees): Proposition 11.5: Consider a sequence of m operations on a splay tree, each one a search, insertion, or deletion, starting from a splay tree T with zero keys. Also, let f(i) denote the number of times the entry i is accessed in the splay tree, that is, its frequency, and let n denote the total number of entries. Assuming that each entry is accessed at least once, then the total running time for performing the sequence of operations is O m+ n ∑ i=1 f(i)log(m/f(i)) ! . We omit the proof of this proposition, but it is not as hard to justify as one might imagine. The remarkable thing is that this proposition states that the amortized running time of accessing an entry i is O(log(m/f(i))).

Chapter 11. Search Trees 11.5 (2,4) Trees In this section, we will consider a data structure known as a (2,4) tree. It is a particular example of a more general structure known as a multiway search tree, in which internal nodes may have more than two children. Other forms of multiway search trees will be discussed in Section 15.3. 11.5.1 Multiway Search Trees Recall that general trees are deﬁned so that internal nodes may have many children. In this section, we discuss how general trees can be used as multiway search trees. Map entries stored in a search tree are pairs of the form (k,v), where k is the key and v is the value associated with the key. Deﬁnition of a Multiway Search Tree Let w be a node of an ordered tree. We say that w is a d-node if w has d children. We deﬁne a multiway search tree to be an ordered tree T that has the following properties, which are illustrated in Figure 11.22a: • Each internal node of T has at least two children. That is, each internal node is a d-node such that d ≥2. • Each internal d-node w of T with children c1,...,cd stores an ordered set of d −1 key-value pairs (k1,v1),..., (kd−1,vd−1), where k1 ≤··· ≤kd−1. • Let us conventionally deﬁne k0 = −∞and kd = +∞. For each entry (k,v) stored at a node in the subtree of w rooted at ci, i = 1,...,d, we have that ki−1 ≤k ≤ki. That is, if we think of the set of keys stored at w as including the special ﬁctitious keys k0 = −∞and kd = +∞, then a key k stored in the subtree of T rooted at a child node ci must be “in between” two keys stored at w. This simple viewpoint gives rise to the rule that a d-node stores d −1 regular keys, and it also forms the basis of the algorithm for searching in a multiway search tree. By the above deﬁnition, the external nodes of a multiway search do not store any data and serve only as “placeholders.” As with our convention for binary search trees (Section 11.1), these can be replaced by null references in practice. A binary search tree can be viewed as a special case of a multiway search tree, where each internal node stores one entry and has two children. Whether internal nodes of a multiway tree have two children or many, however, there is an interesting relationship between the number of key-value pairs and the number of external nodes in a multiway search tree. Proposition 11.6: An n-entry multiway search tree has n+1 external nodes. We leave the justiﬁcation of this proposition as an exercise (C-11.49).

11.5. (2,4) Trees 11 13 6 8 23 24 3 4 5 10 (a) 6 8 5 10 11 13 23 24 3 4 (b) 23 24 3 4 6 8 11 13 5 10 (c) Figure 11.22: (a) A multiway search tree T; (b) search path in T for key 12 (unsuccessful search); (c) search path in T for key 24 (successful search).

Chapter 11. Search Trees Searching in a Multiway Tree Searching for an entry with key k in a multiway search tree T is simple. We perform such a search by tracing a path in T starting at the root. (See Figure 11.22b and c.) When we are at a d-node w during this search, we compare the key k with the keys k1,...,kd−1 stored at w. If k = ki for some i, the search is successfully completed. Otherwise, we continue the search in the child ci of w such that ki−1 < k < ki. (Recall that we conventionally deﬁne k0 = −∞and kd = +∞.) If we reach an external node, then we know that there is no entry with key k in T, and the search terminates unsuccessfully. Data Structures for Representing Multiway Search Trees In Section 8.3.3, we discuss a linked data structure for representing a general tree. This representation can also be used for a multiway search tree. When using a general tree to implement a multiway search tree, we must store at each node one or more key-value pairs associated with that node. That is, we need to store with w a reference to some collection that stores the entries for w. During a search for key k in a multiway search tree, the primary operation needed when navigating a node is ﬁnding the smallest key at that node that is greater than or equal to k. For this reason, it is natural to model the information at a node itself as a sorted map, allowing use of the ceilingEntry(k) method. We say such a map serves as a secondary data structure to support the primary data structure represented by the entire multiway search tree. This reasoning may at ﬁrst seem like a circular argument, since we need a representation of a (secondary) ordered map to represent a (primary) ordered map. We can avoid any circular dependence, however, by using the bootstrapping technique, where we use a simple solution to a problem to create a new, more advanced solution. In the context of a multiway search tree, a natural choice for the secondary structure at each node is the SortedTableMap of Section 10.3.1. Because we want to determine the associated value in case of a match for key k, and otherwise the corresponding child ci such that ki−1 < k < ki, we recommend having each key ki in the secondary structure map to the pair (vi,ci). With such a realization of a multiway search tree T, processing a d-node w while searching for an entry of T with key k can be performed using a binary search operation in O(logd) time. Let dmax denote the maximum number of children of any node of T, and let h denote the height of T. The search time in a multiway search tree is therefore O(hlogdmax). If dmax is a constant, the running time for performing a search is O(h). The primary efﬁciency goal for a multiway search tree is to keep the height as small as possible. We will next discuss a strategy that caps dmax at 4 while guaranteeing a height h that is logarithmic in n, the total number of entries stored in the map.

11.5. (2,4) Trees 11.5.2 (2,4)-Tree Operations One form of a multiway search tree that keeps the tree balanced while using small secondary data structures at each node is the (2,4) tree, also known as a 2-4 tree or 2-3-4 tree. This data structure achieves these goals by maintaining two simple properties (see Figure 11.23): Size Property: Every internal node has at most four children. Depth Property: All the external nodes have the same depth. 3 4 5 10 13 14 Figure 11.23: A (2,4) tree. Again, we assume that external nodes are empty and, for the sake of simplicity, we describe our search and update methods assuming that external nodes are real nodes, although this latter requirement is not strictly needed. Enforcing the size property for (2,4) trees keeps the nodes in the multiway search tree simple. It also gives rise to the alternative name “2-3-4 tree,” since it implies that each internal node in the tree has 2, 3, or 4 children. Another implication of this rule is that we can represent the secondary map stored at each internal node using an unordered list or an ordered array, and still achieve O(1)-time performance for all operations (since dmax = 4). The depth property, on the other hand, enforces an important bound on the height of a (2,4) tree. Proposition 11.7: The height of a (2,4) tree storing n entries is O(logn). Justiﬁcation: Let h be the height of a (2,4) tree T storing n entries. We justify the proposition by showing the claim 2 log(n+1) ≤h ≤log(n+1). (11.9) To justify this claim note ﬁrst that, by the size property, we can have at most 4 nodes at depth 1, at most 42 nodes at depth 2, and so on. Thus, the number of external nodes in T is at most 4h. Likewise, by the depth property and the deﬁnition

Chapter 11. Search Trees of a (2,4) tree, we must have at least 2 nodes at depth 1, at least 22 nodes at depth 2, and so on. Thus, the number of external nodes in T is at least 2h. In addition, by Proposition 11.6, the number of external nodes in T is n+1. Therefore, we obtain 2h ≤n+1 ≤4h. Taking the logarithm in base 2 of the terms for the above inequalities, we get that h ≤log(n+1) ≤2h, which justiﬁes our claim (Formula 11.9) when terms are rearranged. Proposition 11.7 states that the size and depth properties are sufﬁcient for keeping a multiway tree balanced. Moreover, this proposition implies that performing a search in a (2,4) tree takes O(logn) time and that the speciﬁc realization of the secondary structures at the nodes is not a crucial design choice, since the maximum number of children dmax is a constant. Maintaining the size and depth properties requires some effort after performing insertions and deletions in a (2,4) tree, however. We discuss these operations next. Insertion To insert a new entry (k,v), with key k, into a (2,4) tree T, we ﬁrst perform a search for k. Assuming that T has no entry with key k, this search terminates unsuccessfully at an external node z. Let w be the parent of z. We insert the new entry into node w and add a new child y (an external node) to w on the left of z. Our insertion method preserves the depth property, since we add a new external node at the same level as existing external nodes. Nevertheless, it may violate the size property. Indeed, if a node w was previously a 4-node, then it would become a 5-node after the insertion, which causes the tree T to no longer be a (2,4) tree. This type of violation of the size property is called an overﬂow at node w, and it must be resolved in order to restore the properties of a (2,4) tree. Let c1,...,c5 be the children of w, and let k1,...,k4 be the keys stored at w. To remedy the overﬂow at node w, we perform a split operation on w as follows (see Figure 11.24): • Replace w with two nodes w′ and w′′, where ◦w′ is a 3-node with children c1,c2,c3 storing keys k1 and k2. ◦w′′ is a 2-node with children c4,c5 storing key k4. • If w is the root of T, create a new root node u; else, let u be the parent of w. • Insert key k3 into u and make w′ and w′′ children of u, so that if w was child i of u, then w′ and w′′ become children i and i+1 of u, respectively. As a consequence of a split operation on node w, a new overﬂow may occur at the parent u of w. If such an overﬂow occurs, it triggers in turn a split at node u. (See Figure 11.25.) A split operation either eliminates the overﬂow or propagates it into the parent of the current node. We show a sequence of insertions in a (2,4) tree in Figure 11.26.

11.5. (2,4) Trees h1 h2 c3 c2 c1 c5 u w k1 k2 k3 k4 c4 k3 c3 c2 c1 c5 w k1 k2 k4 c4 u h1 h2 w′ c2 c1 c4 c5 k1 k2 k4 h1 k3 h2 u w′′ c3 (a) (b) (c) Figure 11.24: A node split: (a) overﬂow at a 5-node w; (b) the third key of w inserted into the parent u of w; (c) node w replaced with a 3-node w′ and a 2-node w′′. 6 7 8 3 4 15 17 6 7 8 3 4 (a) (b) 6 7 8 13 14 5 10 12 3 4 13 14 6 7 8 3 4 5 10 12 15 (c) (d) 13 14 6 7 8 3 4 5 10 6 7 8 3 4 5 10 13 14 (e) (f) Figure 11.25: An insertion in a (2,4) tree that causes a cascading split: (a) before the insertion; (b) insertion of 17, causing an overﬂow; (c) a split; (d) after the split a new overﬂow occurs; (e) another split, creating a new root node; (f) ﬁnal tree.

Chapter 11. Search Trees 6 12 (a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) Figure 11.26: A sequence of insertions into a (2,4) tree: (a) initial tree with one entry; (b) insertion of 6; (c) insertion of 12; (d) insertion of 15, which causes an overﬂow; (e) split, which causes the creation of a new root node; (f) after the split; (g) insertion of 3; (h) insertion of 5, which causes an overﬂow; (i) split; (j) after the split; (k) insertion of 10; (l) insertion of 8.

11.5. (2,4) Trees Analysis of Insertion in a (2,4) Tree Because dmax is at most 4, the original search for the placement of new key k uses O(1) time at each level, and thus O(logn) time overall, since the height of the tree is O(logn) by Proposition 11.7. The modiﬁcations to a single node to insert a new key and child can be implemented to run in O(1) time, as can a single split operation. The number of cascading split operations is bounded by the height of the tree, and so that phase of the insertion process also runs in O(logn) time. Therefore, the total time to perform an insertion in a (2,4) tree is O(logn). Deletion Let us now consider the removal of an entry with key k from a (2,4) tree T. We begin such an operation by performing a search in T for an entry with key k. Removing an entry from a (2,4) tree can always be reduced to the case where the entry to be removed is stored at a node w whose children are external nodes. Suppose, for instance, that the entry with key k that we wish to remove is stored in the ith entry (ki,vi) at a node z that has internal children. In this case, we swap the entry (ki,vi) with an appropriate entry that is stored at a node w with external children as follows (see Figure 11.27d): 1. We ﬁnd the rightmost internal node w in the subtree rooted at the ith child of z, noting that the children of node w are all external nodes. 2. We swap the entry (ki,vi) at z with the last entry of w. Once we ensure that the entry to remove is stored at a node w with only external children (because either it was already at w or we swapped it into w), we simply remove the entry from w and remove the external node that is the ith child of w. Removing an entry (and a child) from a node w as described above preserves the depth property, for we always remove an external child from a node w with only external children. However, in removing such an external node, we may violate the size property at w. Indeed, if w was previously a 2-node, then it becomes a 1-node with no entries after the removal (Figure 11.27a and d), which is not allowed in a (2,4) tree. This type of violation of the size property is called an underﬂow at node w. To remedy an underﬂow, we check whether an immediate sibling of w is a 3-node or a 4-node. If we ﬁnd such a sibling s, then we perform a transfer operation, in which we move a child of s to w, a key of s to the parent u of w and s, and a key of u to w. (See Figure 11.27b and c.) If w has only one sibling, or if both immediate siblings of w are 2-nodes, then we perform a fusion operation, in which we merge w with a sibling, creating a new node w′, and move a key from the parent u of w to w′. (See Figure 11.27e and f.)

Chapter 11. Search Trees 6 8 13 14 5 10 13 14 u w s (a) (b) w 13 14 s u 13 14 6 10 (c) (d) w 13 14 u 13 14 8 10 w′ u (e) (f) 8 10 8 10 (g) (h) Figure 11.27: A sequence of removals from a (2,4) tree: (a) removal of 4, causing an underﬂow; (b) a transfer operation; (c) after the transfer operation; (d) removal of 12, causing an underﬂow; (e) a fusion operation; (f) after the fusion operation; (g) removal of 13; (h) after removing 13.

11.5. (2,4) Trees A fusion operation at node w may cause a new underﬂow to occur at the parent u of w, which in turn triggers a transfer or fusion at u. (See Figure 11.28.) Hence, the number of fusion operations is bounded by the height of the tree, which is O(logn) by Proposition 11.7. If an underﬂow propagates all the way up to the root, then the root is simply deleted. (See Figure 11.28c and d.) 8 10 w 8 10 u (a) (b) 8 10 u w 6 11 (c) (d) Figure 11.28: A propagating sequence of fusions in a (2,4) tree: (a) removal of 14, which causes an underﬂow; (b) fusion, which causes another underﬂow; (c) second fusion operation, which causes the root to be removed; (d) ﬁnal tree. Performance of (2,4) Trees The asymptotic performance of a (2,4) tree is identical to that of an AVL tree (see Table 11.2) in terms of the sorted map ADT, with guaranteed logarithmic bounds for most operations. The time complexity analysis for a (2,4) tree having n keyvalue pairs is based on the following: • The height of a (2,4) tree storing n entries is O(logn), by Proposition 11.7. • A split, transfer, or fusion operation takes O(1) time. • A search, insertion, or removal of an entry visits O(logn) nodes. Thus, (2,4) trees provide for fast map search and update operations. (2,4) trees also have an interesting relationship to the data structure we discuss next.

Chapter 11. Search Trees 11.6 Red-Black Trees Although AVL trees and (2,4) trees have a number of nice properties, they also have some disadvantages. For instance, AVL trees may require many restructure operations (rotations) to be performed after a deletion, and (2,4) trees may require many split or fusing operations to be performed after an insertion or removal. The data structure we discuss in this section, the red-black tree, does not have these drawbacks; it uses O(1) structural changes after an update in order to stay balanced. Formally, a red-black tree is a binary search tree (see Section 11.1) with nodes colored red and black in a way that satisﬁes the following properties: Root Property: The root is black. External Property: Every external node is black. Red Property: The children of a red node are black. Depth Property: All external nodes have the same black depth, deﬁned as the number of proper ancestors that are black. An example of a red-black tree is shown in Figure 11.29. Figure 11.29: An example of a red-black tree, with “red” nodes drawn in white. The common black depth for this tree is 3. We can make the red-black tree deﬁnition more intuitive by noting an interesting correspondence between red-black trees and (2,4) trees. Namely, given a red-black tree, we can construct a corresponding (2,4) tree by merging every red node w into its parent, storing the entry from w at its parent, and with the children of w becoming ordered children of the parent. For example, the red-black tree in Figure 11.29 corresponds to the (2,4) tree from Figure 11.23, as illustrated in Figure 11.30. The depth property of the red-black tree corresponds to the depth property of the (2,4) tree since exactly one black node of the red-black tree contributes to each node of the corresponding (2,4) tree.

11.6. Red-Black Trees Figure 11.30: An illustration of the correspondance between the red-black tree of Figure 11.29 and the (2,4) tree of Figure 11.23, based on the highlighted grouping of red nodes with their black parents. Conversely, we can transform any (2,4) tree into a corresponding red-black tree by coloring each node w black and then performing the following transformations, as illustrated in Figure 11.31. • If w is a 2-node, then keep the (black) children of w as is. • If w is a 3-node, then create a new red node y, give w’s last two (black) children to y, and make the ﬁrst child of w and y be the two children of w. • If w is a 4-node, then create two new red nodes y and z, give w’s ﬁrst two (black) children to y, give w’s last two (black) children to z, and make y and z be the two children of w. Notice that a red node always has a black parent in this construction. ←→ (a) 13 14 ←→ or (b) ←→ (c) Figure 11.31: Correspondence between nodes of a (2,4) tree and a red-black tree: (a) 2-node; (b) 3-node; (c) 4-node.

Chapter 11. Search Trees Proposition 11.8: The height of a red-black tree storing n entries is O(logn). Justiﬁcation: Let T be a red-black tree storing n entries, and let h be the height of T. We justify this proposition by establishing the following fact: log(n+1) ≤h ≤2log(n+1). Let d be the common black depth of all the external nodes of T. Let T ′ be the (2,4) tree associated with T, and let h′ be the height of T ′. Because of the correspondence between red-black trees and (2,4) trees, we know that h′ = d. Hence, by Proposition 11.7, d = h′ ≤log(n+1). By the red property, h ≤2d. Thus, we obtain h ≤2log(n+1). The other inequality, log(n+1) ≤h, follows from Proposition 8.7 and the fact that T has n internal nodes. 11.6.1 Red-Black Tree Operations The algorithm for searching in a red-black tree T is the same as that for a standard binary search tree (Section 11.1). Thus, searching in a red-black tree takes time proportional to the height of the tree, which is O(logn) by Proposition 11.8. The correspondence between (2,4) trees and red-black trees provides important intuition that we will use in our discussion of how to perform updates in red-black trees; in fact, the update algorithms for red-black trees can seem mysteriously complex without this intuition. Split and fuse operations of a (2,4) tree will be effectively mimicked by recoloring neighboring red-black tree nodes. A rotation within a red-black tree will be used to change orientations of a 3-node between the two forms shown in Figure 11.31(b). Insertion Consider the insertion of a key-value pair (k,v) into a red-black tree T. The algorithm initially proceeds as in a standard binary search tree (Section 11.1.2). Namely, we search for k in T and if we reach an external node, we replace this node with an internal node x, storing the entry and having two external children. If this is the ﬁrst entry in T, and thus x is the root, we color it black. In all other cases, we color x red. That action corresponds to inserting (k,v) into a node of the (2,4) tree T ′ at the lowest internal level. The insertion preserves the root and depth properties of T, but it may violate the red property. Indeed, if x is not the root of T and its parent y is red, then we have a parent and a child (namely, y and x) that are both red. Note that by the root property, y cannot be the root of T, and by the red property (which was previously satisﬁed), the parent z of y must be black. Since x and its parent are red, but x’s grandparent z is black, we call this violation of the red property a double red at node x. To remedy a double red, we consider two cases.

11.6. Red-Black Trees Case 1: The Sibling s of y is Black. (See Figure 11.32.) In this case, the double red denotes the fact that we have added the new node to a corresponding 3-node of the (2,4) tree T ′, effectively creating a malformed 4-node. This formation has one red node, y, that is the parent of another red node, x; we want the two red nodes to be siblings instead. To ﬁx this problem, we perform a trinode restructuring of T. The trinode restructuring (introduced in Section 11.2) is done by the operation restructure(x), which consists of the following steps (see again Figure 11.32): • Take node x, its parent y, and grandparent z, and temporarily relabel them as a, b, and c, in left-to-right order, so that a, b, and c will be visited in this order by an inorder tree traversal. • Replace the grandparent z with the node labeled b, and make nodes a and c the children of b, keeping inorder relationships unchanged. After performing the restructure(x) operation, we color b black and we color a and c red. Thus, the restructuring eliminates the double-red problem. Notice that the portion of any path through the restructured part of the tree is incident to exactly one black node, both before and after the trinode restructuring. Therefore, the black depth of the tree is unaffected. z s x y x z y s y x z s x z y s (a) b a c (b) Figure 11.32: Restructuring a red-black tree to remedy a double red: (a) the four conﬁgurations for x, y, and z before restructuring; (b) after restructuring.

Chapter 11. Search Trees Case 2: The Sibling s of y is Red. (See Figure 11.33.) In this case, the double red denotes an overﬂow in the corresponding (2,4) tree T ′. To ﬁx the problem, we perform the equivalent of a split operation. Namely, we do a recoloring: we color y and s black and their parent z red (unless z is the root, in which case, it remains black). Notice that unless z is the root, the portion of any path through the affected part of the tree is incident to exactly one black node, both before and after the recoloring. Therefore, the black depth of the tree is unaffected by the recoloring unless z is the root, in which case it is increased by one. However, it is possible that the double-red problem reappears after such a recoloring, albeit higher up in the tree T, since z may have a red parent. If the double-red problem reappears at z, then we repeat the consideration of the two cases at z. Thus, a recoloring either eliminates the double-red problem at node x, or propagates it to the grandparent z of x. We continue going up T performing recolorings until we ﬁnally resolve the double-red problem (with either a ﬁnal recoloring or a trinode restructuring). Thus, the number of recolorings caused by an insertion is no more than half the height of tree T, that is, O(logn) by Proposition 11.8. z y x s 10 20 30 40 (a) z y x s 10 20 ...30 ... (b) Figure 11.33: Recoloring to remedy the double-red problem: (a) before recoloring and the corresponding 5-node in the associated (2,4) tree before the split; (b) after recoloring and the corresponding nodes in the associated (2,4) tree after the split. As further examples, Figures 11.34 and 11.35 show a sequence of insertion operations in a red-black tree.

11.6. Red-Black Trees (a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) (l) Figure 11.34: A sequence of insertions in a red-black tree: (a) initial tree; (b) insertion of 7; (c) insertion of 12, which causes a double red; (d) after restructuring; (e) insertion of 15, which causes a double red; (f) after recoloring (the root remains black); (g) insertion of 3; (h) insertion of 5; (i) insertion of 14, which causes a double red; (j) after restructuring; (k) insertion of 18, which causes a double red; (l) after recoloring. (Continues in Figure 11.35.)

Chapter 11. Search Trees (m) (n) (o) (p) (q) Figure 11.35: A sequence of insertions in a red-black tree (continued from Figure 11.34): (m) insertion of 16, which causes a double red; (n) after restructuring; (o) insertion of 17, which causes a double red; (p) after recoloring there is again a double red, to be handled by a restructuring; (q) after restructuring.

11.6. Red-Black Trees Deletion Deleting an entry with key k from a red-black tree T initially proceeds as for a binary search tree (Section 11.1.2). Structurally, the process results in the removal of an internal node (either that originally containing key k or its inorder predecessor) together with a child that is external, and the promotion of its other child. If the removed internal node was red, this structural change does not affect the black depths of any paths in the tree, nor introduce any red violations, and so the resulting tree remains a valid red-black tree. In the corresponding (2,4) tree T ′, this case denotes the shrinking of a 4-node or 3-node. If the removed internal node was black, it must have had black height 1, and therefore either both of its children were external, or it had one red child that was an internal node with two external children. In the latter case, the removed node represents the black part of a corresponding 3node, and we restore the red-black properties by recoloring the promoted child to be black. The most complex case is when the removed node was black and had two external children. In the corresponding (2,4) tree, this denotes the removal of an entry from a 2-node. Without rebalancing, such a change results in a deﬁcit of one for the black depth of the external position p that is the promoted child of the deleted internal node. To preserve the depth property, we temporarily assign the promoted leaf a ﬁctitious double black color. A double black in T denotes an underﬂow in the corresponding (2,4) tree T ′. To remedy a double-black problem at an arbitrary position p, we will consider three cases. Case 1: The Sibling y of p is Black and has a Red Child x. (See Figure 11.36.) We perform a trinode restructuring, as originally described in Section 11.2. The operation restructure(x) takes the node x, its parent y, and grandparent z, labels them temporarily left to right as a, b, and c, and replaces z with the node labeled b, making it the parent of the other two. We color a and c black, and give b the former color of z. Notice that the path to p in the result includes one additional black node after the restructure, while the number of black nodes on paths to any of the other three subtrees illustrated in Figure 11.36 remains unchanged. Therefore, we return p to be colored (regular) black, and the double-black problem is eliminated. Resolving this case corresponds to a transfer operation in the (2,4) tree T ′ between two children of node z. The fact that y has a red child assures us that it represents either a 3-node or a 4-node. In effect, the entry previously stored at z is demoted to become a new 2-node to resolve the deﬁciency, while an entry stored at y or its child is promoted to take the place of the entry previously stored at z.

Chapter 11. Search Trees x p z y 10 20 ...30 ... (a) x p z y 10 20 ...30 ... (b) b a c p ...20 ... (c) Figure 11.36: Restructuring of a red-black tree to remedy the double-black problem: (a) and (b) conﬁgurations before the restructuring, where p is a right child and the associated nodes in the corresponding (2,4) tree before the transfer (two other symmetric conﬁgurations where p is a left child are possible); (c) conﬁguration after the restructuring and the associated nodes in the corresponding (2,4) tree after the transfer. The gray color for node z in parts (a) and (b) and for node b in part (c) denotes the fact that this node may be colored either red or black.

11.6. Red-Black Trees Case 2: The Sibling y of p is Black and Both Children of y are Black. We do a recoloring, beginning by changing the color of p from double black to black and the color of y from black to red. This does not create any red violation, because both children of y are black. To counteract the decrease in black depth for paths passing through y or p, we consider the common parent of p and y, which we denote as z. If z is red, we color it black and the problem has been resolved (see Figure 11.37a). If z is black, we color it double black, thereby propagating the problem higher up the tree (see Figure 11.37b). Resolving this case corresponds to a fusion operation in the corresponding (2,4) tree T ′, as y must represent a 2-node. The case where the problem propagates upward is when parent z also represents a 2-node. z y p z y p (a) z y p y z p (b) Figure 11.37: A recoloring operation, which has neutral effect on the black depth for paths: (a) when z is originally red, the recoloring resolves the double-black problem, ending the process; (b) when z is originally black, it becomes doubleblack, requiring a cascading remedy. Case 3: Sibling y of p is Red. (See Figure 11.38.) Let z denote the common parent of y and p, and note that z must be black, because y is red. The combination of y and z represents a 3-node in the corresponding (2,4) tree T ′. In this case, we perform a rotation about y and z, and then recolor y black and z red. This denotes a reorientation of a 3-node in the corresponding (2,4) tree T ′. We now reconsider the double-black problem at p. After the adjustment, the sibling of p is black, and either Case 1 or Case 2 applies. Furthermore, the next application will be the last, because Case 1 is always terminal and Case 2 will be terminal given that the parent of p is now red.

Chapter 11. Search Trees z y p p z y Figure 11.38: A rotation and recoloring about red node y and black node z in the presence of a double-black problem (a symmetric conﬁguration is possible). This amounts to a change of orientation in the corresponding 3-node of a (2,4) tree. This operation does not affect the black depth of any paths through this portion of the tree, but after the operation, one of the other resolutions to the double-black problem may be applied, as the sibling of p will be black. In Figure 11.39, we show a sequence of deletions on a red-black tree. We illustrate a Case 1 restructuring in parts (c) and (d). We illustrate a Case 2 recoloring in parts (f) and (g). Finally, we show an example of a Case 3 rotation between parts (i) and (j), concluding with a Case 2 recoloring in part (k). Performance of Red-Black Trees The asymptotic performance of a red-black tree is identical to that of an AVL tree or a (2,4) tree in terms of the sorted map ADT, with guaranteed logarithmic time bounds for most operations. (See Table 11.2 for a summary of the AVL performance.) The primary advantage of a red-black tree is that an insertion or deletion requires only a constant number of restructuring operations. (This is in contrast to AVL trees and (2,4) trees, both of which require a logarithmic number of structural changes per map operation in the worst case.) That is, an insertion or deletion in a red-black tree requires logarithmic time for a search, and may require a logarithmic number of recoloring operations that cascade upward. We formalize these facts with the following propositions. Proposition 11.9: The insertion of an entry in a red-black tree storing n entries can be done in O(logn) time and requires O(logn) recolorings and at most one trinode restructuring. Proposition 11.10: The algorithm for deleting an entry from a red-black tree with n entries takes O(logn) time and performs O(logn) recolorings and at most two restructuring operations. The proofs of these propositions are left as Exercises R-11.26 and R-11.27.

11.6. Red-Black Trees (a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) Figure 11.39: A sequence of deletions from a red-black tree: (a) initial tree; (b) removal of 3; (c) removal of 12, causing a black deﬁcit to the right of 7 (handled by restructuring); (d) after restructuring; (e) removal of 17; (f) removal of 18, causing a black deﬁcit to the right of 16 (handled by recoloring); (g) after recoloring; (h) removal of 15; (i) removal of 16, causing a black deﬁcit to the right of 14 (handled initially by a rotation); (j) after the rotation the black deﬁcit needs to be handled by a recoloring; (k) after the recoloring.

Chapter 11. Search Trees 11.6.2 Java Implementation In this section, we will provide an implementation of a RBTreeMap class that inherits from the standard TreeMap class and relies on the balancing framework described in Section 11.2.1. In that framework, each node stores an auxiliary integer that can be used for maintaining balance information. For a red-black tree, we use that integer to represent color, choosing to let value 0 (the default) designate the color black, and value 1 the color red; with this convention, any newly created leaf in the tree will be black. Our implementation begins in Code Fragment 11.14, with constructors for an empty map, and a series of convenient utilities for managing the auxiliary ﬁeld to represent color information. That code fragment continues with support for rebalancing the tree after an insertion is performed. When an entry has been inserted in a tree by the standard search-tree algorithm, it will be stored at a previously external node that was converted to an internal node with two new external children. The rebalanceInsert hook is then called, allowing us the opportunity to modify the tree. Except for the special case where the new element is at the root, we change the color of the node with the new element to red (it had been black when a leaf), and then we consider the possibility that we have a double-red violation. The resolveRed utility closely follows the case analysis described in Section 11.6.1, recurring in the case when the red violation is propagated upward. Code Fragment 11.15 manages the rebalancing process after a deletion, based upon the case analysis described in Section 11.6.1. If the removed node was red, then no other action is necessary; however, if the removed node was black, we must consider a way to restore the depth property. An additional challenge is that by the time the rebalanceDelete method is called, a node has already been removed from the tree (this hook is invoked on the promoted child of that removed node). Fortunately, we can infer the properties of the removed node based upon the redblack tree properties, which were satisﬁed before the deletion. In particular, let p denote the promoted child of the removed node. If a black node with a red child has been deleted, then p will be that red child; we remedy this by coloring p black. Otherwise, if p is not the root, let s denote the removed node’s sibling (which will appear as p’s sibling after the deletion). If the deleted node was black with two black children, we must treat p as a double black node to be remedied. This is the case if, and only if, its sibling’s subtree has a black internal node (because the red-black depth property was satisﬁed prior to the deletion). We therefore test whether s is a black internal node, or a red internal node with an internal node as a child (which must be black due to the red property of the tree). We are able to detect the double-black problem within the rebalanceDelete method of Code Fragment 11.15, and we rely on the recursive remedyDoubleBlack method of that code fragment to resolve the problem.

11.6. Red-Black Trees /∗∗An implementation of a sorted map using a red-black tree. ∗/ public class RBTreeMap<K,V> extends TreeMap<K,V> { /∗∗Constructs an empty map using the natural ordering of keys. ∗/ public RBTreeMap() { super(); } /∗∗Constructs an empty map using the given comparator to order keys. ∗/ public RBTreeMap(Comparator<K> comp) { super(comp); } // we use the inherited aux ﬁeld with convention that 0=black and 1=red // (note that new leaves will be black by default, as aux=0) private boolean isBlack(Position<Entry<K,V>> p) { return tree.getAux(p)==0;} private boolean isRed(Position<Entry<K,V>> p) { return tree.getAux(p)==1; } private void makeBlack(Position<Entry<K,V>> p) { tree.setAux(p, 0); } private void makeRed(Position<Entry<K,V>> p) { tree.setAux(p, 1); } private void setColor(Position<Entry<K,V>> p, boolean toRed) { tree.setAux(p, toRed ? 1 : 0); } /∗∗Overrides the TreeMap rebalancing hook that is called after an insertion. ∗/ protected void rebalanceInsert(Position<Entry<K,V>> p) { if (!isRoot(p)) { makeRed(p); // the new internal node is initially colored red resolveRed(p); // but this may cause a double-red problem } } /∗∗Remedies potential double-red violation above red position p. ∗/ private void resolveRed(Position<Entry<K,V>> p) { Position<Entry<K,V>> parent,uncle,middle,grand; // used in case analysis parent = parent(p); if (isRed(parent)) { // double-red problem exists uncle = sibling(parent); if (isBlack(uncle)) { // Case 1: misshapen 4-node middle = restructure(p); // do trinode restructuring makeBlack(middle); makeRed(left(middle)); makeRed(right(middle)); } else { // Case 2: overfull 5-node makeBlack(parent); // perform recoloring makeBlack(uncle); grand = parent(parent); if (!isRoot(grand)) { makeRed(grand); // grandparent becomes red resolveRed(grand); // recur at red grandparent } } } } Code Fragment 11.14: The RBTreeMap class. (Continues in Code Fragment 11.15.)

Chapter 11. Search Trees /∗∗Overrides the TreeMap rebalancing hook that is called after a deletion. ∗/ protected void rebalanceDelete(Position<Entry<K,V>> p) { if (isRed(p)) // deleted parent was black makeBlack(p); // so this restores black depth else if (!isRoot(p)) { Position<Entry<K,V>> sib = sibling(p); if (isInternal(sib) && (isBlack(sib) || isInternal(left(sib)))) remedyDoubleBlack(p); // sib's subtree has nonzero black height } } /∗∗Remedies a presumed double-black violation at the given (nonroot) position. ∗/ private void remedyDoubleBlack(Position<Entry<K,V>> p) { Position<Entry<K,V>> z = parent(p); Position<Entry<K,V>> y = sibling(p); if (isBlack(y)) { if (isRed(left(y)) || isRed(right(y))) { // Case 1: trinode restructuring Position<Entry<K,V>> x = (isRed(left(y)) ? left(y) : right(y)); Position<Entry<K,V>> middle = restructure(x); setColor(middle, isRed(z)); // root of restructured subtree gets z's old color makeBlack(left(middle)); makeBlack(right(middle)); } else { // Case 2: recoloring makeRed(y); if (isRed(z)) makeBlack(z); // problem is resolved else if (!isRoot(z)) remedyDoubleBlack(z); // propagate the problem } } else { // Case 3: reorient 3-node rotate(y); makeBlack(y); makeRed(z); remedyDoubleBlack(p); // restart the process at p } } } Code Fragment 11.15: Support for deletion in the RBTreeMap class (continued from Code Fragment 11.14).

Chapter 12. Sorting and Selection 12.1 Merge-Sort We have introduced several sorting algorithms thus far, including insertion-sort (see Sections 3.1.2, 7.6, and 9.4.1); selection-sort (see Section 9.4.1); bubble-sort (see Exercise C-7.51); and heap-sort (see Section 9.4.2). In this chapter, we will present four other sorting algorithms, called merge-sort, quick-sort, bucket-sort, and radix-sort, and then discuss the advantages and disadvantages of the various algorithms in Section 12.4. 12.1.1 Divide-and-Conquer The ﬁrst two algorithms we describe in this chapter, merge-sort and quick-sort, use recursion in an algorithmic design pattern called divide-and-conquer. We have already seen the power of recursion in describing algorithms in an elegant manner (see Chapter 5). The divide-and-conquer pattern consists of the following three steps: 1. Divide: If the input size is smaller than a certain threshold (say, one or two elements), solve the problem directly using a straightforward method and return the solution so obtained. Otherwise, divide the input data into two or more disjoint subsets. 2. Conquer: Recursively solve the subproblems associated with the subsets. 3. Combine: Take the solutions to the subproblems and merge them into a solution to the original problem. Using Divide-and-Conquer for Sorting We ﬁrst describe the merge-sort algorithm at a high level, without focusing on whether the data is an array or linked list. (We will soon give concrete implementations for each.) To sort a sequence S with n elements using the three divide-andconquer steps, the merge-sort algorithm proceeds as follows: 1. Divide: If S has zero or one element, return S immediately; it is already sorted. Otherwise (S has at least two elements), remove all the elements from S and put them into two sequences, S1 and S2, each containing about half of the elements of S; that is, S1 contains the ﬁrst ⌊n/2⌋elements of S, and S2 contains the remaining ⌈n/2⌉elements. 2. Conquer: Recursively sort sequences S1 and S2. 3. Combine: Put the elements back into S by merging the sorted sequences S1 and S2 into a sorted sequence. In reference to the divide step, we recall that the notation ⌊x⌋indicates the ﬂoor of x, that is, the largest integer k, such that k ≤x. Similarly, the notation ⌈x⌉indicates the ceiling of x, that is, the smallest integer m, such that x ≤m.

12.1. Merge-Sort We can visualize an execution of the merge-sort algorithm by means of a binary tree T, called the merge-sort tree. Each node of T represents a recursive invocation (or call) of the merge-sort algorithm. We associate with each node v of T the sequence S that is processed by the invocation associated with v. The children of node v are associated with the recursive calls that process the subsequences S1 and S2 of S. The external nodes of T are associated with individual elements of S, corresponding to instances of the algorithm that make no recursive calls. Figure 12.1 summarizes an execution of the merge-sort algorithm by showing the input and output sequences processed at each node of the merge-sort tree. The step-by-step evolution of the merge-sort tree is shown in Figures 12.2 through 12.4. This algorithm visualization in terms of the merge-sort tree helps us analyze the running time of the merge-sort algorithm. In particular, since the size of the input sequence roughly halves at each recursive call of merge-sort, the height of the merge-sort tree is about logn (recall that the base of log is 2 if omitted). (a) (b) Figure 12.1: Merge-sort tree T for an execution of the merge-sort algorithm on a sequence with 8 elements: (a) input sequences processed at each node of T; (b) output sequences generated at each node of T.

Chapter 12. Sorting and Selection (a) (b) (c) (d) (e) (f) Figure 12.2: Visualization of an execution of merge-sort. Each node of the tree represents a recursive call of merge-sort. The nodes drawn with dashed lines represent calls that have not been made yet. The node drawn with thick lines represents the current call. The empty nodes drawn with thin lines represent completed calls. The remaining nodes (drawn with thin lines and not empty) represent calls that are waiting for a child call to return. (Continues in Figure 12.3.)

12.1. Merge-Sort (g) (h) (i) (j) (k) (l) Figure 12.3: Visualization of an execution of merge-sort. (Combined with Figures 12.2 and 12.4.)

Chapter 12. Sorting and Selection (m) (n) (o) (p) Figure 12.4: Visualization of an execution of merge-sort (continued from Figure 12.3). Several calls are omitted between (m) and (n). Note the merging of two halves performed in step (p). Proposition 12.1: The merge-sort tree associated with an execution of mergesort on a sequence of size n has height ⌈logn⌉. We leave the justiﬁcation of Proposition 12.1 as a simple exercise (R-12.1). We will use this proposition to analyze the running time of the merge-sort algorithm. Having given an overview of merge-sort and an illustration of how it works, let us consider each of the steps of this divide-and-conquer algorithm in more detail. Dividing a sequence of size n involves separating it at the element with index ⌈n/2⌉, and recursive calls can be started by passing these smaller sequences as parameters. The difﬁcult step is combining the two sorted sequences into a single sorted sequence. Thus, before we present our analysis of merge-sort, we need to say more about how this is done.

12.1. Merge-Sort 12.1.2 Array-Based Implementation of Merge-Sort We begin by focusing on the case when a sequence of items is represented with an array. The merge method (Code Fragment 12.1) is responsible for the subtask of merging two previously sorted sequences, S1 and S2, with the output copied into S. We copy one element during each pass of the while loop, conditionally determining whether the next element should be taken from S1 or S2. The divide-and-conquer merge-sort algorithm is given in Code Fragment 12.2. We illustrate a step of the merge process in Figure 12.5. During the process, index i represents the number of elements of S1 that have been copied to S, while index j represents the number of elements of S2 that have been copied to S. Assuming S1 and S2 both have at least one uncopied element, we copy the smaller of the two elements being considered. Since i + j objects have been previously copied, the next element is placed in S[i+ j]. (For example, when i + j is 0, the next element is copied to S[0]). If we reach the end of one of the sequences, we must copy the next element from the other. /∗∗Merge contents of arrays S1 and S2 into properly sized array S. ∗/ public static <K> void merge(K[ ] S1, K[ ] S2, K[ ] S, Comparator<K> comp) { int i = 0, j = 0; while (i + j < S.length) { if (j == S2.length || (i < S1.length && comp.compare(S1[i], S2[j]) < 0)) S[i+j] = S1[i++]; // copy ith element of S1 and increment i else S[i+j] = S2[j++]; // copy jth element of S2 and increment j } } Code Fragment 12.1: An implementation of the merge operation for a Java array. S1 S S2 18 19 22 9 10 j i i+ j 11 12 14 S S1 S2 18 19 22 i j i+ j 11 12 14 (a) (b) Figure 12.5: A step in the merge of two sorted arrays for which S2[ j] < S1[i]. We show the arrays before the copy step in (a) and after it in (b).

Chapter 12. Sorting and Selection /∗∗Merge-sort contents of array S. ∗/ public static <K> void mergeSort(K[ ] S, Comparator<K> comp) { int n = S.length; if (n < 2) return; // array is trivially sorted // divide int mid = n/2; K[ ] S1 = Arrays.copyOfRange(S, 0, mid); // copy of ﬁrst half K[ ] S2 = Arrays.copyOfRange(S, mid, n); // copy of second half // conquer (with recursion) mergeSort(S1, comp); // sort copy of ﬁrst half mergeSort(S2, comp); // sort copy of second half // merge results merge(S1, S2, S, comp); // merge sorted halves back into original } Code Fragment 12.2: An implementation of the recursive merge-sort algorithm for a Java array (using the merge method deﬁned in Code Fragment 12.1). We note that methods merge and mergeSort rely on use of a Comparator instance to compare a pair of generic objects that are presumed to belong to a total order. This is the same approach we introduced when deﬁning priority queues in Section 9.2.2, and when studying implementing sorted maps in Chapters 10 and 11. 12.1.3 The Running Time of Merge-Sort We begin by analyzing the running time of the merge algorithm. Let n1 and n2 be the number of elements of S1 and S2, respectively. It is clear that the operations performed inside each pass of the while loop take O(1) time. The key observation is that during each iteration of the loop, one element is copied from either S1 or S2 into S (and that element is considered no further). Therefore, the number of iterations of the loop is n1 +n2. Thus, the running time of algorithm merge is O(n1 +n2). Having analyzed the running time of the merge algorithm used to combine subproblems, let us analyze the running time of the entire merge-sort algorithm, assuming it is given an input sequence of n elements. For simplicity, we restrict our attention to the case where n is a power of 2. We leave it to an exercise (R-12.3) to show that the result of our analysis also holds when n is not a power of 2. When evaluating the merge-sort recursion, we rely on the analysis technique introduced in Section 5.2. We account for the amount of time spent within each recursive call, but excluding any time spent waiting for successive recursive calls to terminate. In the case of our mergeSort method, we account for the time to divide the sequence into two subsequences, and the call to merge to combine the two sorted sequences, but we exclude the two recursive calls to mergeSort.

12.1. Merge-Sort A merge-sort tree T, as portrayed in Figures 12.2 through 12.4, can guide our analysis. Consider a recursive call associated with a node v of the merge-sort tree T. The divide step at node v is straightforward; this step runs in time proportional to the size of the sequence for v, based on the use of slicing to create copies of the two list halves. We have already observed that the merging step also takes time that is linear in the size of the merged sequence. If we let i denote the depth of node v, the time spent at node v is O(n/2i), since the size of the sequence handled by the recursive call associated with v is equal to n/2i. Looking at the tree T more globally, as shown in Figure 12.6, we see that, given our deﬁnition of “time spent at a node,” the running time of merge-sort is equal to the sum of the times spent at the nodes of T. Observe that T has exactly 2i nodes at depth i. This simple observation has an important consequence, for it implies that the overall time spent at all the nodes of T at depth i is O(2i ·n/2i), which is O(n). By Proposition 12.1, the height of T is ⌈logn⌉. Thus, since the time spent at each of the ⌈logn⌉+1 levels of T is O(n), we have the following result: Proposition 12.2: Algorithm merge-sort sorts a sequence S of size n in O(nlogn) time, assuming two elements of S can be compared in O(1) time. Height Time per level Total time: O(nlogn) O(n) O(n) O(logn) O(n) n n/2 n/4 n/4 n/4 n/4 n/2 Figure 12.6: A visual analysis of the running time of merge-sort. Each node represents the time spent in a particular recursive call, labeled with the size of its subproblem.

Chapter 12. Sorting and Selection 12.1.4 Merge-Sort and Recurrence Equations ⋆ There is another way to justify that the running time of the merge-sort algorithm is O(nlogn) (Proposition 12.2). Namely, we can deal more directly with the recursive nature of the merge-sort algorithm. In this section, we will present such an analysis of the running time of merge-sort, and in so doing, introduce the mathematical concept of a recurrence equation (also known as recurrence relation). Let the function t(n) denote the worst-case running time of merge-sort on an input sequence of size n. Since merge-sort is recursive, we can characterize function t(n) by means of an equation where the function t(n) is recursively expressed in terms of itself. In order to simplify our characterization of t(n), let us restrict our attention to the case when n is a power of 2. (We leave the problem of showing that our asymptotic characterization still holds in the general case as an exercise.) In this case, we can specify the deﬁnition of t(n) as t(n) =  b if n ≤1 2t(n/2)+cn otherwise. An expression such as the one above is called a recurrence equation, since the function appears on both the left- and right-hand sides of the equal sign. Although such a characterization is correct and accurate, what we really desire is a big-Oh type of characterization of t(n) that does not involve the function t(n) itself. That is, we want a closed-form characterization of t(n). We can obtain a closed-form solution by applying the deﬁnition of a recurrence equation, assuming n is relatively large. For example, after one more application of the equation above, we can write a new recurrence for t(n) as t(n) = 2(2t(n/22)+(cn/2))+cn = 22t(n/22)+2(cn/2)+cn = 22t(n/22)+2cn. If we apply the equation again, we get t(n) = 23t(n/23) + 3cn. At this point, we should see a pattern emerging, so that after applying this equation i times, we get t(n) = 2it(n/2i)+icn. The issue that remains, then, is to determine when to stop this process. To see when to stop, recall that we switch to the closed form t(n) = b when n ≤1, which will occur when 2i = n. In other words, this will occur when i = logn. Making this substitution, then, yields t(n) = 2lognt(n/2logn)+(logn)cn = nt(1)+cnlogn = nb+cnlogn. That is, we get an alternative justiﬁcation of the fact that t(n) is O(nlogn).

12.1. Merge-Sort 12.1.5 Alternative Implementations of Merge-Sort Sorting Linked Lists The merge-sort algorithm can easily be adapted to use any form of a basic queue as its container type. In Code Fragment 12.3, we provide such an implementation, based on use of the LinkedQueue class from Section 6.2.3. The O(nlogn) bound for merge-sort from Proposition 12.2 applies to this implementation as well, since each basic operation runs in O(1) time when implemented with a linked list. We show an example execution of this version of the merge algorithm in Figure 12.7. /∗∗Merge contents of sorted queues S1 and S2 into empty queue S. ∗/ public static <K> void merge(Queue<K> S1, Queue<K> S2, Queue<K> S, Comparator<K> comp) { while (!S1.isEmpty() && !S2.isEmpty()) { if (comp.compare(S1.ﬁrst(), S2.ﬁrst()) < 0) S.enqueue(S1.dequeue()); // take next element from S1 else S.enqueue(S2.dequeue()); // take next element from S2 } while (!S1.isEmpty()) S.enqueue(S1.dequeue()); // move any elements that remain in S1 while (!S2.isEmpty()) S.enqueue(S2.dequeue()); // move any elements that remain in S2 } /∗∗Merge-sort contents of queue. ∗/ public static <K> void mergeSort(Queue<K> S, Comparator<K> comp) { int n = S.size(); if (n < 2) return; // queue is trivially sorted // divide Queue<K> S1 = new LinkedQueue<>(); // (or any queue implementation) Queue<K> S2 = new LinkedQueue<>(); while (S1.size() < n/2) S1.enqueue(S.dequeue()); // move the ﬁrst n/2 elements to S1 while (!S.isEmpty()) S2.enqueue(S.dequeue()); // move remaining elements to S2 // conquer (with recursion) mergeSort(S1, comp); // sort ﬁrst half mergeSort(S2, comp); // sort second half // merge results merge(S1, S2, S, comp); // merge sorted halves back into original } Code Fragment 12.3: An implementation of merge-sort using a basic queue.

Chapter 12. Sorting and Selection S1 S2 S S1 S2 S (a) (b) S1 S2 S S1 S2 S (c) (d) S1 S2 S S1 S2 S (e) (f) S1 S2 S S1 S2 S (g) (h) S1 S2 S (i) Figure 12.7: Example of an execution of the merge algorithm, as implemented in Code Fragment 12.3 using queues.

12.1. Merge-Sort A Bottom-Up (Nonrecursive) Merge-Sort There is a nonrecursive version of array-based merge-sort, which runs in O(nlogn) time. It is a bit faster than recursive merge-sort in practice, as it avoids the extra overheads of recursive calls and temporary memory at each level. The main idea is to perform merge-sort bottom-up, performing the merges level by level going up the merge-sort tree. Given an input array of elements, we begin by merging every successive pair of elements into sorted runs of length two. We merge these runs into runs of length four, merge these new runs into runs of length eight, and so on, until the array is sorted. To keep the space usage reasonable, we deploy a second array that stores the merged runs (swapping input and output arrays after each iteration). We give a Java implementation in Code Fragment 12.4, using the built-in method System.arraycopy to copy a range of cells between two arrays. A similar bottom-up approach can be used for sorting linked lists. (See Exercise C-12.30.) /∗∗Merges in[start..start+inc−1] and in[start+inc..start+2∗inc−1] into out. ∗/ public static <K> void merge(K[ ] in, K[ ] out, Comparator<K> comp, int start, int inc) { int end1 = Math.min(start + inc, in.length); // boundary for run 1 int end2 = Math.min(start + 2 ∗inc, in.length); // boundary for run 2 int x=start; // index into run 1 int y=start+inc; // index into run 2 int z=start; // index into output while (x < end1 && y < end2) if (comp.compare(in[x], in[y]) < 0) out[z++] = in[x++]; // take next from run 1 else out[z++] = in[y++]; // take next from run 2 if (x < end1) System.arraycopy(in, x, out, z, end1 −x); // copy rest of run 1 else if (y < end2) System.arraycopy(in, y, out, z, end2 −y); // copy rest of run 2 } /∗∗Merge-sort contents of data array. ∗/ public static <K> void mergeSortBottomUp(K[ ] orig, Comparator<K> comp) { int n = orig.length; K[ ] src = orig; // alias for the original K[ ] dest = (K[ ]) new Object[n]; // make a new temporary array K[ ] temp; // reference used only for swapping for (int i=1; i < n; i ∗= 2) { // each iteration sorts all runs of length i for (int j=0; j < n; j += 2∗i) // each pass merges two runs of length i merge(src, dest, comp, j, i); temp = src; src = dest; dest = temp; // reverse roles of the arrays } if (orig != src) System.arraycopy(src, 0, orig, 0, n); // additional copy to get result to original } Code Fragment 12.4: An implementation of the nonrecursive merge-sort algorithm.

Chapter 12. Sorting and Selection 12.2 Quick-Sort The next sorting algorithm we discuss is called quick-sort. Like merge-sort, this algorithm is also based on the divide-and-conquer paradigm, but it uses this technique in a somewhat opposite manner, as all the hard work is done before the recursive calls. High-Level Description of Quick-Sort The quick-sort algorithm sorts a sequence S using a simple recursive approach. The main idea is to apply the divide-and-conquer technique, whereby we divide S into subsequences, recur to sort each subsequence, and then combine the sorted subsequences by a simple concatenation. In particular, the quick-sort algorithm consists of the following three steps (see Figure 12.8): 1. Divide: If S has at least two elements (nothing needs to be done if S has zero or one element), select a speciﬁc element x from S, which is called the pivot. As is common practice, choose the pivot x to be the last element in S. Remove all the elements from S and put them into three sequences: • L, storing the elements in S less than x • E, storing the elements in S equal to x • G, storing the elements in S greater than x Of course, if the elements of S are distinct, then E holds just one element— the pivot itself. 2. Conquer: Recursively sort sequences L and G. 3. Combine: Put back the elements into S in order by ﬁrst inserting the elements of L, then those of E, and ﬁnally those of G. 2. Recur 1. Split using pivot x 3. Concatenate 2. Recur G(> x) L(< x) E(= x) Figure 12.8: A visual schematic of the quick-sort algorithm.

12.2. Quick-Sort Like merge-sort, the execution of quick-sort can be visualized by means of a binary recursion tree, called the quick-sort tree. Figure 12.9 summarizes an execution of the quick-sort algorithm by showing the input and output sequences processed at each node of the quick-sort tree. The step-by-step evolution of the quick-sort tree is shown in Figures 12.10, 12.11, and 12.12. Unlike merge-sort, however, the height of the quick-sort tree associated with an execution of quick-sort is linear in the worst case. This happens, for example, if the sequence consists of n distinct elements and is already sorted. Indeed, in this case, the standard choice of the last element as pivot yields a subsequence L of size n−1, while subsequence E has size 1 and subsequence G has size 0. At each call of quick-sort on subsequence L, the size decreases by 1. Hence, the height of the quick-sort tree is n−1. (a) (b) Figure 12.9: Quick-sort tree T for an execution of the quick-sort algorithm on a sequence with 8 elements: (a) input sequences processed at each node of T; (b) output sequences generated at each node of T. The pivot used at each level of the recursion is shown in bold.

Chapter 12. Sorting and Selection (a) (b) (c) (d) (e) (f) Figure 12.10: Visualization of quick-sort. Each node of the tree represents a recursive call. The nodes drawn with dashed lines represent calls that have not been made yet. The node drawn with thick lines represents the running call. The empty nodes drawn with thin lines represent terminated calls. The remaining nodes represent suspended calls (that is, active calls that are waiting for a child call to return). Note the divide steps performed in (b), (d), and (f). (Continues in Figure 12.11.)

12.2. Quick-Sort (g) (h) (i) (j) (k) (l) Figure 12.11: Visualization of an execution of quick-sort. Note the concatenation step performed in (k). (Continues in Figure 12.12.)

Chapter 12. Sorting and Selection (m) (n) (o) (p) (q) (r) Figure 12.12: Visualization of an execution of quick-sort. Several calls between (p) and (q) have been omitted. Note the concatenation steps performed in (o) and (r). (Continued from Figure 12.11.)

12.2. Quick-Sort Performing Quick-Sort on General Sequences In Code Fragment 12.5, we give an implementation of the quick-sort algorithm that works on any sequence type that operates as a queue. This particular version relies on the LinkedQueue class from Section 6.2.3; we provide a more streamlined implementation of quick-sort using an array-based sequence in Section 12.2.2. Our implementation chooses the ﬁrst item of the queue as the pivot (since it is easily accessible), and then it divides sequence S into queues L, E, and G of elements that are respectively less than, equal to, and greater than the pivot. We then recur on the L and G lists, and transfer elements from the sorted lists L, E, and G back to S. All of the queue operations run in O(1) worst-case time when implemented with a linked list. /∗∗Quick-sort contents of a queue. ∗/ public static <K> void quickSort(Queue<K> S, Comparator<K> comp) { int n = S.size(); if (n < 2) return; // queue is trivially sorted // divide K pivot = S.ﬁrst(); // using ﬁrst as arbitrary pivot Queue<K> L = new LinkedQueue<>(); Queue<K> E = new LinkedQueue<>(); Queue<K> G = new LinkedQueue<>(); while (!S.isEmpty()) { // divide original into L, E, and G K element = S.dequeue(); int c = comp.compare(element, pivot); if (c < 0) // element is less than pivot L.enqueue(element); else if (c == 0) // element is equal to pivot E.enqueue(element); else // element is greater than pivot G.enqueue(element); } // conquer quickSort(L, comp); // sort elements less than pivot quickSort(G, comp); // sort elements greater than pivot // concatenate results while (!L.isEmpty()) S.enqueue(L.dequeue()); while (!E.isEmpty()) S.enqueue(E.dequeue()); while (!G.isEmpty()) S.enqueue(G.dequeue()); } Code Fragment 12.5: Quick-sort for a sequence S implemented as a queue.

Chapter 12. Sorting and Selection Running Time of Quick-Sort We can analyze the running time of quick-sort with the same technique used for merge-sort in Section 12.1.3. Namely, we can identify the time spent at each node of the quick-sort tree T and sum up the running times for all the nodes. Examining Code Fragment 12.5, we see that the divide step and the ﬁnal concatenation of quick-sort can be implemented in linear time. Thus, the time spent at a node v of T is proportional to the input size s(v) of v, deﬁned as the size of the sequence handled by the call of quick-sort associated with node v. Since subsequence E has at least one element (the pivot), the sum of the input sizes of the children of v is at most s(v)−1. Let si denote the sum of the input sizes of the nodes at depth i for a particular quick-sort tree T. Clearly, s0 = n, since the root r of T is associated with the entire sequence. Also, s1 ≤n−1, since the pivot is not propagated to the children of r. More generally, it must be that si < si−1 since the elements of the subsequences at depth i all come from distinct subsequences at depth i−1, and at least one element from depth i−1 does not propagate to depth i because it is in a set E (in fact, one element from each node at depth i−1 does not propagate to depth i). We can therefore bound the overall running time of an execution of quick-sort as O(n·h) where h is the overall height of the quick-sort tree T for that execution. Unfortunately, in the worst case, the height of a quick-sort tree is n−1, as observed in Section 12.2. Thus, quick-sort runs in O(n2) worst-case time. Paradoxically, if we choose the pivot as the last element of the sequence, this worst-case behavior occurs for problem instances when sorting should be easy—if the sequence is already sorted. Given its name, we would expect quick-sort to run quickly, and it often does in practice. The best case for quick-sort on a sequence of distinct elements occurs when subsequences L and G have roughly the same size. In that case, as we saw with merge-sort, the tree has height O(logn) and therefore quick-sort runs in O(nlogn) time; we leave the justiﬁcation of this fact as an exercise (R-12.12). More so, we can observe an O(nlogn) running time even if the split between L and G is not as perfect. For example, if every divide step caused one subsequence to have one-fourth of those elements and the other to have three-fourths of the elements, the height of the tree would remain O(logn) and thus the overall performance O(nlogn). We will see in the next section that introducing randomization in the choice of a pivot will makes quick-sort essentially behave in this way on average, with an expected running time that is O(nlogn).

12.2. Quick-Sort 12.2.1 Randomized Quick-Sort One common method for analyzing quick-sort is to assume that the pivot will always divide the sequence in a reasonably balanced manner. However, we feel such an assumption would presuppose knowledge about the input distribution that is typically not available. For example, we would have to assume that we will rarely be given “almost” sorted sequences to sort, which are actually common in many applications. Fortunately, this assumption is not needed in order for us to match our intuition to quick-sort’s behavior. In general, we desire some way of getting close to the best-case running time for quick-sort. The way to get close to the best-case running time, of course, is for the pivot to divide the input sequence S almost equally. If this outcome were to occur, then it would result in a running time that is asymptotically the same as the best-case running time. That is, having pivots close to the “middle” of the set of elements leads to an O(nlogn) running time for quick-sort. Picking Pivots at Random Since the goal of the partition step of the quick-sort method is to divide the sequence S with sufﬁcient balance, let us introduce randomization into the algorithm and pick as the pivot a random element of the input sequence. That is, instead of picking the pivot as the ﬁrst or last element of S, we pick an element of S at random as the pivot, keeping the rest of the algorithm unchanged. This variation of quick-sort is called randomized quick-sort. The following proposition shows that the expected running time of randomized quick-sort on a sequence with n elements is O(nlogn). This expectation is taken over all the possible random choices the algorithm makes, and is independent of any assumptions about the distribution of the possible input sequences the algorithm is likely to be given. Proposition 12.3: The expected running time of randomized quick-sort on a sequence S of size n is O(nlogn). Justiﬁcation: Let S be a sequence with n elements and let T be the binary tree associated with an execution of randomized quick-sort on S. First, we observe that the running time of the algorithm is proportional to the number of comparisons performed. We consider the recursive call associated with a node of T and observe that during the call, all comparisons are between the pivot element and another element of the input of the call. Thus, we can evaluate the total number of comparisons performed by the algorithm as ∑s∈SC(x), where C(x) is the number of comparisons involving x as a nonpivot element. Next, we will show that for every element x ∈S, the expected value of C(x) is O(logn). Since the expected value of a sum is the sum of the expected values of its terms, an O(logn) bound on the expected value of C(x) implies that randomized quick-sort runs in expected O(nlogn) time.

Chapter 12. Sorting and Selection To show that the expected value of C(x) is O(nlogn) for any x, we ﬁx an arbitrary element x and consider the path of nodes in the tree T associated with recursive calls for which x is part of the input sequence. (See Figure 12.13.) By deﬁnition, C(x) is equal to that path length, as x will take part in one nonpivot comparison per level of the tree until it is chosen as the pivot or is the only element that remains. Let nd denote the input size for the node of that path at depth d of tree T, for 0 ≤d ≤C(x). Since all elements are in the initial recursive call, n0 = n. We know that the input size for any recursive call is at least one less than the size of its parent, and thus that nd+1 ≤nd −1 for any d < C(x). In the worst case, this implies that C(x) ≤n−1, as the recursive process stops if nd = 1 or if x is chosen as the pivot. We can show the stronger claim that the expected value of C(x) is O(logn) based on the random selection of a pivot at each level. The choice of pivot at depth d of this path is considered “good” if nd+1 ≤3nd/4. The choice of a pivot will be good with probability at least 1/2, as there are at least nd/2 elements in the input that, if chosen as pivot, will result in at least nd/4 elements begin placed in each subproblem, thereby leaving x in a group with at most 3nd/4 elements. We conclude by noting that there can be at most log4/3 n such good pivot choices before x is isolated. Since a choice is good with probability at least 1/2, the expected number of recursive calls before achieving log4/3 n good choices is at most 2log4/3 n, which implies that C(x) is O(logn). With a more rigorous analysis, we can show that the running time of randomized quick-sort is O(nlogn) with high probability. (See Exercise C-12.55.) Figure 12.13: An illustration of the analysis of Proposition 12.3 for an execution of randomized quick-sort. We focus on element x = 31, which has value C(x) = 3, as it is the nonpivot element in a comparison with 50, 45, and 17. By our notation, n0 = 10, n1 = 6, n2 = 5, and n3 = 2, and the pivot choices of 50 and 17 are good.

12.2. Quick-Sort 12.2.2 Additional Optimizations for Quick-Sort An algorithm is in-place if it uses only a small amount of memory in addition to that needed for the original input. Our implementation of heap-sort, from Section 9.4.2, is an example of such an in-place sorting algorithm. Our implementation of quick-sort from Code Fragment 12.5 does not qualify as in-place because we use additional containers L, E, and G when dividing a sequence S within each recursive call. Quick-sort of an array-based sequence can be adapted to be in-place, and such an optimization is used in most deployed implementations. Performing the quick-sort algorithm in-place requires a bit of ingenuity, however, for we must use the input sequence itself to store the subsequences for all the recursive calls. We show algorithm quickSortInPlace, which performs in-place quick-sort of an array, in Code Fragment 12.6. In-place quick-sort modiﬁes the input sequence using element swapping and does not explicitly create subsequences. Instead, a subsequence of the input sequence is implicitly represented by a range of positions speciﬁed by a leftmost index a and a rightmost index b. The divide /∗∗Sort the subarray S[a..b] inclusive. ∗/ private static <K> void quickSortInPlace(K[ ] S, Comparator<K> comp, int a, int b) { if (a >= b) return; // subarray is trivially sorted int left = a; int right = b−1; K pivot = S[b]; K temp; // temp object used for swapping while (left <= right) { // scan until reaching value equal or larger than pivot (or right marker) while (left <= right && comp.compare(S[left], pivot) < 0) left++; // scan until reaching value equal or smaller than pivot (or left marker) while (left <= right && comp.compare(S[right], pivot) > 0) right−−; if (left <= right) { // indices did not strictly cross // so swap values and shrink range temp = S[left]; S[left] = S[right]; S[right] = temp; left++; right−−; } } // put pivot into its ﬁnal place (currently marked by left index) temp = S[left]; S[left] = S[b]; S[b] = temp; // make recursive calls quickSortInPlace(S, comp, a, left −1); quickSortInPlace(S, comp, left + 1, b); } Code Fragment 12.6: In-place quick-sort for an array S. The entire array can be sorted as quickSortInPlace(S, comp, 0, S.length−1).

Chapter 12. Sorting and Selection step is performed by scanning the array simultaneously using local variables left, which advances forward, and right, which advances backward, swapping pairs of elements that are in reverse order, as shown in Figure 12.14. When these two indices pass each other, the division step is complete and the algorithm completes by recurring on these two sublists. There is no explicit “combine” step, because the concatenation of the two sublists is implicit to the in-place use of the original list. It is worth noting that if a sequence has duplicate values, we are not explicitly creating three sublists L, E, and G, as in our original quick-sort description. We instead allow elements equal to the pivot (other than the pivot itself) to be dispersed across the two sublists. Exercise R-12.11 explores the subtlety of our implementation in the presence of duplicate keys, and Exercise C-12.34 describes an in-place algorithm that strictly partitions into three sublists L, E, and G. l r (a) l r (b) l r (c) r l (d) l,r (e) r < l (f) (g) Figure 12.14: Divide step of in-place quick-sort, using index l as shorthand for identiﬁer left, and index r as shorthand for identiﬁer right. Index l scans the sequence from left to right, and index r scans the sequence from right to left. A swap is performed when l is at an element as large as the pivot and r is at an element as small as the pivot. A ﬁnal swap with the pivot, in part (f), completes the divide step.

12.2. Quick-Sort Although the implementation we describe in this section for dividing the sequence into two pieces is in-place, we note that the complete quick-sort algorithm needs space for a stack proportional to the depth of the recursion tree, which in this case can be as large as n−1. Admittedly, the expected stack depth is O(logn), which is small compared to n. Nevertheless, a simple trick lets us guarantee the stack size is O(logn). The main idea is to design a nonrecursive version of in-place quick-sort using an explicit stack to iteratively process subproblems (each of which can be represented with a pair of indices marking subarray boundaries). Each iteration involves popping the top subproblem, splitting it in two (if it is big enough), and pushing the two new subproblems. The trick is that when pushing the new subproblems, we should ﬁrst push the larger subproblem and then the smaller one. In this way, the sizes of the subproblems will at least double as we go down the stack; hence, the stack can have depth at most O(logn). We leave the details of this implementation as an exercise (P-12.59). Pivot Selection Our implementation in this section blindly picks the last element as the pivot at each level of the quick-sort recursion. This leaves it susceptible to the Θ(n2)-time worst case, most notably when the original sequence is already sorted, reverse sorted, or nearly sorted. As described in Section 12.2.1, this can be improved upon by using a randomly chosen pivot for each partition step. In practice, another common technique for choosing a pivot is to use the median of tree values, taken respectively from the front, middle, and tail of the array. This median-of-three heuristic will more often choose a good pivot and computing a median of three may require lower overhead than selecting a pivot with a random number generator. For larger data sets, the median of more than three potential pivots might be computed. Hybrid Approaches Although quick-sort has very good performance on large data sets, it has rather high overhead on relatively small data sets. For example, the process of quicksorting a sequence of eight elements, as illustrated in Figures 12.10 through 12.12, involves considerable bookkeeping. In practice, a simple algorithm like insertionsort (Section 7.6) will execute faster when sorting such a short sequence. It is therefore common, in optimized sorting implementations, to use a hybrid approach, with a divide-and-conquer algorithm used until the size of a subsequence falls below some threshold (perhaps 50 elements); insertion-sort can be directly invoked upon portions with length below the threshold. We will further discuss such practical considerations in Section 12.4, when comparing the performance of various sorting algorithms.

Chapter 12. Sorting and Selection 12.3 Studying Sorting through an Algorithmic Lens Recapping our discussions on sorting to this point, we have described several methods with either a worst case or expected running time of O(nlogn) on an input sequence of size n. These methods include merge-sort and quick-sort, described in this chapter, as well as heap-sort (Section 9.4.2). In this section, we will study sorting as an algorithmic problem, addressing general issues about sorting algorithms. 12.3.1 Lower Bound for Sorting A natural ﬁrst question to ask is whether we can sort any faster than O(nlogn) time. Interestingly, if the computational primitive used by a sorting algorithm is the comparison of two elements, this is in fact the best we can do—comparison-based sorting has an Ω(nlogn) worst-case lower bound on its running time. (Recall the notation Ω(·) from Section 4.3.1.) To focus on the main cost of comparison-based sorting, let us only count comparisons, for the sake of a lower bound. Suppose we are given a sequence S = (x0,x1,...,xn−1) that we wish to sort, and assume that all the elements of S are distinct (this is not really a restriction since we are deriving a lower bound). We do not care if S is implemented as an array or a linked list, for the sake of our lower bound, since we are only counting comparisons. Each time a sorting algorithm compares two elements xi and xj (that is, it asks, “is xi < xj?”), there are two outcomes: “yes” or “no.” Based on the result of this comparison, the sorting algorithm may perform some internal calculations (which we are not counting here) and will eventually perform another comparison between two other elements of S, which again will have two outcomes. Therefore, we can represent a comparison-based sorting algorithm with a decision tree T (recall Example 8.5). That is, each internal node v in T corresponds to a comparison and the edges from position v to its children correspond to the computations resulting from either a “yes” or “no” answer. It is important to note that the hypothetical sorting algorithm in question probably has no explicit knowledge of the tree T. The tree simply represents all the possible sequences of comparisons that a sorting algorithm might make, starting from the ﬁrst comparison (associated with the root) and ending with the last comparison (associated with the parent of an external node). Each possible initial order, or permutation, of the elements in S will cause our hypothetical sorting algorithm to execute a series of comparisons, traversing a path in T from the root to some external node. Let us associate with each external node v in T, then, the set of permutations of S that cause our sorting algorithm to end up in v. The most important observation in our lower-bound argument is that each external node v in T can represent the sequence of comparisons for at most one permutation of S. The justiﬁcation for this claim is simple: If two different

12.3. Studying Sorting through an Algorithmic Lens permutations P1 and P2 of S are associated with the same external node, then there are at least two objects xi and xj, such that xi is before xj in P1 but xi is after xj in P2. At the same time, the output associated with v must be a speciﬁc reordering of S, with either xi or xj appearing before the other. But if P1 and P2 both cause the sorting algorithm to output the elements of S in this order, then that implies there is a way to trick the algorithm into outputting xi and xj in the wrong order. Since this cannot be allowed by a correct sorting algorithm, each external node of T must be associated with exactly one permutation of S. We use this property of the decision tree associated with a sorting algorithm to prove the following result: Proposition 12.4: The running time of any comparison-based algorithm for sorting an n-element sequence is Ω(nlogn) in the worst case. Justiﬁcation: The running time of a comparison-based sorting algorithm must be greater than or equal to the height of the decision tree T associated with this algorithm, as described above. (See Figure 12.15.) By the argument above, each external node in T must be associated with one permutation of S. Moreover, each permutation of S must result in a different external node of T. The number of permutations of n objects is n! = n(n −1)(n −2)···2 · 1. Thus, T must have at least n! external nodes. By Proposition 8.7, the height of T is at least log(n!). This immediately justiﬁes the proposition, because there are at least n/2 terms that are greater than or equal to n/2 in the product n!; hence, log(n!) ≥log n  n  = n 2 log n 2, which is Ω(nlogn). (i.e., worst-case running time) Minimum Height log(n!) n! xk < xl ? xg < xh ? xc < xd ? xm < xn ? xa < xb ? xe < xf ? xi < xj ? Figure 12.15: Visualizing the lower bound for comparison-based sorting.

Chapter 12. Sorting and Selection 12.3.2 Linear-Time Sorting: Bucket-Sort and Radix-Sort In the previous section, we showed that Ω(nlogn) time is necessary, in the worst case, to sort an n-element sequence with a comparison-based sorting algorithm. A natural question to ask, then, is whether there are other kinds of sorting algorithms that can be designed to run asymptotically faster than O(nlogn) time. Interestingly, such algorithms exist, but they require special assumptions about the input sequence to be sorted. Even so, such scenarios often arise in practice, such as when sorting integers from a known range or sorting character strings, so discussing them is worthwhile. In this section, we will consider the problem of sorting a sequence of entries, each a key-value pair, where the keys have a restricted type. Bucket-Sort Consider a sequence S of n entries whose keys are integers in the range [0,N −1], for some integer N ≥2, and suppose that S should be sorted according to the keys of the entries. In this case, it is possible to sort S in O(n+N) time. It might seem surprising, but this implies, for example, that if N is O(n), then we can sort S in O(n) time. Of course, the crucial point is that, because of the restrictive assumption about the format of the elements, we can avoid using comparisons. The main idea is to use an algorithm called bucket-sort, which is not based on comparisons, but on using keys as indices into a bucket array B that has cells indexed from 0 to N −1. An entry with key k is placed in the “bucket” B[k], which itself is a sequence (of entries with key k). After inserting each entry of the input sequence S into its bucket, we can put the entries back into S in sorted order by enumerating the contents of the buckets B[0],B[1],...,B[N −1] in order. We describe the bucket-sort algorithm in Code Fragment 12.7. Algorithm bucketSort(S): Input: Sequence S of entries with integer keys in the range [0,N−1] Output: Sequence S sorted in nondecreasing order of the keys let B be an array of n sequences, each of which is initially empty for each entry e in S do let k denote the key of e remove e from S and insert it at the end of bucket (sequence) B[k] for i = 0 to n−1 do for each entry e in sequence B[i] do remove e from B[i] and insert it at the end of S Code Fragment 12.7: Bucket-sort.

12.3. Studying Sorting through an Algorithmic Lens It is easy to see that bucket-sort runs in O(n + N) time and uses O(n + N) space. Hence, bucket-sort is efﬁcient when the range N of values for the keys is small compared to the sequence size n, say N = O(n) or N = O(nlogn). Still, its performance deteriorates as N grows compared to n. An important property of the bucket-sort algorithm is that it works correctly even if there are many different elements with the same key. Indeed, we described it in a way that anticipates such occurrences. Stable Sorting When sorting key-value pairs, an important issue is how equal keys are handled. Let S = ((k0,v0),...,(kn−1,vn−1)) be a sequence of such entries. We say that a sorting algorithm is stable if, for any two entries (ki,vi) and (kj,vj) of S such that ki = kj and (ki,vi) precedes (kj,vj) in S before sorting (that is, i < j), entry (ki,vi) also precedes entry (kj,vj) after sorting. Stability is important for a sorting algorithm because applications may want to preserve the initial order of elements with the same key. Our informal description of bucket-sort in Code Fragment 12.7 guarantees stability as long as we ensure that all sequences act as queues, with elements processed and removed from the front of a sequence and inserted at the back. That is, when initially placing elements of S into buckets, we should process S from front to back, and add each element to the end of its bucket. Subsequently, when transferring elements from the buckets back to S, we should process each B[i] from front to back, with those elements added to the end of S. Radix-Sort One of the reasons that stable sorting is so important is that it allows the bucketsort approach to be applied to more general contexts than to sort integers. Suppose, for example, that we want to sort entries with keys that are pairs (k,l), where k and l are integers in the range [0,N −1], for some integer N ≥2. In a context such as this, it is common to deﬁne an order on these keys using the lexicographic (dictionary) convention, where (k1,l1) < (k2,l2) if k1 < k2 or if k1 = k2 and l1 < l2 (see page 363). This is a pairwise version of the lexicographic comparison function, which can be applied to equal-length character strings, or to tuples of length d. The radix-sort algorithm sorts a sequence S of entries with keys that are pairs, by applying a stable bucket-sort on the sequence twice; ﬁrst using one component of the pair as the key when ordering and then using the second component. But which order is correct? Should we ﬁrst sort on the k’s (the ﬁrst component) and then on the l’s (the second component), or should it be the other way around?

Chapter 12. Sorting and Selection To gain intuition before answering this question, we consider the following example. Example 12.5: Consider the following sequence S (we show only the keys): S = ((3,3),(1,5),(2,5),(1,2),(2,3),(1,7),(3,2),(2,2)). If we sort S stably on the ﬁrst component, then we get the sequence S1 = ((1,5),(1,2),(1,7),(2,5),(2,3),(2,2),(3,3),(3,2)). If we then stably sort this sequence S1 using the second component, we get the sequence S1,2 = ((1,2),(2,2),(3,2),(2,3),(3,3),(1,5),(2,5),(1,7)), which is unfortunately not a sorted sequence. On the other hand, if we ﬁrst stably sort S using the second component, then we get the sequence S2 = ((1,2),(3,2),(2,2),(3,3),(2,3),(1,5),(2,5),(1,7)). If we then stably sort sequence S2 using the ﬁrst component, we get the sequence S2,1 = ((1,2),(1,5),(1,7),(2,2),(2,3),(2,5),(3,2),(3,3)), which is indeed sequence S lexicographically ordered. So, from this example, we are led to believe that we should ﬁrst sort using the second component and then again using the ﬁrst component. This intuition is exactly right. By ﬁrst stably sorting by the second component and then again by the ﬁrst component, we guarantee that if two entries are equal in the second sort (by the ﬁrst component), then their relative order in the starting sequence (which is sorted by the second component) is preserved. Thus, the resulting sequence is guaranteed to be sorted lexicographically every time. We leave to a simple exercise (R-12.19) the determination of how this approach can be extended to triples and other d-tuples of numbers. We can summarize this section as follows: Proposition 12.6: Let S be a sequence of n key-value pairs, each of which has a key (k1,k2,...,kd), where ki is an integer in the range [0,N −1] for some integer N ≥2. We can sort S lexicographically in time O(d(n+N)) using radix-sort. Radix-sort can be applied to any key that can be viewed as a composite of smaller pieces that are to be sorted lexicographically. For example, we can apply it to sort character strings of moderate length, as each individual character can be represented as an integer value. (Some care is needed to properly handle strings with varying lengths.)

12.4. Comparing Sorting Algorithms 12.4 Comparing Sorting Algorithms At this point, it might be useful for us to take a moment and consider all the algorithms we have studied in this book to sort an n-element sequence. Considering Running Time and Other Factors We have studied several methods, such as insertion-sort and selection-sort, that have O(n2)-time behavior in the average and worst case. We have also studied several methods with O(nlogn)-time behavior, including heap-sort, merge-sort, and quick-sort. Finally, the bucket-sort and radix-sort methods run in linear time for certain types of keys. Certainly, the selection-sort algorithm is a poor choice in any application, since it runs in O(n2) time even in the best case. But, of the remaining sorting algorithms, which is the best? As with many things in life, there is no clear “best” sorting algorithm from the remaining candidates. There are trade-offs involving efﬁciency, memory usage, and stability. The sorting algorithm best suited for a particular application depends on the properties of that application. In fact, the default sorting algorithm used by computing languages and systems has evolved greatly over time. We can offer some guidance and observations, therefore, based on the known properties of the “good” sorting algorithms. Insertion-Sort If implemented well, the running time of insertion-sort is O(n + m), where m is the number of inversions (that is, the number of pairs of elements out of order). Thus, insertion-sort is an excellent algorithm for sorting small sequences (say, less than 50 elements), because insertion-sort is simple to program, and small sequences necessarily have few inversions. Also, insertion-sort is quite effective for sorting sequences that are already “almost” sorted. By “almost,” we mean that the number of inversions is small. But the O(n2)-time performance of insertion-sort makes it a poor choice outside of these special contexts. Heap-Sort Heap-sort, on the other hand, runs in O(nlogn) time in the worst case, which is optimal for comparison-based sorting methods. Heap-sort can easily be made to execute in-place, and is a natural choice on small- and medium-sized sequences, when input data can ﬁt into main memory. However, heap-sort tends to be outperformed by both quick-sort and merge-sort on larger sequences. A standard heap-sort does not provide a stable sort, because of the swapping of elements.

Chapter 12. Sorting and Selection Quick-Sort Although its O(n2)-time worst-case performance makes quick-sort susceptible in real-time applications where we must make guarantees on the time needed to complete a sorting operation, we expect its performance to be O(nlogn) time, and experimental studies have shown that it outperforms both heap-sort and merge-sort on many tests. Quick-sort does not naturally provide a stable sort, due to the swapping of elements during the partitioning step. For decades quick-sort was the default choice for a general-purpose, in-memory sorting algorithm. Quick-sort was included as the qsort sorting utility provided in C language libraries, and was the basis for sorting utilities on Unix operating systems for many years. It has long been the standard algorithm for sorting arrays of primitive type in Java. (We discuss sorting of object types below.) Merge-Sort Merge-sort runs in O(nlogn) time in the worst case. It is quite difﬁcult to make merge-sort run in-place for arrays, and without that optimization the extra overhead of allocate a temporary array, and copying between the arrays is less attractive than in-place implementations of heap-sort and quick-sort for sequences that can ﬁt entirely in a computer’s main memory. Even so, merge-sort is an excellent algorithm for situations where the input is stratiﬁed across various levels of the computer’s memory hierarchy (e.g., cache, main memory, external memory). In these contexts, the way that merge-sort processes runs of data in long merge streams makes the best use of all the data brought as a block into a level of memory, thereby reducing the total number of memory transfers. The GNU sorting utility (and most current versions of the Linux operating system) relies on a multiway merge-sort variant. Tim-sort (designed by Tim Peters) is a hybrid approach that is essentially a bottom-up merge-sort that takes advantage of initial runs in the data while using insertion-sort to build additional runs. Tim-sort has been the standard sorting algorithm in Python since 2003, and it has become the default algorithm for sorting arrays of object types, as of Java SE 7. Bucket-Sort and Radix-Sort Finally, if an application involves sorting entries with small integer keys, character strings, or d-tuples of keys from a discrete range, then bucket-sort or radix-sort is an excellent choice, for it runs in O(d(n+N)) time, where [0,N −1] is the range of integer keys (and d = 1 for bucket sort). Thus, if d(n+N) is signiﬁcantly “below” the nlogn function, then this sorting method should run faster than even quick-sort, heap-sort, or merge-sort.

12.5. Selection 12.5 Selection As important as it is, sorting is not the only interesting problem dealing with a total order relation on a set of elements. There are a number of applications in which we are interested in identifying a single element in terms of its rank relative to the sorted order of the entire set. Examples include identifying the minimum and maximum elements, but we may also be interested in, say, identifying the median element, that is, the element such that half of the other elements are smaller and the remaining half are larger. In general, queries that ask for an element with a given rank are called order statistics. Deﬁning the Selection Problem In this section, we discuss the general order-statistic problem of selecting the k th smallest element from an unsorted collection of n comparable elements. This is known as the selection problem. Of course, we can solve this problem by sorting the collection and then indexing into the sorted sequence at index k −1. Using the best comparison-based sorting algorithms, this approach would take O(nlogn) time, which is obviously an overkill for the cases where k = 1 or k = n (or even k = 2, k = 3, k = n −1, or k = n −5), because we can easily solve the selection problem for these values of k in O(n) time. Thus, a natural question to ask is whether we can achieve an O(n) running time for all values of k (including the interesting case of ﬁnding the median, where k = ⌊n/2⌋). 12.5.1 Prune-and-Search We can indeed solve the selection problem in O(n) time for any value of k. Moreover, the technique we use to achieve this result involves an interesting algorithmic design pattern. This design pattern is known as prune-and-search or decreaseand-conquer. In applying this design pattern, we solve a given problem that is deﬁned on a collection of n objects by pruning away a fraction of the n objects and recursively solving the smaller problem. When we have ﬁnally reduced the problem to one deﬁned on a constant-sized collection of objects, we then solve the problem using some brute-force method. Returning back from all the recursive calls completes the construction. In some cases, we can avoid using recursion, in which case we simply iterate the prune-and-search reduction step until we can apply a brute-force method and stop. Incidentally, the binary search method described in Section 5.1.3 is an example of the prune-and-search design pattern.

Chapter 12. Sorting and Selection 12.5.2 Randomized Quick-Select In applying the prune-and-search pattern to ﬁnding the k th smallest element in an unordered sequence of n elements, we describe a simple and practical algorithm, known as randomized quick-select. This algorithm runs in O(n) expected time, taken over all possible random choices made by the algorithm; this expectation does not depend whatsoever on any randomness assumptions about the input distribution. We note though that randomized quick-select runs in O(n2) time in the worst case, the justiﬁcation of which is left as an exercise (R-12.25). We also provide an exercise (C-12.56) for modifying randomized quick-select to deﬁne a deterministic selection algorithm that runs in O(n) worst-case time. The existence of this deterministic algorithm is mostly of theoretical interest, however, since the constant factor hidden by the big-Oh notation is relatively large in that case. Suppose we are given an unsorted sequence S of n comparable elements together with an integer k ∈[1,n]. At a high level, the quick-select algorithm for ﬁnding the k th smallest element in S is similar to the randomized quick-sort algorithm described in Section 12.2.1. We pick a “pivot” element from S at random and use this to subdivide S into three subsequences L, E, and G, storing the elements of S less than, equal to, and greater than the pivot, respectively. In the prune step, we determine which of these subsets contains the desired element, based on the value of k and the sizes of those subsets. We then recur on the appropriate subset, noting that the desired element’s rank in the subset may differ from its rank in the full set. Pseudocode for randomized quick-select is shown in Code Fragment 12.8. Algorithm quickSelect(S,k): Input: Sequence S of n comparable elements, and an integer k ∈[1,n] Output: The k th smallest element of S if n == 1 then return the (ﬁrst) element of S. pick a random (pivot) element x of S and divide S into three sequences: • L, storing the elements in S less than x • E, storing the elements in S equal to x • G, storing the elements in S greater than x if k ≤|L| then return quickSelect(L,k) else if k ≤|L|+|E| then return x {each element in E is equal to x} else return quickSelect(G,k−|L|−|E|) {note the new selection parameter} Code Fragment 12.8: Randomized quick-select algorithm.

12.5. Selection 12.5.3 Analyzing Randomized Quick-Select Showing that randomized quick-select runs in O(n) time requires a simple probabilistic argument. The argument is based on the linearity of expectation, which states that if X and Y are random variables and c is a number, then E(X +Y) = E(X)+E(Y) and E(cX) = cE(X), where we use E(Z) to denote the expected value of the expression Z. Let t(n) be the running time of randomized quick-select on a sequence of size n. Since this algorithm depends on random events, its running time, t(n), is a random variable. We want to bound E(t(n)), the expected value of t(n). Say that a recursive call of our algorithm is “good” if it partitions S so that the size of each of L and G is at most 3n/4. Clearly, a recursive call is good with probability at least 1/2. Let g(n) denote the number of consecutive recursive calls we make, including the present one, before we get a good one. Then we can characterize t(n) using the following recurrence equation: t(n) ≤bn·g(n)+t(3n/4), where b ≥1 is a constant. Applying the linearity of expectation for n > 1, we get E (t(n)) ≤E (bn·g(n)+t(3n/4)) = bn·E (g(n)) +E (t(3n/4)). Since a recursive call is good with probability at least 1/2, and whether a recursive call is good or not is independent of its parent call being good, the expected value of g(n) is at most the expected number of times we must ﬂip a fair coin before it comes up “heads.” That is, E(g(n)) ≤2. Thus, if we let T(n) be shorthand for E(t(n)), then we can write the case for n > 1 as T(n) ≤T(3n/4)+2bn. To convert this relation into a closed form, let us iteratively apply this inequality assuming n is large. So, for example, after two applications, T(n) ≤T((3/4)2n)+2b(3/4)n+2bn. At this point, we should see that the general case is T(n) ≤2bn· ⌈log4/3 n⌉ ∑ i=0 (3/4)i. In other words, the expected running time is at most 2bn times a geometric sum whose base is a positive number less than 1. Thus, by Proposition 4.5, T(n) is O(n). Proposition 12.7: The expected running time of randomized quick-select on a sequence S of size n is O(n), assuming two elements of S can be compared in O(1) time.

Chapter 13. Text Processing 13.1 Abundance of Digitized Text Despite the wealth of multimedia information, text processing remains one of the dominant functions of computers. Computers are used to edit, store, and display documents, and to transport ﬁles over the Internet. Furthermore, digital systems are used to archive a wide range of textual information, and new data is being generated at a rapidly increasing pace. A large corpus can readily surpass a petabyte of data (which is equivalent to a thousand terabytes, or a million gigabytes). Common examples of digital collections that include textual information are: • Snapshots of the World Wide Web, as Internet document formats HTML and XML are primarily text formats, with added tags for multimedia content • All documents stored locally on a user’s computer • Email archives • Compilations of status updates on social networking sites such as Facebook • Feeds from microblogging sites such as Twitter and Tumblr These collections include written text from hundreds of international languages. Furthermore, there are large data sets (such as DNA) that can be viewed computationally as “strings” even though they are not language. In this chapter, we explore some of the fundamental algorithms that can be used to efﬁciently analyze and process large textual data sets. In addition to having interesting applications, text-processing algorithms also highlight some important algorithmic design patterns. We begin by examining the problem of searching for a pattern as a substring of a larger piece of text, for example, when searching for a word in a document. The pattern-matching problem gives rise to the brute-force method, which is often inefﬁcient but has wide applicability. We continue by describing more efﬁcient algorithms for solving the pattern-matching problem, and we examine several special-purpose data structures that can be used to better organize textual data in order to support more efﬁcient runtime queries. Because of the massive size of textual data sets, the issue of compression is important, both in minimizing the number of bits that need to be communicated through a network and to reduce the long-term storage requirements for archives. For text compression, we can apply the greedy method, which often allows us to approximate solutions to hard problems, and for some problems (such as in text compression) actually gives rise to optimal algorithms. Finally, we introduce dynamic programming, an algorithmic technique that can be applied in certain settings to solve a problem in polynomial time, which appears at ﬁrst to require exponential time to solve. We demonstrate the application on this technique to the problem of ﬁnding partial matches between strings that may be similar but not perfectly aligned. This problem arises when making suggestions for a misspelled word, or when trying to match related genetic samples.

13.1. Abundance of Digitized Text 13.1.1 Notations for Character Strings When discussing algorithms for text processing, we use character strings as a model for text. Character strings can come from a wide variety of sources, including scientiﬁc, linguistic, and Internet applications. Indeed, the following are examples of such strings: S = "CGTAAACTGCTTTAATCAAACGC" T = "http://www.wiley.com" The ﬁrst string, S, comes from DNA applications, and the second string, T, is the Internet address (URL) for the publisher of this book. To allow fairly general notions of a string in our algorithm descriptions, we only assume that characters of a string come from a known alphabet, which we denote as Σ. For example, in the context of DNA, there are four symbols in the standard alphabet, Σ = {A,C,G,T}. This alphabet Σ can, of course, be a subset of the ASCII or Unicode character sets, but it could also be something more general. Although we assume that an alphabet has a ﬁxed ﬁnite size, denoted as |Σ|, that size can be nontrivial, as with Java’s treatment of the Unicode alphabet, which allows more than a million distinct characters. We therefore consider the impact of |Σ| in our asymptotic analysis of text-processing algorithms. Java’s String class provides support for representing an immutable sequence of characters, while its StringBuilder class supports mutable character sequences (see Section 1.3). For much of this chapter, we rely on the more primitive representation of a string as a char array, primarily because it allows us to use the standard indexing notation S[i], rather than the String class’s more cumbersome syntax, S.charAt(i). In order to discuss pieces of a string, we denote as a substring of an n-character string P a string of the form P[i]P[i+1]P[i+2]···P[ j], for some 0 ≤i ≤j ≤n−1. To simplify the notation for referring to such substrings in prose, we let P[i.. j] denote the substring of P from index i to index j inclusive. We note that string is technically a substring of itself (taking i = 0 and j = n−1), so if we want to rule this out as a possibility, we must restrict the deﬁnition to proper substrings, which require that either i > 0 or j < n−1. We use the convention that if i > j, then P[i.. j] is equal to the null string, which has length 0. In addition, in order to distinguish some special kinds of substrings, let us refer to any substring of the form P[0.. j], for 0 ≤j ≤n −1, as a preﬁx of P, and any substring of the form P[i..n−1], for 0 ≤i ≤n−1, as a sufﬁx of P. For example, if we again take P to be the string of DNA given above, then "CGTAA" is a preﬁx of P, "CGC" is a sufﬁx of P, and "TTAATC" is a (proper) substring of P. Note that the null string is a preﬁx and a sufﬁx of any other string.

Chapter 13. Text Processing 13.2 Pattern-Matching Algorithms In the classic pattern-matching problem, we are given a text string of length n and a pattern string of length m ≤n, and must determine whether the pattern is a substring of the text. If so, we may want to ﬁnd the lowest index within the text at which the pattern begins, or perhaps all indices at which the pattern begins. The pattern-matching problem is inherent to many behaviors of Java’s String class, such as text.contains(pattern) and text.indexOf(pattern), and is a subtask of more complex string operations such as text.replace(pattern, substitute) and text.split(pattern). In this section, we present three pattern-matching algorithms, with increasing levels of sophistication. Our implementations report the index that begins the leftmost occurrence of the pattern, if found. For a failed search, we adopt the conventions of the indexOf method of Java’s String class, returning −1 as a sentinel. 13.2.1 Brute Force The brute-force algorithmic design pattern is a powerful technique for algorithm design when we have something we wish to search for or when we wish to optimize some function. When applying this technique in a general situation, we typically enumerate all possible conﬁgurations of the inputs involved and pick the best of all these enumerated conﬁgurations. In applying this technique to design a brute-force pattern-matching algorithm, we derive what is probably the ﬁrst algorithm that we might think of for solving the problem—we simply test all the possible placements of the pattern relative to the text. An implementation of this algorithm is shown in Code Fragment 13.1. /∗∗Returns the lowest index at which substring pattern begins in text (or else −1).∗/ public static int ﬁndBrute(char[ ] text, char[ ] pattern) { int n = text.length; int m = pattern.length; for (int i=0; i <= n −m; i++) { // try every starting index within text int k = 0; // k is index into pattern while (k < m && text[i+k] == pattern[k]) // kth character of pattern matches k++; if (k == m) // if we reach the end of the pattern, return i; // substring text[i..i+m-1] is a match } return −1; // search failed } Code Fragment 13.1: An implementation of the brute-force pattern-matching algorithm. (We use character arrays rather than strings to simplify indexing notation.)

13.2. Pattern-Matching Algorithms Performance The analysis of the brute-force pattern-matching algorithm could not be simpler. It consists of two nested loops, with the outer loop indexing through all possible starting indices of the pattern in the text, and the inner loop indexing through each character of the pattern, comparing it to its potentially corresponding character in the text. Thus, the correctness of the brute-force pattern-matching algorithm follows immediately from this exhaustive search approach. The running time of brute-force pattern matching in the worst case is not good, however, because we can perform up to m character comparisons for each candidate alignment of the pattern within the text. Referring to Code Fragment 13.1, we see that the outer for loop is executed at most n−m+1 times, and the inner while loop is executed at most m times. Thus, the worst-case running time of the brute-force method is O(nm). Example 13.1: Suppose we are given the text string text = "abacaabaccabacabaabb" and the pattern string pattern = "abacab" Figure 13.1 illustrates the execution of the brute-force pattern-matching algorithm on this selection of text and pattern. a b a b c c Text: 11 comparisons not shown b a a a b c b Pattern: b a a a a b c a a b a a a b c b c b a a a b b a a a b a c c a b a a c b a Figure 13.1: Example run of the brute-force pattern-matching algorithm. The algorithm performs 27 character comparisons, indicated above with numerical labels.

Chapter 13. Text Processing 13.2.2 The Boyer-Moore Algorithm At ﬁrst, it might seem that it is always necessary to examine every character in the text in order to locate a pattern as a substring or to rule out its existence. But this is not always the case. The Boyer-Moore pattern-matching algorithm, which we will study in this section, can sometimes avoid examining a signiﬁcant fraction of the character in the text. In this section, we will describe a simpliﬁed version of the original algorithm by Boyer and Moore. The main idea of the Boyer-Moore algorithm is to improve the running time of the brute-force algorithm by adding two potentially time-saving heuristics. Roughly stated, these heuristics are as follows: Looking-Glass Heuristic: When testing a possible placement of the pattern against the text, perform the comparisons against the pattern from right-to-left. Character-Jump Heuristic: During the testing of a possible placement of the pattern within the text, a mismatch of character text[i]=c with the corresponding character pattern[k] is handled as follows. If c is not contained anywhere in the pattern, then shift the pattern completely past text[i] = c. Otherwise, shift the pattern until an occurrence of character c gets aligned with text[i]. We will formalize these heuristics shortly, but at an intuitive level, they work as an integrated team to allow us to avoid comparisons with whole groups of characters in the text. In particular, when a mismatch is found near the right end of the pattern, we may end up realigning the pattern beyond the mismatch, without ever examining several characters of the text preceding the mismatch. For example, Figure 13.2 demonstrates a few simple applications of these heuristics. Notice that when the characters e and i mismatch at the right end of the original placement of the pattern, we slide the pattern beyond the mismatched character, without ever examining the ﬁrst four characters of the text. s i · Pattern: · · · · · s · · · e · · · · · · Text: · · · h u s s i h u s s i h u s Figure 13.2: A simple example demonstrating the intuition of the Boyer-Moore pattern-matching algorithm. The original comparison results in a mismatch with character e of the text. Because that character is nowhere in the pattern, the entire pattern is shifted beyond its location. The second comparison is also a mismatch, but the mismatched character s occurs elsewhere in the pattern. The pattern is then shifted so that its last occurrence of s is aligned with the corresponding s in the text. The remainder of the process is not illustrated in this ﬁgure.

13.2. Pattern-Matching Algorithms The example of Figure 13.2 is rather basic, because it only involves mismatches with the last character of the pattern. More generally, when a match is found for that last character, the algorithm continues by trying to extend the match with the second-to-last character of the pattern in its current alignment. That process continues until either matching the entire pattern, or ﬁnding a mismatch at some interior position of the pattern. If a mismatch is found, and the mismatched character of the text does not occur in the pattern, we shift the entire pattern beyond that location, as originally illustrated in Figure 13.2. If the mismatched character occurs elsewhere in the pattern, we must consider two possible subcases depending on whether its last occurrence is before or after the character of the pattern that was mismatched. Those two cases are illustrated in Figure 13.3. In the case of Figure 13.3(b), we slide the pattern only one unit. It would be more productive to slide it rightward until ﬁnding another occurrence of mismatched character text[i] in the pattern, but we do not wish to take time to search (a) · · b · · k j i′ · a · · · · b · · j +1 m−(j+1) Text: Pattern: · · · · · a · · · · · · · · · · i a · (b) · k j i′ · · b · · a · · b · · a k m−k Text: Pattern: · · · · · · · · a · · · · · · · · · · i Figure 13.3: Additional rules for the character-jump heuristic of the Boyer-Moore algorithm. We let i represent the index of the mismatched character in the text, k represent the corresponding index in the pattern, and j represent the index of the last occurrence of text[i] within the pattern. We distinguish two cases: (a) j < k, in which case we shift the pattern by k −j units, and thus, index i advances by m −( j + 1) units; (b) j > k, in which case we shift the pattern by one unit, and index i advances by m−k units.

Chapter 13. Text Processing for another occurrence. The efﬁciency of the Boyer-Moore algorithm relies on quickly determining where a mismatched character occurs elsewhere in the pattern. In particular, we deﬁne a function last(c) as • If c is in the pattern, last(c) is the index of the last (rightmost) occurrence of c in the pattern. Otherwise, we conventionally deﬁne last(c) = −1. If we assume that the alphabet is of ﬁxed, ﬁnite size, and that characters can be converted to indices of an array (for example, by using their character code), the last function can be easily implemented as a lookup table with worst-case O(1)- time access to the value last(c). However, the table would have length equal to the size of the alphabet (rather than the size of the pattern), and time would be required to initialize the entire table. We prefer to use a hash table to represent the last function, with only those characters from the pattern occurring in the map. The space usage for this approach is proportional to the number of distinct alphabet symbols that occur in the pattern, and thus O(max(m,|Σ|)). The expected lookup time remains O(1) (as does the worst-case, if we consider |Σ| a constant). Our complete implementation of the Boyer-Moore pattern-matching algorithm is given in Code Fragment 13.2. /∗∗Returns the lowest index at which substring pattern begins in text (or else −1).∗/ public static int ﬁndBoyerMoore(char[ ] text, char[ ] pattern) { int n = text.length; int m = pattern.length; if (m == 0) return 0; // trivial search for empty string Map<Character,Integer> last = new HashMap<>(); // the 'last' map for (int i=0; i < n; i++) last.put(text[i], −1); // set −1 as default for all text characters for (int k=0; k < m; k++) last.put(pattern[k], k); // rightmost occurrence in pattern is last // start with the end of the pattern aligned at index m−1 of the text int i = m−1; // an index into the text int k = m−1; // an index into the pattern while (i < n) { if (text[i] == pattern[k]) { // a matching character if (k == 0) return i; // entire pattern has been found i−−; // otherwise, examine previous k−−; // characters of text/pattern } else { i += m −Math.min(k, 1 + last.get(text[i])); // case analysis for jump step k = m −1; // restart at end of pattern } } return −1; // pattern was never found } Code Fragment 13.2: An implementation of the Boyer-Moore algorithm.

13.2. Pattern-Matching Algorithms The correctness of the Boyer-Moore pattern-matching algorithm follows from the fact that each time the method makes a shift, it is guaranteed not to “skip” over any possible matches. For last(c) is the location of the last occurrence of c in the pattern. In Figure 13.4, we illustrate the execution of the Boyer-Moore patternmatching algorithm on an input string similar to Example 13.1. c a b c d last(c) −1 a c d a b a a c b a a b c Text: Pattern: b a a a b c b a a a b c b a a a b c b a a a b c b a a a b c b a a a b c b b a a a b a Figure 13.4: An illustration of the Boyer-Moore pattern-matching algorithm, including a summary of the last(c) function. The algorithm performs 13 character comparisons, which are indicated with numerical labels. Performance If using a traditional lookup table, the worst-case running time of the Boyer-Moore algorithm is O(nm + |Σ|). The computation of the last function takes O(m + |Σ|) time, although the dependence on |Σ| is removed if using a hash table. The actual search for the pattern takes O(nm) time in the worst case—the same as the bruteforce algorithm. An example that achieves the worst case for Boyer-Moore is text = n z }| { aaaaaa··· a pattern = b m−1 z }| { aa···a The worst-case performance, however, is unlikely to be achieved for English text; in that case, the Boyer-Moore algorithm is often able to skip large portions of text. Experimental evidence on English text shows that the average number of comparisons done per character is 0.24 for a ﬁve-character pattern string. We have actually presented a simpliﬁed version of the Boyer-Moore algorithm. The original algorithm achieves worst-case running time O(n+ m + |Σ|) by using an alternative shift heuristic for a partially matched text string, whenever it shifts the pattern more than the character-jump heuristic. This alternative shift heuristic is based on applying the main idea from the Knuth-Morris-Pratt pattern-matching algorithm, which we discuss next.

Chapter 13. Text Processing 13.2.3 The Knuth-Morris-Pratt Algorithm In examining the worst-case performances of the brute-force and Boyer-Moore pattern-matching algorithms on speciﬁc instances of the problem, such as that given in Example 13.1, we should notice a major inefﬁciency (at least in the worst case). For a certain alignment of the pattern, if we ﬁnd several matching characters but then detect a mismatch, we ignore all the information gained by the successful comparisons after restarting with the next incremental placement of the pattern. The Knuth-Morris-Pratt (or “KMP”) algorithm, discussed in this section, avoids this waste of information and, in so doing, it achieves a running time of O(n+m), which is asymptotically optimal. That is, in the worst case any pattern-matching algorithm will have to examine all the characters of the text and all the characters of the pattern at least once. The main idea of the KMP algorithm is to precompute self-overlaps between portions of the pattern so that when a mismatch occurs at one location, we immediately know the maximum amount to shift the pattern before continuing the search. A motivating example is shown in Figure 13.5. a · · · · · · · · · · a m a m t o n i m a a g a l t o n i m a a g a l · t a m t o n i m a a g a l Text: Pattern: a a m a g l a m c Figure 13.5: A motivating example for the Knuth-Morris-Pratt algorithm. If a mismatch occurs at the indicated location, the pattern could be shifted to the second alignment, without explicit need to recheck the partial match with the preﬁx ama. If the mismatched character is not an l, then the next potential alignment of the pattern can take advantage of the common a. The Failure Function To implement the KMP algorithm, we will precompute a failure function, f, that indicates the proper shift of the pattern upon a failed comparison. Speciﬁcally, the failure function f(k) is deﬁned as the length of the longest preﬁx of the pattern that is a sufﬁx of the substring pattern[1..k] (note that we did not include pattern[0] here, since we will shift at least one unit). Intuitively, if we ﬁnd a mismatch upon character pattern[k+1], the function f(k) tells us how many of the immediately preceding characters can be reused to restart the pattern. Example 13.2 describes the value of the failure function for the example pattern from Figure 13.5.

13.2. Pattern-Matching Algorithms Example 13.2: Consider the pattern "amalgamation" from Figure 13.5. The Knuth-Morris-Pratt (KMP) failure function, f(k), for the string P is as shown in the following table: k P[k] a m a l g a m a t i o n f(k) Implementation Our implementation of the KMP pattern-matching algorithm is shown in Code Fragment 13.3. It relies on a utility method, computeFailKMP, discussed on the next page, to compute the failure function efﬁciently. The main part of the KMP algorithm is its while loop, each iteration of which performs a comparison between the character at index j in the text and the character at index k in the pattern. If the outcome of this comparison is a match, the algorithm moves on to the next characters in both (or reports a match if reaching the end of the pattern). If the comparison failed, the algorithm consults the failure function for a new candidate character in the pattern, or starts over with the next index in the text if failing on the ﬁrst character of the pattern (since nothing can be reused). /∗∗Returns the lowest index at which substring pattern begins in text (or else −1).∗/ public static int ﬁndKMP(char[ ] text, char[ ] pattern) { int n = text.length; int m = pattern.length; if (m == 0) return 0; // trivial search for empty string int[ ] fail = computeFailKMP(pattern); // computed by private utility int j = 0; // index into text int k = 0; // index into pattern while (j < n) { if (text[j] == pattern[k]) { // pattern[0..k] matched thus far if (k == m −1) return j −m + 1; // match is complete j++; // otherwise, try to extend match k++; } else if (k > 0) k = fail[k−1]; // reuse suﬃx of P[0..k-1] else j++; } return −1; // reached end without match } Code Fragment 13.3: An implementation of the KMP pattern-matching algorithm. The computeFailKMP utility method is given in Code Fragment 13.4.

Chapter 13. Text Processing Constructing the KMP Failure Function To construct the failure function, we use the method shown in Code Fragment 13.4, which is a “bootstrapping” process that compares the pattern to itself as in the KMP algorithm. Each time we have two characters that match, we set f( j) = k+1. Note that since we have j > k throughout the execution of the algorithm, f(k −1) is always well deﬁned when we need to use it. private static int[ ] computeFailKMP(char[ ] pattern) { int m = pattern.length; int[ ] fail = new int[m]; // by default, all overlaps are zero int j = 1; int k = 0; while (j < m) { // compute fail[j] during this pass, if nonzero if (pattern[j] == pattern[k]) { // k + 1 characters match thus far fail[j] = k + 1; j++; k++; } else if (k > 0) // k follows a matching preﬁx k = fail[k−1]; else // no match found starting at j j++; } return fail; } Code Fragment 13.4: An implementation of the computeFailKMP utility in support of the KMP pattern-matching algorithm. Note how the algorithm uses the previous values of the failure function to efﬁciently compute new values. Performance Excluding the computation of the failure function, the running time of the KMP algorithm is clearly proportional to the number of iterations of the while loop. For the sake of the analysis, let us deﬁne s = j −k. Intuitively, s is the total amount by which the pattern has been shifted with respect to the text. Note that throughout the execution of the algorithm, we have s ≤n. One of the following three cases occurs at each iteration of the loop. • If text[ j] = pattern[k], then j and k each increase by 1, thus s is unchanged. • If text[ j] ̸= pattern[k] and k > 0, then j does not change and s increases by at least 1, since in this case s changes from j −k to j −f(k −1); note that this is an addition of k−f(k −1), which is positive because f(k −1) < k. • If text[ j] ̸= pattern[k] and k = 0, then j increases by 1 and s increases by 1, since k does not change.

13.2. Pattern-Matching Algorithms Thus, at each iteration of the loop, either j or s increases by at least 1 (possibly both); hence, the total number of iterations of the while loop in the KMP patternmatching algorithm is at most 2n. Achieving this bound, of course, assumes that we have already computed the failure function for the pattern. The algorithm for computing the failure function runs in O(m) time. Its analysis is analogous to that of the main KMP algorithm, yet with a pattern of length m compared to itself. Thus, we have: Proposition 13.3: The Knuth-Morris-Pratt algorithm performs pattern matching on a text string of length n and a pattern string of length m in O(n+m) time. The correctness of this algorithm follows from the deﬁnition of the failure function. Any comparisons that are skipped are actually unnecessary, for the failure function guarantees that all the ignored comparisons are redundant—they would involve comparing the same matching characters over again. In Figure 13.6, we illustrate the execution of the KMP pattern-matching algorithm on the same input strings as in Example 13.1. Note the use of the failure function to avoid redoing one of the comparisons between a character of the pattern and a character of the text. Also note that the algorithm performs fewer overall comparisons than the brute-force algorithm run on the same strings (Figure 13.1). Failure function: k pattern[k] a b a c a b f(k) a a b c b a a a b c no comparison performed Text: Pattern: b a a a b c b b a a a b a c c a b a a c b a a b c b a a a b c b a a a b c 9 10 11 12 b a a Figure 13.6: An illustration of the KMP pattern-matching algorithm. The primary algorithm performs 19 character comparisons, which are indicated with numerical labels. (Additional comparisons would be performed during the computation of the failure function.)

Chapter 13. Text Processing 13.3 Tries The pattern-matching algorithms presented in Section 13.2 speed up the search in a text by preprocessing the pattern (to compute the last function in the Boyer-Moore algorithm or the failure function in the Knuth-Morris-Pratt algorithm). In this section, we take a complementary approach, namely, we present string searching algorithms that preprocess the text, rather than the pattern. This approach is suitable for applications in which many queries are performed on a ﬁxed text, so that the initial cost of preprocessing the text is compensated by a speedup in each subsequent query (for example, a website that offers pattern matching in Shakespeare’s Hamlet or a search engine that offers Web pages containing the term Hamlet). A trie (pronounced “try”) is a tree-based data structure for storing strings in order to support fast pattern matching. The main application for tries is in information retrieval. Indeed, the name “trie” comes from the word “retrieval.” In an information retrieval application, such as a search for a certain DNA sequence in a genomic database, we are given a collection S of strings, all deﬁned using the same alphabet. The primary query operations that tries support are pattern matching and preﬁx matching. The latter operation involves being given a string X, and looking for all the strings in S that being with X. 13.3.1 Standard Tries Let S be a set of s strings from alphabet Σ such that no string in S is a preﬁx of another string. A standard trie for S is an ordered tree T with the following properties (see Figure 13.7): • Each node of T, except the root, is labeled with a character of Σ. • The children of an internal node of T have distinct labels. • T has s leaves, each associated with a string of S, such that the concatenation of the labels of the nodes on the path from the root to a leaf v of T yields the string of S associated with v. Thus, a trie T represents the strings of S with paths from the root to the leaves of T. Note the importance of assuming that no string in S is a preﬁx of another string. This ensures that each string of S is uniquely associated with a leaf of T. (This is similar to the restriction for preﬁx codes with Huffman coding, as described in Section 13.4.) We can always satisfy this assumption by adding a special character that is not in the original alphabet Σ at the end of each string. An internal node in a standard trie T can have anywhere between 1 and |Σ| children. There is an edge going from the root r to one of its children for each character that is ﬁrst in some string in the collection S. In addition, a path from the root of T to an internal node v at depth k corresponds to a k-character preﬁx

13.3. Tries b e l i l d l y u e c k o a r l s l t l p Figure 13.7: Standard trie for the strings {bear, bell, bid, bull, buy, sell, stock, stop}. X[0..k−1] of a string X of S. In fact, for each character c that can follow the preﬁx X[0..k −1] in a string of the set S, there is a child of v labeled with character c. In this way, a trie concisely stores the common preﬁxes that exist among a set of strings. As a special case, if there are only two characters in the alphabet, then the trie is essentially a binary tree, with some internal nodes possibly having only one child (that is, it may be an improper binary tree). In general, although it is possible that an internal node has up to |Σ| children, in practice the average degree of such nodes is likely to be much smaller. For example, the trie shown in Figure 13.7 has several internal nodes with only one child. On larger data sets, the average degree of nodes is likely to get smaller at greater depths of the tree, because there may be fewer strings sharing the common preﬁx, and thus fewer continuations of that pattern. Furthermore, in many languages, there will be character combinations that are unlikely to naturally occur. The following proposition provides some important structural properties of a standard trie: Proposition 13.4: A standard trie storing a collection S of s strings of total length n from an alphabet Σ has the following properties: • The height of T is equal to the length of the longest string in S. • Every internal node of T has at most |Σ| children. • T has s leaves. • The number of nodes of T is at most n+1. The worst case for the number of nodes of a trie occurs when no two strings share a common nonempty preﬁx; that is, except for the root, all internal nodes have one child.

Chapter 13. Text Processing A trie T for a set S of strings can be used to implement a set or map whose keys are the strings of S. Namely, we perform a search in T for a string X by tracing down from the root the path indicated by the characters in X. If this path can be traced and terminates at a leaf node, then we know X is a string in S. For example, in the trie in Figure 13.7, tracing the path for “bull” ends up at a leaf. If the path cannot be traced or the path can be traced but terminates at an internal node, then X is not a string in S. In the example in Figure 13.7, the path for “bet” cannot be traced and the path for “be” ends at an internal node. Neither such word is in the set S. It is easy to see that the running time of the search for a string of length m is O(m · |Σ|), because we visit at most m + 1 nodes of T and we spend O(|Σ|) time at each node determining the child having the subsequent character as a label. The O(|Σ|) upper bound on the time to locate a child with a given label is achievable, even if the children of a node are unordered, since there are at most |Σ| children. We can improve the time spent at a node to be O(log|Σ|) or expected O(1), by mapping characters to children using a secondary search table or hash table at each node, or by using a direct lookup table of size |Σ| at each node, if |Σ| is sufﬁciently small (as is the case for DNA strings). For these reasons, we typically expect a search for a string of length m to run in O(m) time. From the discussion above, it follows that we can use a trie to perform a special type of pattern matching, called word matching, where we want to determine whether a given pattern matches one of the words of the text exactly. Word matching differs from standard pattern matching because the pattern cannot match an arbitrary substring of the text—only one of its words. To accomplish this, each word of the original document must be added to the trie. (See Figure 13.8.) A simple extension of this scheme supports preﬁx-matching queries. However, arbitrary occurrences of the pattern in the text (for example, the pattern is a proper sufﬁx of a word or spans two words) cannot be efﬁciently performed. To construct a standard trie for a set S of strings, we can use an incremental algorithm that inserts the strings one at a time. Recall the assumption that no string of S is a preﬁx of another string. To insert a string X into the current trie T, we trace the path associated with X in T, creating a new chain of nodes to store the remaining characters of X when we get stuck. The running time to insert X with length m is similar to a search, with worst-case O(m·|Σ|) performance, or expected O(m) if using secondary hash tables at each node. Thus, constructing the entire trie for set S takes expected O(n) time, where n is the total length of the strings of S. There is a potential space inefﬁciency in the standard trie that has prompted the development of the compressed trie, which is also known (for historical reasons) as the Patricia trie. Namely, there are potentially a lot of nodes in the standard trie that have only one child, and the existence of such nodes is a waste. We discuss the compressed trie next.

13.3. Tries e a b e a r ? s e l l 10 11 12 13 14 15 16 s t o c k 17 18 19 20 21 ! s e e a b u l l ? 23 24 25 26 27 28 29 30 31 32 33 34 b u y s t o c k ! 35 36 37 38 39 40 41 42 43 44 45 b i d 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 s t o c k ! b i d s t o c k ! h e a r t h e b e l l ? s t o p ! 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 s e (a) 0,24 l l p 17,40,51,62 l e e c k o a r h s t e a r b e i d l 47,58 u y l l (b) Figure 13.8: Word matching with a standard trie: (a) text to be searched (articles and prepositions, which are also known as stop words, excluded); (b) standard trie for the words in the text, with leaves augmented with indications of the index at which the given work begins in the text. For example, the leaf for the word “stock” notes that the word begins at indices 17, 40, 51, and 62 of the text.

Chapter 13. Text Processing 13.3.2 Compressed Tries A compressed trie is similar to a standard trie but it ensures that each internal node in the trie has at least two children. It enforces this rule by compressing chains of single-child nodes into individual edges. (See Figure 13.9.) Let T be a standard trie. We say that an internal node v of T is redundant if v has one child and is not the root. For example, the trie of Figure 13.7 has eight redundant nodes. Let us also say that a chain of k ≥2 edges, (v0,v1)(v1,v2)···(vk−1,vk), is redundant if: • vi is redundant for i = 1,...,k −1. • v0 and vk are not redundant. We can transform T into a compressed trie by replacing each redundant chain (v0,v1)···(vk−1,vk) of k ≥2 edges into a single edge (v0,vk), relabeling vk with the concatenation of the labels of nodes v1,...,vk. s to p b ck e id ar y ll u ll ell Figure 13.9: Compressed trie for the strings {bear, bell, bid, bull, buy, sell, stock, stop}. (Compare this with the standard trie shown in Figure 13.7.) Notice that, in addition to compression at the leaves, the internal node with label “to” is shared by words “stock” and “stop”. Thus, nodes in a compressed trie are labeled with strings, which are substrings of strings in the collection, rather than with individual characters. The advantage of a compressed trie over a standard trie is that the number of nodes of the compressed trie is proportional to the number of strings and not to their total length, as shown in the following proposition (compare with Proposition 13.4). Proposition 13.5: A compressed trie storing a collection S of s strings from an alphabet of size d has the following properties: • Every internal node of T has at least two children and most d children. • T has s leaves nodes. • The number of nodes of T is O(s).

13.3. Tries The attentive reader may wonder whether the compression of paths provides any signiﬁcant advantage, since it is offset by a corresponding expansion of the node labels. Indeed, a compressed trie is truly advantageous only when it is used as an auxiliary index structure over a collection of strings already stored in a primary structure, and is not required to actually store all the characters of the strings in the collection. Suppose, for example, that the collection S of strings is an array of strings S[0], S[1], ..., S[s −1]. Instead of storing the label X of a node explicitly, we represent it implicitly by a combination of three integers (i, j, k), such that X = S[i][ j..k]; that is, X is the substring of S[i] consisting of the characters from the j th to the k th inclusive. (See the example in Figure 13.10. Also compare with the standard trie of Figure 13.8.) S[2] = S[3] = S[4] = S[5] = S[6] = S[9] = S[8] = S[7] = S[0] = S[1] = k t c o s e l l b e r a s e e l l b u l l r b u y h b i d e s t p o a b e s (a) 5, 2, 2 3, 1, 2 0, 2, 2 3, 3, 4 0, 1, 1 7, 0, 3 1, 0, 0 9, 3, 3 0, 0, 0 6, 1, 2 4, 1, 1 4, 2, 3 1, 1, 1 1, 2, 3 8, 2, 3 2, 2, 3 (b) Figure 13.10: (a) Collection S of strings stored in an array. (b) Compact representation of the compressed trie for S. This additional compression scheme allows us to reduce the total space for the trie itself from O(n) for the standard trie to O(s) for the compressed trie, where n is the total length of the strings in S and s is the number of strings in S. We must still store the different strings in S, of course, but we nevertheless reduce the space for the trie. Searching in a compressed trie is not necessarily faster than in a standard tree, since there is still need to compare every character of the desired pattern with the potentially multicharacter labels while traversing paths in the trie.

Chapter 13. Text Processing 13.3.3 Suﬃx Tries One of the primary applications for tries is for the case when the strings in the collection S are all the sufﬁxes of a string X. Such a trie is called the sufﬁx trie (also known as a sufﬁx tree or position tree) of string X. For example, Figure 13.11a shows the sufﬁx trie for the eight sufﬁxes of string “minimize.” For a sufﬁx trie, the compact representation presented in the previous section can be further simpliﬁed. Namely, the label of each vertex is a pair “j..k” indicating the string X[ j..k]. (See Figure 13.11b.) To satisfy the rule that no sufﬁx of X is a preﬁx of another sufﬁx, we can add a special character, denoted with $, that is not in the original alphabet Σ at the end of X (and thus to every sufﬁx). That is, if string X has length n, we build a trie for the set of n strings X[ j..n−1]$, for j = 0,...,n−1. Saving Space Using a sufﬁx trie allows us to save space over a standard trie by using several space compression techniques, including those used for the compressed trie. The advantage of the compact representation of tries now becomes apparent for sufﬁx tries. Since the total length of the sufﬁxes of a string X of length n is 1+2+···+n = n(n+1) , storing all the sufﬁxes of X explicitly would take O(n2) space. Even so, the sufﬁx trie represents these strings implicitly in O(n) space, as formally stated in the following proposition. Proposition 13.6: The compact representation of a sufﬁx trie T for a string X of length n uses O(n) space. Construction We can construct the sufﬁx trie for a string of length n with an incremental algorithm like the one given in Section 13.3.1. This construction takes O(|Σ|n2) time because the total length of the sufﬁxes is quadratic in n. However, the (compact) sufﬁx trie for a string of length n can be constructed in O(n) time with a specialized algorithm, different from the one for general tries. This linear-time construction algorithm is fairly complex, however, and is not reported here. Still, we can take advantage of the existence of this fast construction algorithm when we want to use a sufﬁx trie to solve other problems.

13.3. Tries e ze ze mize i nimize ze nimize mi nimize (a) 0..1 6..7 6..7 2..7 2..7 2..7 1..1 6..7 7..7 4..7 e m i n i m i z (b) Figure 13.11: (a) Sufﬁx trie T for the string X = "minimize". (b) Compact representation of T, where pair j..k denotes the substring X[ j..k] in the reference string. Using a Suﬃx Trie The sufﬁx trie T for a string X can be used to efﬁciently perform pattern-matching queries on text X. Namely, we can determine whether a pattern is a substring of X by trying to trace a path associated with P in T. P is a substring of X if and only if such a path can be traced. The search down the trie T assumes that nodes in T store some additional information, with respect to the compact representation of the sufﬁx trie: If node v has label j..k and Y is the string of length y associated with the path from the root to v (included), then X[k−y+1..k] = Y. This property ensures that we can compute the start index of the pattern in the text when a match occurs in O(m) time.

Chapter 13. Text Processing 13.3.4 Search Engine Indexing The World Wide Web contains a huge collection of text documents (Web pages). Information about these pages are gathered by a program called a Web crawler, which then stores this information in a special dictionary database. A Web search engine allows users to retrieve relevant information from this database, thereby identifying relevant pages on the Web containing given keywords. In this section, we will present a simpliﬁed model of a search engine. Inverted Files The core information stored by a search engine is a map, called an inverted index or inverted ﬁle, storing key-value pairs (w,L), where w is a word and L is a collection of pages containing word w. The keys (words) in this map are called index terms and should be a set of vocabulary entries and proper nouns as large as possible. The values in this map are called occurrence lists and should cover as many Web pages as possible. We can efﬁciently implement an inverted index with a data structure consisting of the following: 1. An array storing the occurrence lists of the terms (in no particular order). 2. A compressed trie for the set of index terms, where each leaf stores the index of the occurrence list of the associated term. The reason for storing the occurrence lists outside the trie is to keep the size of the trie data structure sufﬁciently small to ﬁt in internal memory. Instead, because of their large total size, the occurrence lists have to be stored on disk. With our data structure, a query for a single keyword is similar to a wordmatching query (Section 13.3.1). Namely, we ﬁnd the keyword in the trie and we return the associated occurrence list. When multiple keywords are given and the desired output are the pages containing all the given keywords, we retrieve the occurrence list of each keyword using the trie and return their intersection. To facilitate the intersection computation, each occurrence list should be implemented with a sequence sorted by address or with a map, to allow efﬁcient set operations. In addition to the basic task of returning a list of pages containing given keywords, search engines provide an important additional service by ranking the pages returned by relevance. Devising fast and accurate ranking algorithms for search engines is a major challenge for computer researchers and electronic commerce companies.

13.4. Text Compression and the Greedy Method 13.4 Text Compression and the Greedy Method In this section, we will consider the important task of text compression. In this problem, we are given a string X deﬁned over some alphabet, such as the ASCII or Unicode character sets, and we want to efﬁciently encode X into a small binary string Y (using only the characters 0 and 1). Text compression is useful in any situation where we wish to reduce bandwidth for digital communications, so as to minimize the time needed to transmit our text. Likewise, text compression is useful for storing large documents more efﬁciently, so as to allow a ﬁxed-capacity storage device to contain as many documents as possible. The method for text compression explored in this section is the Huffman code. Standard encoding schemes, such as ASCII, use ﬁxed-length binary strings to encode characters (with 7 or 8 bits in the traditional or extended ASCII systems, respectively). The Unicode system was originally proposed as a 16-bit ﬁxedlength representation, although common encodings reduce the space usage by allowing common groups of characters, such as those from the ASCII system, with fewer bits. The Huffman code saves space over a ﬁxed-length encoding by using short code-word strings to encode high-frequency characters and long code-word strings to encode low-frequency characters. Furthermore, the Huffman code uses a variable-length encoding speciﬁcally optimized for a given string X over any alphabet. The optimization is based on the use of character frequencies, where we have, for each character c, a count f(c) of the number of times c appears in the string X. To encode the string X, we convert each character in X to a variable-length code-word, and we concatenate all these code-words in order to produce the encoding Y for X. In order to avoid ambiguities, we insist that no code-word in our encoding be a preﬁx of another code-word in our encoding. Such a code is called a preﬁx code, and it simpliﬁes the decoding of Y to retrieve X. (See Figure 13.12.) Even with this restriction, the savings produced by a variable-length preﬁx code can be signiﬁcant, particularly if there is a wide variance in character frequencies (as is the case for natural language text in almost every written language). Huffman’s algorithm for producing an optimal variable-length preﬁx code for X is based on the construction of a binary tree T that represents the code. Each edge in T represents a bit in a code-word, with an edge to a left child representing a “0” and an edge to a right child representing a “1”. Each leaf v is associated with a speciﬁc character, and the code-word for that character is deﬁned by the sequence of bits associated with the edges in the path from the root of T to v. (See Figure 13.12.) Each leaf v has a frequency, f(v), which is simply the frequency in X of the character associated with v. In addition, we give each internal node v in T a frequency, f(v), that is the sum of the frequencies of all the leaves in the subtree rooted at v.

Chapter 13. Text Processing (a) Character a b d e f h i k n o r s t u v Frequency (b) k i o t s n f v u b h d e a r Figure 13.12: An illustration of an example Huffman code for the input string X = "a fast runner need never be afraid of the dark": (a) frequency of each character of X; (b) Huffman tree T for string X. The code for a character c is obtained by tracing the path from the root of T to the leaf where c is stored, and associating a left child with 0 and a right child with 1. For example, the code for "r" is 011, and the code for "h" is 10111. 13.4.1 The Huﬀman Coding Algorithm The Huffman coding algorithm begins with each of the d distinct characters of the string X to encode being the root node of a single-node binary tree. The algorithm proceeds in a series of rounds. In each round, the algorithm takes the two binary trees with the smallest frequencies and merges them into a single binary tree. It repeats this process until only one tree is left. (See Code Fragment 13.5.) Each iteration of the while loop in Huffman’s algorithm can be implemented in O(logd) time using a priority queue represented with a heap. In addition, each iteration takes two nodes out of Q and adds one in, a process that will be repeated d −1 times before exactly one node is left in Q. Thus, this algorithm runs in O(n+ d logd) time. Although a full justiﬁcation of this algorithm’s correctness is beyond our scope here, we note that its intuition comes from a simple idea—any optimal code can be converted into an optimal code in which the code-words for the two lowest-frequency characters, a and b, differ only in their last bit. Repeating the argument for a string with a and b replaced by a character c, gives the following: Proposition 13.7: Huffman’s algorithm constructs an optimal preﬁx code for a string of length n with d distinct characters in O(n+d logd) time.

13.4. Text Compression and the Greedy Method Algorithm Huﬀman(X): Input: String X of length n with d distinct characters Output: Coding tree for X Compute the frequency f(c) of each character c of X. Initialize a priority queue Q. for each character c in X do Create a single-node binary tree T storing c. Insert T into Q with key f(c). while Q.size() > 1 do Entry e1 = Q.removeMin() with e1 having key f1 and value T1. Entry e2 = Q.removeMin() with e2 having key f2 and value T2. Create a new binary tree T with left subtree T1 and right subtree T2. Insert T into Q with key f1 + f2. Entry e = Q.removeMin() with e having tree T as its value. return tree T Code Fragment 13.5: Huffman coding algorithm. 13.4.2 The Greedy Method Huffman’s algorithm for building an optimal encoding is an example application of an algorithmic design pattern called the greedy method. This design pattern is applied to optimization problems, where we are trying to construct some structure while minimizing or maximizing some property of that structure. The general formula for the greedy-method pattern is almost as simple as that for the brute-force method. In order to solve a given optimization problem using the greedy method, we proceed by a sequence of choices. The sequence starts from some well-understood starting condition, and computes the cost for that initial condition. The pattern then asks that we iteratively make additional choices by identifying the decision that achieves the best cost improvement from all of the choices that are currently possible. This approach does not always lead to an optimal solution. But there are several problems that it does work for, and such problems are said to possess the greedy-choice property. This is the property that a global optimal condition can be reached by a series of locally optimal choices (that is, choices that are each the current best from among the possibilities available at the time), starting from a well-deﬁned starting condition. The problem of computing an optimal variable-length preﬁx code is just one example of a problem that possesses the greedy-choice property.

Chapter 13. Text Processing 13.5 Dynamic Programming In this section, we will discuss the dynamic-programming algorithmic design pattern. This technique is similar to the divide-and-conquer technique (Section 12.1.1), in that it can be applied to a wide variety of different problems. Dynamic programming can often be used to produce polynomial-time algorithms to solve problems that seem to require exponential time. In addition, the algorithms that result from applications of the dynamic programming technique are usually quite simple, often needing little more than a few lines of code to describe some nested loops for ﬁlling in a table. 13.5.1 Matrix Chain-Product Rather than starting out with an explanation of the general components of the dynamic programming technique, we begin by giving a classic, concrete example. Suppose we are given a collection of n two-dimensional matrices for which we wish to compute the mathematical product A = A0 ·A1 ·A2 ···An−1, where Ai is a di × di+1 matrix, for i = 0,1,2,...,n −1. In the standard matrix multiplication algorithm (which is the one we will use), to multiply a d×e-matrix B times an e× f-matrix C, we compute the product, A, as A[i][ j] = e−1 ∑ k=0 B[i][k]·C[k][ j]. This deﬁnition implies that matrix multiplication is associative, that is, it implies that B · (C · D) = (B ·C) · D. Thus, we can parenthesize the expression for A any way we wish and we will end up with the same answer. However, we will not necessarily perform the same number of primitive (that is, scalar) multiplications in each parenthesization, as is illustrated in the following example. Example 13.8: Let B be a 2×10-matrix, let C be a 10×50-matrix, and let D be a 50 × 20-matrix. Computing B · (C · D) requires 2 · 10 · 20 + 10 · 50 · 20 = 10400 multiplications, whereas computing (B·C)·D requires 2·10·50+2·50·20 = 3000 multiplications. The matrix chain-product problem is to determine the parenthesization of the expression deﬁning the product A that minimizes the total number of scalar multiplications performed. As the example above illustrates, the differences between parenthesizations can be dramatic, so ﬁnding a good solution can result in signiﬁcant speedups.

13.5. Dynamic Programming Deﬁning Subproblems One way to solve the matrix chain-product problem is to simply enumerate all the possible ways of parenthesizing the expression for A and determine the number of multiplications performed by each one. Unfortunately, the set of all different parenthesizations of the expression for A is equal in number to the set of all different binary trees that have n leaves. This number is exponential in n. Thus, this straightforward (“brute-force”) algorithm runs in exponential time, for there are an exponential number of ways to parenthesize an associative arithmetic expression. We can signiﬁcantly improve the performance achieved by the brute-force algorithm, however, by making a few observations about the nature of the matrix chain-product problem. The ﬁrst is that the problem can be split into subproblems. In this case, we can deﬁne a number of different subproblems, each of which is to compute the best parenthesization for some subexpression Ai·Ai+1 ···A j. As a concise notation, we use Ni,j to denote the minimum number of multiplications needed to compute this subexpression. Thus, the original matrix chain-product problem can be characterized as that of computing the value of N0,n−1. This observation is important, but we need one more in order to apply the dynamic programming technique. Characterizing Optimal Solutions The other important observation we can make about the matrix chain-product problem is that it is possible to characterize an optimal solution to a particular subproblem in terms of optimal solutions to its subproblems. We call this property the subproblem optimality condition. In the case of the matrix chain-product problem, we observe that, no matter how we parenthesize a subexpression, there has to be some ﬁnal matrix multiplication that we perform. That is, a full parenthesization of a subexpression Ai · Ai+1 ···A j has to be of the form (Ai···Ak) · (Ak+1 ···A j), for some k ∈{i,i + 1,..., j −1}. Moreover, for whichever k is the correct one, the products (Ai···Ak) and (Ak+1 ···A j) must also be solved optimally. If this were not so, then there would be a global optimal that had one of these subproblems solved suboptimally. But this is impossible, since we could then reduce the total number of multiplications by replacing the current subproblem solution by an optimal solution for the subproblem. This observation implies a way of explicitly deﬁning the optimization problem for Ni,j in terms of other optimal subproblem solutions. Namely, we can compute Ni,j by considering each place k where we could put the ﬁnal multiplication and taking the minimum over all such choices.

Chapter 13. Text Processing Designing a Dynamic Programming Algorithm We can therefore characterize the optimal subproblem solution, Ni,j, as Ni,j = min i≤k< j{Ni,k +Nk+1,j +didk+1dj+1}, where Ni,i = 0, since no work is needed for a single matrix. That is, Ni,j is the minimum, taken over all possible places to perform the ﬁnal multiplication, of the number of multiplications needed to compute each subexpression plus the number of multiplications needed to perform the ﬁnal matrix multiplication. Notice that there is a sharing of subproblems going on that prevents us from dividing the problem into completely independent subproblems (as we would need to do to apply the divide-and-conquer technique). We can, nevertheless, use the equation for Ni,j to derive an efﬁcient algorithm by computing Ni,j values in a bottom-up fashion, and storing intermediate solutions in a table of Ni,j values. We can begin simply enough by assigning Ni,i = 0 for i = 0,1,...,n−1. We can then apply the general equation for Ni,j to compute Ni,i+1 values, since they depend only on Ni,i and Ni+1,i+1 values that are available. Given the Ni,i+1 values, we can then compute the Ni,i+2 values, and so on. Therefore, we can build Ni,j values up from previously computed values until we can ﬁnally compute the value of N0,n−1, which is the number that we are searching for. A Java implementation of this dynamic programming solution is given in Code Fragment 13.6; we use techniques from Section 3.1.5 for working with a two-dimensional array in Java. public static int[ ][ ] matrixChain(int[ ] d) { int n = d.length −1; // number of matrices int[ ][ ] N = new int[n][n]; // n-by-n matrix; initially zeros for (int b=1; b < n; b++) // number of products in subchain for (int i=0; i < n −b; i++) { // start of subchain int j = i + b; // end of subchain N[i][j] = Integer.MAX VALUE; // used as ’inﬁnity’ for (int k=i; k < j; k++) N[i][j] = Math.min(N[i][j], N[i][k] + N[k+1][j] + d[i]∗d[k+1]∗d[j+1]); } return N; } Code Fragment 13.6: Dynamic programming algorithm for the matrix chainproduct problem. Thus, we can compute N0,n−1 with an algorithm that consists primarily of three nested loops (the third of which computes the min term). Each of these loops iterates at most n times per execution, with a constant amount of additional work within. Therefore, the total running time of this algorithm is O(n3).

13.5. Dynamic Programming 13.5.2 DNA and Text Sequence Alignment A common text-processing problem, which arises in genetics and software engineering, is to test the similarity between two text strings. In a genetics application, the two strings could correspond to two strands of DNA, for which we want to compute similarities. Likewise, in a software engineering application, the two strings could come from two versions of source code for the same program, for which we want to determine changes made from one version to the next. Indeed, determining the similarity between two strings is so common that the Unix and Linux operating systems have a built-in program, named diff, for comparing text ﬁles. Given a string X = x0x1x2 ···xn−1, a subsequence of X is any string that is of the form xi1xi2 ···xik, where ij < ij+1; that is, it is a sequence of characters that are not necessarily contiguous but are nevertheless taken in order from X. For example, the string AAAG is a subsequence of the string CGATAATTGAGA. The DNA and text similarity problem we address here is the longest common subsequence (LCS) problem. In this problem, we are given two character strings, X = x0x1x2 ···xn−1 and Y = y0y1y2 ···ym−1, over some alphabet (such as the alphabet {A,C,G,T} common in computational genomics) and are asked to ﬁnd a longest string S that is a subsequence of both X and Y. One way to solve the longest common subsequence problem is to enumerate all subsequences of X and take the largest one that is also a subsequence of Y. Since each character of X is either in or not in a subsequence, there are potentially 2n different subsequences of X, each of which requires O(m) time to determine whether it is a subsequence of Y. Thus, this brute-force approach yields an exponential-time algorithm that runs in O(2nm) time, which is very inefﬁcient. Fortunately, the LCS problem is efﬁciently solvable using dynamic programming. The Components of a Dynamic Programming Solution As mentioned above, the dynamic programming technique is used primarily for optimization problems, where we wish to ﬁnd the “best” way of doing something. We can apply the dynamic programming technique in such situations if the problem has certain properties: Simple Subproblems: There has to be some way of repeatedly breaking the global optimization problem into subproblems. Moreover, there should be a way to parameterize subproblems with just a few indices, like i, j, k, and so on. Subproblem Optimization: An optimal solution to the global problem must be a composition of optimal subproblem solutions. Subproblem Overlap: Optimal solutions to unrelated subproblems can contain subproblems in common.

Chapter 13. Text Processing Applying Dynamic Programming to the LCS Problem Recall that in the LCS problem, we are given two character strings, X and Y, of length n and m, respectively, and are asked to ﬁnd a longest string S that is a subsequence of both X and Y. Since X and Y are character strings, we have a natural set of indices with which to deﬁne subproblems—indices into the strings X and Y. Let us deﬁne a subproblem, therefore, as that of computing the value L j,k, which we will use to denote the length of a longest string that is a subsequence of both the ﬁrst j characters of X and the ﬁrst k characters of Y, that is of preﬁxes X[0.. j −1] and Y[0..k−1]. If either j = 0 or k = 0, then L j,k is trivially deﬁned as 0. When both j ≥1 and k ≥1, this deﬁnition allows us to rewrite L j,k recursively in terms of optimal subproblem solutions. This deﬁnition depends on which of two cases we are in. (See Figure 13.13.) • xj−1 = yk−1. In this case, we have a match between the last character of X[0.. j −1] and the last character of Y[0..k −1]. We claim that this character belongs to a longest common subsequence of X[0.. j −1] and Y[0..k −1]. To justify this claim, let us suppose it is not true. There has to be some longest common subsequence xa1xa2 ...xac = yb1yb2 ...ybc. If xac = xj−1 or ybc = yk−1, then we get the same sequence by setting ac = j −1 and bc = k −1. Alternately, if xac ̸= xj−1 and ybc ̸= yk−1, then we can get an even longer common subsequence by adding xj−1 = yk−1 to the end. Thus, a longest common subsequence of X[0.. j −1] and Y[0..k −1] ends with xj−1. Therefore, we set L j,k = 1+L j−1,k−1 if xj−1 = yk−1. • xj−1 ̸= yk−1. In this case, we cannot have a common subsequence that includes both xj−1 and yk−1. That is, we can have a common subsequence end with xj−1 or one that ends with yk−1 (or possibly neither), but certainly not both. Therefore, we set L j,k = max{L j−1,k , L j,k−1} if xj−1 ̸= yk−1. 6 7 8 9 10 11 G T T C C T A A T A C A T A A T T G G A G A 0 1 2 3 4 5 6 7 8 9 X = Y = 1 2 3 4 5 5 6 7 8 9 10 G T T C C T A A T C A T A A T T G G G A 0 1 2 3 4 5 6 7 8 X = Y = 1 2 3 4 L10,12 = 1+L9,11 L9,11 = max(L9,10, L8,11) (a) (b) Figure 13.13: The two cases in the longest common subsequence algorithm for computing L j,k when j,k ≥1: (a) xj−1 = yk−1; (b) xj−1 ̸= yk−1.

13.5. Dynamic Programming The LCS Algorithm The deﬁnition of L j,k satisﬁes subproblem optimization, for we cannot have a longest common subsequence without also having longest common subsequences for the subproblems. Also, it uses subproblem overlap, because a subproblem solution L j,k can be used in several other problems (namely, the problems L j+1,k, L j,k+1, and L j+1,k+1). Turning this deﬁnition of L j,k into an algorithm is actually quite straightforward. We create an (n+1)×(m+1) array, L, deﬁned for 0 ≤j ≤n and 0 ≤k ≤m. We initialize all entries to 0, in particular so that all entries of the form L j,0 and L0,k are zero. Then, we iteratively build up values in L until we have Ln,m, the length of a longest common subsequence of X and Y. We give a Java implementation of this algorithm in Code Fragment 13.7. /∗∗Returns table such that L[j][k] is length of LCS for X[0..j−1] and Y[0..k−1]. ∗/ public static int[ ][ ] LCS(char[ ] X, char[ ] Y) { int n = X.length; int m = Y.length; int[ ][ ] L = new int[n+1][m+1]; for (int j=0; j < n; j++) for (int k=0; k < m; k++) if (X[j] == Y[k]) // align this match L[j+1][k+1] = L[j][k] + 1; else // choose to ignore one character L[j+1][k+1] = Math.max(L[j][k+1], L[j+1][k]); return L; } Code Fragment 13.7: Dynamic programming algorithm for the LCS problem. The running time of the algorithm of the LCS algorithm is easy to analyze, for it is dominated by two nested for loops, with the outer one iterating n times and the inner one iterating m times. Since the if-statement and assignment inside the loop each requires O(1) primitive operations, this algorithm runs in O(nm) time. Thus, the dynamic programming technique can be applied to the longest common subsequence problem to improve signiﬁcantly over the exponential-time brute-force solution to the LCS problem. The LCS method of Code Fragment 13.7 computes the length of the longest common subsequence (stored as Ln,m), but not the subsequence itself. Fortunately, it is easy to extract the actual longest common subsequence if given the complete table of L j,k values computed by the LCS method. The solution can be reconstructed back to front by reverse engineering the calculation of length Ln,m. At any position L j,k, if xj = yk, then the length is based on the common subsequence associated with length L j−1,k−1, followed by common character xj. We can record xj as part of the sequence, and then continue the analysis from L j−1,k−1. If xj ̸= yk, then we can move to the larger of L j,k−1 and L j−1,k. We continue this process until reaching

Chapter 13. Text Processing some L j,k = 0 (for example, if j or k is 0 as a boundary case). A Java implementation of this strategy is given in Code Fragment 13.8. This method constructs a longest common subsequence in O(n+ m) additional time, since each pass of the while loop decrements either j or k (or both). An illustration of the algorithm for computing the longest common subsequence is given in Figure 13.14. /∗∗Returns the longest common substring of X and Y, given LCS table L. ∗/ public static char[ ] reconstructLCS(char[ ] X, char[ ] Y, int[ ][ ] L) { StringBuilder solution = new StringBuilder(); int j = X.length; int k = Y.length; while (L[j][k] > 0) // common characters remain if (X[j−1] == Y[k−1]) { solution.append(X[j−1]); j−−; k−−; } else if (L[j−1][k] >= L[j][k−1]) j−−; else k−−; // return left-to-right version, as char array return solution.reverse().toString().toCharArray(); } Code Fragment 13.8: Reconstructing the longest common subsequence. 10 11 12 6 7 8 9 10 11 G T T C C T A A T A C A T A A T T G G A G A Y = 0 1 2 3 4 5 6 7 8 9 X = 1 2 3 4 5 Figure 13.14: Illustration of the algorithm for constructing a longest common subsequence from the array L. A diagonal step on the highlighted path represents the use of a common character (with that character’s respective indices in the sequences highlighted in the margins).

Chapter 14. Graph Algorithms 14.1 Graphs A graph is a way of representing relationships that exist between pairs of objects. That is, a graph is a set of objects, called vertices, together with a collection of pairwise connections between them, called edges. Graphs have applications in modeling many domains, including mapping, transportation, computer networks, and electrical engineering. By the way, this notion of a “graph” should not be confused with bar charts and function plots, as these kinds of “graphs” are unrelated to the topic of this chapter. Viewed abstractly, a graph G is simply a set V of vertices and a collection E of pairs of vertices from V, called edges. Thus, a graph is a way of representing connections or relationships between pairs of objects from some set V. Incidentally, some books use different terminology for graphs and refer to what we call vertices as nodes and what we call edges as arcs. We use the terms “vertices” and “edges.” Edges in a graph are either directed or undirected. An edge (u,v) is said to be directed from u to v if the pair (u,v) is ordered, with u preceding v. An edge (u,v) is said to be undirected if the pair (u,v) is not ordered. Undirected edges are sometimes denoted with set notation, as {u,v}, but for simplicity we use the pair notation (u,v), noting that in the undirected case (u,v) is the same as (v,u). Graphs are typically visualized by drawing the vertices as ovals or rectangles and the edges as segments or curves connecting pairs of ovals and rectangles. The following are some examples of directed and undirected graphs. Example 14.1: We can visualize collaborations among the researchers of a certain discipline by constructing a graph whose vertices are associated with the researchers themselves, and whose edges connect pairs of vertices associated with researchers who have coauthored a paper or book. (See Figure 14.1.) Such edges are undirected because coauthorship is a symmetric relation; that is, if A has coauthored something with B, then B necessarily has coauthored something with A. Chiang Goldwasser Tamassia Goodrich Garg Snoeyink Tollis Vitter Preparata Figure 14.1: Graph of coauthorship among some authors.

14.1. Graphs Example 14.2: We can associate with an object-oriented program a graph whose vertices represent the classes deﬁned in the program, and whose edges indicate inheritance between classes. There is an edge from a vertex v to a vertex u if the class for v inherits from the class for u. Such edges are directed because the inheritance relation only goes in one direction (that is, it is asymmetric). If all the edges in a graph are undirected, then we say the graph is an undirected graph. Likewise, a directed graph, also called a digraph, is a graph whose edges are all directed. A graph that has both directed and undirected edges is often called a mixed graph. Note that an undirected or mixed graph can be converted into a directed graph by replacing every undirected edge (u,v) by the pair of directed edges (u,v) and (v,u). It is often useful, however, to keep undirected and mixed graphs represented as they are, for such graphs have several applications, as in the following example. Example 14.3: A city map can be modeled as a graph whose vertices are intersections or dead ends, and whose edges are stretches of streets without intersections. This graph has both undirected edges, which correspond to stretches of two-way streets, and directed edges, which correspond to stretches of one-way streets. Thus, in this way, a graph modeling a city map is a mixed graph. Example 14.4: Physical examples of graphs are present in the electrical wiring and plumbing networks of a building. Such networks can be modeled as graphs, where each connector, ﬁxture, or outlet is viewed as a vertex, and each uninterrupted stretch of wire or pipe is viewed as an edge. Such graphs are actually components of much larger graphs, namely the local power and water distribution networks. Depending on the speciﬁc aspects of these graphs that we are interested in, we may consider their edges as undirected or directed, for, in principle, water can ﬂow in a pipe and current can ﬂow in a wire in either direction. The two vertices joined by an edge are called the end vertices (or endpoints) of the edge. If an edge is directed, its ﬁrst endpoint is its origin and the other is the destination of the edge. Two vertices u and v are said to be adjacent if there is an edge whose end vertices are u and v. An edge is said to be incident to a vertex if the vertex is one of the edge’s endpoints. The outgoing edges of a vertex are the directed edges whose origin is that vertex. The incoming edges of a vertex are the directed edges whose destination is that vertex. The degree of a vertex v, denoted deg(v), is the number of incident edges of v. The in-degree and out-degree of a vertex v are the number of the incoming and outgoing edges of v, and are denoted indeg(v) and outdeg(v), respectively.

Chapter 14. Graph Algorithms Example 14.5: We can study air transportation by constructing a graph G, called a ﬂight network, whose vertices are associated with airports, and whose edges are associated with ﬂights. (See Figure 14.2.) In graph G, the edges are directed because a given ﬂight has a speciﬁc travel direction. The endpoints of an edge e in G correspond respectively to the origin and destination of the ﬂight corresponding to e. Two airports are adjacent in G if there is a ﬂight that ﬂies between them, and an edge e is incident to a vertex v in G if the ﬂight for e ﬂies to or from the airport for v. The outgoing edges of a vertex v correspond to the outbound ﬂights from v’s airport, and the incoming edges correspond to the inbound ﬂights to v’s airport. Finally, the in-degree of a vertex v of G corresponds to the number of inbound ﬂights to v’s airport, and the out-degree of a vertex v in G corresponds to the number of outbound ﬂights. ORD MIA NW 35 AA 903 DL 247 DL 335 AA 49 AA 411 AA 523 UA 120 UA 877 SW 45 AA 1387 DFW LAX SFO BOS JFK Figure 14.2: Example of a directed graph representing a ﬂight network. The endpoints of edge UA 120 are LAX and ORD; hence, LAX and ORD are adjacent. The in-degree of DFW is 3, and the out-degree of DFW is 2. The deﬁnition of a graph refers to the group of edges as a collection, not a set, thus allowing two undirected edges to have the same end vertices, and for two directed edges to have the same origin and the same destination. Such edges are called parallel edges or multiple edges. A ﬂight network can contain parallel edges (Example 14.5), such that multiple edges between the same pair of vertices could indicate different ﬂights operating on the same route at different times of the day. Another special type of edge is one that connects a vertex to itself. Namely, we say that an edge (undirected or directed) is a self-loop if its two endpoints coincide. A self-loop may occur in a graph associated with a city map (Example 14.3), where it would correspond to a “circle” (a curving street that returns to its starting point). With few exceptions, graphs do not have parallel edges or self-loops. Such graphs are said to be simple. Thus, we can usually say that the edges of a simple graph are a set of vertex pairs (and not just a collection). Throughout this chapter, we will assume that a graph is simple unless otherwise speciﬁed.

14.1. Graphs A path is a sequence of alternating vertices and edges that starts at a vertex and ends at a vertex such that each edge is incident to its predecessor and successor vertex. A cycle is a path that starts and ends at the same vertex, and that includes at least one edge. We say that a path is simple if each vertex in the path is distinct, and we say that a cycle is simple if each vertex in the cycle is distinct, except for the ﬁrst and last one. A directed path is a path such that all edges are directed and are traversed along their direction. A directed cycle is similarly deﬁned. For example, in Figure 14.2, (BOS, NW 35, JFK, AA 1387, DFW) is a directed simple path, and (LAX, UA 120, ORD, UA 877, DFW, AA 49, LAX) is a directed simple cycle. Note that a directed graph may have a cycle consisting of two edges with opposite direction between the same pair of vertices, for example (ORD, UA 877, DFW, DL 335, ORD) in Figure 14.2. A directed graph is acyclic if it has no directed cycles. For example, if we were to remove the edge UA 877 from the graph in Figure 14.2, the remaining graph is acyclic. If a graph is simple, we may omit the edges when describing path P or cycle C, as these are well deﬁned, in which case P is a list of adjacent vertices and C is a cycle of adjacent vertices. Example 14.6: Given a graph G representing a city map (see Example 14.3), we can model a couple driving to dinner at a recommended restaurant as traversing a path though G. If they know the way, and do not accidentally go through the same intersection twice, then they traverse a simple path in G. Likewise, we can model the entire trip the couple takes, from their home to the restaurant and back, as a cycle. If they go home from the restaurant in a completely different way than how they went, not even going through the same intersection twice, then their entire round trip is a simple cycle. Finally, if they travel along one-way streets for their entire trip, we can model their night out as a directed cycle. Given vertices u and v of a (directed) graph G, we say that u reaches v, and that v is reachable from u, if G has a (directed) path from u to v. In an undirected graph, the notion of reachability is symmetric, that is to say, u reaches v if an only if v reaches u. However, in a directed graph, it is possible that u reaches v but v does not reach u, because a directed path must be traversed according to the respective directions of the edges. A graph is connected if, for any two vertices, there is a path between them. A directed graph ⃗G is strongly connected if for any two vertices u and v of ⃗G, u reaches v and v reaches u. (See Figure 14.3 for some examples.) A subgraph of a graph G is a graph H whose vertices and edges are subsets of the vertices and edges of G, respectively. A spanning subgraph of G is a subgraph of G that contains all the vertices of the graph G. If a graph G is not connected, its maximal connected subgraphs are called the connected components of G. A forest is a graph without cycles. A tree is a connected forest, that is, a connected graph without cycles. A spanning tree of a graph is a spanning subgraph that is a tree. (Note that this deﬁnition of a tree is somewhat different from the one given in Chapter 8, as there is not necessarily a designated root.)

Chapter 14. Graph Algorithms DFW MIA ORD JFK BOS SFO LAX DFW SFO MIA ORD JFK BOS LAX (a) (b) JFK BOS LAX DFW ORD MIA SFO DFW MIA ORD JFK BOS SFO LAX (c) (d) Figure 14.3: Examples of reachability in a directed graph: (a) a directed path from BOS to LAX is highlighted; (b) a directed cycle (ORD, MIA, DFW, LAX, ORD) is highlighted; its vertices induce a strongly connected subgraph; (c) the subgraph of the vertices and edges reachable from ORD is highlighted; (d) the removal of the dashed edges results in a directed acyclic graph. Example 14.7: Perhaps the most talked about graph today is the Internet, which can be viewed as a graph whose vertices are computers and whose (undirected) edges are communication connections between pairs of computers on the Internet. The computers and the connections between them in a single domain, like wiley.com, form a subgraph of the Internet. If this subgraph is connected, then two users on computers in this domain can send email to one another without having their information packets ever leave their domain. Suppose the edges of this subgraph form a spanning tree. This implies that, if even a single connection goes down (for example, because someone pulls a communication cable out of the back of a computer in this domain), then this subgraph will no longer be connected.

14.1. Graphs In the propositions that follow, we explore a few important properties of graphs. Proposition 14.8: If G is a graph with m edges and vertex set V, then ∑ v in V deg(v) = 2m. Justiﬁcation: An edge (u,v) is counted twice in the summation above; once by its endpoint u and once by its endpoint v. Thus, the total contribution of the edges to the degrees of the vertices is twice the number of edges. Proposition 14.9: If G is a directed graph with m edges and vertex set V, then ∑ v in V indeg(v) = ∑ v in V outdeg(v) = m. Justiﬁcation: In a directed graph, an edge (u,v) contributes one unit to the out-degree of its origin u and one unit to the in-degree of its destination v. Thus, the total contribution of the edges to the out-degrees of the vertices is equal to the number of edges, and similarly for the in-degrees. We next show that a simple graph with n vertices has O(n2) edges. Proposition 14.10: Let G be a simple graph with n vertices and m edges. If G is undirected, then m ≤n(n−1)/2, and if G is directed, then m ≤n(n−1). Justiﬁcation: Suppose that G is undirected. Since no two edges can have the same endpoints and there are no self-loops, the maximum degree of a vertex in G is n−1 in this case. Thus, by Proposition 14.8, 2m ≤n(n−1). Now suppose that G is directed. Since no two edges can have the same origin and destination, and there are no self-loops, the maximum in-degree of a vertex in G is n−1 in this case. Thus, by Proposition 14.9, m ≤n(n−1). There are a number of simple properties of trees, forests, and connected graphs. Proposition 14.11: Let G be an undirected graph with n vertices and m edges. • If G is connected, then m ≥n−1. • If G is a tree, then m = n−1. • If G is a forest, then m ≤n−1.

Chapter 14. Graph Algorithms 14.1.1 The Graph ADT A graph is a collection of vertices and edges. We model the abstraction as a combination of three data types: Vertex, Edge, and Graph. A Vertex is a lightweight object that stores an arbitrary element provided by the user (e.g., an airport code); we assume the element can be retrieved with the getElement() method. An Edge also stores an associated object (e.g., a ﬂight number, travel distance, cost), which is returned by its getElement() method. The primary abstraction for a graph is the Graph ADT. We presume that a graph can be either undirected or directed, with the designation declared upon construction; recall that a mixed graph can be represented as a directed graph, modeling edge {u,v} as a pair of directed edges (u,v) and (v,u). The Graph ADT includes the following methods: numVertices(): Returns the number of vertices of the graph. vertices(): Returns an iteration of all the vertices of the graph. numEdges(): Returns the number of edges of the graph. edges(): Returns an iteration of all the edges of the graph. getEdge(u, v): Returns the edge from vertex u to vertex v, if one exists; otherwise return null. For an undirected graph, there is no difference between getEdge(u, v) and getEdge(v, u). endVertices(e): Returns an array containing the two endpoint vertices of edge e. If the graph is directed, the ﬁrst vertex is the origin and the second is the destination. opposite(v, e): For edge e incident to vertex v, returns the other vertex of the edge; an error occurs if e is not incident to v. outDegree(v): Returns the number of outgoing edges from vertex v. inDegree(v): Returns the number of incoming edges to vertex v. For an undirected graph, this returns the same value as does outDegree(v). outgoingEdges(v): Returns an iteration of all outgoing edges from vertex v. incomingEdges(v): Returns an iteration of all incoming edges to vertex v. For an undirected graph, this returns the same collection as does outgoingEdges(v). insertVertex(x): Creates and returns a new Vertex storing element x. insertEdge(u, v, x): Creates and returns a new Edge from vertex u to vertex v, storing element x; an error occurs if there already exists an edge from u to v. removeVertex(v): Removes vertex v and all its incident edges from the graph. removeEdge(e): Removes edge e from the graph.

14.2. Data Structures for Graphs 14.2 Data Structures for Graphs In this section, we introduce four data structures for representing a graph. In each representation, we maintain a collection to store the vertices of a graph. However, the four representations differ greatly in the way they organize the edges. • In an edge list, we maintain an unordered list of all edges. This minimally sufﬁces, but there is no efﬁcient way to locate a particular edge (u,v), or the set of all edges incident to a vertex v. • In an adjacency list, we additionally maintain, for each vertex, a separate list containing those edges that are incident to the vertex. This organization allows us to more efﬁciently ﬁnd all edges incident to a given vertex. • An adjacency map is similar to an adjacency list, but the secondary container of all edges incident to a vertex is organized as a map, rather than as a list, with the adjacent vertex serving as a key. This allows more efﬁcient access to a speciﬁc edge (u,v), for example, in O(1) expected time with hashing. • An adjacency matrix provides worst-case O(1) access to a speciﬁc edge (u,v) by maintaining an n × n matrix, for a graph with n vertices. Each slot is dedicated to storing a reference to the edge (u,v) for a particular pair of vertices u and v; if no such edge exists, the slot will store null. A summary of the performance of these structures is given in Table 14.1. Method Edge List Adj. List Adj. Map Adj. Matrix numVertices() O(1) O(1) O(1) O(1) numEdges() O(1) O(1) O(1) O(1) vertices() O(n) O(n) O(n) O(n) edges() O(m) O(m) O(m) O(m) getEdge(u, v) O(m) O(min(du,dv)) O(1) exp. O(1) outDegree(v) O(m) O(1) O(1) O(n) inDegree(v) outgoingEdges(v) O(m) O(dv) O(dv) O(n) incomingEdges(v) insertVertex(x) O(1) O(1) O(1) O(n2) removeVertex(v) O(m) O(dv) O(dv) O(n2) insertEdge(u, v, x) O(1) O(1) O(1) exp. O(1) removeEdge(e) O(1) O(1) O(1) exp. O(1) Table 14.1: A summary of the running times for the methods of the graph ADT, using the graph representations discussed in this section. We let n denote the number of vertices, m the number of edges, and dv the degree of vertex v. Note that the adjacency matrix uses O(n2) space, while all other structures use O(n+m) space.

Chapter 14. Graph Algorithms 14.2.1 Edge List Structure The edge list structure is possibly the simplest, though not the most efﬁcient, representation of a graph G. All vertex objects are stored in an unordered list V, and all edge objects are stored in an unordered list E. We illustrate an example of the edge list structure for a graph G in Figure 14.4. h e g v u w z f z e f g h V E v u w (a) (b) Figure 14.4: (a) A graph G; (b) schematic representation of the edge list structure for G. Notice that an edge object refers to the two vertex objects that correspond to its endpoints, but that vertices do not refer to incident edges. To support the many methods of the Graph ADT (Section 14.1), we assume the following additional features of an edge list representation. Collections V and E are represented with doubly linked lists using our LinkedPositionalList class from Chapter 7. Vertex Objects The vertex object for a vertex v storing element x has instance variables for: • A reference to element x, to support the getElement() method. • A reference to the position of the vertex instance in the list V, thereby allowing v to be efﬁciently removed from V if it were removed from the graph. Edge Objects The edge object for an edge e storing element x has instance variables for: • A reference to element x, to support the getElement() method. • References to the vertex objects associated with the endpoint vertices of e. These will allow for constant-time support for methods endVertices(e) and opposite(v, e). • A reference to the position of the edge instance in list E, thereby allowing e to be efﬁciently removed from E if it were removed from the graph.

14.2. Data Structures for Graphs Performance of the Edge List Structure The performance of an edge list structure in fulﬁlling the graph ADT is summarized in Table 14.2. We begin by discussing the space usage, which is O(n + m) for representing a graph with n vertices and m edges. Each individual vertex or edge instance uses O(1) space, and the additional lists V and E use space proportional to their number of entries. In terms of running time, the edge list structure does as well as one could hope in terms of reporting the number of vertices or edges, or in producing an iteration of those vertices or edges. By querying the respective list V or E, the numVertices and numEdges methods run in O(1) time, and by iterating through the appropriate list, the methods vertices and edges run respectively in O(n) and O(m) time. The most signiﬁcant limitations of an edge list structure, especially when compared to the other graph representations, are the O(m) running times of methods getEdge(u, v), outDegree(v), and outgoingEdges(v) (and corresponding methods inDegree and incomingEdges). The problem is that with all edges of the graph in an unordered list E, the only way to answer those queries is through an exhaustive inspection of all edges. Finally, we consider the methods that update the graph. It is easy to add a new vertex or a new edge to the graph in O(1) time. For example, a new edge can be added to the graph by creating an Edge instance storing the given element as data, adding that instance to the positional list E, and recording its resulting Position within E as an attribute of the edge. That stored position can later be used to locate and remove this edge from E in O(1) time, and thus implement the method removeEdge(e). It is worth discussing why the removeVertex(v) method has a running time of O(m). As stated in the graph ADT, when a vertex v is removed from the graph, all edges incident to v must also be removed (otherwise, we would have a contradiction of edges that refer to vertices that are not part of the graph). To locate the incident edges to the vertex, we must examine all edges of E. Method Running Time numVertices(), numEdges() O(1) vertices() O(n) edges() O(m) getEdge(u, v), outDegree(v), outgoingEdges(v) O(m) insertVertex(x), insertEdge(u, v, x), removeEdge(e) O(1) removeVertex(v) O(m) Table 14.2: Running times of the methods of a graph implemented with the edge list structure. The space used is O(n+m), where n is the number of vertices and m is the number of edges.

Chapter 14. Graph Algorithms 14.2.2 Adjacency List Structure The adjacency list structure for a graph adds extra information to the edge list structure that supports direct access to the incident edges (and thus to the adjacent vertices) of each vertex. Speciﬁcally, for each vertex v, we maintain a collection I(v), called the incidence collection of v, whose entries are edges incident to v. In the case of a directed graph, outgoing and incoming edges can be respectively stored in two separate collections, Iout(v) and Iin(v). Traditionally, the incidence collection I(v) for a vertex v is a list, which is why we call this way of representing a graph the adjacency list structure. We require that the primary structure for an adjacency list maintain the collection V of vertices in a way so that we can locate the secondary structure I(v) for a given vertex v in O(1) time. This could be done by using a positional list to represent V, with each Vertex instance maintaining a direct reference to its I(v) incidence collection; we illustrate such an adjacency list structure of a graph in Figure 14.5. If vertices can be uniquely numbered from 0 to n−1, we could instead use a primary array-based structure to access the appropriate secondary lists. The primary beneﬁt of an adjacency list is that the collection I(v) (or more speciﬁcally, Iout(v)) contains exactly those edges that should be reported by the method outgoingEdges(v). Therefore, we can implement this method by iterating the edges of I(v) in O(deg(v)) time, where deg(v) is the degree of vertex v. This is the best possible outcome for any graph representation, because there are deg(v) edges to be reported. h e g v u w z f f h h g e f e g u v w z V (a) (b) Figure 14.5: (a) An undirected graph G; (b) a schematic representation of the adjacency list structure for G. Collection V is the primary list of vertices, and each vertex has an associated list of incident edges. Although not diagrammed as such, we presume that each edge of the graph is represented with a unique Edge instance that maintains references to its endpoint vertices, and that E is a list of all edges.

14.2. Data Structures for Graphs Performance of the Adjacency List Structure Table 14.3 summarizes the performance of the adjacency list structure implementation of a graph, assuming that the primary collection V and E, and all secondary collections I(v) are implemented with doubly linked lists. Asymptotically, the space requirements for an adjacency list are the same as an edge list structure, using O(n + m) space for a graph with n vertices and m edges. It is clear that the primary lists of vertices and edges use O(n + m) space. In addition, the sum of the lengths of all secondary lists is O(m), for reasons that were formalized in Propositions 14.8 and 14.9. In short, an undirected edge (u,v) is referenced in both I(u) and I(v), but its presence in the graph results in only a constant amount of additional space. We have already noted that the outgoingEdges(v) method can be achieved in O(deg(v)) time based on use of I(v). For a directed graph, this is more speciﬁcally O(outdeg(v)) based on use of Iout(v). The outDegree(v) method of the graph ADT can run in O(1) time, assuming collection I(v) can report its size in similar time. To locate a speciﬁc edge for implementing getEdge(u, v), we can search through either I(u) and I(v) (or for a directed graph, either Iout(u) or Iin(v)). By choosing the smaller of the two, we get O(min(deg(u),deg(v))) running time. The rest of the bounds in Table 14.3 can be achieved with additional care. To efﬁciently support deletions of edges, an edge (u,v) would need to maintain a reference to its positions within both I(u) and I(v), so that it could be deleted from those collections in O(1) time. To remove a vertex v, we must also remove any incident edges, but at least we can locate those edges in O(deg(v)) time. Method Running Time numVertices(), numEdges() O(1) vertices() O(n) edges() O(m) getEdge(u, v) O(min(deg(u),deg(v))) outDegree(v), inDegree(v) O(1) outgoingEdges(v), incomingEdges(v) O(deg(v)) insertVertex(x), insertEdge(u, v, x) O(1) removeEdge(e) O(1) removeVertex(v) O(deg(v)) Table 14.3: Running times of the methods of a graph implemented with the adjacency list structure. The space used is O(n+m), where n is the number of vertices and m is the number of edges.

Chapter 14. Graph Algorithms 14.2.3 Adjacency Map Structure In the adjacency list structure, we assume that the secondary incidence collections are implemented as unordered linked lists. Such a collection I(v) uses space proportional to O(deg(v)), allows an edge to be added or removed in O(1) time, and allows an iteration of all edges incident to vertex v in O(deg(v)) time. However, the best implementation of getEdge(u, v) requires O(min(deg(u),deg(v))) time, because we must search through either I(u) or I(v). We can improve the performance by using a hash-based map to implement I(v) for each vertex v. Speciﬁcally, we let the opposite endpoint of each incident edge serve as a key in the map, with the edge structure serving as the value. We call such a graph representation an adjacency map. (See Figure 14.6.) The space usage for an adjacency map remains O(n+ m), because I(v) uses O(deg(v)) space for each vertex v, as with the adjacency list. The advantage of the adjacency map, relative to an adjacency list, is that the getEdge(u, v) method can be implemented in expected O(1) time by searching for vertex u as a key in I(v), or vice versa. This provides a likely improvement over the adjacency list, while retaining the worst-case bound of O(min(deg(u),deg(v))). In comparing the performance of adjacency map to other representations (see Table 14.1), we ﬁnd that it essentially achieves optimal running times for all methods, making it an excellent all-purpose choice as a graph representation. h e g v u w z f g h w h u u w v g e f e w v u z f v w z V (a) (b) Figure 14.6: (a) An undirected graph G; (b) a schematic representation of the adjacency map structure for G. Each vertex maintains a secondary map in which neighboring vertices serve as keys, with the connecting edges as associated values. As with the adjacency list, we presume that there is also an overall list E of all Edge instances.

14.2. Data Structures for Graphs 14.2.4 Adjacency Matrix Structure The adjacency matrix structure for a graph G augments the edge list structure with a matrix A (that is, a two-dimensional array, as in Section 3.1.5), which allows us to locate an edge between a given pair of vertices in worst-case constant time. In the adjacency matrix representation, we think of the vertices as being the integers in the set {0,1,...,n−1} and the edges as being pairs of such integers. This allows us to store references to edges in the cells of a two-dimensional n × n array A. Speciﬁcally, the cell A[i][ j] holds a reference to the edge (u,v), if it exists, where u is the vertex with index i and v is the vertex with index j. If there is no such edge, then A[i][ j] = null. We note that array A is symmetric if graph G is undirected, as A[i][ j] = A[ j][i] for all pairs i and j. (See Figure 14.7.) The most signiﬁcant advantage of an adjacency matrix is that any edge (u,v) can be accessed in worst-case O(1) time; recall that the adjacency map supports that operation in O(1) expected time. However, several operation are less efﬁcient with an adjacency matrix. For example, to ﬁnd the edges incident to vertex v, we must presumably examine all n entries in the row associated with v; recall that an adjacency list or map can locate those edges in optimal O(deg(v)) time. Adding or removing vertices from a graph is problematic, as the matrix must be resized. Furthermore, the O(n2) space usage of an adjacency matrix is typically far worse than the O(n + m) space required of the other representations. Although, in the worst case, the number of edges in a dense graph will be proportional to n2, most real-world graphs are sparse. In such cases, use of an adjacency matrix is inefﬁcient. However, if a graph is dense, the constants of proportionality of an adjacency matrix can be smaller than that of an adjacency list or map. In fact, if edges do not have auxiliary data, a boolean adjacency matrix can use one bit per edge slot, such that A[i][ j] = true if and only if associated (u,v) is an edge. h e g v u w z f h u v w z e e g g f f h (a) (b) Figure 14.7: (a) An undirected graph G; (b) a schematic representation of the auxiliary adjacency matrix structure for G, in which n vertices are mapped to indices 0 to n−1. Although not diagrammed as such, we presume that there is a unique Edge instance for each edge, and that it maintains references to its endpoint vertices. We also assume that there is a secondary edge list (not pictured), to allow the edges() method to run in O(m) time, for a graph with m edges.

Chapter 14. Graph Algorithms 14.2.5 Java Implementation In this section, we provide an implementation of the Graph ADT, based on the adjacency map representation, as described in Section 14.2.3. We use positional lists to represent each of the primary lists V and E, as originally described in the edge list representation. Additionally, for each vertex v, we use a hash-based map to represent the secondary incidence map I(v). To gracefully support both undirected and directed graphs, each vertex maintains two different map references: outgoing and incoming. In the directed case, these are initialized to two distinct map instances, representing Iout(v) and Iin(v), respectively. In the case of an undirected graph, we assign both outgoing and incoming as aliases to a single map instance. Our implementation is organized as follows. We assume deﬁnitions for Vertex, Edge, and Graph interfaces, although for the sake of brevity, we do not include those deﬁnitions in the book (they are available online). We then deﬁne a concrete AdjacencyMapGraph class, with nested classes InnerVertex and InnerEdge to implement the vertex and edge abstractions. These classes use generic parameters V and E to designate the element type stored respectively at vertices and edges. We begin in Code Fragment 14.1, with the deﬁnitions of the InnerVertex and InnerEdge classes (although in reality, those deﬁnitions should be nested within the following AdjacencyMapGraph class). Note well how the InnerVertex constructor initializes the outgoing and incoming instance variables depending on whether the overall graph is undirected or directed. Code Fragments 14.2 and 14.3 contain the core implementation of the class AdjacencyMapGraph. A graph instance maintains a boolean variable that designates whether the graph is directed, and it maintains the vertex list and edge list. Although not shown in these code fragments, our implementation includes private validate methods that perform type conversions between the public Vertex and Edge interface types to the concrete InnerVertex and InnerEdge classes, while also performing some error checking. This design is similar to the validate method of the LinkedPositionalList class (see Code Fragment 7.10 of Section 7.3.3), which converts an outward Position to the underlying Node type for that class. The most complex methods are those that modify the graph. When insertVertex is called, we must create a new InnerVertex instance, add that vertex to the list of vertices, and record its position within that list (so that we can efﬁciently delete it from the list if the vertex is removed from the graph). When inserting an edge (u,v), we must also create a new instance, add it to the edge list, and record its position, yet we must also add the new edge to the outgoing adjacency map for vertex u, and the incoming map for vertex v. Code Fragment 14.3 contains code for removeVertex as well; the implementation of removeEdge is not included, but is available in the online version of the code.

14.2. Data Structures for Graphs /∗∗A vertex of an adjacency map graph representation. ∗/ private class InnerVertex<V> implements Vertex<V> { private V element; private Position<Vertex<V>> pos; private Map<Vertex<V>, Edge<E>> outgoing, incoming; /∗∗Constructs a new InnerVertex instance storing the given element. ∗/ public InnerVertex(V elem, boolean graphIsDirected) { element = elem; outgoing = new ProbeHashMap<>(); if (graphIsDirected) incoming = new ProbeHashMap<>(); else incoming = outgoing; // if undirected, alias outgoing map } /∗∗Returns the element associated with the vertex. ∗/ public V getElement() { return element; } /∗∗Stores the position of this vertex within the graph's vertex list. ∗/ public void setPosition(Position<Vertex<V>> p) { pos = p; } /∗∗Returns the position of this vertex within the graph's vertex list. ∗/ public Position<Vertex<V>> getPosition() { return pos; } /∗∗Returns reference to the underlying map of outgoing edges. ∗/ public Map<Vertex<V>, Edge<E>> getOutgoing() { return outgoing; } /∗∗Returns reference to the underlying map of incoming edges. ∗/ public Map<Vertex<V>, Edge<E>> getIncoming() { return incoming; } } //------------ end of InnerVertex class ------------ /∗∗An edge between two vertices. ∗/ private class InnerEdge<E> implements Edge<E> { private E element; private Position<Edge<E>> pos; private Vertex<V>[ ] endpoints; /∗∗Constructs InnerEdge instance from u to v, storing the given element. ∗/ public InnerEdge(Vertex<V> u, Vertex<V> v, E elem) { element = elem; endpoints = (Vertex<V>[ ]) new Vertex[ ]{u,v}; // array of length 2 } /∗∗Returns the element associated with the edge. ∗/ public E getElement() { return element; } /∗∗Returns reference to the endpoint array. ∗/ public Vertex<V>[ ] getEndpoints() { return endpoints; } /∗∗Stores the position of this edge within the graph's vertex list. ∗/ public void setPosition(Position<Edge<E>> p) { pos = p; } /∗∗Returns the position of this edge within the graph's vertex list. ∗/ public Position<Edge<E>> getPosition() { return pos; } } //------------ end of InnerEdge class ------------ Code Fragment 14.1: InnerVertex and InnerEdge classes (to be nested within the AdjacencyMapGraph class). Interfaces Vertex<V> and Edge<E> are not shown.

Chapter 14. Graph Algorithms public class AdjacencyMapGraph<V,E> implements Graph<V,E> { // nested InnerVertex and InnerEdge classes deﬁned here... private boolean isDirected; private PositionalList<Vertex<V>> vertices = new LinkedPositionalList<>(); private PositionalList<Edge<E>> edges = new LinkedPositionalList<>(); /∗∗Constructs an empty graph (either undirected or directed). ∗/ public AdjacencyMapGraph(boolean directed) { isDirected = directed; } /∗∗Returns the number of vertices of the graph ∗/ public int numVertices() { return vertices.size(); } /∗∗Returns the vertices of the graph as an iterable collection ∗/ public Iterable<Vertex<V>> vertices() { return vertices; } /∗∗Returns the number of edges of the graph ∗/ public int numEdges() { return edges.size(); } /∗∗Returns the edges of the graph as an iterable collection ∗/ public Iterable<Edge<E>> edges() { return edges; } /∗∗Returns the number of edges for which vertex v is the origin. ∗/ public int outDegree(Vertex<V> v) { InnerVertex<V> vert = validate(v); return vert.getOutgoing().size(); } /∗∗Returns an iterable collection of edges for which vertex v is the origin. ∗/ public Iterable<Edge<E>> outgoingEdges(Vertex<V> v) { InnerVertex<V> vert = validate(v); return vert.getOutgoing().values(); // edges are the values in the adjacency map } /∗∗Returns the number of edges for which vertex v is the destination. ∗/ public int inDegree(Vertex<V> v) { InnerVertex<V> vert = validate(v); return vert.getIncoming().size(); } /∗∗Returns an iterable collection of edges for which vertex v is the destination. ∗/ public Iterable<Edge<E>> incomingEdges(Vertex<V> v) { InnerVertex<V> vert = validate(v); return vert.getIncoming().values(); // edges are the values in the adjacency map } public Edge<E> getEdge(Vertex<V> u, Vertex<V> v) { /∗∗Returns the edge from u to v, or null if they are not adjacent. ∗/ InnerVertex<V> origin = validate(u); return origin.getOutgoing().get(v); // will be null if no edge from u to v } /∗∗Returns the vertices of edge e as an array of length two. ∗/ public Vertex<V>[ ] endVertices(Edge<E> e) { InnerEdge<E> edge = validate(e); return edge.getEndpoints(); } Code Fragment 14.2: AdjacencyMapGraph class deﬁnition. (Continues in Code Fragment 14.3.) The validate(v) and validate(e) methods are available online.

14.2. Data Structures for Graphs /∗∗Returns the vertex that is opposite vertex v on edge e. ∗/ public Vertex<V> opposite(Vertex<V> v, Edge<E> e) throws IllegalArgumentException { InnerEdge<E> edge = validate(e); Vertex<V>[ ] endpoints = edge.getEndpoints(); if (endpoints[0] == v) return endpoints[1]; else if (endpoints[1] == v) return endpoints[0]; else throw new IllegalArgumentException("v is not incident to this edge"); } /∗∗Inserts and returns a new vertex with the given element. ∗/ public Vertex<V> insertVertex(V element) { InnerVertex<V> v = new InnerVertex<>(element, isDirected); v.setPosition(vertices.addLast(v)); return v; } /∗∗Inserts and returns a new edge between u and v, storing given element. ∗/ public Edge<E> insertEdge(Vertex<V> u, Vertex<V> v, E element) throws IllegalArgumentException { if (getEdge(u,v) == null) { InnerEdge<E> e = new InnerEdge<>(u, v, element); e.setPosition(edges.addLast(e)); InnerVertex<V> origin = validate(u); InnerVertex<V> dest = validate(v); origin.getOutgoing().put(v, e); dest.getIncoming().put(u, e); return e; } else throw new IllegalArgumentException("Edge from u to v exists"); } /∗∗Removes a vertex and all its incident edges from the graph. ∗/ public void removeVertex(Vertex<V> v) { InnerVertex<V> vert = validate(v); // remove all incident edges from the graph for (Edge<E> e : vert.getOutgoing().values()) removeEdge(e); for (Edge<E> e : vert.getIncoming().values()) removeEdge(e); // remove this vertex from the list of vertices vertices.remove(vert.getPosition()); } } Code Fragment 14.3: AdjacencyMapGraph class deﬁnition (continued from Code Fragment 14.2). We omit the removeEdge method, for brevity.

Chapter 14. Graph Algorithms 14.3 Graph Traversals Greek mythology tells of an elaborate labyrinth that was built to house the monstrous Minotaur, which was part bull and part man. This labyrinth was so complex that neither beast nor human could escape it. No human, that is, until the Greek hero, Theseus, with the help of the king’s daughter, Ariadne, decided to implement a graph traversal algorithm. Theseus fastened a ball of thread to the door of the labyrinth and unwound it as he traversed the twisting passages in search of the monster. Theseus obviously knew about good algorithm design, for, after ﬁnding and defeating the beast, Theseus easily followed the string back out of the labyrinth to the loving arms of Ariadne. Formally, a traversal is a systematic procedure for exploring a graph by examining all of its vertices and edges. A traversal is efﬁcient if it visits all the vertices and edges in time proportional to their number, that is, in linear time. Graph traversal algorithms are key to answering many fundamental questions about graphs involving the notion of reachability, that is, in determining how to travel from one vertex to another while following paths of a graph. Interesting problems that deal with reachability in an undirected graph G include the following: • Computing a path from vertex u to vertex v, or reporting that no such path exists. • Given a start vertex s of G, computing, for every vertex v of G, a path with the minimum number of edges between s and v, or reporting that no such path exists. • Testing whether G is connected. • Computing a spanning tree of G, if G is connected. • Computing the connected components of G. • Identifying a cycle in G, or reporting that G has no cycles. Interesting problems that deal with reachability in a directed graph ⃗G include the following: • Computing a directed path from vertex u to vertex v, or reporting that no such path exists. • Finding all the vertices of ⃗G that are reachable from a given vertex s. • Determine whether ⃗G is acyclic. • Determine whether ⃗G is strongly connected. In the remainder of this section, we will present two efﬁcient graph traversal algorithms, called depth-ﬁrst search and breadth-ﬁrst search, respectively.

14.3. Graph Traversals 14.3.1 Depth-First Search The ﬁrst traversal algorithm we consider in this section is depth-ﬁrst search (DFS). Depth-ﬁrst search is useful for testing a number of properties of graphs, including whether there is a path from one vertex to another and whether or not a graph is connected. Depth-ﬁrst search in a graph G is analogous to wandering in a labyrinth with a string and a can of paint without getting lost. We begin at a speciﬁc starting vertex s in G, which we initialize by ﬁxing one end of our string to s and painting s as “visited.” The vertex s is now our “current” vertex. In general, if we call our current vertex u, we traverse G by considering an arbitrary edge (u,v) incident to the current vertex u. If the edge (u,v) leads us to a vertex v that is already visited (that is, painted), we ignore that edge. If, on the other hand, (u,v) leads to an unvisited vertex v, then we unroll our string, and go to v. We then paint v as “visited,” and make it the current vertex, repeating the computation above. Eventually, we will get to a “dead end,” that is, a current vertex v such that all the edges incident to v lead to vertices already visited. To get out of this impasse, we roll our string back up, backtracking along the edge that brought us to v, going back to a previously visited vertex u. We then make u our current vertex and repeat the computation above for any edges incident to u that we have not yet considered. If all of u’s incident edges lead to visited vertices, then we again roll up our string and backtrack to the vertex we came from to get to u, and repeat the procedure at that vertex. Thus, we continue to backtrack along the path that we have traced so far until we ﬁnd a vertex that has yet unexplored edges, take one such edge, and continue the traversal. The process terminates when our backtracking leads us back to the start vertex s, and there are no more unexplored edges incident to s. The pseudocode for a depth-ﬁrst search traversal starting at a vertex u (see Code Fragment 14.4) follows our analogy with string and paint. We use recursion to implement the string analogy, and we assume that we have a mechanism (the paint analogy) to determine whether a vertex or edge has been previously explored. Algorithm DFS(G, u): Input: A graph G and a vertex u of G Output: A collection of vertices reachable from u, with their discovery edges Mark vertex u as visited. for each of u’s outgoing edges, e = (u,v) do if vertex v has not been visited then Record edge e as the discovery edge for vertex v. Recursively call DFS(G, v). Code Fragment 14.4: The DFS algorithm.

Chapter 14. Graph Algorithms Classifying Graph Edges with DFS An execution of depth-ﬁrst search can be used to analyze the structure of a graph, based upon the way in which edges are explored during the traversal. The DFS process naturally identiﬁes what is known as the depth-ﬁrst search tree rooted at a starting vertex s. Whenever an edge e = (u,v) is used to discover a new vertex v during the DFS algorithm of Code Fragment 14.4, that edge is known as a discovery edge or tree edge, as oriented from u to v. All other edges that are considered during the execution of DFS are known as nontree edges, which take us to a previously visited vertex. In the case of an undirected graph, we will ﬁnd that all nontree edges that are explored connect the current vertex to one that is an ancestor of it in the DFS tree. We will call such an edge a back edge. When performing a DFS on a directed graph, there are three possible kinds of nontree edges: • back edges, which connect a vertex to an ancestor in the DFS tree • forward edges, which connect a vertex to a descendant in the DFS tree • cross edges, which connect a vertex to a vertex that is neither its ancestor nor its descendant An example application of the DFS algorithm on a directed graph is shown in Figure 14.8, demonstrating each type of nontree edge. An example application of the DFS algorithm on an undirected graph is shown in Figure 14.9. BOS JFK ORD MIA SFO LAX DFW SFO MIA JFK DFW BOS ORD LAX (a) (b) Figure 14.8: An example of a DFS in a directed graph, starting at vertex (BOS): (a) intermediate step, where, for the ﬁrst time, a considered edge leads to an already visited vertex (DFW); (b) the completed DFS. The tree edges are shown with thick blue lines, the back edges are shown with dashed blue lines, and the forward and cross edges are shown with dotted black lines. The order in which the vertices are visited is indicated by a label next to each vertex. The edge (ORD,DFW) is a back edge, but (DFW,ORD) is a forward edge. Edge (BOS,SFO) is a forward edge, and (SFO,LAX) is a cross edge.

14.3. Graph Traversals A C D E F G H I J K L M N O P B A C D E F G H I J K L M N O P B (a) (b) A C D E F G H I J K L M N O P B A C D E F G H I J K L M N O P B (c) (d) A C D E F G H I J K L M N O P B A C D E F G H I J K L M N O P B (e) (f) Figure 14.9: Example of depth-ﬁrst search traversal on an undirected graph starting at vertex A. We assume that a vertex’s adjacencies are considered in alphabetical order. Visited vertices and explored edges are highlighted, with discovery edges drawn as solid lines and nontree (back) edges as dashed lines: (a) input graph; (b) path of tree edges, traced from A until back edge (G,C) is examined; (c) reaching F, which is a dead end; (d) after backtracking to I, resuming with edge (I,M), and hitting another dead end at O; (e) after backtracking to G, continuing with edge (G,L), and hitting another dead end at H; (f) ﬁnal result.

Chapter 14. Graph Algorithms Properties of a Depth-First Search There are a number of observations that we can make about the depth-ﬁrst search algorithm, many of which derive from the way the DFS algorithm partitions the edges of a graph G into groups. We will begin with the most signiﬁcant property. Proposition 14.12: Let G be an undirected graph on which a DFS traversal starting at a vertex s has been performed. Then the traversal visits all vertices in the connected component of s, and the discovery edges form a spanning tree of the connected component of s. Justiﬁcation: Suppose there is at least one vertex w in s’s connected component not visited, and let v be the ﬁrst unvisited vertex on some path from s to w (we may have v = w). Since v is the ﬁrst unvisited vertex on this path, it has a neighbor u that was visited. But when we visited u, we must have considered the edge (u,v); hence, it cannot be correct that v is unvisited. Therefore, there are no unvisited vertices in s’s connected component. Since we only follow a discovery edge when we go to an unvisited vertex, we will never form a cycle with such edges. Therefore, the discovery edges form a connected subgraph without cycles, hence a tree. Moreover, this is a spanning tree because, as we have just seen, the depth-ﬁrst search visits each vertex in the connected component of s. Proposition 14.13: Let ⃗G be a directed graph. Depth-ﬁrst search on ⃗G starting at a vertex s visits all the vertices of ⃗G that are reachable from s. Also, the DFS tree contains directed paths from s to every vertex reachable from s. Justiﬁcation: Let Vs be the subset of vertices of ⃗G visited by DFS starting at vertex s. We want to show that Vs contains s and every vertex reachable from s belongs to Vs. Suppose now, for the sake of a contradiction, that there is a vertex w reachable from s that is not in Vs. Consider a directed path from s to w, and let (u,v) be the ﬁrst edge on such a path taking us out of Vs, that is, u is in Vs but v is not in Vs. When DFS reaches u, it explores all the outgoing edges of u, and thus must also reach vertex v via edge (u,v). Hence, v should be in Vs, and we have obtained a contradiction. Therefore, Vs must contain every vertex reachable from s. We prove the second fact by induction on the steps of the algorithm. We claim that each time a discovery edge (u,v) is identiﬁed, there exists a directed path from s to v in the DFS tree. Since u must have previously been discovered, there exists a path from s to u, so by appending the edge (u,v) to that path, we have a directed path from s to v. Note that since back edges always connect a vertex v to a previously visited vertex u, each back edge implies a cycle in G, consisting of the discovery edges from u to v plus the back edge (u,v).

14.3. Graph Traversals Running Time of Depth-First Search In terms of its running time, depth-ﬁrst search is an efﬁcient method for traversing a graph. Note that DFS is called at most once on each vertex (since it gets marked as visited), and therefore every edge is examined at most twice for an undirected graph, once from each of its end vertices, and at most once in a directed graph, from its origin vertex. If we let ns ≤n be the number of vertices reachable from a vertex s, and ms ≤m be the number of incident edges to those vertices, a DFS starting at s runs in O(ns+ms) time, provided the following conditions are satisﬁed: • The graph is represented by a data structure such that creating and iterating through the outgoingEdges(v) takes O(deg(v)) time, and the opposite(v, e) method takes O(1) time. The adjacency list structure is one such structure, but the adjacency matrix structure is not. • We have a way to “mark” a vertex or edge as explored, and to test if a vertex or edge has been explored in O(1) time. We discuss ways of implementing DFS to achieve this goal in the next section. Given the assumptions above, we can solve a number of interesting problems. Proposition 14.14: Let G be an undirected graph with n vertices and m edges. A DFS traversal of G can be performed in O(n + m) time, and can be used to solve the following problems in O(n+m) time: • Computing a path between two given vertices of G, if one exists. • Testing whether G is connected. • Computing a spanning tree of G, if G is connected. • Computing the connected components of G. • Computing a cycle in G, or reporting that G has no cycles. Proposition 14.15: Let ⃗G be a directed graph with n vertices and m edges. A DFS traversal of ⃗G can be performed in O(n + m) time, and can be used to solve the following problems in O(n+m) time: • Computing a directed path between two given vertices of ⃗G, if one exists. • Computing the set of vertices of ⃗G that are reachable from a given vertex s. • Testing whether ⃗G is strongly connected. • Computing a directed cycle in ⃗G, or reporting that ⃗G is acyclic. The justiﬁcation of Propositions 14.14 and 14.15 is based on algorithms that use slightly modiﬁed versions of the DFS algorithm as subroutines. We will explore some of those extensions in the remainder of this section.

Chapter 14. Graph Algorithms 14.3.2 DFS Implementation and Extensions We will begin by providing a Java implementation of the depth-ﬁrst search algorithm. We originally described the algorithm with pseudocode in Code Fragment 14.4. In order to implement it, we must have a mechanism for keeping track of which vertices have been visited, and for recording the resulting DFS tree edges. For this bookkeeping, we use two auxiliary data structures. First, we maintain a set, named known, containing vertices that have already been visited. Second, we keep a map, named forest, that associates, with a vertex v, the edge e of the graph that is used to discover v (if any). Our DFS method is presented in Code Fragment 14.5. /∗∗Performs depth-ﬁrst search of Graph g starting at Vertex u. ∗/ public static <V,E> void DFS(Graph<V,E> g, Vertex<V> u, Set<Vertex<V>> known, Map<Vertex<V>,Edge<E>> forest) { known.add(u); // u has been discovered for (Edge<E> e : g.outgoingEdges(u)) { // for every outgoing edge from u Vertex<V> v = g.opposite(u, e); if (!known.contains(v)) { forest.put(v, e); // e is the tree edge that discovered v DFS(g, v, known, forest); // recursively explore from v } } } Code Fragment 14.5: Recursive implementation of depth-ﬁrst search on a graph, starting at a designated vertex u. As an outcome of a call, visited vertices are added to the known set, and discovery edges are added to the forest. Our DFS method does not make any assumption about how the Set or Map instances are implemented; however, the O(n + m) running-time analysis of the previous section does presume that we can “mark” a vertex as explored or test the status of a vertex in O(1) time. If we use hash-based implementations of the set and map structure, then all of their operations run in O(1) expected time, and the overall algorithm runs in O(n+m) time with very high probability. In practice, this is a compromise we are willing to accept. If vertices can be numbered from 0,...,n−1 (a common assumption for graph algorithms), then the set and map can be implemented more directly as a lookup table, with a vertex label used as an index into an array of size n. In that case, the necessary set and map operations run in worst-case O(1) time. Alternatively, we can “decorate” each vertex with the auxiliary information, either by leveraging the generic type of the element that is stored with each vertex, or by redesigning the Vertex type to store additional ﬁelds. That would allow marking operations to be performed in O(1)-time, without any assumption about vertices being numbered.

14.3. Graph Traversals Reconstructing a Path from u to v We can use the basic DFS method as a tool to identify the (directed) path leading from vertex u to v, if v is reachable from u. This path can easily be reconstructed from the information that was recorded in the forest of discovery edges during the traversal. Code Fragment 14.6 provides an implementation of a secondary method that produces an ordered list of vertices on the path from u to v, if given the map of discovery edges that was computed by the original DFS method. To reconstruct the path, we begin at the end of the path, examining the forest of discovery edges to determine what edge was used to reach vertex v. We then determine the opposite vertex of that edge and repeat the process to determine what edge was used to discover it. By continuing this process until reaching u, we can construct the entire path. Assuming constant-time lookup in the forest map, the path reconstruction takes time proportional to the length of the path, and therefore, it runs in O(n) time (in addition to the time originally spent calling DFS). /∗∗Returns an ordered list of edges comprising the directed path from u to v. ∗/ public static <V,E> PositionalList<Edge<E>> constructPath(Graph<V,E> g, Vertex<V> u, Vertex<V> v, Map<Vertex<V>,Edge<E>> forest) { PositionalList<Edge<E>> path = new LinkedPositionalList<>(); if (forest.get(v) != null) { // v was discovered during the search Vertex<V> walk = v; // we construct the path from back to front while (walk != u) { Edge<E> edge = forest.get(walk); path.addFirst(edge); // add edge to *front* of path walk = g.opposite(walk, edge); // repeat with opposite endpoint } } return path; } Code Fragment 14.6: Method to reconstruct a directed path from u to v, given the trace of discovery from a DFS started at u. The method returns an ordered list of vertices on the path. Testing for Connectivity We can use the basic DFS method to determine whether a graph is connected. In the case of an undirected graph, we simply start a depth-ﬁrst search at an arbitrary vertex and then test whether known.size() equals n at the conclusion. If the graph is connected, then by Proposition 14.12, all vertices will have been discovered; conversely, if the graph is not connected, there must be at least one vertex v that is not reachable from u, and that will not be discovered.

Chapter 14. Graph Algorithms For directed graph, ⃗G, we may wish to test whether it is strongly connected, that is, whether for every pair of vertices u and v, both u reaches v and v reaches u. If we start an independent call to DFS from each vertex, we could determine whether this was the case, but those n calls when combined would run in O(n(n+m)). However, we can determine if ⃗G is strongly connected much faster than this, requiring only two depth-ﬁrst searches. We begin by performing a depth-ﬁrst search of our directed graph ⃗G starting at an arbitrary vertex s. If there is any vertex of ⃗G that is not visited by this traversal, and is not reachable from s, then the graph is not strongly connected. If this ﬁrst depth-ﬁrst search visits each vertex of ⃗G, we need to then check whether s is reachable from all other vertices. Conceptually, we can accomplish this by making a copy of graph ⃗G, but with the orientation of all edges reversed. A depth-ﬁrst search starting at s in the reversed graph will reach every vertex that could reach s in the original. In practice, a better approach than making a new graph is to reimplement a version of the DFS method that loops through all incoming edges to the current vertex, rather than all outgoing edges. Since this algorithm makes just two DFS traversals of ⃗G, it runs in O(n+m) time. Computing All Connected Components When a graph is not connected, the next goal we may have is to identify all of the connected components of an undirected graph, or the strongly connected components of a directed graph. We will begin by discussing the undirected case. If an initial call to DFS fails to reach all vertices of a graph, we can restart a new call to DFS at one of those unvisited vertices. An implementation of such a comprehensive DFSComplete method is given in Code Fragment 14.7. It returns a map that represents a DFS forest for the entire graph. We say this is a forest rather than a tree, because the graph may not be connected. Vertices that serve as roots of DFS trees within this forest will not have discovery edges and will not appear as keys in the returned map. Therefore, the number of connected components of the graph g is equal to g.numVertices() −forest.size(). /∗∗Performs DFS for the entire graph and returns the DFS forest as a map. ∗/ public static <V,E> Map<Vertex<V>,Edge<E>> DFSComplete(Graph<V,E> g) { Set<Vertex<V>> known = new HashSet<>(); Map<Vertex<V>,Edge<E>> forest = new ProbeHashMap<>(); for (Vertex<V> u : g.vertices()) if (!known.contains(u)) DFS(g, u, known, forest); // (re)start the DFS process at u return forest; } Code Fragment 14.7: Top-level method that returns a DFS forest for an entire graph.

14.3. Graph Traversals We can further determine which vertices are in which component, either by examining the structure of the forest that is returned, or by making a minor modiﬁcation to the core DFS method to tag each vertex with a component number when it is ﬁrst discovered. (See Exercise C-14.43.) Although the DFSComplete method makes multiple calls to the original DFS method, the total time spent by a call to DFSComplete is O(n+ m). For an undirected graph, recall from our original analysis on page 635 that a single call to DFS starting at vertex s runs in time O(ns +ms) where ns is the number of vertices reachable from s, and ms is the number of incident edges to those vertices. Because each call to DFS explores a different component, the sum of ns +ms terms is n+m. The situation is more complex for ﬁnding strongly connected components of a directed graph. The O(n+ m) total bound for a call to DFSComplete applies to the directed case as well, because when restarting the process, we proceed with the existing set of known vertices. This ensures that the DFS subroutine is called once on each vertex, and therefore that each outgoing edge is explored only once during the entire process. As an example, consider again the graph of Figure 14.8. If we were to start the original DFS method at vertex ORD, the known set of vertices would become { ORD, DFW, SFO, LAX, MIA }. If restarting the DFS method at vertex BOS, the outgoing edges to vertices SFO and MIA would not result in further recursion, because those vertices are marked as known. However, the forest returned by a single call to DFSComplete does not represent the strongly connected components of the graph. There exists an approach for computing those components in O(n + m) time, making use of two calls to DFSComplete, but the details are beyond the scope of this book. Detecting Cycles with DFS For both undirected and directed graphs, a cycle exists if and only if a back edge exists relative to the DFS traversal of that graph. It is easy to see that if a back edge exists, a cycle exists by taking the back edge from the descendant to its ancestor and then following the tree edges back to the descendant. Conversely, if a cycle exists in the graph, there must be a back edge relative to a DFS (although we do not prove this fact here). Algorithmically, detecting a back edge in the undirected case is easy, because all edges are either tree edges or back edges. In the case of a directed graph, additional modiﬁcations to the core DFS implementation are needed to properly categorize a nontree edge as a back edge. When a directed edge is explored leading to a previously visited vertex, we must recognize whether that vertex is an ancestor of the current vertex. This can be accomplished, for example, by maintaining another set, with all vertices upon which a recursive call to DFS is currently active. We leave details as an exercise (C-14.42).

Chapter 14. Graph Algorithms 14.3.3 Breadth-First Search The advancing and backtracking of a depth-ﬁrst search, as described in the previous section, deﬁnes a traversal that could be physically traced by a single person exploring a graph. In this section, we will consider another algorithm for traversing a connected component of a graph, known as a breadth-ﬁrst search (BFS). The BFS algorithm is more akin to sending out, in all directions, many explorers who collectively traverse a graph in coordinated fashion. A BFS proceeds in rounds and subdivides the vertices into levels. BFS starts at vertex s, which is at level 0. In the ﬁrst round, we paint as “visited,” all vertices adjacent to the start vertex s; these vertices are one step away from the beginning and are placed into level 1. In the second round, we allow all explorers to go two steps (i.e., edges) away from the starting vertex. These new vertices, which are adjacent to level 1 vertices and not previously assigned to a level, are placed into level 2 and marked as “visited.” This process continues in similar fashion, terminating when no new vertices are found in a level. A Java implementation of BFS is given in Code Fragment 14.8. We follow a convention similar to that of DFS (Code Fragment 14.5), maintaining a known set of vertices, and storing the BFS tree edges in a map. We illustrate a BFS traversal in Figure 14.10. /∗∗Performs breadth-ﬁrst search of Graph g starting at Vertex u. ∗/ public static <V,E> void BFS(Graph<V,E> g, Vertex<V> s, Set<Vertex<V>> known, Map<Vertex<V>,Edge<E>> forest) { PositionalList<Vertex<V>> level = new LinkedPositionalList<>(); known.add(s); level.addLast(s); // ﬁrst level includes only s while (!level.isEmpty()) { PositionalList<Vertex<V>> nextLevel = new LinkedPositionalList<>(); for (Vertex<V> u : level) for (Edge<E> e : g.outgoingEdges(u)) { Vertex<V> v = g.opposite(u, e); if (!known.contains(v)) { known.add(v); forest.put(v, e); // e is the tree edge that discovered v nextLevel.addLast(v); // v will be further considered in next pass } } level = nextLevel; // relabel ’next’ level to become the current } } Code Fragment 14.8: Implementation of breadth-ﬁrst search on a graph, starting at a designated vertex s.

14.3. Graph Traversals F H I J K L M N O P A B C E D G B I J K L M N O P C D G F E A H (a) (b) A J K L M N O P B C E D H G F I K L M N O P I H G F A B C D E J (c) (d) F G H I J K L M N O P A B C D E F G H I J K L M N O P A B C D E (e) (f) Figure 14.10: Example of breadth-ﬁrst search traversal, where the edges incident to a vertex are considered in alphabetical order of the adjacent vertices. The discovery edges are shown with solid lines and the nontree (cross) edges are shown with dashed lines: (a) starting the search at A; (b) discovery of level 1; (c) discovery of level 2; (d) discovery of level 3; (e) discovery of level 4; (f) discovery of level 5.

Chapter 14. Graph Algorithms When discussing DFS, we described a classiﬁcation of nontree edges being either back edges, which connect a vertex to one of its ancestors, forward edges, which connect a vertex to one of its descendants, or cross edges, which connect a vertex to another vertex that is neither its ancestor nor its descendant. For BFS on an undirected graph, all nontree edges are cross edges (see Exercise C-14.46), and for BFS on a directed graph, all nontree edges are either back edges or cross edges (see Exercise C-14.47). The BFS traversal algorithm has a number of interesting properties, some of which we explore in the proposition that follows. Most notably, a path in a breadthﬁrst search tree rooted at vertex s to any other vertex v is guaranteed to be the shortest such path from s to v in terms of the number of edges. Proposition 14.16: Let G be an undirected or directed graph on which a BFS traversal starting at vertex s has been performed. Then • The traversal visits all vertices of G that are reachable from s. • For each vertex v at level i, the path of the BFS tree T between s and v has i edges, and any other path of G from s to v has at least i edges. • If (u,v) is an edge that is not in the BFS tree, then the level number of v can be at most 1 greater than the level number of u. We leave the justiﬁcation of this proposition as Exercise C-14.49. The analysis of the running time of BFS is similar to the one of DFS, with the algorithm running in O(n + m) time, or more speciﬁcally, in O(ns + ms) time if ns is the number of vertices reachable from vertex s, and ms ≤m is the number of incident edges to those vertices. To explore the entire graph, the process can be restarted at another vertex, akin to the DFSComplete method of Code Fragment 14.7. The actual path from vertex s to vertex v can be reconstructed using the constructPath method of Code Fragment 14.6 Proposition 14.17: Let G be a graph with n vertices and m edges represented with the adjacency list structure. A BFS traversal of G takes O(n+m) time. Although our implementation of BFS in Code Fragment 14.8 progresses level by level, the BFS algorithm can also be implemented using a single FIFO queue to represent the current fringe of the search. Starting with the source vertex in the queue, we repeatedly remove the vertex from the front of the queue and insert any of its unvisited neighbors to the back of the queue. (See Exercise C-14.50.) In comparing the capabilities of DFS and BFS, both can be used to efﬁciently ﬁnd the set of vertices that are reachable from a given source, and to determine paths to those vertices. However, BFS guarantees that those paths use as few edges as possible. For an undirected graph, both algorithms can be used to test connectivity, to identify connected components, or to locate a cycle. For directed graphs, the DFS algorithm is better suited for certain tasks, such as ﬁnding a directed cycle in the graph, or in identifying the strongly connected components.

14.4. Transitive Closure 14.4 Transitive Closure We have seen that graph traversals can be used to answer basic questions of reachability in a directed graph. In particular, if we are interested in knowing whether there is a path from vertex u to vertex v in a graph, we can perform a DFS or BFS traversal starting at u and observe whether v is discovered. If representing a graph with an adjacency list or adjacency map, we can answer the question of reachability for u and v in O(n+m) time (see Propositions 14.15 and 14.17). In certain applications, we may wish to answer many reachability queries more efﬁciently, in which case it may be worthwhile to precompute a more convenient representation of a graph. For example, the ﬁrst step for a service that computes driving directions from an origin to a destination might be to assess whether the destination is reachable. Similarly, in an electricity network, we may wish to be able to quickly determine whether current ﬂows from one particular vertex to another. Motivated by such applications, we introduce the following deﬁnition. The transitive closure of a directed graph ⃗G is itself a directed graph ⃗G∗such that the vertices of ⃗G∗are the same as the vertices of ⃗G, and ⃗G∗has an edge (u,v), whenever ⃗G has a directed path from u to v (including the case where (u,v) is an edge of the original ⃗G). If a graph is represented as an adjacency list or adjacency map, we can compute its transitive closure in O(n(n+m)) time by making use of n graph traversals, one from each starting vertex. For example, a DFS starting at vertex u can be used to determine all vertices reachable from u, and thus a collection of edges originating with u in the transitive closure. In the remainder of this section, we explore an alternative technique for computing the transitive closure of a directed graph that is particularly well suited for when a directed graph is represented by a data structure that supports O(1)-time lookup for the getEdge(u, v) method (for example, the adjacency-matrix structure). Let ⃗G be a directed graph with n vertices and m edges. We compute the transitive closure of ⃗G in a series of rounds. We initialize ⃗G0 = ⃗G. We also arbitrarily number the vertices of ⃗G as v1,v2,..., vn. We then begin the computation of the rounds, beginning with round 1. In a generic round k, we construct directed graph ⃗Gk starting with ⃗Gk = ⃗Gk−1 and adding to ⃗Gk the directed edge (vi,vj) if directed graph ⃗Gk−1 contains both the edges (vi,vk) and (vk,vj). In this way, we will enforce a simple rule embodied in the proposition that follows. Proposition 14.18: For i = 1,..., n, directed graph ⃗Gk has an edge (vi,vj) if and only if directed graph ⃗G has a directed path from vi to vj, whose intermediate vertices (if any) are in the set {v1,...,vk}. In particular, ⃗Gn is equal to ⃗G∗, the transitive closure of ⃗G.

Chapter 14. Graph Algorithms Proposition 14.18 suggests a simple algorithm for computing the transitive closure of ⃗G that is based on the series of rounds to compute each ⃗Gk. This algorithm is known as the Floyd-Warshall algorithm, and its pseudocode is given in Code Fragment 14.9. We illustrate an example run of the Floyd-Warshall algorithm in Figure 14.11. Algorithm FloydWarshall(⃗G): Input: A directed graph ⃗G with n vertices Output: The transitive closure ⃗G∗of ⃗G let v1,v2,..., vn be an arbitrary numbering of the vertices of ⃗G ⃗G0 = ⃗G for k = 1 to n do ⃗Gk = ⃗Gk−1 for all i, j in {1,..., n} with i ̸= j and i, j ̸= k do if both edges (vi,vk) and (vk,vj) are in ⃗Gk−1 then add edge (vi,vj) to ⃗Gk (if it is not already present) return ⃗Gn Code Fragment 14.9: Pseudocode for the Floyd-Warshall algorithm. This algorithm computes the transitive closure ⃗G∗of G by incrementally computing a series of directed graphs ⃗G0,⃗G1,..., ⃗Gn, for k = 1,..., n. From this pseudocode, we can easily analyze the running time of the FloydWarshall algorithm assuming that the data structure representing G supports methods getEdge and insertEdge in O(1) time. The main loop is executed n times and the inner loop considers each of O(n2) pairs of vertices, performing a constant-time computation for each one. Thus, the total running time of the Floyd-Warshall algorithm is O(n3). From the description and analysis above we may immediately derive the following proposition. Proposition 14.19: Let ⃗G be a directed graph with n vertices, and let ⃗G be represented by a data structure that supports lookup and update of adjacency information in O(1) time. Then the Floyd-Warshall algorithm computes the transitive closure ⃗G∗of ⃗G in O(n3) time. Performance of the Floyd-Warshall Algorithm Asymptotically, the O(n3) running time of the Floyd-Warshall algorithm is no better than that achieved by repeatedly running DFS, once from each vertex, to compute the reachability. However, the Floyd-Warshall algorithm matches the asymptotic bounds of the repeated DFS when a graph is dense, or when a graph is sparse but represented as an adjacency matrix. (See Exercise R-14.13.)

14.4. Transitive Closure v7 v2 v6 v4 v1 v5 v3 DFW MIA SFO ORD JFK BOS LAX v7 v2 v6 v4 v1 v5 v3 MIA SFO ORD JFK LAX BOS DFW (a) (b) v7 v2 v6 v4 v1 v5 v3 LAX MIA SFO ORD JFK BOS DFW v7 v2 v6 v4 v5 v3 v1 JFK LAX DFW BOS SFO ORD MIA (c) (d) v4 v1 v5 v7 v3 v2 v6 SFO ORD MIA LAX BOS JFK DFW v4 v1 v5 v7 v3 v2 v6 LAX BOS JFK DFW SFO ORD MIA (e) (f) Figure 14.11: Sequence of directed graphs computed by the Floyd-Warshall algorithm: (a) initial directed graph ⃗G = ⃗G0 and numbering of the vertices; (b) directed graph ⃗G1; (c) ⃗G2; (d) ⃗G3; (e) ⃗G4; (f) ⃗G5. Note that ⃗G5 = ⃗G6 = ⃗G7. If directed graph ⃗Gk−1 has the edges (vi,vk) and (vk,vj), but not the edge (vi,vj), in the drawing of directed graph ⃗Gk, we show edges (vi,vk) and (vk,vj) with dashed lines, and edge (vi,vj) with a thick line. For example, in (b) existing edges (MIA,LAX) and (LAX,ORD) result in new edge (MIA,ORD).

Chapter 14. Graph Algorithms The importance of the Floyd-Warshall algorithm is that it is much easier to implement than repeated DFS, and much faster in practice because there are relatively few low-level operations hidden within the asymptotic notation. The algorithm is particularly well suited for the use of an adjacency matrix, as a single bit can be used to designate the reachability modeled as an edge (u,v) in the transitive closure. However, note that repeated calls to DFS results in better asymptotic performance when the graph is sparse and represented using an adjacency list or adjacency map. In that case, a single DFS runs in O(n+m) time, and so the transitive closure can be computed in O(n2 +nm) time, which is preferable to O(n3). Java Implementation We will conclude with a Java implementation of the Floyd-Warshall algorithm, as presented in Code Fragment 14.10. Although the pseudocode for the algorithm describes a series of directed graphs ⃗G0,⃗G1,..., ⃗Gn, we directly modify the original graph, repeatedly adding new edges to the closure as we progress through rounds of the Floyd-Warshall algorithm. Also, the pseudocode for the algorithm describes the loops based on vertices being indexed from 0 to n−1. With our graph ADT, we prefer to use Java’s foreach loop syntax directly on the vertices of the graph. Therefore, in Code Fragment 14.10, variables i, j, and k are references to vertices, not integer indices into the sequence of vertices. Finally, we make one additional optimization in the Java implementation, relative to the pseudocode, by not bothering to iterate through values of j unless we have veriﬁed that edge (i,k) exists in the current version of the closure. /∗∗Converts graph g into its transitive closure. ∗/ public static <V,E> void transitiveClosure(Graph<V,E> g) { for (Vertex<V> k : g.vertices()) for (Vertex<V> i : g.vertices()) // verify that edge (i,k) exists in the partial closure if (i != k && g.getEdge(i,k) != null) for (Vertex<V> j : g.vertices()) // verify that edge (k,j) exists in the partial closure if (i != j && j != k && g.getEdge(k,j) != null) // if (i,j) not yet included, add it to the closure if (g.getEdge(i,j) == null) g.insertEdge(i, j, null); } Code Fragment 14.10: Java implementation of the Floyd-Warshall algorithm.

14.5. Directed Acyclic Graphs 14.5 Directed Acyclic Graphs Directed graphs without directed cycles are encountered in many applications. Such a directed graph is often referred to as a directed acyclic graph, or DAG, for short. Applications of such graphs include the following: • Prerequisites between courses of an academic program. • Inheritance between classes of an object-oriented program. • Scheduling constraints between the tasks of a project. We will explore this latter application further in the following example: Example 14.20: In order to manage a large project, it is convenient to break it up into a collection of smaller tasks. The tasks, however, are rarely independent, because scheduling constraints exist between them. (For example, in a house building project, the task of ordering nails obviously precedes the task of nailing shingles to the roof deck.) Clearly, scheduling constraints cannot have circularities, because they would make the project impossible. (For example, in order to get a job you need to have work experience, but in order to get work experience you need to have a job.) The scheduling constraints impose restrictions on the order in which the tasks can be executed. Namely, if a constraint says that task a must be completed before task b is started, then a must precede b in the order of execution of the tasks. Thus, if we model a feasible set of tasks as vertices of a directed graph, and we place a directed edge from u to v whenever the task for u must be executed before the task for v, then we deﬁne a directed acyclic graph. 14.5.1 Topological Ordering The example above motivates the following deﬁnition. Let ⃗G be a directed graph with n vertices. A topological ordering of ⃗G is an ordering v1,...,vn of the vertices of ⃗G such that for every edge (vi,vj) of ⃗G, it is the case that i < j. That is, a topological ordering is an ordering such that any directed path in ⃗G traverses vertices in increasing order. Note that a directed graph may have more than one topological ordering. (See Figure 14.12.) Proposition 14.21: ⃗G has a topological ordering if and only if it is acyclic. Justiﬁcation: The necessity (the “only if” part of the statement) is easy to demonstrate. Suppose ⃗G is topologically ordered. Assume, for the sake of a contradiction, that ⃗G has a cycle consisting of edges (vi0,vi1),(vi1,vi2),...,(vik−1,vi0). Because of the topological ordering, we must have i0 < i1 < ··· < ik−1 < i0, which is clearly impossible. Thus, ⃗G must be acyclic.

Chapter 14. Graph Algorithms F C D G B H E A F C D G B H E A (a) (b) Figure 14.12: Two topological orderings of the same acyclic directed graph. We now argue the sufﬁciency of the condition (the “if” part). Suppose ⃗G is acyclic. We will give an algorithmic description of how to build a topological ordering for ⃗G. Since ⃗G is acyclic, ⃗G must have a vertex with no incoming edges (that is, with in-degree 0). Let v1 be such a vertex. Indeed, if v1 did not exist, then in tracing a directed path from an arbitrary start vertex, we would eventually encounter a previously visited vertex, thus contradicting the acyclicity of ⃗G. If we remove v1 from ⃗G, together with its outgoing edges, the resulting directed graph is still acyclic. Hence, the resulting directed graph also has a vertex with no incoming edges, and we let v2 be such a vertex. By repeating this process until the directed graph becomes empty, we obtain an ordering v1,...,vn of the vertices of ⃗G. Because of the construction above, if (vi,vj) is an edge of ⃗G, then vi must be deleted before vj can be deleted, and thus, i < j. Therefore, v1,...,vn is a topological ordering. Proposition 14.21’s justiﬁcation suggests an algorithm for computing a topological ordering of a directed graph, which we call topological sorting. We present a Java implementation of the technique in Code Fragment 14.11, and an example execution of the algorithm in Figure 14.13. Our implementation uses a map, named inCount, to map each vertex v to a counter that represents the current number of incoming edges to v, excluding those coming from vertices that have previously been added to the topological order. As was the case with our graph traversals, a hash-based map only provides O(1) expected time access to its entries, rather than worst-case time. This could easily be converted to worst-case time if vertices could be indexed from 0 to n−1, or if we store the count as a ﬁeld of the vertex instance. As a side effect, the topological sorting algorithm of Code Fragment 14.11 also tests whether the given directed graph ⃗G is acyclic. Indeed, if the algorithm terminates without ordering all the vertices, then the subgraph of the vertices that have not been ordered must contain a directed cycle.

14.5. Directed Acyclic Graphs /∗∗Returns a list of verticies of directed acyclic graph g in topological order. ∗/ public static <V,E> PositionalList<Vertex<V>> topologicalSort(Graph<V,E> g) { // list of vertices placed in topological order PositionalList<Vertex<V>> topo = new LinkedPositionalList<>(); // container of vertices that have no remaining constraints Stack<Vertex<V>> ready = new LinkedStack<>(); // map keeping track of remaining in-degree for each vertex Map<Vertex<V>, Integer> inCount = new ProbeHashMap<>(); for (Vertex<V> u : g.vertices()) { inCount.put(u, g.inDegree(u)); // initialize with actual in-degree if (inCount.get(u) == 0) // if u has no incoming edges, ready.push(u); // it is free of constraints } while (!ready.isEmpty()) { Vertex<V> u = ready.pop(); topo.addLast(u); for (Edge<E> e : g.outgoingEdges(u)) { // consider all outgoing neighbors of u Vertex<V> v = g.opposite(u, e); inCount.put(v, inCount.get(v) −1); // v has one less constraint without u if (inCount.get(v) == 0) ready.push(v); } } return topo; } Code Fragment 14.11: Java implementation for the topological sorting algorithm. (We show an example execution of this algorithm in Figure 14.13.) Proposition 14.22: Let ⃗G be a directed graph with n vertices and m edges, using an adjacency list representation. The topological sorting algorithm runs in O(n+m) time using O(n) auxiliary space, and either computes a topological ordering of ⃗G or fails to include some vertices, which indicates that ⃗G has a directed cycle. Justiﬁcation: The initial recording of the n in-degrees uses O(n) time based on the inDegree method. Say that a vertex u is visited by the topological sorting algorithm when u is removed from the ready list. A vertex u can be visited only when inCount.get(u) is 0, which implies that all its predecessors (vertices with outgoing edges into u) were previously visited. As a consequence, any vertex that is on a directed cycle will never be visited, and any other vertex will be visited exactly once. The algorithm traverses all the outgoing edges of each visited vertex once, so its running time is proportional to the number of outgoing edges of the visited vertices. In accordance with Proposition 14.9, the running time is (n+ m). Regarding the space usage, observe that containers topo, ready, and inCount have at most one entry per vertex, and therefore use O(n) space.

Chapter 14. Graph Algorithms G C H D F B E A E F C A B D G H E F C A B D G H (a) (b) (c) E F C A B D G H F C D A B G H E C B D F A G H E (d) (e) (f) E F H G D C A B C G H B A D F E E F H G D C A B (g) (h) (i) Figure 14.13: Example of a run of algorithm topologicalSort (Code Fragment 14.11). The label near a vertex shows its current inCount value, and its eventual rank in the resulting topological order. The highlighted vertex is one with inCount equal to zero that will become the next vertex in the topological order. Dashed lines denote edges that have already been examined, which are no longer reﬂected in the inCount values.

14.6. Shortest Paths 14.6 Shortest Paths As we saw in Section 14.3.3, the breadth-ﬁrst search strategy can be used to ﬁnd a path with as few edges as possible from some starting vertex to every other vertex in a connected graph. This approach makes sense in cases where each edge is as good as any other, but there are many situations where this approach is not appropriate. For example, we might want to use a graph to represent the roads between cities, and we might be interested in ﬁnding the fastest way to travel cross-country. In this case, it is probably not appropriate for all the edges to be equal to each other, for some inter-city distances will likely be much larger than others. Likewise, we might be using a graph to represent a computer network (such as the Internet), and we might be interested in ﬁnding the fastest way to route a data packet between two computers. In this case, it again may not be appropriate for all the edges to be equal to each other, for some connections in a computer network are typically much faster than others (for example, some edges might represent low-bandwidth connections, while others might represent high-speed, ﬁber-optic connections). It is natural, therefore, to consider graphs whose edges are not weighted equally. 14.6.1 Weighted Graphs A weighted graph is a graph that has a numeric (for example, integer) label w(e) associated with each edge e, called the weight of edge e. For e = (u,v), we let notation w(u,v) = w(e). We show an example of a weighted graph in Figure 14.14. BOS JFK MIA ORD DFW SFO LAX Figure 14.14: A weighted graph whose vertices represent major U.S. airports and whose edge weights represent distances in miles. This graph has a path from JFK to LAX of total weight 2,777 (going through ORD and DFW). This is the minimumweight path in the graph from JFK to LAX.

Chapter 14. Graph Algorithms Deﬁning Shortest Paths in a Weighted Graph Let G be a weighted graph. The length (or weight) of a path is the sum of the weights of the edges of P. That is, if P = ((v0,v1),(v1,v2),...,(vk−1,vk)), then the length of P, denoted w(P), is deﬁned as w(P) = k−1 ∑ i=0 w(vi,vi+1). The distance from a vertex u to a vertex v in G, denoted d(u,v), is the length of a minimum-length path (also called shortest path) from u to v, if such a path exists. People often use the convention that d(u,v) = ∞if there is no path at all from u to v in G. Even if there is a path from u to v in G, however, if there is a cycle in G whose total weight is negative, the distance from u to v may not be deﬁned. For example, suppose vertices in G represent cities, and the weights of edges in G represent how much money it costs to go from one city to another. If someone were willing to actually pay us to go from say JFK to ORD, then the “cost” of the edge (JFK,ORD) would be negative. If someone else were willing to pay us to go from ORD to JFK, then there would be a negative-weight cycle in G and distances would no longer be deﬁned. That is, anyone could now build a path (with cycles) in G from any city A to another city B that ﬁrst goes to JFK and then cycles as many times as he or she likes from JFK to ORD and back, before going on to B. The existence of such paths would allow us to build arbitrarily low negative-cost paths (and, in this case, make a fortune in the process). But distances cannot be arbitrarily low negative numbers. Thus, any time we use edge weights to represent distances, we must be careful not to introduce any negative-weight cycles. Suppose we are given a weighted graph G, and we are asked to ﬁnd a shortest path from some vertex s to each other vertex in G, viewing the weights on the edges as distances. In this section, we explore efﬁcient ways of ﬁnding all such shortest paths, if they exist. The ﬁrst algorithm we discuss is for the simple, yet common, case when all the edge weights in G are nonnegative (that is, w(e) ≥0 for each edge e of G); hence, we know in advance that there are no negative-weight cycles in G. Recall that the special case of computing a shortest path when all weights are equal to one was solved with the BFS traversal algorithm presented in Section 14.3.3. There is an interesting approach for solving this single-source problem based on the greedy-method design pattern (Section 13.4.2). Recall that in this pattern we solve the problem at hand by repeatedly selecting the best choice from among those available in each iteration. This paradigm can often be used in situations where we are trying to optimize some cost function over a collection of objects. We can add objects to our collection, one at a time, always picking the next one that optimizes the function from among those yet to be chosen.

14.6. Shortest Paths 14.6.2 Dijkstra’s Algorithm The main idea in applying the greedy-method pattern to the single-source shortestpath problem is to perform a “weighted” breadth-ﬁrst search starting at the source vertex s. In particular, we can use the greedy method to develop an algorithm that iteratively grows a “cloud” of vertices out of s, with the vertices entering the cloud in order of their distances from s. Thus, in each iteration, the next vertex chosen is the vertex outside the cloud that is closest to s. The algorithm terminates when no more vertices are outside the cloud (or when those outside the cloud are not connected to those within the cloud), at which point we have a shortest path from s to every vertex of G that is reachable from s. This approach is a simple, but nevertheless powerful, example of the greedy-method design pattern. Applying the greedy method to the single-source, shortest-path problem, results in an algorithm known as Dijkstra’s algorithm. Edge Relaxation Let us deﬁne a label D[v] for each vertex v in V, which we use to approximate the distance in G from s to v. The meaning of these labels is that D[v] will always store the length of the best path we have found so far from s to v. Initially, D[s] = 0 and D[v] = ∞for each v ̸= s, and we deﬁne the set C, which is our “cloud” of vertices, to initially be the empty set. At each iteration of the algorithm, we select a vertex u not in C with smallest D[u] label, and we pull u into C. (In general, we will use a priority queue to select among the vertices outside the cloud.) In the very ﬁrst iteration we will, of course, pull s into C. Once a new vertex u is pulled into C, we update the label D[v] of each vertex v that is adjacent to u and is outside of C, to reﬂect the fact that there may be a new and better way to get to v via u. This update operation is known as a relaxation procedure, for it takes an old estimate and checks if it can be improved to get closer to its true value. The speciﬁc edge relaxation operation is as follows: Edge Relaxation: if D[u]+w(u,v) < D[v] then D[v] = D[u]+w(u,v) Algorithm Description and Example We give the pseudocode for Dijkstra’s algorithm in Code Fragment 14.12, and illustrate several iterations of Dijkstra’s algorithm in Figures 14.15 through 14.17.

Chapter 14. Graph Algorithms Algorithm ShortestPath(G,s): Input: A directed or undirected graph G with nonnegative edge weights, and a distinguished vertex s of G. Output: The length of a shortest path from s to v for each vertex v of G. Initialize D[s] = 0 and D[v] = ∞for each vertex v ̸= s. Let a priority queue Q contain all the vertices of G using the D labels as keys. while Q is not empty do {pull a new vertex u into the cloud} u = value returned by Q.removeMin() for each edge (u,v) such that v is in Q do {perform the relaxation procedure on edge (u,v)} if D[u]+w(u,v) < D[v] then D[v] = D[u]+w(u,v) Change the key of vertex v in Q to D[v]. return the label D[v] of each vertex v Code Fragment 14.12: Pseudocode for Dijkstra’s algorithm, solving the singlesource shortest-path problem for an undirected or directed graph. PVD BWI DFW LAX ORD MIA SFO BOS JFK ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ PVD DFW JFK MIA ORD BWI LAX BOS SFO ∞ ∞ ∞ ∞ ∞ (a) (b) Figure 14.15: An example execution of Dijkstra’s shortest-path algorithm on a weighted graph. The start vertex is BWI. A box next to each vertex v stores the label D[v]. The edges of the shortest-path tree are drawn as thick arrows, and for each vertex u outside the “cloud” we show the current best edge for pulling in u with a thick line. (Continues in Figure 14.16.)

14.6. Shortest Paths PVD DFW LAX ORD MIA SFO BOS JFK BWI ∞ ∞ PVD DFW LAX ORD MIA SFO BOS JFK BWI ∞ ∞ (c) (d) PVD DFW LAX ORD MIA SFO BOS JFK BWI ∞ PVD SFO LAX ORD MIA BOS JFK BWI DFW ∞ (e) (f) PVD DFW LAX ORD BOS JFK BWI SFO MIA PVD LAX ORD BOS JFK BWI SFO MIA DFW (g) (h) Figure 14.16: An example execution of Dijkstra’s shortest-path algorithm on a weighted graph. (Continued from Figure 14.15; continues in Figure 14.17.)

Chapter 14. Graph Algorithms PVD LAX ORD BOS JFK BWI MIA DFW SFO PVD ORD BOS JFK BWI MIA DFW SFO LAX (i) (j) Figure 14.17: An example execution of Dijkstra’s shortest-path algorithm on a weighted graph. (Continued from Figure 14.16.) Why It Works The interesting aspect of the Dijkstra algorithm is that, at the moment a vertex u is pulled into C, its label D[u] stores the correct length of a shortest path from v to u. Thus, when the algorithm terminates, it will have computed the shortest-path distance from s to every vertex of G. That is, it will have solved the single-source shortest-path problem. It is probably not immediately clear why Dijkstra’s algorithm correctly ﬁnds the shortest path from the start vertex s to each other vertex u in the graph. Why is it that the distance from s to u is equal to the value of the label D[u] at the time vertex u is removed from the priority queue Q and added to the cloud C? The answer to this question depends on there being no negative-weight edges in the graph, for it allows the greedy method to work correctly, as we show in the proposition that follows. Proposition 14.23: In Dijkstra’s algorithm, whenever a vertex v is pulled into the cloud, the label D[v] is equal to d(s,v), the length of a shortest path from s to v. Justiﬁcation: Suppose that D[v] > d(s,v) for some vertex v in V, and let z be the ﬁrst vertex the algorithm pulled into the cloud C (that is, removed from Q) such that D[z] > d(s,z). There is a shortest path P from s to z (for otherwise d(s,z) = ∞= D[z]). Let us therefore consider the moment when z is pulled into C, and let y be the ﬁrst vertex of P (when going from s to z) that is not in C at this moment. Let x be the predecessor of y in path P (note that we could have x = s). (See Figure 14.18.) We know, by our choice of y, that x is already in C at this point.

14.6. Shortest Paths the ﬁrst “wrong” vertex picked z picked implies that D[z] ≤D[y] P D[z] > d(s,z) C D[y] = d(s,y) y z s x D[x] = d(s,x) Figure 14.18: A schematic illustration for the justiﬁcation of Proposition 14.23. Moreover, D[x] = d(s,x), since z is the ﬁrst incorrect vertex. When x was pulled into C, we tested (and possibly updated) D[y] so that we had at that point D[y] ≤D[x]+w(x,y) = d(s,x)+w(x,y). But since y is the next vertex on the shortest path from s to z, this implies that D[y] = d(s,y). But we are now at the moment when we are picking z, not y, to join C; hence, D[z] ≤D[y]. It should be clear that a subpath of a shortest path is itself a shortest path. Hence, since y is on the shortest path from s to z, d(s,y)+d(y,z) = d(s,z). Moreover, d(y,z) ≥0 because there are no negative-weight edges. Therefore, D[z] ≤D[y] = d(s,y) ≤d(s,y)+d(y,z) = d(s,z). But this contradicts the deﬁnition of z; hence, there can be no such vertex z. The Running Time of Dijkstra’s Algorithm In this section, we analyze the time complexity of Dijkstra’s algorithm. We denote with n and m the number of vertices and edges of the input graph G, respectively. We assume that the edge weights can be added and compared in constant time. Because of the high level of the description we gave for Dijkstra’s algorithm in Code Fragment 14.12, analyzing its running time requires that we give more details on its implementation. Speciﬁcally, we should indicate the data structures used and how they are implemented.

Chapter 14. Graph Algorithms Let us ﬁrst assume that we are representing the graph G using an adjacency list or adjacency map structure. This data structure allows us to step through the vertices adjacent to u during the relaxation step in time proportional to their number. Therefore, the time spent in the management of the nested for loop, and the number of iterations of that loop, is ∑ u in VG outdeg(u), which is O(m) by Proposition 14.9. The outer while loop executes O(n) times, since a new vertex is added to the cloud during each iteration. This still does not settle all the details for the algorithm analysis, however, for we must say more about how to implement the other principal data structure in the algorithm—the priority queue Q. Referring back to Code Fragment 14.12 in search of priority queue operations, we ﬁnd that n vertices are originally inserted into the priority queue; since these are the only insertions, the maximum size of the queue is n. In each of n iterations of the while loop, a call to removeMin is made to extract the vertex u with smallest D label from Q. Then, for each neighbor v of u, we perform an edge relaxation, and may potentially update the key of v in the queue. Thus, we actually need an implementation of an adaptable priority queue (Section 9.5), in which case the key of a vertex v is changed using the method replaceKey(e, k), where e is the priority queue entry associated with vertex v. In the worst case, there could be one such update for each edge of the graph. Overall, the running time of Dijkstra’s algorithm is bounded by the sum of the following: • n insertions into Q. • n calls to the removeMin method on Q. • m calls to the replaceKey method on Q. If Q is an adaptable priority queue implemented as a heap, then each of the above operations run in O(logn), and so the overall running time for Dijkstra’s algorithm is O((n+m)logn). Note that if we wish to express the running time as a function of n only, then it is O(n2 logn) in the worst case. Let us now consider an alternative implementation for the adaptable priority queue Q using an unsorted sequence. (See Exercise P-9.52.) This, of course, requires that we spend O(n) time to extract the minimum element, but it affords very fast key updates, provided Q supports location-aware entries (Section 9.5.1). Speciﬁcally, we can implement each key update done in a relaxation step in O(1) time—we simply change the key value once we locate the entry in Q to update. Hence, this implementation results in a running time that is O(n2 +m), which can be simpliﬁed to O(n2) since G is simple.

14.6. Shortest Paths Comparing the Two Implementations We have two choices for implementing the adaptable priority queue with locationaware entries in Dijkstra’s algorithm: a heap implementation, which yields a running time of O((n + m)logn), and an unsorted sequence implementation, which yields a running time of O(n2). Since both implementations would be fairly simple to code, they are about equal in terms of the programming sophistication needed. These two implementations are also about equal in terms of the constant factors in their worst-case running times. Looking only at these worst-case times, we prefer the heap implementation when the number of edges in the graph is small (that is, when m < n2/logn), and we prefer the sequence implementation when the number of edges is large (that is, when m > n2/logn). Proposition 14.24: Given a weighted graph G with n vertices and m edges, such that the weight of each edge is nonnegative, and a vertex s of G, Dijkstra’s algorithm can compute the distance from s to all other vertices of G in the better of O(n2) or O((n+m)logn) time. We note that an advanced priority queue implementation, known as a Fibonacci heap, can be used to implement Dijkstra’s algorithm in O(m+nlogn) time. Programming Dijkstra’s Algorithm in Java Having given a pseudocode description of Dijkstra’s algorithm, let us now present Java code for performing Dijkstra’s algorithm, assuming we are given a graph whose edge elements are nonnegative integer weights. Our implementation of the algorithm is in the form of a method, shortestPathLengths, that takes a graph and a designated source vertex as parameters. (See Code Fragment 14.13.) It returns a map, named cloud, storing the shortest-path distance d(s,v) for each vertex v that is reachable from the source. We rely on our HeapAdaptablePriorityQueue developed in Section 9.5.2 as an adaptable priority queue. As we have done with other algorithms in this chapter, we rely on hash-based maps to store auxiliary data (in this case, mapping v to its distance bound D[v] and its adaptable priority queue entry). The expected O(1)-time access to elements of these dictionaries could be converted to worst-case bounds, either by numbering vertices from 0 to n−1 to use as indices into an array, or by storing the information within each vertex’s element. The pseudocode for Dijkstra’s algorithm begins by assigning D[v] = ∞for each v other than the source; we rely on the special value Integer.MAX VALUE in Java to provide a sufﬁcient numeric value to model inﬁnity. However, we avoid including vertices with this “inﬁnite” distance in the resulting cloud that is returned by the method. The use of this numeric limit could be avoided altogether by waiting to add a vertex to the priority queue until after an edge that reaches it is relaxed. (See Exercise C-14.62.)

Chapter 14. Graph Algorithms /∗∗Computes shortest-path distances from src vertex to all reachable vertices of g. ∗/ public static <V> Map<Vertex<V>, Integer> shortestPathLengths(Graph<V,Integer> g, Vertex<V> src) { // d.get(v) is upper bound on distance from src to v Map<Vertex<V>, Integer> d = new ProbeHashMap<>(); // map reachable v to its d value Map<Vertex<V>, Integer> cloud = new ProbeHashMap<>(); // pq will have vertices as elements, with d.get(v) as key AdaptablePriorityQueue<Integer, Vertex<V>> pq; pq = new HeapAdaptablePriorityQueue<>(); // maps from vertex to its pq locator Map<Vertex<V>, Entry<Integer,Vertex<V>>> pqTokens; pqTokens = new ProbeHashMap<>(); // for each vertex v of the graph, add an entry to the priority queue, with // the source having distance 0 and all others having inﬁnite distance for (Vertex<V> v : g.vertices()) { if (v == src) d.put(v,0); else d.put(v, Integer.MAX VALUE); pqTokens.put(v, pq.insert(d.get(v), v)); // save entry for future updates } // now begin adding reachable vertices to the cloud while (!pq.isEmpty()) { Entry<Integer, Vertex<V>> entry = pq.removeMin(); int key = entry.getKey(); Vertex<V> u = entry.getValue(); cloud.put(u, key); // this is actual distance to u pqTokens.remove(u); // u is no longer in pq for (Edge<Integer> e : g.outgoingEdges(u)) { Vertex<V> v = g.opposite(u,e); if (cloud.get(v) == null) { // perform relaxation step on edge (u,v) int wgt = e.getElement(); if (d.get(u) + wgt < d.get(v)) { // better path to v? d.put(v, d.get(u) + wgt); // update the distance pq.replaceKey(pqTokens.get(v), d.get(v)); // update the pq entry } } } } return cloud; // this only includes reachable vertices } Code Fragment 14.13: Java implementation of Dijkstra’s algorithm for computing the shortest-path distances from a single source. We assume that e.getElement() for edge e represents the weight of that edge.

14.6. Shortest Paths Reconstructing a Shortest-Path Tree Our pseudocode description of Dijkstra’s algorithm in Code Fragment 14.12 and our implementation in Code Fragment 14.13 compute the value D[v], for each vertex v, that is the length of a shortest path from the source vertex s to v. However, those forms of the algorithm do not explicitly compute the actual paths that achieve those distances. Fortunately, it is possible to represent shortest paths from source s to every reachable vertex in a graph using a compact data structure known as a shortest-path tree. This is possible because if a shortest path from s to v passes through an intermediate vertex u, it must begin with a shortest path from s to u. We next demonstrate that a shortest-path tree rooted at source s can be reconstructed in O(n+ m) time, given the D[v] values produced by Dijkstra’s algorithm using s as the source. As we did when representing the DFS and BFS trees, we will map each vertex v ̸= s to a parent u (possibly, u = s), such that u is the vertex immediately before v on a shortest path from s to v. If u is the vertex just before v on a shortest path from s to v, it must be that D[u]+w(u,v) = D[v]. Conversely, if the above equation is satisﬁed, then a shortest path from s to u followed by the edge (u,v) is a shortest path to v. Our implementation in Code Fragment 14.14 reconstructs a tree based on this logic, testing all incoming edges to each vertex v, looking for a (u,v) that satisﬁes the key equation. The running time is O(n+m), as we consider each vertex and all incoming edges to those vertices. (See Proposition 14.9.) /∗∗ ∗Reconstructs a shortest-path tree rooted at vertex s, given distance map d. ∗The tree is represented as a map from each reachable vertex v (other than s) ∗to the edge e = (u,v) that is used to reach v from its parent u in the tree. ∗/ public static <V> Map<Vertex<V>,Edge<Integer>> spTree(Graph<V,Integer> g, Vertex<V> s, Map<Vertex<V>,Integer> d) { Map<Vertex<V>, Edge<Integer>> tree = new ProbeHashMap<>(); for (Vertex<V> v : d.keySet()) if (v != s) for (Edge<Integer> e : g.incomingEdges(v)) { // consider INCOMING edges Vertex<V> u = g.opposite(v, e); int wgt = e.getElement(); if (d.get(v) == d.get(u) + wgt) tree.put(v, e); // edge is is used to reach v } return tree; } Code Fragment 14.14: Java method that reconstructs a single-source shortest-path tree, based on knowledge of the shortest-path distances.

Chapter 14. Graph Algorithms 14.7 Minimum Spanning Trees Suppose we wish to connect all the computers in a new ofﬁce building using the least amount of cable. We can model this problem using an undirected, weighted graph G whose vertices represent the computers, and whose edges represent all the possible pairs (u,v) of computers, where the weight w(u,v) of edge (u,v) is equal to the amount of cable needed to connect computer u to computer v. Rather than computing a shortest-path tree from some particular vertex v, we are interested instead in ﬁnding a tree T that contains all the vertices of G and has the minimum total weight over all such trees. Algorithms for ﬁnding such a tree are the focus of this section. Problem Deﬁnition Given an undirected, weighted graph G, we are interested in ﬁnding a tree T that contains all the vertices in G and minimizes the sum w(T) = ∑ (u,v) in T w(u,v). A tree, such as this, that contains every vertex of a connected graph G is said to be a spanning tree, and the problem of computing a spanning tree T with smallest total weight is known as the minimum spanning tree (or MST) problem. The development of efﬁcient algorithms for the minimum spanning tree problem predates the modern notion of computer science itself. In this section, we discuss two classic algorithms for solving the MST problem. These algorithms are both applications of the greedy method, which, as was discussed brieﬂy in the previous section, is based on choosing objects to join a growing collection by iteratively picking an object that minimizes some cost function. The ﬁrst algorithm we discuss is the Prim-Jarn´ık algorithm, which grows the MST from a single root vertex, much in the same way as Dijkstra’s shortest-path algorithm. The second algorithm we discuss is Kruskal’s algorithm, which “grows” the MST in clusters by considering edges in nondecreasing order of their weights. In order to simplify the description of the algorithms, we assume, in the following, that the input graph G is undirected (that is, all its edges are undirected) and simple (that is, it has no self-loops and no parallel edges). Hence, we denote the edges of G as unordered vertex pairs (u,v). Before we discuss the details of these algorithms, however, let us give a crucial fact about minimum spanning trees that forms the basis of the algorithms.

14.7. Minimum Spanning Trees A Crucial Fact about Minimum Spanning Trees The two MST algorithms we discuss are based on the greedy method, which in this case depends crucially on the following fact. (See Figure 14.19.) V1 V2 e min-weight “bridge” edge e Belongs to a Minimum Spanning Tree Figure 14.19: An illustration of the crucial fact about minimum spanning trees. Proposition 14.25: Let G be a weighted connected graph, and let V1 and V2 be a partition of the vertices of G into two disjoint nonempty sets. Furthermore, let e be an edge in G with minimum weight from among those with one endpoint in V1 and the other in V2. There is a minimum spanning tree T that has e as one of its edges. Justiﬁcation: Let T be a minimum spanning tree of G. If T does not contain edge e, the addition of e to T must create a cycle. Therefore, there is some edge f ̸= e of this cycle that has one endpoint in V1 and the other in V2. Moreover, by the choice of e, w(e) ≤w( f). If we remove f from T ∪{e}, we obtain a spanning tree whose total weight is no more than before. Since T was a minimum spanning tree, this new tree must also be a minimum spanning tree. In fact, if the weights in G are distinct, then the minimum spanning tree is unique; we leave the justiﬁcation of this less crucial fact as an exercise (C-14.64). In addition, note that Proposition 14.25 remains valid even if the graph G contains negative-weight edges or negative-weight cycles, unlike the algorithms we presented for shortest paths.

Chapter 14. Graph Algorithms 14.7.1 Prim-Jarn´ık Algorithm In the Prim-Jarn´ık algorithm, we grow a minimum spanning tree from a single cluster starting from some “root” vertex s. The main idea is similar to that of Dijkstra’s algorithm. We will begin with some vertex s, deﬁning the initial “cloud” of vertices C. Then, in each iteration, we choose a minimum-weight edge e = (u,v), connecting a vertex u in the cloud C to a vertex v outside of C. The vertex v is then brought into the cloud C and the process is repeated until a spanning tree is formed. Again, the crucial fact about minimum spanning trees comes into play, for by always choosing the smallest-weight edge joining a vertex inside C to one outside C, we are assured of always adding a valid edge to the MST. To efﬁciently implement this approach, we can take another cue from Dijkstra’s algorithm. We maintain a label D[v] for each vertex v outside the cloud C, so that D[v] stores the weight of the minimum observed edge for joining v to the cloud C. (In Dijkstra’s algorithm, this label measured the full path length from starting vertex s to v, including an edge (u,v).) These labels serve as keys in a priority queue used to decide which vertex is next in line to join the cloud. We give the pseudocode in Code Fragment 14.15. Algorithm PrimJarnik(G): Input: An undirected, weighted, connected graph G with n vertices and m edges Output: A minimum spanning tree T for G Pick any vertex s of G D[s] = 0 for each vertex v ̸= s do D[v] = ∞ Initialize T = ∅. Initialize a priority queue Q with an entry (D[v],v) for each vertex v. For each vertex v, maintain connect(v) as the edge achieving D[v] (if any). while Q is not empty do Let u be the value of the entry returned by Q.removeMin(). Connect vertex u to T using edge connect(e). for each edge e′ = (u,v) such that v is in Q do {check if edge (u,v) better connects v to T} if w(u,v) < D[v] then D[v] = w(u,v) connect(v) = e′. Change the key of vertex v in Q to D[v]. return the tree T Code Fragment 14.15: The Prim-Jarn´ık algorithm for the MST problem.

14.7. Minimum Spanning Trees Analyzing the Prim-Jarn´ık Algorithm The implementation issues for the Prim-Jarn´ık algorithm are similar to those for Dijkstra’s algorithm, relying on an adaptable priority queue Q (Section 9.5.1). We initially perform n insertions into Q, later perform n extract-min operations, and may update a total of m priorities as part of the algorithm. Those steps are the primary contributions to the overall running time. With a heap-based priority queue, each operation runs in O(logn) time, and the overall time for the algorithm is O((n + m)logn), which is O(mlogn) for a connected graph. Alternatively, we can achieve O(n2) running time by using an unsorted list as a priority queue. Illustrating the Prim-Jarn´ık Algorithm We illustrate the Prim-Jarn´ık algorithm in Figures 14.20 and 14.21. PVD JFK BWI ORD MIA LAX DFW SFO BOS PVD SFO BOS JFK BWI ORD MIA LAX DFW (a) (b) PVD SFO BOS JFK BWI ORD MIA LAX DFW PVD SFO BOS JFK BWI ORD MIA LAX DFW (c) (d) Figure 14.20: An illustration of the Prim-Jarn´ık MST algorithm, starting with vertex PVD. (Continues in Figure 14.21.)

Chapter 14. Graph Algorithms PVD DFW BOS JFK BWI ORD MIA SFO LAX PVD DFW BOS JFK BWI ORD MIA SFO LAX (e) (f) PVD LAX BOS JFK BWI ORD MIA DFW SFO PVD LAX BOS JFK BWI ORD MIA DFW SFO (g) (h) PVD LAX BOS JFK BWI ORD MIA DFW SFO PVD LAX BOS JFK BWI ORD MIA DFW SFO (i) (j) Figure 14.21: An illustration of the Prim-Jarn´ık MST algorithm. (Continued from Figure 14.20.)

14.7. Minimum Spanning Trees 14.7.2 Kruskal’s Algorithm In this section, we will introduce Kruskal’s algorithm for constructing a minimum spanning tree. While the Prim-Jarn´ık algorithm builds the MST by growing a single tree until it spans the graph, Kruskal’s algorithm maintains many smaller trees in a forest, repeatedly merging pairs of trees until a single tree spans the graph. Initially, each vertex is in its own cluster. The algorithm then considers each edge in turn, ordered by increasing weight. If an edge e connects vertices in two different clusters, then e is added to the set of edges of the minimum spanning tree, and the two trees are merged with the addition of e. If, on the other hand, e connects two vertices in the same cluster, then e is discarded. Once the algorithm has added enough edges to form a spanning tree, it terminates and outputs this tree as the minimum spanning tree. We give pseudocode for Kruskal’s MST algorithm in Code Fragment 14.16 and we show an example of this algorithm in Figures 14.22, 14.23, and 14.24. Algorithm Kruskal(G): Input: A simple connected weighted graph G with n vertices and m edges Output: A minimum spanning tree T for G for each vertex v in G do Deﬁne an elementary cluster C(v) = {v}. Initialize a priority queue Q to contain all edges in G, using the weights as keys. T = ∅ {T will ultimately contain the edges of an MST} while T has fewer than n−1 edges do (u,v) = value returned by Q.removeMin() Let C(u) be the cluster containing u, and let C(v) be the cluster containing v. if C(u) ̸= C(v) then Add edge (u,v) to T. Merge C(u) and C(v) into one cluster. return tree T Code Fragment 14.16: Kruskal’s algorithm for the MST problem. As was the case with the Prim-Jarn´ık algorithm, the correctness of Kruskal’s algorithm is based upon the crucial fact about minimum spanning trees from Proposition 14.25. Each time Kruskal’s algorithm adds an edge (u,v) to the minimum spanning tree T, we can deﬁne a partitioning of the set of vertices V (as in the proposition) by letting V1 be the cluster containing v and letting V2 contain the rest of the vertices in V. This clearly deﬁnes a disjoint partitioning of the vertices of V and, more importantly, since we are extracting edges from Q in order by their weights, e must be a minimum-weight edge with one vertex in V1 and the other in V2. Thus, Kruskal’s algorithm always adds a valid minimum spanning tree edge.

Chapter 14. Graph Algorithms PVD BWI LAX BOS JFK MIA ORD SFO DFW PVD LAX JFK BOS SFO MIA ORD BWI DFW (a) (b) PVD DFW LAX JFK BOS MIA ORD BWI SFO PVD MIA DFW SFO LAX BOS JFK BWI ORD (c) (d) PVD ORD MIA DFW SFO LAX JFK BOS BWI PVD BOS BWI MIA DFW SFO LAX JFK ORD (e) (f) Figure 14.22: Example of an execution of Kruskal’s MST algorithm on a graph with integer weights. We show the clusters as shaded regions and we highlight the edge being considered in each iteration. (Continues in Figure 14.23.)

14.7. Minimum Spanning Trees PVD ORD BWI MIA DFW SFO LAX BOS JFK PVD ORD BWI BOS MIA DFW SFO LAX JFK (g) (h) PVD ORD BWI BOS MIA DFW SFO LAX JFK PVD BWI ORD MIA DFW SFO LAX BOS JFK (i) (j) PVD BOS JFK BWI ORD MIA DFW SFO LAX PVD JFK BWI BOS MIA DFW SFO LAX ORD (k) (l) Figure 14.23: An example of an execution of Kruskal’s MST algorithm. Rejected edges are shown dashed. (Continues in Figure 14.24.)

Chapter 14. Graph Algorithms PVD BOS JFK BWI ORD MIA DFW SFO LAX PVD ORD JFK BWI MIA DFW SFO LAX BOS (m) (n) Figure 14.24: Example of an execution of Kruskal’s MST algorithm (continued). The edge considered in (n) merges the last two clusters, which concludes this execution of Kruskal’s algorithm. (Continued from Figure 14.23.) The Running Time of Kruskal’s Algorithm There are two primary contributions to the running time of Kruskal’s algorithm. The ﬁrst is the need to consider the edges in nondecreasing order of their weights, and the second is the management of the cluster partition. Analyzing its running time requires that we give more details on its implementation. The ordering of edges by weight can be implemented in O(mlogm), either by use of a sorting algorithm or a priority queue Q. If that queue is implemented with a heap, we can initialize Q in O(mlogm) time by repeated insertions, or in O(m) time using bottom-up heap construction (see Section 9.3.4), and the subsequent calls to removeMin each run in O(logm) time, since the queue has size O(m). We note that since m is O(n2) for a simple graph, O(logm) is the same as O(logn). Therefore, the running time due to the ordering of edges is O(mlogn). The remaining task is the management of clusters. To implement Kruskal’s algorithm, we must be able to ﬁnd the clusters for vertices u and v that are endpoints of an edge e, to test whether those two clusters are distinct, and if so, to merge those two clusters into one. None of the data structures we have studied thus far are well suited for this task. However, we conclude this chapter by formalizing the problem of managing disjoint partitions, and introducing efﬁcient union-ﬁnd data structures. In the context of Kruskal’s algorithm, we perform at most 2m “ﬁnd” operations and n −1 “union” operations. We will see that a simple union-ﬁnd structure can perform that combination of operations in O(m + nlogn) time (see Proposition 14.26), and a more advanced structure can support an even faster time. For a connected graph, m ≥n−1; therefore, the bound of O(mlogn) time for ordering the edges dominates the time for managing the clusters. We conclude that the running time of Kruskal’s algorithm is O(mlogn).

14.7. Minimum Spanning Trees Java Implementation Code Fragment 14.17 presents a Java implementation of Kruskal’s algorithm. The minimum spanning tree is returned in the form of a list of edges. As a consequence of Kruskal’s algorithm, those edges will be reported in nondecreasing order of their weights. Our implementation assumes use of a Partition class for managing the cluster partition. An implementation of the Partition class is presented in Section 14.7.3. /∗∗Computes a minimum spanning tree of graph g using Kruskal's algorithm. ∗/ public static <V> PositionalList<Edge<Integer>> MST(Graph<V,Integer> g) { // tree is where we will store result as it is computed PositionalList<Edge<Integer>> tree = new LinkedPositionalList<>(); // pq entries are edges of graph, with weights as keys PriorityQueue<Integer, Edge<Integer>> pq = new HeapPriorityQueue<>(); // union-ﬁnd forest of components of the graph Partition<Vertex<V>> forest = new Partition<>(); // map each vertex to the forest position Map<Vertex<V>,Position<Vertex<V>>> positions = new ProbeHashMap<>(); for (Vertex<V> v : g.vertices()) positions.put(v, forest.makeGroup(v)); for (Edge<Integer> e : g.edges()) pq.insert(e.getElement(), e); int size = g.numVertices(); // while tree not spanning and unprocessed edges remain... while (tree.size() != size −1 && !pq.isEmpty()) { Entry<Integer, Edge<Integer>> entry = pq.removeMin(); Edge<Integer> edge = entry.getValue(); Vertex<V>[ ] endpoints = g.endVertices(edge); Position<Vertex<V>> a = forest.ﬁnd(positions.get(endpoints[0])); Position<Vertex<V>> b = forest.ﬁnd(positions.get(endpoints[1])); if (a != b) { tree.addLast(edge); forest.union(a,b); } } return tree; } Code Fragment 14.17: Java implementation of Kruskal’s algorithm for the minimum spanning tree problem. The Partition class is discussed in Section 14.7.3.

Chapter 14. Graph Algorithms 14.7.3 Disjoint Partitions and Union-Find Structures In this section, we consider a data structure for managing a partition of elements into a collection of disjoint sets. Our initial motivation is in support of Kruskal’s minimum spanning tree algorithm, in which a forest of disjoint trees is maintained, with occasional merging of neighboring trees. More generally, the disjoint partition problem can be applied to various models of discrete growth. We formalize the problem with the following model. A partition data structure manages a universe of elements that are organized into disjoint sets (that is, an element belongs to one and only one of these sets). Unlike with the Set ADT, we do not expect to be able to iterate through the contents of a set, nor to efﬁciently test whether a given set includes a given element. To avoid confusion with such notions of a set, we will refer to the sets of our partition as clusters. However, we will not require an explicit structure for each cluster, instead allowing the organization of clusters to be implicit. To differentiate between one cluster and another, we assume that at any point in time, each cluster has a designated element that we refer to as the leader of the cluster. Formally, we deﬁne the methods of a partition ADT using positions, each of which stores an element x. The partition ADT supports the following methods. makeCluster(x): Creates a singleton cluster containing new element x and returns its position. union(p, q): Merges the clusters containing positions p and q. ﬁnd(p): Returns the position of the leader of the cluster containing position p. Sequence Implementation A simple implementation of a partition with a total of n elements uses a collection of sequences, one for each cluster, where the sequence for a cluster A stores element positions. Each position object stores a reference to its associated element x, and a reference to the sequence storing p, since this sequence is representing the cluster containing p’s element. (See Figure 14.25.) With this representation, we can easily perform the makeCluster(x) and ﬁnd(p) operations in O(1) time, allowing the ﬁrst position in a sequence to serve as the “leader.” Operation union(p, q) requires that we join two sequences into one and update the cluster references of the positions in one of the two. We choose to implement this operation by removing all the positions from the sequence with smaller size, and inserting them in the sequence with larger size. Each time we take a position from the smaller cluster A and insert it into the larger cluster B, we update the cluster reference for that position to now point to B. Hence, the operation union(p, q) takes time O(min(np,nq)), where np (resp. nq) is the cardinality of the

14.7. Minimum Spanning Trees C B A Figure 14.25: Sequence-based implementation of a partition consisting of three clusters: A = {1,4,7}, B = {2,3,6,9}, and C = {5,8,10,11,12}. cluster containing position p (resp. q). Clearly, this time is O(n) if there are n elements in the partition universe. However, we next present an amortized analysis that shows this implementation to be much better than appears from this worst-case analysis. Proposition 14.26: When using the sequence-based partition implementation, performing a series of k makeCluster, union, and ﬁnd operations on an initially empty partition involving at most n elements takes O(k +nlogn) time. Justiﬁcation: We use the accounting method and assume that one cyber-dollar can pay for the time to perform a ﬁnd operation, a makeCluster operation, or the movement of a position object from one sequence to another in a union operation. In the case of a ﬁnd or makeCluster operation, we charge the operation itself 1 cyber-dollar. In the case of a union operation, we assume that 1 cyber-dollar pays for the constant-time work in comparing the sizes of the two sequences, and that we charge 1 cyber-dollar to each position that we move from the smaller cluster to the larger cluster. Clearly, the 1 cyber-dollar charged for each ﬁnd and makeCluster operation, together with the ﬁrst cyber-dollar collected for each union operation, accounts for a total of k cyber-dollars. Consider, then, the number of charges made to positions on behalf of union operations. The important observation is that each time we move a position from one cluster to another, the size of that position’s cluster at least doubles. Thus, each position is moved from one cluster to another at most logn times; hence, each position can be charged at most O(logn) times. Since we assume that the partition is initially empty, there are O(n) different elements referenced in the given series of operations, which implies that the total time for moving elements during the union operations is O(nlogn).

Chapter 14. Graph Algorithms A Tree-Based Partition Implementation ⋆ An alternative data structure for representing a partition uses a collection of trees to store the n elements, where each tree is associated with a different cluster. In particular, we implement each tree with a linked data structure whose nodes serve as the position objects. (See Figure 14.26.) We view each position p as being a node having an instance variable, element, referring to its element x, and an instance variable, parent, referring to its parent node. By convention, if p is the root of its tree, we set p’s parent reference to itself. Figure 14.26: Tree-based implementation of a partition consisting of three clusters: A = {1,4,7}, B = {2,3,6,9}, and C = {5,8,10,11,12}. With this partition data structure, operation ﬁnd(p) is performed by walking up from position p to the root of its tree, which takes O(n) time in the worst case. Operation union(p, q) can be implemented by making one of the trees a subtree of the other. This can be done by ﬁrst locating the two roots, and then in O(1) additional time by setting the parent reference of one root to point to the other root. See Figure 14.27 for an example of both operations. (a) (b) Figure 14.27: Tree-based implementation of a partition: (a) operation union(p, q); (b) operation ﬁnd(p), where p denotes the position object for element 12.

14.7. Minimum Spanning Trees At ﬁrst, this implementation may seem to be no better than the sequence-based data structure, but we add the following two simple heuristics to make it run faster. Union-by-Size: With each position p, store the number of elements in the subtree rooted at p. In a union operation, make the root of the smaller cluster become a child of the other root, and update the size ﬁeld of the larger root. Path Compression: In a ﬁnd operation, for each position q that the ﬁnd visits, reset the parent of q to the root. (See Figure 14.28.) (a) (b) Figure 14.28: Path-compression heuristic: (a) path traversed by operation ﬁnd on element 12; (b) restructured tree. A surprising property of this data structure, when implemented using the unionby-size and path-compression heuristics, is that performing a series of k operations involving n elements takes O(klog∗n) time, where log∗n is the log-star function, which is the inverse of the tower-of-twos function. Intuitively, log∗n is the number of times that one can iteratively take the logarithm (base 2) of a number before getting a number smaller than 2. Table 14.4 shows a few sample values. minimum n 22 = 4 222 = 16 = 65,536 = 265,536 log∗n Table 14.4: Some values of log∗n and critical values for its inverse. Proposition 14.27: When using the tree-based partition representation with both union-by-size and path compression, performing a series of k makeCluster, union, and ﬁnd operations on an initially empty partition involving at most n elements takes O(klog∗n) time. Although the analysis for this data structure is rather complex, its implementation is quite straightforward. We conclude with a Java implementation of the structure, given in Code Fragment 14.18.

Chapter 14. Graph Algorithms /∗∗A Union-Find structure for maintaining disjoint sets. ∗/ public class Partition<E> { //--------------- nested Locator class ------------- private class Locator<E> implements Position<E> { public E element; public int size; public Locator<E> parent; public Locator(E elem) { element = elem; size = 1; parent = this; // convention for a cluster leader } public E getElement() { return element; } } //--------- end of nested Locator class --------- /∗∗Makes a new cluster containing element e and returns its position. ∗/ public Position<E> makeCluster(E e) { return new Locator<E>(e); } /∗∗ ∗Finds the cluster containing the element identiﬁed by Position p ∗and returns the Position of the cluster's leader. ∗/ public Position<E> ﬁnd(Position<E> p) { Locator<E> loc = validate(p); if (loc.parent != loc) loc.parent = (Locator<E>) ﬁnd(loc.parent); // overwrite parent after recursion return loc.parent; } /∗∗Merges the clusters containing elements with positions p and q (if distinct). ∗/ public void union(Position<E> p, Position<E> q) { Locator<E> a = (Locator<E>) ﬁnd(p); Locator<E> b = (Locator<E>) ﬁnd(q); if (a != b) if (a.size > b.size) { b.parent = a; a.size += b.size; } else { a.parent = b; b.size += a.size; } } } Code Fragment 14.18: Java implementation of a Partition class using union-by-size and path compression. We omit the validate method due to space limitation.

Chapter 15. Memory Management and B-Trees 15.1 Memory Management Computer memory is organized into a sequence of words, each of which typically consists of 4, 8, or 16 bytes (depending on the computer). These memory words are numbered from 0 to N −1, where N is the number of memory words available to the computer. The number associated with each memory word is known as its memory address. Thus, the memory in a computer can be viewed as basically one giant array of memory words, as portrayed in Figure 15.1. Figure 15.1: Memory addresses. In order to run programs and store information, the computer’s memory must be managed so as to determine what data is stored in what memory cells. In this section, we discuss the basics of memory management, most notably describing the way in which memory is allocated for various purposes in a Java program, and the way in which portions of memory are deallocated and reclaimed, when no longer needed. 15.1.1 Stacks in the Java Virtual Machine A Java program is typically compiled into a sequence of byte codes that serve as “machine” instructions for a well-deﬁned model—the Java Virtual Machine (JVM). The deﬁnition of the JVM is at the heart of the deﬁnition of the Java language itself. By compiling Java code into the JVM byte codes, rather than the machine language of a speciﬁc CPU, a Java program can be run on any computer that has a program that can emulate the JVM. Stacks have an important application to the runtime environment of Java programs. A running Java program (more precisely, a running Java thread) has a private stack, called the Java method stack or just Java stack for short, which is used to keep track of local variables and other important information on methods as they are invoked during execution. (See Figure 15.2.) More speciﬁcally, during the execution of a Java program, the Java Virtual Machine (JVM) maintains a stack whose elements are descriptors of the currently active (that is, nonterminated) invocations of methods. These descriptors are called frames. A frame for some invocation of method “fool” stores the current values of the local variables and parameters of method fool, as well as information on method “cool” that called fool and on what needs to be returned to method “cool”.

15.1. Memory Management Java Program main() { cool(i); int i=5; } cool(int j) { fool(k); } int k=7; fool: PC = 320 fool(int m) { } m = 7 cool: PC = 216 j = 5 k = 7 main: PC = 14 i = 5 Java Stack Figure 15.2: An example of a Java method stack: method fool has just been called by method cool, which itself was previously called by method main. Note the values of the program counter, parameters, and local variables stored in the stack frames. When the invocation of method fool terminates, the invocation of method cool will resume its execution at instruction 217, which is obtained by incrementing the value of the program counter stored in the stack frame. Keeping Track of the Program Counter The JVM keeps a special variable, called the program counter, to maintain the address of the statement the JVM is currently executing in the program. When a method “cool” invokes another method “fool”, the current value of the program counter is recorded in the frame of the current invocation of cool (so the JVM will know where to return to when method fool is done). At the top of the Java stack is the frame of the running method, that is, the method that currently has control of the execution. The remaining elements of the stack are frames of the suspended methods, that is, methods that have invoked another method and are currently waiting for it to return control to them upon its termination. The order of the elements in the stack corresponds to the chain of invocations of the currently active methods. When a new method is invoked, a frame for this method is pushed onto the stack. When it terminates, its frame is popped from the stack and the JVM resumes the processing of the previously suspended method.

Chapter 15. Memory Management and B-Trees Implementing Recursion One of the beneﬁts of using a stack to implement method invocation is that it allows programs to use recursion. That is, it allows a method to call itself, as discussed in Chapter 5. We implicitly described the concept of the call stack and the use of frames within our portrayal of recursion traces in that chapter. Interestingly, early programming languages, such as Cobol and Fortran, did not originally use call stacks to implement function and procedure calls. But because of the elegance and efﬁciency that recursion allows, all modern programming languages, including the modern versions of classic languages like Cobol and Fortran, utilize a runtime stack for method and procedure calls. Each box of a recursion trace corresponds to a frame of the Java method stack. At any point in time, the contents of the Java method stack corresponds to the chain of boxes from the initial method invocation to the current one. To better illustrate how a runtime stack allows recursive methods, we refer back to the Java implementation of the classic recursive deﬁnition of the factorial function, n! = n(n−1)(n−2)···1, with the code originally given in Code Fragment 5.1, and the recursion trace in Figure 5.1. The ﬁrst time we call method factorial(n), its stack frame includes a local variable storing the value n. The method recursively calls itself to compute (n−1)!, which pushes a new frame on the Java runtime stack. In turn, this recursive invocation calls itself to compute (n−2)!, etc. The chain of recursive invocations, and thus the runtime stack, only grows up to size n + 1, with the most deeply nested call being factorial(0), which returns 1 without any further recursion. The runtime stack allows several invocations of the factorial method to exist simultaneously. Each has a frame that stores the value of its parameter n as well as the value to be returned. When the ﬁrst recursive call eventually terminates, it returns (n−1)!, which is then multiplied by n to compute n! for the original call of the factorial method. The Operand Stack Interestingly, there is actually another place where the JVM uses a stack. Arithmetic expressions, such as ((a+b)∗(c+d))/e, are evaluated by the JVM using an operand stack. A simple binary operation, such as a+b, is computed by pushing a on the stack, pushing b on the stack, and then calling an instruction that pops the top two items from the stack, performs the binary operation on them, and pushes the result back onto the stack. Likewise, instructions for writing and reading elements to and from memory involve the use of pop and push methods for the operand stack. Thus, the JVM uses a stack to evaluate arithmetic expressions in Java.

15.1. Memory Management 15.1.2 Allocating Space in the Memory Heap We have already discussed (in Section 15.1.1) how the Java Virtual Machine allocates a method’s local variables in that method’s frame on the Java runtime stack. The Java stack is not the only kind of memory available for program data in Java, however. Dynamic Memory Allocation Memory for an object can also be allocated dynamically during a method’s execution, by having that method utilize the special new operator built into Java. For example, the following Java statement creates an array of integers whose size is given by the value of variable k: int[ ] items = new int[k]; The size of the array above is known only at runtime. Moreover, the array may continue to exist even after the method that created it terminates. Thus, the memory for this array cannot be allocated on the Java stack. The Memory Heap Instead of using the Java stack for this object’s memory, Java uses memory from another area of storage—the memory heap (which should not be confused with the “heap” data structure presented in Chapter 9). We illustrate this memory area, together with the other memory areas, in a Java Virtual Machine in Figure 15.3. The storage available in the memory heap is divided into blocks, which are contiguous array-like “chunks” of memory that may be of variable or ﬁxed sizes. To simplify the discussion, let us assume that blocks in the memory heap are of a ﬁxed size, say, 1,024 bytes, and that one block is big enough for any object we might want to create. (Efﬁciently handling the more general case is actually an interesting research problem.) Program Code Java Stack Memory Heap Free Memory fixed size − doesn’t grow grows into higher memory grows into lower memory Figure 15.3: A schematic view of the layout of memory addresses in the Java Virtual Machine.

Chapter 15. Memory Management and B-Trees Memory Allocation Algorithms The Java Virtual Machine deﬁnition requires that the memory heap be able to quickly allocate memory for new objects, but it does not specify the algorithm that should be used to do this. One popular method is to keep contiguous “holes” of available free memory in a linked list, called the free list. The links joining these holes are stored inside the holes themselves, since their memory is not being used. As memory is allocated and deallocated, the collection of holes in the free lists changes, with the unused memory being separated into disjoint holes divided by blocks of used memory. This separation of unused memory into separate holes is known as fragmentation. The problem is that it becomes more difﬁcult to ﬁnd large continuous chunks of memory, when needed, even though an equivalent amount of memory may be unused (yet fragmented). Two kinds of fragmentation can occur. Internal fragmentation occurs when a portion of an allocated memory block is unused. For example, a program may request an array of size 1000, but only use the ﬁrst 100 cells of this array. A runtime environment can not do much to reduce internal fragmentation. External fragmentation, on the other hand, occurs when there is a signiﬁcant amount of unused memory between several contiguous blocks of allocated memory. Since the runtime environment has control over where to allocate memory when it is requested (for example, when the new keyword is used in Java), the runtime environment should allocate memory in a way to try to reduce external fragmentation. Several heuristics have been suggested for allocating memory from the heap so as to minimize external fragmentation. The best-ﬁt algorithm searches the entire free list to ﬁnd the hole whose size is closest to the amount of memory being requested. The ﬁrst-ﬁt algorithm searches from the beginning of the free list for the ﬁrst hole that is large enough. The next-ﬁt algorithm is similar, in that it also searches the free list for the ﬁrst hole that is large enough, but it begins its search from where it left off previously, viewing the free list as a circularly linked list (Section 3.3). The worst-ﬁt algorithm searches the free list to ﬁnd the largest hole of available memory, which might be done faster than a search of the entire free list if this list were maintained as a priority queue (Chapter 9). In each algorithm, the requested amount of memory is subtracted from the chosen memory hole and the leftover part of that hole is returned to the free list. Although it might sound good at ﬁrst, the best-ﬁt algorithm tends to produce the worst external fragmentation, since the leftover parts of the chosen holes tend to be small. The ﬁrst-ﬁt algorithm is fast, but it tends to produce a lot of external fragmentation at the front of the free list, which slows down future searches. The next-ﬁt algorithm spreads fragmentation more evenly throughout the memory heap, thus keeping search times low. This spreading also makes it more difﬁcult to allocate large blocks, however. The worst-ﬁt algorithm attempts to avoid this problem by keeping contiguous sections of free memory as large as possible.

15.1. Memory Management 15.1.3 Garbage Collection In some languages, like C and C++, the memory space for objects must be explicitly deallocated by the programmer, which is a duty often overlooked by beginning programmers and is the source of frustrating programming errors even for experienced programmers. The designers of Java instead placed the burden of memory management entirely on the runtime environment. As mentioned above, memory for objects is allocated from the memory heap and the space for the instance variables of a running Java program are placed in its method stacks, one for each running thread (for the simple programs discussed in this book there is typically just one running thread). Since instance variables in a method stack can refer to objects in the memory heap, all the variables and objects in the method stacks of running threads are called root objects. All those objects that can be reached by following object references that start from a root object are called live objects. The live objects are the active objects currently being used by the running program; these objects should not be deallocated. For example, a running Java program may store, in a variable, a reference to a sequence S that is implemented using a doubly linked list. The reference variable to S is a root object, while the object for S is a live object, as are all the node objects that are referenced from this object and all the elements that are referenced from these node objects. From time to time, the Java virtual machine (JVM) may notice that available space in the memory heap is becoming scarce. At such times, the JVM can elect to reclaim the space that is being used for objects that are no longer live, and return the reclaimed memory to the free list. This reclamation process is known as garbage collection. There are several different algorithms for garbage collection, but one of the most used is the mark-sweep algorithm. The Mark-Sweep Algorithm In the mark-sweep garbage collection algorithm, we associate a “mark” bit with each object that identiﬁes whether that object is live. When we determine at some point that garbage collection is needed, we suspend all other activity and clear the mark bits of all the objects currently allocated in the memory heap. We then trace through the Java stacks of the currently running threads and we mark all the root objects in these stacks as “live.” We must then determine all the other live objects— the ones that are reachable from the root objects. To do this efﬁciently, we can perform a depth-ﬁrst search (see Section 14.3.1) on the directed graph that is deﬁned by objects referencing other objects. In this case, each object in the memory heap is viewed as a vertex in a directed graph, and the reference from one object to another is viewed as a directed edge. By performing a directed DFS from each root object, we can correctly identify and mark each live object. This process is known as the “mark” phase.

Chapter 15. Memory Management and B-Trees Once this process has completed, we then scan through the memory heap and reclaim any space that is being used for an object that has not been marked. At this time, we can also optionally coalesce all the allocated space in the memory heap into a single block, thereby eliminating external fragmentation for the time being. This scanning and reclamation process is known as the “sweep” phase, and when it completes, we resume running the suspended program. Thus, the mark-sweep garbage collection algorithm will reclaim unused space in time proportional to the number of live objects and their references plus the size of the memory heap. Performing DFS In-Place The mark-sweep algorithm correctly reclaims unused space in the memory heap, but there is an important issue we must face during the mark phase. Since we are reclaiming memory space at a time when available memory is scarce, we must take care not to use extra space during the garbage collection itself. The trouble is that the DFS algorithm, in the recursive way we have described it in Section 14.3.1, can use space proportional to the number of vertices in the graph. In the case of garbage collection, the vertices in our graph are the objects in the memory heap; hence, we probably don’t have this much memory to use. So our only alternative is to ﬁnd a way to perform DFS in-place rather than recursively. The main idea for performing DFS in-place is to simulate the recursion stack using the edges of the graph (which in the case of garbage collection correspond to object references). When we traverse an edge from a visited vertex v to a new vertex w, we change the edge (v,w) stored in v’s adjacency list to point back to v’s parent in the DFS tree. When we return back to v (simulating the return from the “recursive” call at w), we can now switch the edge we modiﬁed to point back to w. Of course, we need to have some way of identifying which edge we need to change back. One possibility is to number the references going out of v as 1, 2, and so on, and store, in addition to the mark bit (which we are using for the “visited” tag in our DFS), a count identiﬁer that tells us which edges we have modiﬁed. Using a count identiﬁer requires an extra word of storage per object. This extra word can be avoided in some implementations, however. For example, many implementations of the Java virtual machine represent an object as a composition of a reference with a type identiﬁer (which indicates if this object is an Integer or some other type) and as a reference to the other objects or data ﬁelds for this object. Since the type reference is always supposed to be the ﬁrst element of the composition in such implementations, we can use this reference to “mark” the edge we changed when leaving an object v and going to some object w. We simply swap the reference at v that refers to the type of v with the reference at v that refers to w. When we return to v, we can quickly identify the edge (v,w) we changed, because it will be the ﬁrst reference in the composition for v, and the position of the reference to v’s type will tell us the place where this edge belongs in v’s adjacency list.

15.2. Memory Hierarchies and Caching 15.2 Memory Hierarchies and Caching With the increased use of computing in society, software applications must manage extremely large data sets. Such applications include the processing of online ﬁnancial transactions, the organization and maintenance of databases, and analyses of customers’ purchasing histories and preferences. The amount of data can be so large that the overall performance of algorithms and data structures sometimes depends more on the time to access the data than on the speed of the CPU. 15.2.1 Memory Systems In order to accommodate large data sets, computers have a hierarchy of different kinds of memories, which vary in terms of their size and distance from the CPU. Closest to the CPU are the internal registers that the CPU itself uses. Access to such locations is very fast, but there are relatively few such locations. At the second level in the hierarchy are one or more memory caches. This memory is considerably larger than the register set of a CPU, but accessing it takes longer. At the third level in the hierarchy is the internal memory, which is also known as main memory or core memory. The internal memory is considerably larger than the cache memory, but also requires more time to access. Another level in the hierarchy is the external memory, which usually consists of disks, CD drives, DVD drives, and/or tapes. This memory is very large, but it is also very slow. Data stored through an external network can be viewed as yet another level in this hierarchy, with even greater storage capacity, but even slower access. Thus, the memory hierarchy for computers can be viewed as consisting of ﬁve or more levels, each of which is larger and slower than the previous level. (See Figure 15.4.) During the execution of a program, data is routinely copied from one level of the hierarchy to a neighboring level, and these transfers can become a computational bottleneck. External Memory Internal Memory Caches Registers CPU Bigger Network Storage Faster Figure 15.4: The memory hierarchy.

Chapter 15. Memory Management and B-Trees 15.2.2 Caching Strategies The signiﬁcance of the memory hierarchy on the performance of a program depends greatly upon the size of the problem we are trying to solve and the physical characteristics of the computer system. Often, the bottleneck occurs between two levels of the memory hierarchy—the one that can hold all data items and the level just below that one. For a problem that can ﬁt entirely in main memory, the two most important levels are the cache memory and the internal memory. Access times for internal memory can be as much as 10 to 100 times longer than those for cache memory. It is desirable, therefore, to be able to perform most memory accesses in cache memory. For a problem that does not ﬁt entirely in main memory, on the other hand, the two most important levels are the internal memory and the external memory. Here the differences are even more dramatic, for access times for disks, the usual general-purpose external-memory device, are typically as much as 100,000 to 1,000,000 times longer than those for internal memory. To put this latter ﬁgure into perspective, imagine there is a student in Baltimore who wants to send a request-for-money message to his parents in Chicago. If the student sends his parents an email message, it can arrive at their home computer in about ﬁve seconds. Think of this mode of communication as corresponding to an internal-memory access by a CPU. A mode of communication corresponding to an external-memory access that is 500,000 times slower would be for the student to walk to Chicago and deliver his message in person, which would take about a month if he can average 20 miles per day. Thus, we should make as few accesses to external memory as possible. Most algorithms are not designed with the memory hierarchy in mind, in spite of the great variance between access times for the different levels. Indeed, all of the algorithm analyses thus far described in this book have assumed that all memory accesses are equal. This assumption might seem, at ﬁrst, to be a great oversight— and one we are only addressing now in the ﬁnal chapter—but there are good reasons why it is actually a reasonable assumption to make. One justiﬁcation for this assumption is that it is often necessary to assume that all memory accesses take the same amount of time, since speciﬁc device-dependent information about memory sizes is often hard to come by. In fact, information about memory size may be difﬁcult to get. For example, a Java program that is designed to run on many different computer platforms cannot easily be deﬁned in terms of a speciﬁc computer architecture conﬁguration. We can certainly use architecturespeciﬁc information, if we have it (and we will show how to exploit such information later in this chapter). But once we have optimized our software for a certain architecture conﬁguration, our software will no longer be device-independent. Fortunately, such optimizations are not always necessary, primarily because of the second justiﬁcation for the equal-time memory-access assumption.

15.2. Memory Hierarchies and Caching Caching and Blocking Another justiﬁcation for the memory-access equality assumption is that operating system designers have developed general mechanisms that allow most memory accesses to be fast. These mechanisms are based on two important locality-ofreference properties that most software possesses: • Temporal locality: If a program accesses a certain memory location, then there is increased likelihood that it accesses that same location again in the near future. For example, it is common to use the value of a counter variable in several different expressions, including one to increment the counter’s value. In fact, a common adage among computer architects is that a program spends 90% of its time in 10% of its code. • Spatial locality: If a program accesses a certain memory location, then there is increased likelihood that it soon accesses other locations that are near this one. For example, a program using an array may be likely to access the locations of this array in a sequential or near-sequential manner. Computer scientists and engineers have performed extensive software proﬁling experiments to justify the claim that most software possesses both of these kinds of locality of reference. For example, a nested for loop used to repeatedly scan through an array will exhibit both kinds of locality. Temporal and spatial localities have, in turn, given rise to two fundamental design choices for multilevel computer memory systems (which are present in the interface between cache memory and internal memory, and also in the interface between internal memory and external memory). The ﬁrst design choice is called virtual memory. This concept consists of providing an address space as large as the capacity of the secondary-level memory, and of transferring data located in the secondary level into the primary level, when they are addressed. Virtual memory does not limit the programmer to the constraint of the internal memory size. The concept of bringing data into primary memory is called caching, and it is motivated by temporal locality. By bringing data into primary memory, we are hoping that it will be accessed again soon, and we will be able to respond quickly to all the requests for this data that come in the near future. The second design choice is motivated by spatial locality. Speciﬁcally, if data stored at a secondary-level memory location ℓis accessed, then we bring into primary-level memory a large block of contiguous locations that include the location ℓ. (See Figure 15.5.) This concept is known as blocking, and it is motivated by the expectation that other secondary-level memory locations close to ℓwill soon be accessed. In the interface between cache memory and internal memory, such blocks are often called cache lines, and in the interface between internal memory and external memory, such blocks are often called pages.

Chapter 15. Memory Management and B-Trees A block in the external memory address space A block on disk 0 1 2 3 ... 1024 ... 2048 ... Figure 15.5: Blocks in external memory. When implemented with caching and blocking, virtual memory often allows us to perceive secondary-level memory as being faster than it really is. There is still a problem, however. Primary-level memory is much smaller than secondarylevel memory. Moreover, because memory systems use blocking, any program of substance will likely reach a point where it requests data from secondary-level memory, but the primary memory is already full of blocks. In order to fulﬁll the request and maintain our use of caching and blocking, we must remove some block from primary memory to make room for a new block from secondary memory in this case. Deciding which block to evict brings up a number of interesting data structure and algorithm design issues. Caching in Web Browsers For motivation, we will consider a related problem that arises when revisiting information presented in Web pages. To exploit temporal locality of reference, it is often advantageous to store copies of Web pages in a cache memory, so these pages can be quickly retrieved when requested again. This effectively creates a two-level memory hierarchy, with the cache serving as the smaller, quicker internal memory, and the network being the external memory. In particular, suppose we have a cache memory that has m “slots” that can contain Web pages. We assume that a Web page can be placed in any slot of the cache. This is known as a fully associative cache. As a browser executes, it requests different Web pages. Each time the browser requests such a Web page p, the browser determines (using a quick test) if p is unchanged and currently contained in the cache. If p is contained in the cache, then the browser satisﬁes the request using the cached copy. If p is not in the cache, however, the page for p is requested over the Internet and transferred into the cache. If one of the m slots in the cache is available, then the browser assigns p to one of the empty slots. But if all the m cells of the cache are occupied, then the computer must determine which previously viewed Web page to evict before bringing in p to take its place. There are, of course, many different policies that can be used to determine the page to evict.

15.2. Memory Hierarchies and Caching Page Replacement Algorithms Some of the better-known page replacement policies include the following (see Figure 15.6): • First-in, ﬁrst-out (FIFO): Evict the page that has been in the cache the longest, that is, the page that was transferred to the cache furthest in the past. • Least recently used (LRU): Evict the page whose last request occurred furthest in the past. In addition, we can consider a simple and purely random strategy: • Random: Choose a page at random to evict from the cache. Figure 15.6: The Random, FIFO, and LRU page replacement policies. The Random strategy is one of the easiest policies to implement, for it only requires a random or pseudorandom number generator. The overhead involved in implementing this policy is an O(1) additional amount of work per page replacement. Moreover, there is no additional overhead for each page request, other than to determine whether a page request is in the cache or not. Still, this policy makes no attempt to take advantage of any temporal locality exhibited by a user’s browsing.

Chapter 15. Memory Management and B-Trees The FIFO strategy is quite simple to implement, as it only requires a queue Q to store references to the pages in the cache. Pages are enqueued in Q when they are referenced by a browser, and then are brought into the cache. When a page needs to be evicted, the computer simply performs a dequeue operation on Q to determine which page to evict. Thus, this policy also requires O(1) additional work per page replacement. Also, the FIFO policy incurs no additional overhead for page requests. Moreover, it tries to take some advantage of temporal locality. The LRU strategy goes a step further than the FIFO strategy, for the LRU strategy explicitly takes advantage of temporal locality as much as possible, by always evicting the page that was least-recently used. From a policy point of view, this is an excellent approach, but it is costly from an implementation point of view. That is, its way of optimizing temporal and spatial locality is fairly costly. Implementing the LRU strategy requires the use of an adaptable priority queue Q that supports updating the priority of existing pages. If Q is implemented with a sorted sequence based on a linked list, then the overhead for each page request and page replacement is O(1). When we insert a page in Q or update its key, the page is assigned the highest key in Q and is placed at the end of the list, which can also be done in O(1) time. Even though the LRU strategy has constant-time overhead, using the implementation above, the constant factors involved, in terms of the additional time overhead and the extra space for the priority queue Q, make this policy less attractive from a practical point of view. Since these different page replacement policies have different trade-offs between implementation difﬁculty and the degree to which they seem to take advantage of localities, it is natural for us to ask for some kind of comparative analysis of these methods to see which one, if any, is the best. From a worst-case point of view, the FIFO and LRU strategies have fairly unattractive competitive behavior. For example, suppose we have a cache containing m pages, and consider the FIFO and LRU methods for performing page replacement for a program that has a loop that repeatedly requests m + 1 pages in a cyclic order. Both the FIFO and LRU policies perform badly on such a sequence of page requests, because they perform a page replacement on every page request. Thus, from a worst-case point of view, these policies are almost the worst we can imagine—they require a page replacement on every page request. This worst-case analysis is a little too pessimistic, however, for it focuses on each protocol’s behavior for one bad sequence of page requests. An ideal analysis would be to compare these methods over all possible page-request sequences. Of course, this is impossible to do exhaustively, but there have been a great number of experimental simulations done on page-request sequences derived from real programs. Based on these experimental comparisons, the LRU strategy has been shown to be usually superior to the FIFO strategy, which is usually better than the Random strategy.

15.3. External Searching and B-Trees 15.3 External Searching and B-Trees Consider the problem of maintaining a large collection of items that does not ﬁt in main memory, such as a typical database. In this context, we refer to the secondarymemory blocks as disk blocks. Likewise, we refer to the transfer of a block between secondary memory and primary memory as a disk transfer. Recalling the great time difference that exists between main memory accesses and disk accesses, the main goal of maintaining such a collection in external memory is to minimize the number of disk transfers needed to perform a query or update. We refer to this count as the I/O complexity of the algorithm involved. Some Ineﬃcient External-Memory Representations A typical operation we would like to support is the search for a key in a map. If we were to store n items unordered in a doubly linked list, searching for a particular key within the list requires n transfers in the worst case, since each link hop we perform on the linked list might access a different block of memory. We can reduce the number of block transfers by storing the sequence in an array. A sequential search of an array can be performed using only O(n/B) block transfers because of spatial locality of reference, where B denotes the number of elements that ﬁt into a block. This is because the block transfer when accessing the ﬁrst element of the array actually retrieves the ﬁrst B elements, and so on with each successive block. It is worth noting that the bound of O(n/B) transfers is only achieved when using an array of primitives in Java. For an array of objects, the array stores the sequence of references; the actual objects that are referenced are not necessarily stored near each other in memory, and so there may be n distinct block transfers in the worst case. If a sequence is stored in sorted order within an array, a binary search performs O(log2 n) transfers, which is a nice improvement. But we do not get signiﬁcant beneﬁt from block transfers because each query during a binary search is likely in a different block of the sequence. As usual, update operations are expensive for a sorted array. Since these simple implementations are I/O inefﬁcient, we should consider the logarithmic-time internal-memory strategies that use balanced binary trees (for example, AVL trees or red-black trees) or other search structures with logarithmic average-case query and update times (for example, skip lists or splay trees). Typically, each node accessed for a query or update in one of these structures will be in a different block. Thus, these methods all require O(log2 n) transfers in the worst case to perform a query or update operation. But we can do better! We can perform map queries and updates using only O(logB n) = O(logn/logB) transfers.

Chapter 15. Memory Management and B-Trees 15.3.1 (a,b) Trees To reduce the number of external-memory accesses when searching, we can represent our map using a multiway search tree (Section 11.5.1). This approach gives rise to a generalization of the (2,4) tree data structure known as the (a,b) tree. An (a,b) tree is a multiway search tree such that each node has between a and b children and stores between a−1 and b−1 entries. The algorithms for searching, inserting, and removing entries in an (a,b) tree are straightforward generalizations of the corresponding ones for (2,4) trees. The advantage of generalizing (2,4) trees to (a,b) trees is that a parameterized class of trees provides a ﬂexible search structure, where the size of the nodes and the running time of the various map operations depends on the parameters a and b. By setting the parameters a and b appropriately with respect to the size of disk blocks, we can derive a data structure that achieves good external-memory performance. Deﬁnition of an (a,b) Tree An (a,b) tree, where parameters a and b are integers such that 2 ≤a ≤(b+1)/2, is a multiway search tree T with the following additional restrictions: Size Property: Each internal node has at least a children, unless it is the root, and has at most b children. Depth Property: All the external nodes have the same depth. Proposition 15.1: The height of an (a,b) tree storing n entries is Ω(logn/logb) and O(logn/loga). Justiﬁcation: Let T be an (a,b) tree storing n entries, and let h be the height of T. We justify the proposition by establishing the following bounds on h: logb log(n+1) ≤h ≤ loga log n+1 +1. By the size and depth properties, the number n′′ of external nodes of T is at least 2ah−1 and at most bh. By Proposition 11.6, n′′ = n+1. Thus, 2ah−1 ≤n+1 ≤bh. Taking the logarithm in base 2 of each term, we get (h−1)loga+1 ≤log(n+1) ≤hlogb. An algebraic manipulation of these inequalities completes the justiﬁcation.

15.3. External Searching and B-Trees Search and Update Operations We recall that in a multiway search tree T, each node w of T holds a secondary structure M(w), which is itself a map (Section 11.5.1). If T is an (a,b) tree, then M(w) stores at most b entries. Let f(b) denote the time for performing a search in a map, M(w). The search algorithm in an (a,b) tree is exactly like the one for multiway search trees given in Section 11.5.1. Hence, searching in an (a,b) tree T with n entries takes O( f(b) loga logn) time. Note that if b is considered a constant (and thus a is also), then the search time is O(logn). The main application of (a,b) trees is for maps stored in external memory. Namely, to minimize disk accesses, we select the parameters a and b so that each tree node occupies a single disk block (so that f(b) = 1 if we wish to simply count block transfers). Providing the right a and b values in this context gives rise to a data structure known as the B-tree, which we will describe shortly. Before we describe this structure, however, let us discuss how insertions and removals are handled in (a,b) trees. The insertion algorithm for an (a,b) tree is similar to that for a (2,4) tree. An overﬂow occurs when an entry is inserted into a b-node v, which becomes an illegal (b+ 1)-node. (Recall that a node in a multiway tree is a d-node if it has d children.) To remedy an overﬂow, we split node w by moving the median entry of w into the parent of w and replacing w with a ⌈(b+1)/2⌉-node w′ and a ⌊(b+1)/2⌋- node w′′. We can now see the reason for requiring a ≤(b+ 1)/2 in the deﬁnition of an (a,b) tree. Note that as a consequence of the split, we need to build the secondary structures M(w′) and M(w′′). Removing an entry from an (a,b) tree is similar to what was done for (2,4) trees. An underﬂow occurs when a key is removed from an a-node w, distinct from the root, which causes w to become an illegal (a−1)-node. To remedy an underﬂow, we perform a transfer with a sibling of w that is not an a-node or we perform a fusion of w with a sibling that is an a-node. The new node w′ resulting from the fusion is a (2a−1)-node, which is another reason for requiring a ≤(b+1)/2. Table 15.1 shows the performance of a map realized with an (a,b) tree. Method Running Time get O  f(b) loga logn  put O  g(b) loga logn  remove O  g(b) loga logn  Table 15.1: Time bounds for an n-entry map realized by an (a,b) tree T. We assume the secondary structure of the nodes of T support search in f(b) time, and split and fusion operations in g(b) time, for some functions f(b) and g(b), which can be made to be O(1) when we are only counting disk transfers.

Chapter 15. Memory Management and B-Trees 15.3.2 B-Trees A version of the (a,b) tree data structure, which is the best-known method for maintaining a map in external memory, is called the “B-tree.” (See Figure 15.7.) A B-tree of order d is an (a,b) tree with a = ⌈d/2⌉and b = d. Since we discussed the standard map query and update methods for (a,b) trees above, we restrict our discussion here to the I/O complexity of B-trees. Figure 15.7: A B-tree of order 6. An important property of B-trees is that we can choose d so that the d children references and the d −1 keys stored at a node can ﬁt compactly into a single disk block, implying that d is proportional to B. This choice allows us to assume that a and b are also proportional to B in the analysis of the search and update operations on (a,b) trees. Thus, f(b) and g(b) are both O(1), for each time we access a node to perform a search or an update operation, we need only perform a single disk transfer. As we have already observed above, each search or update requires that we examine at most O(1) nodes for each level of the tree. Therefore, any map search or update operation on a B-tree requires only O(log⌈d/2⌉n), that is, O(logn/logB), disk transfers. For example, an insert operation proceeds down the B-tree to locate the node in which to insert the new entry. If the node would overﬂow (to have d +1 children) because of this addition, then this node is split into two nodes that have ⌊(d + 1)/2⌋and ⌈(d + 1)/2⌉children, respectively. This process is then repeated at the next level up, and will continue for at most O(logB n) levels. Likewise, if a remove operation results in a node underﬂow (to have ⌈d/2⌉−1 children), then we move references from a sibling node with at least ⌈d/2⌉+ 1 children or we perform a fusion operation of this node with its sibling (and repeat this computation at the parent). As with the insert operation, this will continue up the B-tree for at most O(logB n) levels. The requirement that each internal node have at least ⌈d/2⌉children implies that each disk block used to support a B-tree is at least half full. Thus, we have the following: Proposition 15.2: A B-tree with n entries has I/O complexity O(logB n) for search or update operation, and uses O(n/B) blocks, where B is the size of a block.

15.4. External-Memory Sorting 15.4 External-Memory Sorting In addition to data structures, such as maps, that need to be implemented in external memory, there are many algorithms that must also operate on input sets that are too large to ﬁt entirely into internal memory. In this case, the objective is to solve the algorithmic problem using as few block transfers as possible. The most classic domain for such external-memory algorithms is the sorting problem. Multiway Merge-Sort An efﬁcient way to sort a set S of n objects in external memory amounts to a simple external-memory variation on the familiar merge-sort algorithm. The main idea behind this variation is to merge many recursively sorted lists at a time, thereby reducing the number of levels of recursion. Speciﬁcally, a high-level description of this multiway merge-sort method is to divide S into d subsets S1, S2, ..., Sd of roughly equal size, recursively sort each subset Si, and then simultaneously merge all d sorted lists into a sorted representation of S. If we can perform the merge process using only O(n/B) disk transfers, then, for large enough values of n, the total number of transfers performed by this algorithm satisﬁes the following recurrence equation: t(n) = d ·t(n/d)+cn/B, for some constant c ≥1. We can stop the recursion when n ≤B, since we can perform a single block transfer at this point, getting all of the objects into internal memory, and then sort the set with an efﬁcient internal-memory algorithm. Thus, the stopping criterion for t(n) is t(n) = 1 if n/B ≤1. This implies a closed-form solution that t(n) is O((n/B)logd(n/B)), which is O((n/B)log(n/B)/logd). Thus, if we can choose d to be Θ(M/B), where M is the size of the internal memory, then the worst-case number of block transfers performed by this multiway mergesort algorithm will be quite low. For reasons given in the next section, we choose d = (M/B)−1. The only aspect of this algorithm left to specify, then, is how to perform the d-way merge using only O(n/B) block transfers.

Chapter 15. Memory Management and B-Trees 15.4.1 Multiway Merging In a standard merge-sort (Section 12.1), the merge process combines two sorted sequences into one by repeatedly taking the smaller of the items at the front of the two respective lists. In a d-way merge, we repeatedly ﬁnd the smallest among the items at the front of the d sequences and place it as the next element of the merged sequence. We continue until all elements are included. In the context of an external-memory sorting algorithm, if main memory has size M and each block has size B, we can store up to M/B blocks within main memory at any given time. We speciﬁcally choose d = (M/B) −1 so that we can afford to keep one block from each input sequence in main memory at any given time, and to have one additional block to use as a buffer for the merged sequence. (See Figure 15.8.) Q Figure 15.8: A d-way merge with d = 5 and B = 4. Blocks that currently reside in main memory are shaded. We maintain the smallest unprocessed element from each input sequence in main memory, requesting the next block from a sequence when the preceding block has been exhausted. Similarly, we use one block of internal memory to buffer the merged sequence, ﬂushing that block to external memory when full. In this way, the total number of transfers performed during a single d-way merge is O(n/B), since we scan each block of list Si once, and we write out each block of the merged list S′ once. In terms of computation time, choosing the smallest of d values can trivially be performed using O(d) operations. If we are willing to devote O(d) internal memory, we can maintain a priority queue identifying the smallest element from each sequence, thereby performing each step of the merge in O(logd) time by removing the minimum element and replacing it with the next element from the same sequence. Hence, the internal time for the d-way merge is O(nlogd). Proposition 15.3: Given an array-based sequence S of n elements stored in external memory, we can sort S with O((n/B)log(n/B)/log(M/B)) block transfers and O(nlogn) internal computations, where M is the size of the internal memory and B is the size of a block.

C H A P T E R 1 Introduction In this chapter, we discuss the aims and goals of this text and brieﬂy review programming concepts and discrete mathematics. We will r See that how a program performs for reasonably large input is just as important as its performance on moderate amounts of input. r Summarize the basic mathematical background needed for the rest of the book. r Brieﬂy review recursion. r Summarize some important features of Java that are used throughout the text. 1.1 What’s the Book About? Suppose you have a group of N numbers and would like to determine the kth largest. This is known as the selection problem. Most students who have had a programming course or two would have no difﬁculty writing a program to solve this problem. There are quite a few “obvious” solutions. One way to solve this problem would be to read the N numbers into an array, sort the array in decreasing order by some simple algorithm such as bubblesort, and then return the element in position k. A somewhat better algorithm might be to read the ﬁrst k elements into an array and sort them (in decreasing order). Next, each remaining element is read one by one. As a new element arrives, it is ignored if it is smaller than the kth element in the array. Otherwise, it is placed in its correct spot in the array, bumping one element out of the array. When the algorithm ends, the element in the kth position is returned as the answer. Both algorithms are simple to code, and you are encouraged to do so. The natural questions, then, are which algorithm is better and, more important, is either algorithm good enough? A simulation using a random ﬁle of 30 million elements and k = 15,000,000 will show that neither algorithm ﬁnishes in a reasonable amount of time; each requires several days of computer processing to terminate (albeit eventually with a correct answer). An alternative method, discussed in Chapter 7, gives a solution in about a second. Thus, although our proposed algorithms work, they cannot be considered good algorithms, because they are entirely impractical for input sizes that a third algorithm can handle in a reasonable amount of time.

Chapter 1 Introduction t h i s w a t s o a h g f g d t Figure 1.1 Sample word puzzle A second problem is to solve a popular word puzzle. The input consists of a twodimensional array of letters and a list of words. The object is to ﬁnd the words in the puzzle. These words may be horizontal, vertical, or diagonal in any direction. As an example, the puzzle shown in Figure 1.1 contains the words this, two, fat, and that. The word this begins at row 1, column 1, or (1,1), and extends to (1,4); two goes from (1,1) to (3,1); fat goes from (4,1) to (2,3); and that goes from (4,4) to (1,1). Again, there are at least two straightforward algorithms that solve the problem. For each word in the word list, we check each ordered triple (row, column, orientation) for the presence of the word. This amounts to lots of nested for loops but is basically straightforward. Alternatively, for each ordered quadruple (row, column, orientation, number of characters) that doesn’t run off an end of the puzzle, we can test whether the word indicated is in the word list. Again, this amounts to lots of nested for loops. It is possible to save some time if the maximum number of characters in any word is known. It is relatively easy to code up either method of solution and solve many of the real-life puzzles commonly published in magazines. These typically have 16 rows, 16 columns, and 40 or so words. Suppose, however, we consider the variation where only the puzzle board is given and the word list is essentially an English dictionary. Both of the solutions proposed require considerable time to solve this problem and therefore are not acceptable. However, it is possible, even with a large word list, to solve the problem in a matter of seconds. An important concept is that, in many problems, writing a working program is not good enough. If the program is to be run on a large data set, then the running time becomes an issue. Throughout this book we will see how to estimate the running time of a program for large inputs and, more important, how to compare the running times of two programs without actually coding them. We will see techniques for drastically improving the speed of a program and for determining program bottlenecks. These techniques will enable us to ﬁnd the section of the code on which to concentrate our optimization efforts. 1.2 Mathematics Review This section lists some of the basic formulas you need to memorize or be able to derive and reviews basic proof techniques.

1.2 Mathematics Review 1.2.1 Exponents XAXB = XA+B XA XB = XA−B (XA)B = XAB XN + XN = 2XN ̸= X2N 2N + 2N = 2N+1 1.2.2 Logarithms In computer science, all logarithms are to the base 2 unless speciﬁed otherwise. Deﬁnition 1.1. XA = B if and only if logX B = A Several convenient equalities follow from this deﬁnition. Theorem 1.1. logA B = logC B logC A; A, B, C > 0, A ̸= 1 Proof. Let X = logC B, Y = logC A, and Z = logA B. Then, by the deﬁnition of logarithms, CX = B, CY = A, and AZ = B. Combining these three equalities yields CX = B = (CY)Z. Therefore, X = YZ, which implies Z = X/Y, proving the theorem. Theorem 1.2. log AB = log A + log B; A, B > 0 Proof. Let X = log A, Y = log B, and Z = log AB. Then, assuming the default base of 2, 2X = A, 2Y = B, and 2Z = AB. Combining the last three equalities yields 2X2Y = AB = 2Z. Therefore, X + Y = Z, which proves the theorem. Some other useful formulas, which can all be derived in a similar manner, follow. log A/B = log A −log B log(AB) = B log A log X < X for all X > 0 log 1 = 0, log 2 = 1, log 1,024 = 10, log 1,048,576 = 20

Chapter 1 Introduction 1.2.3 Series The easiest formulas to remember are N  i=0 2i = 2N+1 −1 and the companion, N  i=0 Ai = AN+1 −1 A −1 In the latter formula, if 0 < A < 1, then N  i=0 Ai ≤ 1 −A and as N tends to ∞, the sum approaches 1/(1 −A). These are the “geometric series” formulas. We can derive the last formula for ∞ i=0 Ai (0 < A < 1) in the following manner. Let S be the sum. Then S = 1 + A + A2 + A3 + A4 + A5 + · · · Then AS = A + A2 + A3 + A4 + A5 + · · · If we subtract these two equations (which is permissible only for a convergent series), virtually all the terms on the right side cancel, leaving S −AS = 1 which implies that S = 1 −A We can use this same technique to compute ∞ i=1 i/2i, a sum that occurs frequently. We write S = 1 2 + 2 22 + 3 23 + 4 24 + 5 25 + · · · and multiply by 2, obtaining 2S = 1 + 2 2 + 3 22 + 4 23 + 5 24 + 6 25 + · · ·

1.2 Mathematics Review Subtracting these two equations yields S = 1 + 1 2 + 1 22 + 1 23 + 1 24 + 1 25 + · · · Thus, S = 2. Another type of common series in analysis is the arithmetic series. Any such series can be evaluated from the basic formula. N  i=1 i = N(N + 1) ≈N2 For instance, to ﬁnd the sum 2 + 5 + 8 + · · · + (3k −1), rewrite it as 3(1 + 2 + 3 + · · · + k) −(1 + 1 + 1 + · · · + 1), which is clearly 3k(k + 1)/2 −k. Another way to remember this is to add the ﬁrst and last terms (total 3k + 1), the second and next to last terms (total 3k + 1), and so on. Since there are k/2 of these pairs, the total sum is k(3k + 1)/2, which is the same answer as before. The next two formulas pop up now and then but are fairly uncommon. N  i=1 i2 = N(N + 1)(2N + 1) ≈N3 N  i=1 ik ≈Nk+1 |k + 1| k ̸= −1 When k = −1, the latter formula is not valid. We then need the following formula, which is used far more in computer science than in other mathematical disciplines. The numbers HN are known as the harmonic numbers, and the sum is known as a harmonic sum. The error in the following approximation tends to γ ≈0.57721566, which is known as Euler’s constant. HN = N  i=1 i ≈loge N These two formulas are just general algebraic manipulations. N  i=1 f(N) = N f(N) N  i=n0 f(i) = N  i=1 f(i) − n0−1  i=1 f(i) 1.2.4 Modular Arithmetic We say that A is congruent to B modulo N, written A ≡B (mod N), if N divides A −B. Intuitively, this means that the remainder is the same when either A or B is divided by N. Thus, 81 ≡61 ≡1 (mod 10). As with equality, if A ≡B (mod N), then A + C ≡ B + C (mod N) and AD ≡BD (mod N).

Chapter 1 Introduction Often, N is a prime number. In that case, there are three important theorems. First, if N is prime, then ab ≡0 (mod N) is true if and only if a ≡0 (mod N) or b ≡0 (mod N). In other words, if a prime number N divides a product of two numbers, it divides at least one of the two numbers. Second, if N is prime, then the equation ax ≡1 (mod N) has a unique solution (mod N), for all 0 < a < N. This solution 0 < x < N, is the multiplicative inverse. Third, if N is prime, then the equation x2 ≡a (mod N) has either two solutions (mod N), for all 0 < a < N, or no solutions. There are many theorems that apply to modular arithmetic, and some of them require extraordinary proofs in number theory. We will use modular arithmetic sparingly, and the preceding theorems will sufﬁce. 1.2.5 The P Word The two most common ways of proving statements in data structure analysis are proof by induction and proof by contradiction (and occasionally proof by intimidation, used by professors only). The best way of proving that a theorem is false is by exhibiting a counterexample. Proof by Induction A proof by induction has two standard parts. The ﬁrst step is proving a base case, that is, establishing that a theorem is true for some small (usually degenerate) value(s); this step is almost always trivial. Next, an inductive hypothesis is assumed. Generally this means that the theorem is assumed to be true for all cases up to some limit k. Using this assumption, the theorem is then shown to be true for the next value, which is typically k + 1. This proves the theorem (as long as k is ﬁnite). As an example, we prove that the Fibonacci numbers, F0 = 1, F1 = 1, F2 = 2, F3 = 3, F4 = 5, . . . , Fi = Fi−1 +Fi−2, satisfy Fi < (5/3)i, for i ≥1. (Some deﬁnitions have F0 = 0, which shifts the series.) To do this, we ﬁrst verify that the theorem is true for the trivial cases. It is easy to verify that F1 = 1 < 5/3 and F2 = 2 < 25/9; this proves the basis. We assume that the theorem is true for i = 1, 2, . . . , k; this is the inductive hypothesis. To prove the theorem, we need to show that Fk+1 < (5/3)k+1. We have Fk+1 = Fk + Fk−1 by the deﬁnition, and we can use the inductive hypothesis on the right-hand side, obtaining Fk+1 < (5/3)k + (5/3)k−1 < (3/5)(5/3)k+1 + (3/5)2(5/3)k+1 < (3/5)(5/3)k+1 + (9/25)(5/3)k+1

1.2 Mathematics Review which simpliﬁes to Fk+1 < (3/5 + 9/25)(5/3)k+1 < (24/25)(5/3)k+1 < (5/3)k+1 proving the theorem. As a second example, we establish the following theorem. Theorem 1.3. If N ≥1, then N i=1 i2 = N(N+1)(2N+1) Proof. The proof is by induction. For the basis, it is readily seen that the theorem is true when N = 1. For the inductive hypothesis, assume that the theorem is true for 1 ≤k ≤N. We will establish that, under this assumption, the theorem is true for N + 1. We have N+1  i=1 i2 = N  i=1 i2 + (N + 1)2 Applying the inductive hypothesis, we obtain N+1  i=1 i2 = N(N + 1)(2N + 1) + (N + 1)2 = (N + 1) N(2N + 1) + (N + 1)  = (N + 1)2N2 + 7N + 6 = (N + 1)(N + 2)(2N + 3) Thus, N+1  i=1 i2 = (N + 1)[(N + 1) + 1][2(N + 1) + 1] proving the theorem. Proof by Counterexample The statement Fk ≤k2 is false. The easiest way to prove this is to compute F11 =144>112. Proof by Contradiction Proof by contradiction proceeds by assuming that the theorem is false and showing that this assumption implies that some known property is false, and hence the original assumption was erroneous. A classic example is the proof that there is an inﬁnite number of primes. To

Chapter 1 Introduction prove this, we assume that the theorem is false, so that there is some largest prime Pk. Let P1, P2, . . . , Pk be all the primes in order and consider N = P1P2P3 · · · Pk + 1 Clearly, N is larger than Pk, so by assumption N is not prime. However, none of P1, P2, . . . , Pk divides N exactly, because there will always be a remainder of 1. This is a contradiction, because every number is either prime or a product of primes. Hence, the original assumption, that Pk is the largest prime, is false, which implies that the theorem is true. 1.3 A Brief Introduction to Recursion Most mathematical functions that we are familiar with are described by a simple formula. For instance, we can convert temperatures from Fahrenheit to Celsius by applying the formula C = 5(F −32)/9 Given this formula, it is trivial to write a Java method; with declarations and braces removed, the one-line formula translates to one line of Java. Mathematical functions are sometimes deﬁned in a less standard form. As an example, we can deﬁne a function f, valid on nonnegative integers, that satisﬁes f(0) = 0 and f(x) = 2f(x −1) + x2. From this deﬁnition we see that f(1) = 1, f(2) = 6, f(3) = 21, and f(4) = 58. A function that is deﬁned in terms of itself is called recursive. Java allows functions to be recursive.1 It is important to remember that what Java provides is merely an attempt to follow the recursive spirit. Not all mathematically recursive functions are efﬁciently (or correctly) implemented by Java’s simulation of recursion. The idea is that the recursive function f ought to be expressible in only a few lines, just like a nonrecursive function. Figure 1.2 shows the recursive implementation of f. Lines 3 and 4 handle what is known as the base case, that is, the value for which the function is directly known without resorting to recursion. Just as declaring f(x) = 2f(x−1)+x2 is meaningless, mathematically, without including the fact that f(0) = 0, the recursive Java method doesn’t make sense without a base case. Line 6 makes the recursive call. There are several important and possibly confusing points about recursion. A common question is: Isn’t this just circular logic? The answer is that although we are deﬁning a method in terms of itself, we are not deﬁning a particular instance of the method in terms of itself. In other words, evaluating f(5) by computing f(5) would be circular. Evaluating f(5) by computing f(4) is not circular—unless, of course, f(4) is evaluated by eventually computing f(5). The two most important issues are probably the how and why questions. 1 Using recursion for numerical calculations is usually a bad idea. We have done so to illustrate the basic points.

1.3 A Brief Introduction to Recursion public static int f( int x ) { if( x == 0 ) return 0; else return 2 * f( x - 1 ) + x * x; } Figure 1.2 A recursive method In Chapter 3, the how and why issues are formally resolved. We will give an incomplete description here. It turns out that recursive calls are handled no differently from any others. If f is called with the value of 4, then line 6 requires the computation of 2 ∗f(3) + 4 ∗4. Thus, a call is made to compute f(3). This requires the computation of 2 ∗f(2) + 3 ∗3. Therefore, another call is made to compute f(2). This means that 2 ∗f(1) + 2 ∗2 must be evaluated. To do so, f(1) is computed as 2 ∗f(0) + 1 ∗1. Now, f(0) must be evaluated. Since this is a base case, we know a priori that f(0) = 0. This enables the completion of the calculation for f(1), which is now seen to be 1. Then f(2), f(3), and ﬁnally f(4) can be determined. All the bookkeeping needed to keep track of pending calls (those started but waiting for a recursive call to complete), along with their variables, is done by the computer automatically. An important point, however, is that recursive calls will keep on being made until a base case is reached. For instance, an attempt to evaluate f(−1) will result in calls to f(−2), f(−3), and so on. Since this will never get to a base case, the program won’t be able to compute the answer (which is undeﬁned anyway). Occasionally, a much more subtle error is made, which is exhibited in Figure 1.3. The error in Figure 1.3 is that bad(1) is deﬁned, by line 6, to be bad(1). Obviously, this doesn’t give any clue as to what bad(1) actually is. The computer will thus repeatedly make calls to bad(1) in an attempt to resolve its values. Eventually, its bookkeeping system will run out of space, and the program will terminate abnormally. Generally, we would say that this method doesn’t work for one special case but is correct otherwise. This isn’t true here, since bad(2) calls bad(1). Thus, bad(2) cannot be evaluated either. Furthermore, bad(3), bad(4), and bad(5) all make calls to bad(2). Since bad(2) is unevaluable, none of these values are either. In fact, this public static int bad( int n ) { if( n == 0 ) return 0; else return bad( n / 3 + 1 ) + n - 1; } Figure 1.3 A nonterminating recursive method

Chapter 1 Introduction program doesn’t work for any nonnegative value of n, except 0. With recursive programs, there is no such thing as a “special case.” These considerations lead to the ﬁrst two fundamental rules of recursion: 1. Base cases. You must always have some base cases, which can be solved without recursion. 2. Making progress. For the cases that are to be solved recursively, the recursive call must always be to a case that makes progress toward a base case. Throughout this book, we will use recursion to solve problems. As an example of a nonmathematical use, consider a large dictionary. Words in dictionaries are deﬁned in terms of other words. When we look up a word, we might not always understand the deﬁnition, so we might have to look up words in the deﬁnition. Likewise, we might not understand some of those, so we might have to continue this search for a while. Because the dictionary is ﬁnite, eventually either (1) we will come to a point where we understand all of the words in some deﬁnition (and thus understand that deﬁnition and retrace our path through the other deﬁnitions) or (2) we will ﬁnd that the deﬁnitions are circular and we are stuck, or that some word we need to understand for a deﬁnition is not in the dictionary. Our recursive strategy to understand words is as follows: If we know the meaning of a word, then we are done; otherwise, we look the word up in the dictionary. If we understand all the words in the deﬁnition, we are done; otherwise, we ﬁgure out what the deﬁnition means by recursively looking up the words we don’t know. This procedure will terminate if the dictionary is well deﬁned but can loop indeﬁnitely if a word is either not deﬁned or circularly deﬁned. Printing Out Numbers Suppose we have a positive integer, n, that we wish to print out. Our routine will have the heading printOut(n). Assume that the only I/O routines available will take a single-digit number and output it to the terminal. We will call this routine printDigit; for example, printDigit(4) will output a 4 to the terminal. Recursion provides a very clean solution to this problem. To print out 76234, we need to ﬁrst print out 7623 and then print out 4. The second step is easily accomplished with the statement printDigit(n%10), but the ﬁrst doesn’t seem any simpler than the original problem. Indeed it is virtually the same problem, so we can solve it recursively with the statement printOut(n/10). This tells us how to solve the general problem, but we still need to make sure that the program doesn’t loop indeﬁnitely. Since we haven’t deﬁned a base case yet, it is clear that we still have something to do. Our base case will be printDigit(n) if 0 ≤n < 10. Now printOut(n) is deﬁned for every positive number from 0 to 9, and larger numbers are deﬁned in terms of a smaller positive number. Thus, there is no cycle. The entire method is shown in Figure 1.4.

1.3 A Brief Introduction to Recursion public static void printOut( int n ) /* Print nonnegative n */ { if( n >= 10 ) printOut( n / 10 ); printDigit( n % 10 ); } Figure 1.4 Recursive routine to print an integer We have made no effort to do this efﬁciently. We could have avoided using the mod routine (which can be very expensive) because n%10 = n −⌊n/10⌋∗10.2 Recursion and Induction Let us prove (somewhat) rigorously that the recursive number-printing program works. To do so, we’ll use a proof by induction. Theorem 1.4. The recursive number-printing algorithm is correct for n ≥0. Proof (by induction on the number of digits in n). First, if n has one digit, then the program is trivially correct, since it merely makes a call to printDigit. Assume then that printOut works for all numbers of k or fewer digits. A number of k + 1 digits is expressed by its ﬁrst k digits followed by its least signiﬁcant digit. But the number formed by the ﬁrst k digits is exactly ⌊n/10⌋, which, by the inductive hypothesis, is correctly printed, and the last digit is n mod 10, so the program prints out any (k + 1)-digit number correctly. Thus, by induction, all numbers are correctly printed. This proof probably seems a little strange in that it is virtually identical to the algorithm description. It illustrates that in designing a recursive program, all smaller instances of the same problem (which are on the path to a base case) may be assumed to work correctly. The recursive program needs only to combine solutions to smaller problems, which are “magically” obtained by recursion, into a solution for the current problem. The mathematical justiﬁcation for this is proof by induction. This gives the third rule of recursion: 3. Design rule. Assume that all the recursive calls work. This rule is important because it means that when designing recursive programs, you generally don’t need to know the details of the bookkeeping arrangements, and you don’t have to try to trace through the myriad of recursive calls. Frequently, it is extremely difﬁcult to track down the actual sequence of recursive calls. Of course, in many cases this is an indication of a good use of recursion, since the computer is being allowed to work out the complicated details. 2 ⌊x⌋is the largest integer that is less than or equal to x.

Chapter 1 Introduction The main problem with recursion is the hidden bookkeeping costs. Although these costs are almost always justiﬁable, because recursive programs not only simplify the algorithm design but also tend to give cleaner code, recursion should never be used as a substitute for a simple for loop. We’ll discuss the overhead involved in recursion in more detail in Section 3.6. When writing recursive routines, it is crucial to keep in mind the four basic rules of recursion: 1. Base cases. You must always have some base cases, which can be solved without recursion. 2. Making progress. For the cases that are to be solved recursively, the recursive call must always be to a case that makes progress toward a base case. 3. Design rule. Assume that all the recursive calls work. 4. Compound interest rule. Never duplicate work by solving the same instance of a problem in separate recursive calls. The fourth rule, which will be justiﬁed (along with its nickname) in later sections, is the reason that it is generally a bad idea to use recursion to evaluate simple mathematical functions, such as the Fibonacci numbers. As long as you keep these rules in mind, recursive programming should be straightforward. 1.4 Implementing Generic Components Pre-Java 5 An important goal of object-oriented programming is the support of code reuse. An important mechanism that supports this goal is the generic mechanism: If the implementation is identical except for the basic type of the object, a generic implementation can be used to describe the basic functionality. For instance, a method can be written to sort an array of items; the logic is independent of the types of objects being sorted, so a generic method could be used. Unlike many of the newer languages (such as C++, which uses templates to implement generic programming), before version 1.5, Java did not support generic implementations directly. Instead, generic programming was implemented using the basic concepts of inheritance. This section describes how generic methods and classes can be implemented in Java using the basic principles of inheritance. Direct support for generic methods and classes was announced by Sun in June 2001 as a future language addition. Finally, in late 2004, Java 5 was released and provided support for generic methods and classes. However, using generic classes requires an understanding of the pre-Java 5 idioms for generic programming. As a result, an understanding of how inheritance is used to implement generic programs is essential, even in Java 5.

1.4 Implementing Generic Components Pre-Java 5 1.4.1 Using Object for Genericity The basic idea in Java is that we can implement a generic class by using an appropriate superclass, such as Object. An example is the MemoryCell class shown in Figure 1.5. There are two details that must be considered when we use this strategy. The ﬁrst is illustrated in Figure 1.6, which depicts a main that writes a "37" to a MemoryCell object and then reads from the MemoryCell object. To access a speciﬁc method of the object, we must downcast to the correct type. (Of course, in this example, we do not need the downcast, since we are simply invoking the toString method at line 9, and this can be done for any object.) A second important detail is that primitive types cannot be used. Only reference types are compatible with Object. A standard workaround to this problem is discussed momentarily. // MemoryCell class // Object read( ) --> Returns the stored value // void write( Object x ) --> x is stored public class MemoryCell { // Public methods public Object read( ) { return storedValue; } public void write( Object x ) { storedValue = x; } // Private internal data representation private Object storedValue; } Figure 1.5 A generic MemoryCell class (pre-Java 5) public class TestMemoryCell { public static void main( String [ ] args ) { MemoryCell m = new MemoryCell( ); m.write( "37" ); String val = (String) m.read( ); System.out.println( "Contents are: " + val ); } } Figure 1.6 Using the generic MemoryCell class (pre-Java 5)

Chapter 1 Introduction 1.4.2 Wrappers for Primitive Types When we implement algorithms, often we run into a language typing problem: We have an object of one type, but the language syntax requires an object of a different type. This technique illustrates the basic theme of a wrapper class. One typical use is to store a primitive type, and add operations that the primitive type either does not support or does not support correctly. In Java, we have already seen that although every reference type is compatible with Object, the eight primitive types are not. As a result, Java provides a wrapper class for each of the eight primitive types. For instance, the wrapper for the int type is Integer. Each wrapper object is immutable (meaning its state can never change), stores one primitive value that is set when the object is constructed, and provides a method to retrieve the value. The wrapper classes also contain a host of static utility methods. As an example, Figure 1.7 shows how we can use the MemoryCell to store integers. 1.4.3 Using Interface Types for Genericity Using Object as a generic type works only if the operations that are being performed can be expressed using only methods available in the Object class. Consider, for example, the problem of ﬁnding the maximum item in an array of items. The basic code is type-independent, but it does require the ability to compare any two objects and decide which is larger and which is smaller. Thus we cannot simply ﬁnd the maximum of an array of Object—we need more information. The simplest idea would be to ﬁnd the maximum of an array of Comparable. To determine order, we can use the compareTo method that we know must be available for all Comparables. The code to do this is shown in Figure 1.8, which provides a main that ﬁnds the maximum in an array of String or Shape. It is important to mention a few caveats. First, only objects that implement the Comparable interface can be passed as elements of the Comparable array. Objects that have a compareTo method but do not declare that they implement Comparable are not Comparable, and do not have the requisite IS-A relationship. Thus, it is presumed that Shape implements public class WrapperDemo { public static void main( String [ ] args ) { MemoryCell m = new MemoryCell( ); m.write( new Integer( 37 ) ); Integer wrapperVal = (Integer) m.read( ); int val = wrapperVal.intValue( ); System.out.println( "Contents are: " + val ); } } Figure 1.7 An illustration of the Integer wrapper class

1.4 Implementing Generic Components Pre-Java 5 class FindMaxDemo { /** * Return max item in arr. * Precondition: arr.length > 0 */ public static Comparable findMax( Comparable [ ] arr ) { int maxIndex = 0; for( int i = 1; i < arr.length; i++ ) if( arr[ i ].compareTo( arr[ maxIndex ] ) > 0 ) maxIndex = i; return arr[ maxIndex ]; } /** * Test findMax on Shape and String objects. */ public static void main( String [ ] args ) { Shape [ ] sh1 = { new Circle( 2.0 ), new Square( 3.0 ), new Rectangle( 3.0, 4.0 ) }; String [ ] st1 = { "Joe", "Bob", "Bill", "Zeke" }; System.out.println( findMax( sh1 ) ); System.out.println( findMax( st1 ) ); } } Figure 1.8 A generic findMax routine, with demo using shapes and strings (pre-Java 5) the Comparable interface, perhaps comparing areas of Shapes. It is also implicit in the test program that Circle, Square, and Rectangle are subclasses of Shape. Second, if the Comparable array were to have two objects that are incompatible (e.g., a String and a Shape), the compareTo method would throw a ClassCastException. This is the expected (indeed, required) behavior. Third, as before, primitives cannot be passed as Comparables, but the wrappers work because they implement the Comparable interface. Fourth, it is not required that the interface be a standard library interface. Finally, this solution does not always work, because it might be impossible to declare that a class implements a needed interface. For instance, the class might be a library class,

Chapter 1 Introduction while the interface is a user-deﬁned interface. And if the class is ﬁnal, we can’t extend it to create a new class. Section 1.6 offers another solution for this problem, which is the function object. The function object uses interfaces also and is perhaps one of the central themes encountered in the Java library. 1.4.4 Compatibility of Array Types One of the difﬁculties in language design is how to handle inheritance for aggregate types. Suppose that Employee IS-A Person. Does this imply that Employee[] IS-A Person[]? In other words, if a routine is written to accept Person[] as a parameter, can we pass an Employee[] as an argument? At ﬁrst glance, this seems like a no-brainer, and Employee[] should be type-compatible with Person[]. However, this issue is trickier than it seems. Suppose that in addition to Employee, Student IS-A Person. Suppose the Employee[] is type-compatible with Person[]. Then consider this sequence of assignments: Person[] arr = new Employee[ 5 ]; // compiles: arrays are compatible arr[ 0 ] = new Student( ... ); // compiles: Student IS-A Person Both assignments compile, yet arr[0] is actually referencing an Employee, and Student IS-NOT-A Employee. Thus we have type confusion. The runtime system cannot throw a ClassCastException since there is no cast. The easiest way to avoid this problem is to specify that the arrays are not typecompatible. However, in Java the arrays are type-compatible. This is known as a covariant array type. Each array keeps track of the type of object it is allowed to store. If an incompatible type is inserted into the array, the Virtual Machine will throw an ArrayStoreException. The covariance of arrays was needed in earlier versions of Java because otherwise the calls on lines 29 and 30 in Figure 1.8 would not compile. 1.5 Implementing Generic Components Using Java 5 Generics Java 5 supports generic classes that are very easy to use. However, writing generic classes requires a little more work. In this section, we illustrate the basics of how generic classes and methods are written. We do not attempt to cover all the constructs of the language, which are quite complex and sometimes tricky. Instead, we show the syntax and idioms that are used throughout this book.

1.5 Implementing Generic Components Using Java 5 Generics 1.5.1 Simple Generic Classes and Interfaces Figure 1.9 shows a generic version of the MemoryCell class previously depicted in Figure 1.5. Here, we have changed the name to GenericMemoryCell because neither class is in a package and thus the names cannot be the same. When a generic class is speciﬁed, the class declaration includes one or more type parameters enclosed in angle brackets <> after the class name. Line 1 shows that the GenericMemoryCell takes one type parameter. In this instance, there are no explicit restrictions on the type parameter, so the user can create types such as GenericMemoryCell<String> and GenericMemoryCell<Integer> but not GenericMemoryCell<int>. Inside the GenericMemoryCell class declaration, we can declare ﬁelds of the generic type and methods that use the generic type as a parameter or return type. For example, in line 5 of Figure 1.9, the write method for GenericMemoryCell<String> requires a parameter of type String. Passing anything else will generate a compiler error. Interfaces can also be declared as generic. For example, prior to Java 5 the Comparable interface was not generic, and its compareTo method took an Object as the parameter. As a result, any reference variable passed to the compareTo method would compile, even if the variable was not a sensible type, and only at runtime would the error be reported as a ClassCastException. In Java 5, the Comparable class is generic, as shown in Figure 1.10. The String class, for instance, now implements Comparable<String> and has a compareTo method that takes a String as a parameter. By making the class generic, many of the errors that were previously only reported at runtime become compile-time errors. public class GenericMemoryCell<AnyType> { public AnyType read( ) { return storedValue; } public void write( AnyType x ) { storedValue = x; } private AnyType storedValue; } Figure 1.9 Generic implementation of the MemoryCell class package java.lang; public interface Comparable<AnyType> { public int compareTo( AnyType other ); } Figure 1.10 Comparable interface, Java 5 version which is generic

Chapter 1 Introduction 1.5.2 Autoboxing/Unboxing The code in Figure 1.7 is annoying to write because using the wrapper class requires creation of an Integer object prior to the call to write, and then the extraction of the int value from the Integer, using the intValue method. Prior to Java 5, this is required because if an int is passed in a place where an Integer object is required, the compiler will generate an error message, and if the result of an Integer object is assigned to an int, the compiler will generate an error message. This resulting code in Figure 1.7 accurately reﬂects the distinction between primitive types and reference types, yet it does not cleanly express the programmer’s intent of storing ints in the collection. Java 5 rectiﬁes this situation. If an int is passed in a place where an Integer is required, the compiler will insert a call to the Integer constructor behind the scenes. This is known as autoboxing. And if an Integer is passed in a place where an int is required, the compiler will insert a call to the intValue method behind the scenes. This is known as auto-unboxing. Similar behavior occurs for the seven other primitive/wrapper pairs. Figure 1.11a illustrates the use of autoboxing and unboxing in Java 5. Note that the entities referenced in the GenericMemoryCell are still Integer objects; int cannot be substituted for Integer in the GenericMemoryCell instantiations. 1.5.3 The Diamond Operator In Figure 1.11a, line 5 is annoying because since m is of type GenericMemoryCell<Integer>, it is obvious that object being created must also be GenericMemoryCell<Integer>; any other type parameter would generate a compiler error. Java 7 adds a new language feature, known as the diamond operator, that allows line 5 to be rewritten as GenericMemoryCell<Integer> m = new GenericMemoryCell<>( ); The diamond operator simpliﬁes the code, with no cost to the developer, and we use it throughout the text. Figure 1.11b shows the Java 7 version, incorporating the diamond operator. class BoxingDemo { public static void main( String [ ] args ) { GenericMemoryCell<Integer> m = new GenericMemoryCell<Integer>( ); m.write( 37 ); int val = m.read( ); System.out.println( "Contents are: " + val ); } } Figure 1.11a Autoboxing and unboxing (Java 5)

1.5 Implementing Generic Components Using Java 5 Generics class BoxingDemo { public static void main( String [ ] args ) { GenericMemoryCell<Integer> m = new GenericMemoryCell<>( ); m.write( 5 ); int val = m.read( ); System.out.println( "Contents are: " + val ); } } Figure 1.11b Autoboxing and unboxing (Java 7, using diamond operator) 1.5.4 Wildcards with Bounds Figure 1.12 shows a static method that computes the total area in an array of Shapes (we assume Shape is a class with an area method; Circle and Square extend Shape). Suppose we want to rewrite the method so that it works with a parameter that is Collection<Shape>. Collection is described in Chapter 3; for now, the only important thing about it is that it stores a collection of items that can be accessed with an enhanced for loop. Because of the enhanced for loop, the code should be identical, and the resulting code is shown in Figure 1.13. If we pass a Collection<Shape>, the code works. However, what happens if we pass a Collection<Square>? The answer depends on whether a Collection<Square> IS-A Collection<Shape>. Recall from Section 1.4.4 that the technical term for this is whether we have covariance. In Java, as we mentioned in Section 1.4.4, arrays are covariant. So Square[] IS-A Shape[]. On the one hand, consistency would suggest that if arrays are covariant, then collections should be covariant too. On the other hand, as we saw in Section 1.4.4, the covariance of arrays leads to code that compiles but then generates a runtime exception (an ArrayStoreException). Because the entire reason to have generics is to generate compiler public static double totalArea( Shape [ ] arr ) { double total = 0; for( Shape s : arr ) if( s != null ) total += s.area( ); return total; } Figure 1.12 totalArea method for Shape[]

Chapter 1 Introduction public static double totalArea( Collection<Shape> arr ) { double total = 0; for( Shape s : arr ) if( s != null ) total += s.area( ); return total; } Figure 1.13 totalArea method that does not work if passed a Collection<Square> public static double totalArea( Collection<? extends Shape> arr ) { double total = 0; for( Shape s : arr ) if( s != null ) total += s.area( ); return total; } Figure 1.14 totalArea method revised with wildcards that works if passed a Collection<Square> errors rather than runtime exceptions for type mismatches, generic collections are not covariant. As a result, we cannot pass a Collection<Square> as a parameter to the method in Figure 1.13. What we are left with is that generics (and the generic collections) are not covariant (which makes sense), but arrays are. Without additional syntax, users would tend to avoid collections because the lack of covariance makes the code less ﬂexible. Java 5 makes up for this with wildcards. Wildcards are used to express subclasses (or superclasses) of parameter types. Figure 1.14 illustrates the use of wildcards with a bound to write a totalArea method that takes as parameter a Collection<T>, where T IS-A Shape. Thus, Collection<Shape> and Collection<Square> are both acceptable parameters. Wildcards can also be used without a bound (in which case extends Object is presumed) or with super instead of extends (to express superclass rather than subclass); there are also some other syntax uses that we do not discuss here. 1.5.5 Generic Static Methods In some sense, the totalArea method in Figure 1.14 is generic, since it works for different types. But there is no speciﬁc type parameter list, as was done in the GenericMemoryCell

1.5 Implementing Generic Components Using Java 5 Generics public static <AnyType> boolean contains( AnyType [ ] arr, AnyType x ) { for( AnyType val : arr ) if( x.equals( val ) ) return true; return false; } Figure 1.15 Generic static method to search an array class declaration. Sometimes the speciﬁc type is important perhaps because one of the following reasons apply: 1. The type is used as the return type. 2. The type is used in more than one parameter type. 3. The type is used to declare a local variable. If so, then an explicit generic method with type parameters must be declared. For instance, Figure 1.15 illustrates a generic static method that performs a sequential search for value x in array arr. By using a generic method instead of a nongeneric method that uses Object as the parameter types, we can get compile-time errors if searching for an Apple in an array of Shapes. The generic method looks much like the generic class in that the type parameter list uses the same syntax. The type parameters in a generic method precede the return type. 1.5.6 Type Bounds Suppose we want to write a findMax routine. Consider the code in Figure 1.16. This code cannot work because the compiler cannot prove that the call to compareTo at line 6 is valid; compareTo is guaranteed to exist only if AnyType is Comparable. We can solve this problem public static <AnyType> AnyType findMax( AnyType [ ] arr ) { int maxIndex = 0; for( int i = 1; i < arr.length; i++ ) if( arr[ i ].compareTo( arr[ maxIndex ] ) > 0 ) maxIndex = i; return arr[ maxIndex ]; } Figure 1.16 Generic static method to ﬁnd largest element in an array that does not work

Chapter 1 Introduction public static <AnyType extends Comparable<? super AnyType>> AnyType findMax( AnyType [ ] arr ) { int maxIndex = 0; for( int i = 1; i < arr.length; i++ ) if( arr[ i ].compareTo( arr[ maxIndex ] ) > 0 ) maxIndex = i; return arr[ maxIndex ]; } Figure 1.17 Generic static method to ﬁnd largest element in an array. Illustrates a bounds on the type parameter by using a type bound. The type bound is speciﬁed inside the angle brackets <>, and it speciﬁes properties that the parameter types must have. A naïve attempt is to rewrite the signature as public static <AnyType extends Comparable> ... This is naïve because, as we know, the Comparable interface is now generic. Although this code would compile, a better attempt would be public static <AnyType extends Comparable<AnyType>> ... However, this attempt is not satisfactory. To see the problem, suppose Shape implements Comparable<Shape>. Suppose Square extends Shape. Then all we know is that Square implements Comparable<Shape>. Thus, a Square IS-A Comparable<Shape>, but it IS-NOT-A Comparable<Square>! As a result, what we need to say is that AnyType IS-A Comparable<T> where T is a superclass of AnyType. Since we do not need to know the exact type T, we can use a wildcard. The resulting signature is public static <AnyType extends Comparable<? super AnyType>> Figure 1.17 shows the implementation of findMax. The compiler will accept arrays of types T only such that T implements the Comparable<S> interface, where T IS-A S. Certainly the bounds declaration looks like a mess. Fortunately, we won’t see anything more complicated than this idiom. 1.5.7 Type Erasure Generic types, for the most part, are constructs in the Java language but not in the Virtual Machine. Generic classes are converted by the compiler to nongeneric classes by a process known as type erasure. The simpliﬁed version of what happens is that the compiler generates a raw class with the same name as the generic class with the type parameters removed. The type variables are replaced with their bounds, and when calls are made

1.5 Implementing Generic Components Using Java 5 Generics to generic methods that have an erased return type, casts are inserted automatically. If a generic class is used without a type parameter, the raw class is used. One important consequence of type erasure is that the generated code is not much different than the code that programmers have been writing before generics and in fact is not any faster. The signiﬁcant beneﬁt is that the programmer does not have to place casts in the code, and the compiler will do signiﬁcant type checking. 1.5.8 Restrictions on Generics There are numerous restrictions on generic types. Every one of the restrictions listed here is required because of type erasure. Primitive Types Primitive types cannot be used for a type parameter. Thus GenericMemoryCell<int> is illegal. You must use wrapper classes. instanceof tests instanceof tests and typecasts work only with raw type. In the following code GenericMemoryCell<Integer> cell1 = new GenericMemoryCell<>( ); cell1.write( 4 ); Object cell = cell1; GenericMemoryCell<String> cell2 = (GenericMemoryCell<String>) cell; String s = cell2.read( ); the typecast succeeds at runtime since all types are GenericMemoryCell. Eventually, a runtime error results at the last line because the call to read tries to return a String but cannot. As a result, the typecast will generate a warning, and a corresponding instanceof test is illegal. Static Contexts In a generic class, static methods and ﬁelds cannot refer to the class’s type variables since, after erasure, there are no type variables. Further, since there is really only one raw class, static ﬁelds are shared among the class’s generic instantiations. Instantiation of Generic Types It is illegal to create an instance of a generic type. If T is a type variable, the statement T obj = new T( ); // Right-hand side is illegal is illegal. T is replaced by its bounds, which could be Object (or even an abstract class), so the call to new cannot make sense. Generic Array Objects It is illegal to create an array of a generic type. If T is a type variable, the statement T [ ] arr = new T[ 10 ]; // Right-hand side is illegal

Chapter 1 Introduction is illegal. T would be replaced by its bounds, which would probably be Object, and then the cast (generated by type erasure) to T[] would fail because Object[] IS-NOT-A T[]. Because we cannot create arrays of generic objects, generally we must create an array of the erased type and then use a typecast. This typecast will generate a compiler warning about an unchecked type conversion. Arrays of Parameterized Types Instantiation of arrays of parameterized types is illegal. Consider the following code: GenericMemoryCell<String> [ ] arr1 = new GenericMemoryCell<>[ 10 ]; GenericMemoryCell<Double> cell = new GenericMemoryCell<>( ); cell.write( 4.5 ); Object [ ] arr2 = arr1; arr2[ 0 ] = cell; String s = arr1[ 0 ].read( ); Normally, we would expect that the assignment at line 4, which has the wrong type, would generate an ArrayStoreException. However, after type erasure, the array type is GenericMemoryCell[], and the object added to the array is GenericMemoryCell, so there is no ArrayStoreException. Thus, this code has no casts, yet it will eventually generate a ClassCastException at line 5, which is exactly the situation that generics are supposed to avoid. 1.6 Function Objects In Section 1.5, we showed how to write generic algorithms. As an example, the generic method in Figure 1.16 can be used to ﬁnd the maximum item in an array. However, that generic method has an important limitation: It works only for objects that implement the Comparable interface, using compareTo as the basis for all comparison decisions. In many situations, this approach is not feasible. For instance, it is a stretch to presume that a Rectangle class will implement Comparable, and even if it does, the compareTo method that it has might not be the one we want. For instance, given a 2-by-10 rectangle and a 5-by-5 rectangle, which is the larger rectangle? The answer would depend on whether we are using area or width to decide. Or perhaps if we are trying to ﬁt the rectangle through an opening, the larger rectangle is the rectangle with the larger minimum dimension. As a second example, if we wanted to ﬁnd the maximum string (alphabetically last) in an array of strings, the default compareTo does not ignore case distinctions, so “ZEBRA” would be considered to precede “alligator” alphabetically, which is probably not what we want. The solution in these situations is to rewrite findMax to accept two parameters: an array of objects and a comparison function that explains how to decide which of two objects is the larger and which is the smaller. In effect, the objects no longer know how to compare themselves; instead, this information is completely decoupled from the objects in the array. An ingenious way to pass functions as parameters is to notice that an object contains both data and methods, so we can deﬁne a class with no data and one method and pass

1.6 Function Objects // Generic findMax, with a function object. // Precondition: a.size( ) > 0. public static <AnyType> AnyType findMax( AnyType [ ] arr, Comparator<? super AnyType> cmp ) { int maxIndex = 0; for( int i = 1; i < arr.size( ); i++ ) if( cmp.compare( arr[ i ], arr[ maxIndex ] ) > 0 ) maxIndex = i; return arr[ maxIndex ]; } class CaseInsensitiveCompare implements Comparator<String> { public int compare( String lhs, String rhs ) { return lhs.compareToIgnoreCase( rhs ); } } class TestProgram { public static void main( String [ ] args ) { String [ ] arr = { "ZEBRA", "alligator", "crocodile" }; System.out.println( findMax( arr, new CaseInsensitiveCompare( ) ) ) } } Figure 1.18 Using a function object as a second parameter to findMax; output is ZEBRA an instance of the class. In effect, a function is being passed by placing it inside an object. This object is commonly known as a function object. Figure 1.18 shows the simplest implementation of the function object idea. findMax takes a second parameter, which is an object of type Comparator. The Comparator interface is speciﬁed in java.util and contains a compare method. This interface is shown in Figure 1.19. Any class that implements the Comparator<AnyType> interface type must have a method named compare that takes two parameters of the generic type (AnyType) and returns an int, following the same general contract as compareTo. Thus, in Figure 1.18, the call to compare at line 9 can be used to compare array items. The bounded wildcard at line 4 is used to signal that if we are ﬁnding the maximum in an array of items, the comparator must know how to compare items, or objects of the items’ supertype. To use this version of findMax, at line 26, we can see that findMax is called by passing an array of String and an object that

Chapter 1 Introduction package java.util; public interface Comparator<AnyType> { int compare( AnyType lhs, AnyType rhs ); } Figure 1.19 The Comparator interface implements Comparator<String>. This object is of type CaseInsensitiveCompare, which is a class we write. In Chapter 4, we will give an example of a class that needs to order the items it stores. We will write most of the code using Comparable and show the adjustments needed to use the function objects. Elsewhere in the book, we will avoid the detail of function objects to keep the code as simple as possible, knowing that it is not difﬁcult to add function objects later.

C H A P T E R 2 Algorithm Analysis An algorithm is a clearly speciﬁed set of simple instructions to be followed to solve a problem. Once an algorithm is given for a problem and decided (somehow) to be correct, an important step is to determine how much in the way of resources, such as time or space, the algorithm will require. An algorithm that solves a problem but requires a year is hardly of any use. Likewise, an algorithm that requires hundreds of gigabytes of main memory is not (currently) useful on most machines. In this chapter, we shall discuss r How to estimate the time required for a program. r How to reduce the running time of a program from days or years to fractions of a second. r The results of careless use of recursion. r Very efﬁcient algorithms to raise a number to a power and to compute the greatest common divisor of two numbers. 2.1 Mathematical Background The analysis required to estimate the resource use of an algorithm is generally a theoretical issue, and therefore a formal framework is required. We begin with some mathematical deﬁnitions. Throughout the book we will use the following four deﬁnitions: Deﬁnition 2.1. T(N) = O(f(N)) if there are positive constants c and n0 such that T(N) ≤cf(N) when N ≥n0. Deﬁnition 2.2. T(N) = (g(N)) if there are positive constants c and n0 such that T(N) ≥cg(N) when N ≥n0. Deﬁnition 2.3. T(N) = (h(N)) if and only if T(N) = O(h(N)) and T(N) = (h(N)).

Chapter 2 Algorithm Analysis Deﬁnition 2.4. T(N) = o(p(N)) if for all positive constants c there exists an n0 such that T(N) < cp(N) when N > n0. Less formally, T(N) = o(p(N)) if T(N) = O(p(N)) and T(N) ̸= (p(N)). The idea of these deﬁnitions is to establish a relative order among functions. Given two functions, there are usually points where one function is smaller than the other function, so it does not make sense to claim, for instance, f(N) < g(N). Thus, we compare their relative rates of growth. When we apply this to the analysis of algorithms, we shall see why this is the important measure. Although 1,000N is larger than N2 for small values of N, N2 grows at a faster rate, and thus N2 will eventually be the larger function. The turning point is N = 1,000 in this case. The ﬁrst deﬁnition says that eventually there is some point n0 past which c · f(N) is always at least as large as T(N), so that if constant factors are ignored, f(N) is at least as big as T(N). In our case, we have T(N) = 1,000N, f(N) = N2, n0 = 1,000, and c = 1. We could also use n0 = 10 and c = 100. Thus, we can say that 1,000N = O(N2) (order N-squared). This notation is known as Big-Oh notation. Frequently, instead of saying “order . . . ,” one says “Big-Oh . . . .” If we use the traditional inequality operators to compare growth rates, then the ﬁrst deﬁnition says that the growth rate of T(N) is less than or equal to (≤) that of f(N). The second deﬁnition, T(N) = (g(N)) (pronounced “omega”), says that the growth rate of T(N) is greater than or equal to (≥) that of g(N). The third deﬁnition, T(N) = (h(N)) (pronounced “theta”), says that the growth rate of T(N) equals (=) the growth rate of h(N). The last deﬁnition, T(N) = o(p(N)) (pronounced “little-oh”), says that the growth rate of T(N) is less than (<) the growth rate of p(N). This is different from Big-Oh, because Big-Oh allows the possibility that the growth rates are the same. To prove that some function T(N) = O(f(N)), we usually do not apply these deﬁnitions formally but instead use a repertoire of known results. In general, this means that a proof (or determination that the assumption is incorrect) is a very simple calculation and should not involve calculus, except in extraordinary circumstances (not likely to occur in an algorithm analysis). When we say that T(N) = O(f(N)), we are guaranteeing that the function T(N) grows at a rate no faster than f(N); thus f(N) is an upper bound on T(N). Since this implies that f(N) = (T(N)), we say that T(N) is a lower bound on f(N). As an example, N3 grows faster than N2, so we can say that N2 = O(N3) or N3 = (N2). f(N) = N2 and g(N) = 2N2 grow at the same rate, so both f(N) = O(g(N)) and f(N) = (g(N)) are true. When two functions grow at the same rate, then the decision of whether or not to signify this with () can depend on the particular context. Intuitively, if g(N) = 2N2, then g(N) = O(N4), g(N) = O(N3), and g(N) = O(N2) are all technically correct, but the last option is the best answer. Writing g(N) = (N2) says not only that g(N) = O(N2), but also that the result is as good (tight) as possible.

2.1 Mathematical Background Function Name c Constant log N Logarithmic log2 N Log-squared N Linear N log N N2 Quadratic N3 Cubic 2N Exponential Figure 2.1 Typical growth rates The important things to know are Rule 1. If T1(N) = O(f(N)) and T2(N) = O(g(N)), then (a) T1(N) + T2(N) = O(f(N) + g(N)) (intuitively and less formally it is O(max(f(N), g(N))) ), (b) T1(N) ∗T2(N) = O(f(N) ∗g(N)). Rule 2. If T(N) is a polynomial of degree k, then T(N) = (Nk). Rule 3. logk N = O(N) for any constant k. This tells us that logarithms grow very slowly. This information is sufﬁcient to arrange most of the common functions by growth rate (see Figure 2.1). Several points are in order. First, it is very bad style to include constants or low-order terms inside a Big-Oh. Do not say T(N) = O(2N2) or T(N) = O(N2 +N). In both cases, the correct form is T(N) = O(N2). This means that in any analysis that will require a Big-Oh answer, all sorts of shortcuts are possible. Lower-order terms can generally be ignored, and constants can be thrown away. Considerably less precision is required in these cases. Second, we can always determine the relative growth rates of two functions f(N) and g(N) by computing limN→∞f(N)/g(N), using L’Hôpital’s rule if necessary.1 The limit can have four possible values: r The limit is 0: This means that f(N) = o(g(N)). r The limit is c ̸= 0: This means that f(N) = (g(N)). 1 L’Hôpital’s rule states that if limN→∞f(N) = ∞and limN→∞g(N) = ∞, then limN→∞f(N)/g(N) = limN→∞f′(N)/g′(N), where f′(N) and g′(N) are the derivatives of f(N) and g(N), respectively.

Chapter 2 Algorithm Analysis r The limit is ∞: This means that g(N) = o(f(N)). r The limit does not exist: There is no relation (this will not happen in our context). Using this method almost always amounts to overkill. Usually the relation between f(N) and g(N) can be derived by simple algebra. For instance, if f(N) = N log N and g(N) = N1.5, then to decide which of f(N) and g(N) grows faster, one really needs to determine which of log N and N0.5 grows faster. This is like determining which of log2 N or N grows faster. This is a simple problem, because it is already known that N grows faster than any power of a log. Thus, g(N) grows faster than f(N). One stylistic note: It is bad to say f(N) ≤O(g(N)), because the inequality is implied by the deﬁnition. It is wrong to write f(N) ≥O(g(N)), which does not make sense. As an example of the typical kinds of analysis that are performed, consider the problem of downloading a ﬁle over the Internet. Suppose there is an initial 3-sec delay (to set up a connection), after which the download proceeds at 1.5 M(bytes)/sec. Then it follows that if the ﬁle is N megabytes, the time to download is described by the formula T(N) = N/1.5 + 3. This is a linear function. Notice that the time to download a 1,500M ﬁle (1,003 sec) is approximately (but not exactly) twice the time to download a 750M ﬁle (503 sec). This is typical of a linear function. Notice, also, that if the speed of the connection doubles, both times decrease, but the 1,500M ﬁle still takes approximately twice the time to download as a 750M ﬁle. This is the typical characteristic of linear-time algorithms, and it is why we write T(N) = O(N), ignoring constant factors. (Although using Big-Theta would be more precise, Big-Oh answers are typically given.) Observe, too, that this behavior is not true of all algorithms. For the ﬁrst selection algorithm described in Section 1.1, the running time is controlled by the time it takes to perform a sort. For a simple sorting algorithm, such as the suggested bubble sort, when the amount of input doubles, the running time increases by a factor of four for large amounts of input. This is because those algorithms are not linear. Instead, as we will see when we discuss sorting, trivial sorting algorithms are O(N2), or quadratic. 2.2 Model In order to analyze algorithms in a formal framework, we need a model of computation. Our model is basically a normal computer, in which instructions are executed sequentially. Our model has the standard repertoire of simple instructions, such as addition, multiplication, comparison, and assignment, but, unlike the case with real computers, it takes exactly one time unit to do anything (simple). To be reasonable, we will assume that, like a modern computer, our model has ﬁxed-size (say, 32-bit) integers and that there are no fancy operations, such as matrix inversion or sorting, that clearly cannot be done in one time unit. We also assume inﬁnite memory. This model clearly has some weaknesses. Obviously, in real life, not all operations take exactly the same time. In particular, in our model one disk read counts the same as an addition, even though the addition is typically several orders of magnitude faster. Also, by assuming inﬁnite memory, we ignore the fact that the cost of a memory access can increase when slower memory is used due to larger memory requirements.

2.3 What to Analyze 2.3 What to Analyze The most important resource to analyze is generally the running time. Several factors affect the running time of a program. Some, such as the compiler and computer used, are obviously beyond the scope of any theoretical model, so, although they are important, we cannot deal with them here. The other main factors are the algorithm used and the input to the algorithm. Typically, the size of the input is the main consideration. We deﬁne two functions, Tavg(N) and Tworst(N), as the average and worst-case running time, respectively, used by an algorithm on input of size N. Clearly, Tavg(N) ≤Tworst(N). If there is more than one input, these functions may have more than one argument. Occasionally the best-case performance of an algorithm is analyzed. However, this is often of little interest, because it does not represent typical behavior. Average-case performance often reﬂects typical behavior, while worst-case performance represents a guarantee for performance on any possible input. Notice, also, that, although in this chapter we analyze Java code, these bounds are really bounds for the algorithms rather than programs. Programs are an implementation of the algorithm in a particular programming language, and almost always the details of the programming language do not affect a Big-Oh answer. If a program is running much more slowly than the algorithm analysis suggests, there may be an implementation inefﬁciency. This is more common in languages (like C++) where arrays can be inadvertently copied in their entirety, instead of passed with references. However, this can occur in Java, too. Thus in future chapters we will analyze the algorithms rather than the programs. Generally, the quantity required is the worst-case time, unless otherwise speciﬁed. One reason for this is that it provides a bound for all input, including particularly bad input, which an average-case analysis does not provide. The other reason is that average-case bounds are usually much more difﬁcult to compute. In some instances, the deﬁnition of “average” can affect the result. (For instance, what is average input for the following problem?) As an example, in the next section, we shall consider the following problem: Maximum Subsequence Sum Problem. Given (possibly negative) integers A1, A2, . . . , AN, ﬁnd the maximum value of j k=i Ak. (For convenience, the maximum subsequence sum is 0 if all the integers are negative.) Example: For input −2, 11, −4, 13, −5, −2, the answer is 20 (A2 through A4). This problem is interesting mainly because there are so many algorithms to solve it, and the performance of these algorithms varies drastically. We will discuss four algorithms to solve this problem. The running time on some computer (the exact computer is unimportant) for these algorithms is given in Figure 2.2. There are several important things worth noting in this table. For a small amount of input, the algorithms all run in a blink of the eye, so if only a small amount of input is expected, it might be silly to expend a great deal of effort to design a clever algorithm. On the other hand, there is a large market these days for rewriting programs that were written ﬁve years ago based on a no-longer-valid assumption of small input size. These

Chapter 2 Algorithm Analysis Algorithm Time Input Size O(N3) O(N2) O(N log N) O(N) N = 100 0.000159 0.000006 0.000005 0.000002 N = 1,000 0.095857 0.000371 0.000060 0.000022 N = 10,000 86.67 0.033322 0.000619 0.000222 N = 100,000 NA 3.33 0.006700 0.002205 N = 1,000,000 NA NA 0.074870 0.022711 Figure 2.2 Running times of several algorithms for maximum subsequence sum (in seconds) programs are now too slow, because they used poor algorithms. For large amounts of input, algorithm 4 is clearly the best choice (although algorithm 3 is still usable). Second, the times given do not include the time required to read the input. For algorithm 4, the time merely to read in the input from a disk is likely to be an order of magnitude larger than the time required to solve the problem. This is typical of many efﬁcient algorithms. Reading the data is generally the bottleneck; once the data are read, the problem can be solved quickly. For inefﬁcient algorithms this is not true, and signiﬁcant computer resources must be used. Thus it is important that, whenever possible, algorithms be efﬁcient enough not to be the bottleneck of a problem. Notice that algorithm 4, which is linear, exhibits the nice behavior that as the problem size increases by a factor of ten, the running time also increases by a factor of ten. Running Time Input Size (N) Linear O(N log N) Quadratic Cubic Figure 2.3 Plot (N vs. time) of various algorithms

2.4 Running Time Calculations Running Time 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Input Size (N) Linear O(N log N) Quadratic Cubic Figure 2.4 Plot (N vs. time) of various algorithms Algorithm 2, which is quadratic, does not have this behavior; a tenfold increase in input size yields roughly a hundredfold (102) increase in running time. And algorithm 1, which is cubic, yields a thousandfold (103) increase in running time. We would expect algorithm 1 to take nearly 9,000 seconds (or two and half hours) to complete for N = 100,000. Similarly, we would expect algorithm 2 to take roughly 333 seconds to complete for N = 1,000,000. However, it is possible that Algorithm 2 could take somewhat longer to complete due to the fact that N = 1,000,000 could also yield slower memory accesses than N = 100,000 on modern computers, depending on the size of the memory cache. Figure 2.3 shows the growth rates of the running times of the four algorithms. Even though this graph encompasses only values of N ranging from 10 to 100, the relative growth rates are still evident. Although the graph for the O(N log N) algorithm seems linear, it is easy to verify that it is not by using a straight-edge (or piece of paper). Although the graph for the O(N) algorithm seems constant, this is only because for small values of N, the constant term is larger than the linear term. Figure 2.4 shows the performance for larger values. It dramatically illustrates how useless inefﬁcient algorithms are for even moderately large amounts of input. 2.4 Running Time Calculations There are several ways to estimate the running time of a program. The previous table was obtained empirically. If two programs are expected to take similar times, probably the best way to decide which is faster is to code them both up and run them!

Chapter 2 Algorithm Analysis Generally, there are several algorithmic ideas, and we would like to eliminate the bad ones early, so an analysis is usually required. Furthermore, the ability to do an analysis usually provides insight into designing efﬁcient algorithms. The analysis also generally pinpoints the bottlenecks, which are worth coding carefully. To simplify the analysis, we will adopt the convention that there are no particular units of time. Thus, we throw away leading constants. We will also throw away low-order terms, so what we are essentially doing is computing a Big-Oh running time. Since Big-Oh is an upper bound, we must be careful never to underestimate the running time of the program. In effect, the answer provided is a guarantee that the program will terminate within a certain time period. The program may stop earlier than this, but never later. 2.4.1 A Simple Example Here is a simple program fragment to calculate N i=1 i3: public static int sum( int n ) { int partialSum; partialSum = 0; for( int i = 1; i <= n; i++ ) partialSum += i * i * i; return partialSum; } The analysis of this fragment is simple. The declarations count for no time. Lines 1 and 4 count for one unit each. Line 3 counts for four units per time executed (two multiplications, one addition, and one assignment) and is executed N times, for a total of 4N units. Line 2 has the hidden costs of initializing i, testing i ≤N, and incrementing i. The total cost of all these is 1 to initialize, N + 1 for all the tests, and N for all the increments, which is 2N + 2. We ignore the costs of calling the method and returning, for a total of 6N + 4. Thus, we say that this method is O(N). If we had to perform all this work every time we needed to analyze a program, the task would quickly become infeasible. Fortunately, since we are giving the answer in terms of Big-Oh, there are lots of shortcuts that can be taken without affecting the ﬁnal answer. For instance, line 3 is obviously an O(1) statement (per execution), so it is silly to count precisely whether it is two, three, or four units; it does not matter. Line 1 is obviously insigniﬁcant compared with the for loop, so it is silly to waste time here. This leads to several general rules. 2.4.2 General Rules Rule 1—for loops. The running time of a for loop is at most the running time of the statements inside the for loop (including tests) times the number of iterations.

2.4 Running Time Calculations Rule 2—Nested loops. Analyze these inside out. The total running time of a statement inside a group of nested loops is the running time of the statement multiplied by the product of the sizes of all the loops. As an example, the following program fragment is O(N2): for( i = 0; i < n; i++ ) for( j = 0; j < n; j++ ) k++; Rule 3—Consecutive Statements. These just add (which means that the maximum is the one that counts; see rule 1(a) on page 31). As an example, the following program fragment, which has O(N) work followed by O(N2) work, is also O(N2): for( i = 0; i < n; i++ ) a[ i ] = 0; for( i = 0; i < n; i++ ) for( j = 0; j < n; j++ ) a[ i ] += a[ j ] + i + j; Rule 4—if/else. For the fragment if( condition ) S1 else S2 the running time of an if/else statement is never more than the running time of the test plus the larger of the running times of S1 and S2. Clearly, this can be an overestimate in some cases, but it is never an underestimate. Other rules are obvious, but a basic strategy of analyzing from the inside (or deepest part) out works. If there are method calls, these must be analyzed ﬁrst. If there are recursive methods, there are several options. If the recursion is really just a thinly veiled for loop, the analysis is usually trivial. For instance, the following method is really just a simple loop and is O(N): public static long factorial( int n ) { if( n <= 1 ) return 1; else return n * factorial( n - 1 ); }

Chapter 2 Algorithm Analysis This example is really a poor use of recursion. When recursion is properly used, it is difﬁcult to convert the recursion into a simple loop structure. In this case, the analysis will involve a recurrence relation that needs to be solved. To see what might happen, consider the following program, which turns out to be a horrible use of recursion: public static long fib( int n ) { if( n <= 1 ) return 1; else return fib( n - 1 ) + fib( n - 2 ); } At ﬁrst glance, this seems like a very clever use of recursion. However, if the program is coded up and run for values of N around 40, it becomes apparent that this program is terribly inefﬁcient. The analysis is fairly simple. Let T(N) be the running time for the method call fib(n). If N = 0 or N = 1, then the running time is some constant value, which is the time to do the test at line 1 and return. We can say that T(0) = T(1) = 1 because constants do not matter. The running time for other values of N is then measured relative to the running time of the base case. For N > 2, the time to execute the method is the constant work at line 1 plus the work at line 3. Line 3 consists of an addition and two method calls. Since the method calls are not simple operations, they must be analyzed by themselves. The ﬁrst method call is fib(n - 1) and hence, by the deﬁnition of T, requires T(N −1) units of time. A similar argument shows that the second method call requires T(N −2) units of time. The total time required is then T(N −1) + T(N −2) + 2, where the 2 accounts for the work at line 1 plus the addition at line 3. Thus, for N ≥2, we have the following formula for the running time of fib(n): T(N) = T(N −1) + T(N −2) + 2 Since ﬁb(N) = ﬁb(N −1) + ﬁb(N −2), it is easy to show by induction that T(N) ≥ﬁb(N). In Section 1.2.5, we showed that ﬁb(N) < (5/3)N. A similar calculation shows that (for N > 4) ﬁb(N) ≥(3/2)N, and so the running time of this program grows exponentially. This is about as bad as possible. By keeping a simple array and using a for loop, the running time can be reduced substantially. This program is slow because there is a huge amount of redundant work being performed, violating the fourth major rule of recursion (the compound interest rule), which was presented in Section 1.3. Notice that the ﬁrst call on line 3, fib(n - 1), actually computes fib(n - 2) at some point. This information is thrown away and recomputed by the second call on line 3. The amount of information thrown away compounds recursively and results in the huge running time. This is perhaps the ﬁnest example of the maxim “Don’t compute anything more than once” and should not scare you away from using recursion. Throughout this book, we shall see outstanding uses of recursion.

2.4 Running Time Calculations 2.4.3 Solutions for the Maximum Subsequence Sum Problem We will now present four algorithms to solve the maximum subsequence sum problem posed earlier. The ﬁrst algorithm, which merely exhaustively tries all possibilities, is depicted in Figure 2.5. The indices in the for loop reﬂect the fact that in Java, arrays begin at 0, instead of 1. Also, the algorithm does not compute the actual subsequences; additional code is required to do this. Convince yourself that this algorithm works (this should not take much convincing). The running time is O(N3) and is entirely due to lines 13 and 14, which consist of an O(1) statement buried inside three nested for loops. The loop at line 8 is of size N. The second loop has size N −i which could be small but could also be of size N. We must assume the worst, with the knowledge that this could make the ﬁnal bound a bit high. The third loop has size j −i + 1, which, again, we must assume is of size N. The total is O(1 · N · N · N) = O(N3). Line 6 takes only O(1) total, and lines 16 and 17 take only O(N2) total, since they are easy expressions inside only two loops. It turns out that a more precise analysis, taking into account the actual size of these loops, shows that the answer is (N3) and that our estimate above was a factor of 6 too high (which is all right, because constants do not matter). This is generally true in these kinds of problems. The precise analysis is obtained from the sum N−1 i=0 N−1 j=i j k=i 1, /** * Cubic maximum contiguous subsequence sum algorithm. */ public static int maxSubSum1( int [ ] a ) { int maxSum = 0; for( int i = 0; i < a.length; i++ ) for( int j = i; j < a.length; j++ ) { int thisSum = 0; for( int k = i; k <= j; k++ ) thisSum += a[ k ]; if( thisSum > maxSum ) maxSum = thisSum; } return maxSum; } Figure 2.5 Algorithm 1

Chapter 2 Algorithm Analysis which tells how many times line 14 is executed. The sum can be evaluated inside out, using formulas from Section 1.2.3. In particular, we will use the formulas for the sum of the ﬁrst N integers and ﬁrst N squares. First we have j k=i 1 = j −i + 1 Next we evaluate N−1  j=i (j −i + 1) = (N −i + 1)(N −i) This sum is computed by observing that it is just the sum of the ﬁrst N −i integers. To complete the calculation, we evaluate N−1  i=0 (N −i + 1)(N −i) = N  i=1 (N −i + 1)(N −i + 2) = 1 N  i=1 i2 −  N + 3 	 N  i=1 i + 1 2(N2 + 3N + 2) N  i=1 = 1 N(N + 1)(2N + 1) −  N + 3 	 N(N + 1) + N2 + 3N + 2 N = N3 + 3N2 + 2N We can avoid the cubic running time by removing a for loop. This is not always possible, but in this case there are an awful lot of unnecessary computations present in the algorithm. The inefﬁciency that the improved algorithm corrects can be seen by noticing that j k=i Ak = Aj + j−1 k=i Ak, so the computation at lines 13 and 14 in algorithm 1 is unduly expensive. Figure 2.6 shows an improved algorithm. Algorithm 2 is clearly O(N2); the analysis is even simpler than before. There is a recursive and relatively complicated O(N log N) solution to this problem, which we now describe. If there didn’t happen to be an O(N) (linear) solution, this would be an excellent example of the power of recursion. The algorithm uses a “divide-andconquer” strategy. The idea is to split the problem into two roughly equal subproblems, which are then solved recursively. This is the “divide” part. The “conquer” stage consists of patching together the two solutions of the subproblems, and possibly doing a small amount of additional work, to arrive at a solution for the whole problem. In our case, the maximum subsequence sum can be in one of three places. Either it occurs entirely in the left half of the input, or entirely in the right half, or it crosses the middle and is in both halves. The ﬁrst two cases can be solved recursively. The last case can be obtained by ﬁnding the largest sum in the ﬁrst half that includes the last element

2.4 Running Time Calculations /** * Quadratic maximum contiguous subsequence sum algorithm. */ public static int maxSubSum2( int [ ] a ) { int maxSum = 0; for( int i = 0; i < a.length; i++ ) { int thisSum = 0; for( int j = i; j < a.length; j++ ) { thisSum += a[ j ]; if( thisSum > maxSum ) maxSum = thisSum; } } return maxSum; } Figure 2.6 Algorithm 2 in the ﬁrst half, and the largest sum in the second half that includes the ﬁrst element in the second half. These two sums can then be added together. As an example, consider the following input: First Half Second Half −3 −2 −1 −2 The maximum subsequence sum for the ﬁrst half is 6 (elements A1 through A3) and for the second half is 8 (elements A6 through A7). The maximum sum in the ﬁrst half that includes the last element in the ﬁrst half is 4 (elements A1 through A4), and the maximum sum in the second half that includes the ﬁrst element in the second half is 7 (elements A5 through A7). Thus, the maximum sum that spans both halves and goes through the middle is 4 + 7 = 11 (elements A1 through A7). We see, then, that among the three ways to form a large maximum subsequence, for our example, the best way is to include elements from both halves. Thus, the answer is 11. Figure 2.7 shows an implementation of this strategy. The code for algorithm 3 deserves some comment. The general form of the call for the recursive method is to pass the input array along with the left and right borders, which

Chapter 2 Algorithm Analysis /** * Recursive maximum contiguous subsequence sum algorithm. * Finds maximum sum in subarray spanning a[left..right]. * Does not attempt to maintain actual best sequence. */ private static int maxSumRec( int [ ] a, int left, int right ) { if( left == right ) // Base case if( a[ left ] > 0 ) return a[ left ]; else return 0; int center = ( left + right ) / 2; int maxLeftSum = maxSumRec( a, left, center ); int maxRightSum = maxSumRec( a, center + 1, right ); int maxLeftBorderSum = 0, leftBorderSum = 0; for( int i = center; i >= left; i-- ) { leftBorderSum += a[ i ]; if( leftBorderSum > maxLeftBorderSum ) maxLeftBorderSum = leftBorderSum; } int maxRightBorderSum = 0, rightBorderSum = 0; for( int i = center + 1; i <= right; i++ ) { rightBorderSum += a[ i ]; if( rightBorderSum > maxRightBorderSum ) maxRightBorderSum = rightBorderSum; } return max3( maxLeftSum, maxRightSum, maxLeftBorderSum + maxRightBorderSum ); } /** * Driver for divide-and-conquer maximum contiguous * subsequence sum algorithm. */ public static int maxSubSum3( int [ ] a ) { return maxSumRec( a, 0, a.length - 1 ); } Figure 2.7 Algorithm 3

2.4 Running Time Calculations delimit the portion of the array that is operated upon. A one-line driver program sets this up by passing the borders 0 and N −1 along with the array. Lines 8 to 12 handle the base case. If left == right, there is one element, and it is the maximum subsequence if the element is nonnegative. The case left > right is not possible unless N is negative (although minor perturbations in the code could mess this up). Lines 15 and 16 perform the two recursive calls. We can see that the recursive calls are always on a smaller problem than the original, although minor perturbations in the code could destroy this property. Lines 18 to 24 and 26 to 32 calculate the two maximum sums that touch the center divider. The sum of these two values is the maximum sum that spans both halves. The routine max3 (not shown) returns the largest of the three possibilities. Algorithm 3 clearly requires more effort to code than either of the two previous algorithms. However, shorter code does not always mean better code. As we have seen in the earlier table showing the running times of the algorithms, this algorithm is considerably faster than the other two for all but the smallest of input sizes. The running time is analyzed in much the same way as for the program that computes the Fibonacci numbers. Let T(N) be the time it takes to solve a maximum subsequence sum problem of size N. If N = 1, then the program takes some constant amount of time to execute lines 8 to 12, which we shall call one unit. Thus, T(1) = 1. Otherwise, the program must perform two recursive calls, the two for loops between lines 19 and 32, and some small amount of bookkeeping, such as lines 14 and 18. The two for loops combine to touch every element in the subarray, and there is constant work inside the loops, so the time expended in lines 19 to 32 is O(N). The code in lines 8 to 14, 18, 26, and 34 is all a constant amount of work and can thus be ignored compared with O(N). The remainder of the work is performed in lines 15 and 16. These lines solve two subsequence problems of size N/2 (assuming N is even). Thus, these lines take T(N/2) units of time each, for a total of 2T(N/2). The total time for the algorithm then is 2T(N/2) + O(N). This gives the equations T(1) = 1 T(N) = 2T(N/2) + O(N) To simplify the calculations, we can replace the O(N) term in the equation above with N; since T(N) will be expressed in Big-Oh notation anyway, this will not affect the answer. In Chapter 7, we shall see how to solve this equation rigorously. For now, if T(N) = 2T(N/2)+N, and T(1) = 1, then T(2) = 4 = 2∗2, T(4) = 12 = 4∗3, T(8) = 32 = 8∗4, and T(16) = 80 = 16∗5. The pattern that is evident, and can be derived, is that if N = 2k, then T(N) = N ∗(k + 1) = N log N + N = O(N log N). This analysis assumes N is even, since otherwise N/2 is not deﬁned. By the recursive nature of the analysis, it is really valid only when N is a power of 2, since otherwise we eventually get a subproblem that is not an even size, and the equation is invalid. When N is not a power of 2, a somewhat more complicated analysis is required, but the Big-Oh result remains unchanged. In future chapters, we will see several clever applications of recursion. Here, we present a fourth algorithm to ﬁnd the maximum subsequence sum. This algorithm is simpler to implement than the recursive algorithm and also is more efﬁcient. It is shown in Figure 2.8.

Chapter 2 Algorithm Analysis /** * Linear-time maximum contiguous subsequence sum algorithm. */ public static int maxSubSum4( int [ ] a ) { int maxSum = 0, thisSum = 0; for( int j = 0; j < a.length; j++ ) { thisSum += a[ j ]; if( thisSum > maxSum ) maxSum = thisSum; else if( thisSum < 0 ) thisSum = 0; } return maxSum; } Figure 2.8 Algorithm 4 It should be clear why the time bound is correct, but it takes a little thought to see why the algorithm actually works. To sketch the logic, note that, like algorithms 1 and 2, j is representing the end of the current sequence, while i is representing the start of the current sequence. It happens that the use of i can be optimized out of the program if we do not need to know where the actual best subsequence is, so in designing the algorithm, let’s pretend that i is needed, and that we are trying to improve algorithm 2. One observation is that if a[i] is negative, then it cannot possibly represent the start of the optimal sequence, since any subsequence that begins by including a[i] would be improved by beginning with a[i+1]. Similarly, any negative subsequence cannot possibly be a preﬁx of the optimal subsequence (same logic). If, in the inner loop, we detect that the subsequence from a[i] to a[j] is negative, then we can advance i. The crucial observation is that not only can we advance i to i+1, but we can also actually advance it all the way to j+1. To see this, let p be any index between i+1 and j. Any subsequence that starts at index p is not larger than the corresponding subsequence that starts at index i and includes the subsequence from a[i] to a[p-1], since the latter subsequence is not negative (j is the ﬁrst index that causes the subsequence starting at index i to become negative). Thus advancing i to j+1 is risk free: we cannot miss an optimal solution. This algorithm is typical of many clever algorithms: The running time is obvious, but the correctness is not. For these algorithms, formal correctness proofs (more formal than the sketch above) are almost always required; even then, however, many people still are not convinced. In addition, many of these algorithms require trickier programming, leading to longer development. But when these algorithms work, they run quickly, and we can

2.4 Running Time Calculations test much of the code logic by comparing it with an inefﬁcient (but easily implemented) brute-force algorithm using small input sizes. An extra advantage of this algorithm is that it makes only one pass through the data, and once a[i] is read and processed, it does not need to be remembered. Thus, if the array is on a disk or is being transmitted over the Internet, it can be read sequentially, and there is no need to store any part of it in main memory. Furthermore, at any point in time, the algorithm can correctly give an answer to the subsequence problem for the data it has already read (the other algorithms do not share this property). Algorithms that can do this are called online algorithms. An online algorithm that requires only constant space and runs in linear time is just about as good as possible. 2.4.4 Logarithms in the Running Time The most confusing aspect of analyzing algorithms probably centers around the logarithm. We have already seen that some divide-and-conquer algorithms will run in O(N log N) time. Besides divide-and-conquer algorithms, the most frequent appearance of logarithms centers around the following general rule: An algorithm is O(log N) if it takes constant (O(1)) time to cut the problem size by a fraction (which is usually 1 2). On the other hand, if constant time is required to merely reduce the problem by a constant amount (such as to make the problem smaller by 1), then the algorithm is O(N). It should be obvious that only special kinds of problems can be O(log N). For instance, if the input is a list of N numbers, an algorithm must take (N) merely to read the input in. Thus, when we talk about O(log N) algorithms for these kinds of problems, we usually presume that the input is preread. We provide three examples of logarithmic behavior. Binary Search The ﬁrst example is usually referred to as binary search. Binary Search. Given an integer X and integers A0, A1, . . . , AN−1, which are presorted and already in memory, ﬁnd i such that Ai = X, or return i = −1 if X is not in the input. The obvious solution consists of scanning through the list from left to right and runs in linear time. However, this algorithm does not take advantage of the fact that the list is sorted and is thus not likely to be best. A better strategy is to check if X is the middle element. If so, the answer is at hand. If X is smaller than the middle element, we can apply the same strategy to the sorted subarray to the left of the middle element; likewise, if X is larger than the middle element, we look to the right half. (There is also the case of when to stop.) Figure 2.9 shows the code for binary search (the answer is mid). As usual, the code reﬂects Java’s convention that arrays begin with index 0. Clearly, all the work done inside the loop takes O(1) per iteration, so the analysis requires determining the number of times around the loop. The loop starts with high - low = N −1 and ﬁnishes with high - low ≥−1. Every time through the loop the value high - low must be at least halved from its previous value; thus, the number of times around the loop is at most ⌈log(N −1)⌉+ 2. (As an example, if high - low = 128, then

Chapter 2 Algorithm Analysis /** * Performs the standard binary search. * @return index where item is found, or -1 if not found. */ public static <AnyType extends Comparable<? super AnyType>> int binarySearch( AnyType [ ] a, AnyType x ) { int low = 0, high = a.length - 1; while( low <= high ) { int mid = ( low + high ) / 2; if( a[ mid ].compareTo( x ) < 0 ) low = mid + 1; else if( a[ mid ].compareTo( x ) > 0 ) high = mid - 1; else return mid; // Found } return NOT_FOUND; // NOT_FOUND is defined as -1 } Figure 2.9 Binary search the maximum values of high - low after each iteration are 64, 32, 16, 8, 4, 2, 1, 0, −1.) Thus, the running time is O(log N). Equivalently, we could write a recursive formula for the running time, but this kind of brute-force approach is usually unnecessary when you understand what is really going on and why. Binary search can be viewed as our ﬁrst data structure implementation. It supports the contains operation in O(log N) time, but all other operations (in particular insert) require O(N) time. In applications where the data are static (that is, insertions and deletions are not allowed), this could be very useful. The input would then need to be sorted once, but afterward accesses would be fast. An example is a program that needs to maintain information about the periodic table of elements (which arises in chemistry and physics). This table is relatively stable, as new elements are added infrequently. The element names could be kept sorted. Since there are only about 118 elements, at most eight accesses would be required to ﬁnd an element. Performing a sequential search would require many more accesses. Euclid’s Algorithm A second example is Euclid’s algorithm for computing the greatest common divisor. The greatest common divisor (gcd) of two integers is the largest integer that divides both. Thus, gcd(50, 15) = 5. The algorithm in Figure 2.10 computes gcd(M, N), assuming M ≥N. (If N > M, the ﬁrst iteration of the loop swaps them.)

2.4 Running Time Calculations public static long gcd( long m, long n ) { while( n != 0 ) { long rem = m % n; m = n; n = rem; } return m; } Figure 2.10 Euclid’s algorithm The algorithm works by continually computing remainders until 0 is reached. The last nonzero remainder is the answer. Thus, if M = 1,989 and N = 1,590, then the sequence of remainders is 399, 393, 6, 3, 0. Therefore, gcd(1989, 1590) = 3. As the example shows, this is a fast algorithm. As before, estimating the entire running time of the algorithm depends on determining how long the sequence of remainders is. Although log N seems like a good answer, it is not at all obvious that the value of the remainder has to decrease by a constant factor, since we see that the remainder went from 399 to only 393 in the example. Indeed, the remainder does not decrease by a constant factor in one iteration. However, we can prove that after two iterations, the remainder is at most half of its original value. This would show that the number of iterations is at most 2 log N = O(log N) and establish the running time. This proof is easy, so we include it here. It follows directly from the following theorem. Theorem 2.1. If M > N, then M mod N < M/2. Proof. There are two cases. If N ≤M/2, then since the remainder is smaller than N, the theorem is true for this case. The other case is N > M/2. But then N goes into M once with a remainder M −N < M/2, proving the theorem. One might wonder if this is the best bound possible, since 2 log N is about 20 for our example, and only seven operations were performed. It turns out that the constant can be improved slightly, to roughly 1.44 log N, in the worst case (which is achievable if M and N are consecutive Fibonacci numbers). The average-case performance of Euclid’s algorithm requires pages and pages of highly sophisticated mathematical analysis, and it turns out that the average number of iterations is about (12 ln 2 ln N)/π2 + 1.47. Exponentiation Our last example in this section deals with raising an integer to a power (which is also an integer). Numbers that result from exponentiation are generally quite large, so an analysis works only if we can assume that we have a machine that can store such large integers

Chapter 2 Algorithm Analysis public static long pow( long x, int n ) { if( n == 0 ) return 1; if( n == 1 ) return x; if( isEven( n ) ) return pow( x * x, n / 2 ); else return pow( x * x, n / 2 ) * x; } Figure 2.11 Efﬁcient exponentiation (or a compiler that can simulate this). We will count the number of multiplications as the measurement of running time. The obvious algorithm to compute XN uses N−1 multiplications. A recursive algorithm can do better. N ≤1 is the base case of the recursion. Otherwise, if N is even, we have XN = XN/2 · XN/2, and if N is odd, XN = X(N−1)/2 · X(N−1)/2 · X. For instance, to compute X62, the algorithm does the following calculations, which involve only nine multiplications: X3 = (X2)X, X7 = (X3) 2X, X15 = (X7) 2X, X31 = (X15) 2X, X62 = (X31) The number of multiplications required is clearly at most 2 log N, because at most two multiplications (if N is odd) are required to halve the problem. Again, a recurrence formula can be written and solved. Simple intuition obviates the need for a brute-force approach. Figure 2.11 implements this idea.2 It is sometimes interesting to see how much the code can be tweaked without affecting correctness. In Figure 2.11, lines 5 to 6 are actually unnecessary, because if N is 1, then line 10 does the right thing. Line 10 can also be rewritten as return pow( x, n - 1 ) * x; without affecting the correctness of the program. Indeed, the program will still run in O(log N), because the sequence of multiplications is the same as before. However, all of the following alternatives for line 8 are bad, even though they look correct: 8a return pow( pow( x, 2 ), n / 2 ); 8b return pow( pow( x, n / 2 ), 2 ); 8c return pow( x, n / 2 ) * pow( x, n / 2 ); 2 Java provides a BigInteger class that can be used to manipulate arbitrarily large integers. Translating Figure 2.11 to use BigInteger instead of long is straightforward.

C H A P T E R 3 Lists, Stacks, and Queues This chapter discusses three of the most simple and basic data structures. Virtually every signiﬁcant program will use at least one of these structures explicitly, and a stack is always implicitly used in a program, whether or not you declare one. Among the highlights of this chapter, we will r Introduce the concept of Abstract Data Types (ADTs). r Show how to efﬁciently perform operations on lists. r Introduce the stack ADT and its use in implementing recursion. r Introduce the queue ADT and its use in operating systems and algorithm design. In this chapter, we provide code that implements a signiﬁcant subset of two library classes: ArrayList and LinkedList. 3.1 Abstract Data Types (ADTs) An abstract data type (ADT) is a set of objects together with a set of operations. Abstract data types are mathematical abstractions; nowhere in an ADT’s deﬁnition is there any mention of how the set of operations is implemented. Objects such as lists, sets, and graphs, along with their operations, can be viewed as abstract data types, just as integers, reals, and booleans are data types. Integers, reals, and booleans have operations associated with them, and so do abstract data types. For the set ADT, we might have such operations as add, remove, and contains. Alternatively, we might only want the two operations union and ﬁnd, which would deﬁne a different ADT on the set. The Java class allows for the implementation of ADTs, with appropriate hiding of implementation details. Thus any other part of the program that needs to perform an operation on the ADT can do so by calling the appropriate method. If for some reason implementation details need to be changed, it should be easy to do so by merely changing the routines that perform the ADT operations. This change, in a perfect world, would be completely transparent to the rest of the program. There is no rule telling us which operations must be supported for each ADT; this is a design decision. Error handling and tie breaking (where appropriate) are also generally up to the program designer. The three data structures that we will study in this chapter are

Chapter 3 Lists, Stacks, and Queues primary examples of ADTs. We will see how each can be implemented in several ways, but if they are done correctly, the programs that use them will not necessarily need to know which implementation was used. 3.2 The List ADT We will deal with a general list of the form A0, A1, A2, . . ., AN−1. We say that the size of this list is N. We will call the special list of size 0 an empty list. For any list except the empty list, we say that Ai follows (or succeeds) Ai−1 (i < N) and that Ai−1 precedes Ai (i > 0). The ﬁrst element of the list is A0, and the last element is AN−1. We will not deﬁne the predecessor of A0 or the successor of AN−1. The position of element Ai in a list is i. Throughout this discussion, we will assume, to simplify matters, that the elements in the list are integers, but in general, arbitrarily complex elements are allowed (and easily handled by a generic Java class). Associated with these “deﬁnitions” is a set of operations that we would like to perform on the list ADT. Some popular operations are printList and makeEmpty, which do the obvious things; find, which returns the position of the ﬁrst occurrence of an item; insert and remove, which generally insert and remove some element from some position in the list; and findKth, which returns the element in some position (speciﬁed as an argument). If the list is 34, 12, 52, 16, 12, then find(52) might return 2; insert(x,2) might make the list into 34, 12, x, 52, 16, 12 (if we insert into the position given); and remove(52) might turn that list into 34, 12, x, 16, 12. Of course, the interpretation of what is appropriate for a method is entirely up to the programmer, as is the handling of special cases (for example, what does find(1) return above?). We could also add operations such as next and previous, which would take a position as argument and return the position of the successor and predecessor, respectively. 3.2.1 Simple Array Implementation of Lists All these instructions can be implemented just by using an array. Although arrays are created with a ﬁxed capacity, we can create a different array with double the capacity when needed. This solves the most serious problem with using an array, namely that historically, to use an array, an estimate of the maximum size of the list was required. This estimate is not needed in Java, or any modern programming language. The following code fragment illustrates how an array, arr, which initially has length 10, can be expanded as needed: int [ ] arr = new int[ 10 ]; ... // Later on we decide arr needs to be larger. int [ ] newArr = new int[ arr.length * 2 ]; for( int i = 0; i < arr.length; i++ ) newArr[ i ] = arr[ i ]; arr = newArr;

3.2 The List ADT An array implementation allows printList to be carried out in linear time, and the findKth operation takes constant time, which is as good as can be expected. However, insertion and deletion are potentially expensive, depending on where the insertions and deletions occur. In the worst case, inserting into position 0 (in other words, at the front of the list) requires pushing the entire array down one spot to make room, and deleting the ﬁrst element requires shifting all the elements in the list up one spot, so the worst case for these operations is O(N). On average, half of the list needs to be moved for either operation, so linear time is still required. On the other hand, if all the operations occur at the high end of the list, then no elements need to be shifted, and then adding and deleting take O(1) time. There are many situations where the list is built up by insertions at the high end, and then only array accesses (i.e., findKth operations) occur. In such a case, the array is a suitable implementation. However, if insertions and deletions occur throughout the list, and in particular, at the front of the list, then the array is not a good option. The next subsection deals with the alternative: the linked list. 3.2.2 Simple Linked Lists In order to avoid the linear cost of insertion and deletion, we need to ensure that the list is not stored contiguously, since otherwise entire parts of the list will need to be moved. Figure 3.1 shows the general idea of a linked list. The linked list consists of a series of nodes, which are not necessarily adjacent in memory. Each node contains the element and a link to a node containing its successor. We call this the next link. The last cell’s next link references null. To execute printList or find(x) we merely start at the ﬁrst node in the list and then traverse the list by following the next links. This operation is clearly linear-time, as in the array implementation, although the constant is likely to be larger than if an array implementation were used. The findKth operation is no longer quite as efﬁcient as an array implementation; findKth(i) takes O(i) time and works by traversing down the list in the obvious manner. In practice, this bound is pessimistic, because frequently the calls to findKth are in sorted order (by i). As an example, findKth(2), findKth(3), findKth(4), and findKth(6) can all be executed in one scan down the list. The remove method can be executed in one next reference change. Figure 3.2 shows the result of deleting the third element in the original list. The insert method requires obtaining a new node from the system by using a new call and then executing two reference maneuvers. The general idea is shown in Figure 3.3. The dashed line represents the old next reference. A0 A1 A2 A3 A4 Figure 3.1 A linked list

Chapter 3 Lists, Stacks, and Queues A0 A1 A2 A3 A4 Figure 3.2 Deletion from a linked list A0 A1 A2 A3 A4 X Figure 3.3 Insertion into a linked list As we can see, in principle, if we know where a change is to be made, inserting or removing an item from a linked list does not require moving lots of items and instead involves only a constant number of changes to node links. The special case of adding to the front or removing the ﬁrst item is thus a constanttime operation, presuming of course that a link to the front of the linked list is maintained. The special case of adding at the end (i.e., making the new item as the last item) can be constant-time, as long as we maintain a link to the last node. Thus, a typical linked list keeps links to both ends of the list. Removing the last item is trickier, because we have to ﬁnd the next-to-last item, change its next link to null, and then update the link that maintains the last node. In the classic linked list, where each node stores a link to its next node, having a link to the last node provides no information about the next-to-last node. The obvious idea of maintaining a third link to the next-to-last node doesn’t work, because it too would need to be updated during a remove. Instead, we have every node maintain a link to its previous node in the list. This is shown in Figure 3.4 and is known as a doubly linked list. first last a b c d Figure 3.4 A doubly linked list

3.3 Lists in the Java Collections API 3.3 Lists in the Java Collections API The Java language includes, in its library, an implementation of common data structures. This part of the language is popularly known as the Collections API. The List ADT is one of the data structures implemented in the Collections API. We will see some others in Chapters 4 and 5. 3.3.1 Collection Interface The Collections API resides in package java.util. The notion of a collection, which stores a collection of identically typed objects, is abstracted in the Collection interface. Figure 3.5 shows the most important parts of this interface (some methods are not shown). Many of the methods in the Collection interface do the obvious things that their names suggest. So size returns the number of items in the collection; isEmpty returns true if and only if the size of the collection is zero. contains returns true if x is in the collection. Note that the interface doesn’t specify how the collection decides if x is in the collection—this is determined by the actual classes that implement the Collection interface. add and remove add and remove item x from the collection, returning true if the operation succeeds and false if it fails for a plausible (nonexceptional) reason. For instance, a remove can fail if the item is not present in the collection, and if the particular collection does not allow duplicates, then add can fail when an attempt is made to insert a duplicate. The Collection interface extends the Iterable interface. Classes that implement the Iterable interface can have the enhanced for loop used on them to view all their items. For instance, the routine in Figure 3.6 can be used to print all the items in any collection. The implementation of this version of print is identical, character-for-character, with a corresponding implementation that could be used if coll had type AnyType[]. 3.3.2 Iterator s Collections that implement the Iterable interface must provide a method named iterator that returns an object of type Iterator. The Iterator is an interface deﬁned in package java.util and is shown in Figure 3.7. public interface Collection<AnyType> extends Iterable<AnyType> { int size( ); boolean isEmpty( ); void clear( ); boolean contains( AnyType x ); boolean add( AnyType x ); boolean remove( AnyType x ); java.util.Iterator<AnyType> iterator( ); } Figure 3.5 Subset of the Collection interface in package java.util

Chapter 3 Lists, Stacks, and Queues public static <AnyType> void print( Collection<AnyType> coll ) { for( AnyType item : coll ) System.out.println( item ); } Figure 3.6 Using the enhanced for loop on an Iterable type public interface Iterator<AnyType> { boolean hasNext( ); AnyType next( ); void remove( ); } Figure 3.7 The Iterator interface in package java.util The idea of the Iterator is that via the iterator method, each collection can create, and return to the client, an object that implements the Iterator interface and stores internally its notion of a current position. Each call to next gives the next item in the collection (that has not yet been seen). Thus the ﬁrst call to next gives the ﬁrst item, the second call gives the second item, and so forth. hasNext can be used to tell you if there is a next item. When the compiler sees an enhanced for loop being used on an object that is Iterable, it mechanically replaces the enhanced for loop with calls to the iterator method to obtain an Iterator and then calls to next and hasNext. Thus the previously seen print routine is rewritten by the compiler as shown in Figure 3.8. Because of the limited set of methods available in the Iterator interface, it is hard to use the Iterator for anything more than a simple traversal through the Collection. The Iterator interface also contains a method called remove. With this method you can remove the last item returned by next (after which you cannot call remove again until after another public static <AnyType> void print( Collection<AnyType> coll ) { Iterator<AnyType> itr = coll.iterator( ); while( itr.hasNext( ) ) { AnyType item = itr.next( ); System.out.println( item ); } } Figure 3.8 The enhanced for loop on an Iterable type rewritten by the compiler to use an iterator

3.3 Lists in the Java Collections API call to next). Although the Collection interface also contains a remove method, there are presumably advantages to using the Iterator’s remove method instead. The main advantage of the Iterator’s remove method is that the Collection’s remove method must ﬁrst ﬁnd the item to remove. Presumably it is much less expensive to remove an item if you know exactly where it is. An example that we will see in the next section removes every other item in the collection. This code is easy to write with an iterator, and potentially more efﬁcient than using the Collection’s remove method. When using the iterator directly (rather than indirectly via an enhanced for loop) it is important to keep in mind a fundamental rule: If you make a structural change to the collection being iterated (i.e., an add, remove, or clear method is applied on the collection), then the iterator is no longer valid (and a ConcurrentModificationException is thrown on subsequent attempts to use the iterator). This is necessary to avoid ugly situations in which the iterator is prepared to give a certain item as the next item, and then that item is either removed, or perhaps a new item is inserted just prior to the next item. This means that you shouldn’t obtain an iterator until immediately prior to the need to use it. However, if the iterator invokes its remove method, then the iterator is still valid. This is a second reason to prefer the iterator’s remove method sometimes. 3.3.3 The List Interface, ArrayList, and LinkedList The collection that concerns us the most in this section is the list, which is speciﬁed by the List interface in package java.util. The List interface extends Collection, so it contains all the methods in the Collection interface, plus a few others. Figure 3.9 illustrates the most important of these methods. get and set allow the client to access or change an item at the speciﬁed position in the list, given by its index, idx. Index 0 is the front of the list, index size()-1 represents the last item in the list, and index size() represents the position where a newly added item can be placed. add allows the placement of a new item in position idx (pushing subsequent items one position higher). Thus, an add at position 0 is adding at the front, whereas an add at position size() is adding an item as the new last item. In addition to the standard remove that takes AnyType as a parameter, remove is overloaded to remove an item at a speciﬁed position. Finally, the List interface speciﬁes the listIterator method that produces a more public interface List<AnyType> extends Collection<AnyType> { AnyType get( int idx ); AnyType set( int idx, AnyType newVal ); void add( int idx, AnyType x ); void remove( int idx ); ListIterator<AnyType> listIterator( int pos ); } Figure 3.9 Subset of the List interface in package java.util

Chapter 3 Lists, Stacks, and Queues complicated iterator than normally expected. The ListIterator interface is discussed in Section 3.3.5. There are two popular implementations of the List ADT. The ArrayList provides a growable array implementation of the List ADT. The advantage of using the ArrayList is that calls to get and set take constant time. The disadvantage is that insertion of new items and removal of existing items is expensive, unless the changes are made at the end of the ArrayList. The LinkedList provides a doubly linked list implementation of the List ADT. The advantage of using the LinkedList is that insertion of new items and removal of existing items is cheap, provided that the position of the changes is known. This means that adds and removes from the front of the list are constant-time operations, so much so that the LinkedList provides methods addFirst and removeFirst, addLast and removeLast, and getFirst and getLast to efﬁciently add, remove, and access the items at both ends of the list. The disadvantage is that the LinkedList is not easily indexable, so calls to get are expensive unless they are very close to one of the ends of the list (if the call to get is for an item near the back of the list, the search can proceed from the back of the list). To see the differences, we look at some methods that operate on a List. First, suppose we construct a List by adding items at the end. public static void makeList1( List<Integer> lst, int N ) { lst.clear( ); for( int i = 0; i < N; i++ ) lst.add( i ); } Regardless of whether an ArrayList or LinkedList is passed as a parameter, the running time of makeList1 is O(N) because each call to add, being at the end of the list, takes constant time (the occasional expansion of the ArrayList is safe to ignore). On the other hand, if we construct a List by adding items at the front, public static void makeList2( List<Integer> lst, int N ) { lst.clear( ); for( int i = 0; i < N; i++ ) lst.add( 0, i ); } the running time is O(N) for a LinkedList, but O(N2) for an ArrayList, because in an ArrayList, adding at the front is an O(N) operation. The next routine attempts to compute the sum of the numbers in a List: public static int sum( List<Integer> lst ) { int total = 0; for( int i = 0; i < N; i++ ) total += lst.get( i ); return total; }

3.3 Lists in the Java Collections API Here, the running time is O(N) for an ArrayList, but O(N2) for a LinkedList, because in a LinkedList, calls to get are O(N) operations. Instead, use an enhanced for loop, which will make the running time O(N) for any List, because the iterator will efﬁciently advance from one item to the next. Both ArrayList and LinkedList are inefﬁcient for searches, so calls to the Collection contains and remove methods (that take an AnyType as parameter) take linear time. In an ArrayList, there is a notion of a capacity, which represents the size of the underlying array. The ArrayList automatically increases the capacity as needed to ensure that it is at least as large as the size of the list. If an early estimate of the size is available, ensureCapacity can set the capacity to a sufﬁciently large amount to avoid a later expansion of the array capacity. Also, trimToSize can be used after all ArrayList adds are completed to avoid wasted space. 3.3.4 Example: Using remove on a LinkedList As an example, we provide a routine that removes all even-valued items in a list. Thus, if the list contains 6, 5, 1, 4, 2, then after the method is invoked it will contain 5, 1. There are several possible ideas for an algorithm that deletes items from the list as they are encountered. Of course, one idea is to construct a new list containing all the odd numbers, and then clear the original list and copy the odd numbers back into it. But we are more interested in writing a clean version that avoids making a copy and instead removes items from the list as they are encountered. This is almost certainly a losing strategy for an ArrayList, since removing from almost anywhere in an ArrayList is expensive. In a LinkedList, there is some hope, as we know that removing from a known position can be done efﬁciently by rearranging some links. Figure 3.10 shows the ﬁrst attempt. On an ArrayList, as expected, the remove is not efﬁcient, so the routine takes quadratic time. A LinkedList exposes two problems. First, the call to get is not efﬁcient, so the routine takes quadratic time. Additionally, the call to remove is equally inefﬁcient, because it is expensive to get to position i. Figure 3.11 shows one attempt to rectify the problem. Instead of using get, we use an iterator to step through the list. This is efﬁcient. But then we use the Collection’s remove public static void removeEvensVer1( List<Integer> lst ) { int i = 0; while( i < lst.size( ) ) if( lst.get( i ) % 2 == 0 ) lst.remove( i ); else i++; } Figure 3.10 Removes the even numbers in a list; quadratic on all types of lists

Chapter 3 Lists, Stacks, and Queues public static void removeEvensVer2( List<Integer> lst ) { for( Integer x : lst ) if( x % 2 == 0 ) lst.remove( x ); } Figure 3.11 Removes the even numbers in a list; doesn’t work because of ConcurrentModificationException public static void removeEvensVer3( List<Integer> lst ) { Iterator<Integer> itr = lst.iterator( ); while( itr.hasNext( ) ) if( itr.next( ) % 2 == 0 ) itr.remove( ); } Figure 3.12 Removes the even numbers in a list; quadratic on ArrayList, but linear time for LinkedList method to remove an even-valued item. This is not an efﬁcient operation because the remove method has to search for the item again, which takes linear time. But if we run the code, we ﬁnd out that the situation is even worse: The program generates an exception because when an item is removed, the underlying iterator used by the enhanced for loop is invalidated. (The code in Figure 3.10 explains why: we cannot expect the enhanced for loop to understand that it must advance only if an item is not removed.) Figure 3.12 shows an idea that works: After the iterator ﬁnds an even-valued item, we can use the iterator to remove the value it has just seen. For a LinkedList, the call to the iterator’s remove method is only constant time, because the iterator is at (or near) the node that needs to be removed. Thus, for a LinkedList, the entire routine takes linear time, rather than quadratic time. For an ArrayList, even though the iterator is at the point that needs to be removed, the remove is still expensive, because array items must be shifted, so as expected, the entire routine still takes quadratic time for an ArrayList. If we run the code in Figure 3.12, passing a LinkedList<Integer>, it takes 0.039 seconds for an 800,000-item list, and 0.073 seconds for a 1,600,000 item LinkedList, and is clearly a linear-time routine, because the running time increases by the same factor as the input size. When we pass an ArrayList<Integer>, the routine takes almost ﬁve minutes for an 800,000-item ArrayList, and about twenty minutes for a 1,600,000-item ArrayList; the fourfold increase in running time when the input increases by only a factor of two is consistent with quadratic behavior.

3.4 Implementation of ArrayList public interface ListIterator<AnyType> extends Iterator<AnyType> { boolean hasPrevious( ); AnyType previous( ); void add( AnyType x ); void set( AnyType newVal ); } Figure 3.13 Subset of the ListIterator interface in package java.util (a) (b) (c) Figure 3.14 (a) Normal starting point: next returns 5, previous is illegal, add places item before 5; (b) next returns 8, previous returns 5, add places item between 5 and 8; (c) next is illegal, previous returns 9, add places item after 9 3.3.5 ListIterators Figure 3.13 shows that a ListIterator extends the functionality of an Iterator for Lists. previous and hasPrevious allow traversal of the list from the back to the front. add places a new item into the list in the current position. The notion of the current position is abstracted by viewing the iterator as being between the item that would be given by a call to next and the item that would be given by a call to previous, an abstraction that is illustrated in Figure 3.14. add is a constant-time operation for a LinkedList but is expensive for an ArrayList. set changes the last value seen by the iterator and is convenient for LinkedLists. As an example, it can be used to subtract 1 from all the even numbers in a List, which would be hard to do on a LinkedList without using the ListIterator’s set method. 3.4 Implementation of ArrayList In this section, we provide the implementation of a usable ArrayList generic class. To avoid ambiguities with the library class, we will name our class MyArrayList. We do not provide a MyCollection or MyList interface; rather, MyArrayList is standalone. Before examining the (nearly one hundred lines of) MyArrayList code, we outline the main details. 1. The MyArrayList will maintain the underlying array, the array capacity, and the current number of items stored in the MyArrayList.

Chapter 3 Lists, Stacks, and Queues 2. The MyArrayList will provide a mechanism to change the capacity of the underlying array. The capacity is changed by obtaining a new array, copying the old array into the new array, and allowing the Virtual Machine to reclaim the old array. 3. The MyArrayList will provide an implementation of get and set. 4. The MyArrayList will provide basic routines, such as size, isEmpty, and clear, which are typically one-liners; a version of remove; and also two versions of add. The add routines will increase capacity if the size and capacity are the same. 5. The MyArrayList will provide a class that implements the Iterator interface. This class will store the index of the next item in the iteration sequence and provide implementations of next, hasNext, and remove. The MyArrayList’s iterator method simply returns a newly constructed instance of the class that implements the Iterator interface. 3.4.1 The Basic Class Figure 3.15 and Figure 3.16 show the MyArrayList class. Like its Collections API counterpart, there is some error checking to ensure valid bounds; however, in order to concentrate on the basics of writing the iterator class, we do not check for a structural modiﬁcation that could invalidate an iterator, nor do we check for an illegal iterator remove. These checks are shown in the subsequent implementation of MyLinkedList in Section 3.5 and are exactly the same for both list implementations. As shown on lines 5–6, the MyArrayList stores the size and array as its data members. A host of short routines, namely clear, doClear (used to avoid having the constructor invoke an overridable method), size, trimToSize, isEmpty, get, and set, are implemented in lines 11 to 38. The ensureCapacity routine is shown at lines 40 to 49. Expanding capacity is done with the same logic outlined earlier: saving a reference to the original array at line 45, allocation of a new array at line 46, and copying of the old contents at lines 47 to 48. As shown at lines 42 to 43, the ensureCapacity routine can also be used to shrink the underlying array, but only if the speciﬁed new capacity is at least as large as the size. If it isn’t, the ensureCapacity request is ignored. At line 46, we see an idiom that is required because generic array creation is illegal. Instead, we create an array of the generic type’s bound and then use an array cast. This will generate a compiler warning but is unavoidable in the implementation of generic collections. Two versions of add are shown. The ﬁrst adds at the end of the list and is trivially implemented by calling the more general version that adds at the speciﬁed position. That version is computationally expensive because it requires shifting elements that are at or after the speciﬁed position an additional position higher. add may require increasing capacity. Expanding capacity is very expensive, so if the capacity is expanded, it is made twice as large as the size to avoid having to change the capacity again unless the size increases dramatically (the +1 is used in case the size is 0). The remove method is similar to add, in that elements that are at or after the speciﬁed position must be shifted to one position lower. The remaining routine deals with the iterator method and the implementation of the associated iterator class. In Figure 3.16, this is shown at lines 77 to 96. The iterator

public class MyArrayList<AnyType> implements Iterable<AnyType> { private static final int DEFAULT_CAPACITY = 10; private int theSize; private AnyType [ ] theItems; public MyArrayList( ) { doClear( ); } public void clear( ) { doClear( ); } private void doClear( ) { theSize = 0; ensureCapacity( DEFAULT_CAPACITY ); } public int size( ) { return theSize; } public boolean isEmpty( ) { return size( ) == 0; } public void trimToSize( ) { ensureCapacity( size( ) ); } public AnyType get( int idx ) { if( idx < 0 || idx >= size( ) ) throw new ArrayIndexOutOfBoundsException( ); return theItems[ idx ]; } public AnyType set( int idx, AnyType newVal ) { if( idx < 0 || idx >= size( ) ) throw new ArrayIndexOutOfBoundsException( ); AnyType old = theItems[ idx ]; theItems[ idx ] = newVal; return old; } public void ensureCapacity( int newCapacity ) { if( newCapacity < theSize ) return; AnyType [ ] old = theItems; theItems = (AnyType []) new Object[ newCapacity ]; for( int i = 0; i < size( ); i++ ) theItems[ i ] = old[ i ]; } Figure 3.15 MyArrayList class (Part 1 of 2)

public boolean add( AnyType x ) { add( size( ), x ); return true; } public void add( int idx, AnyType x ) { if( theItems.length == size( ) ) ensureCapacity( size( ) * 2 + 1 ); for( int i = theSize; i > idx; i-- ) theItems[ i ] = theItems[ i - 1 ]; theItems[ idx ] = x; theSize++; } public AnyType remove( int idx ) { AnyType removedItem = theItems[ idx ]; for( int i = idx; i < size( ) - 1; i++ ) theItems[ i ] = theItems[ i + 1 ]; theSize--; return removedItem; } public java.util.Iterator<AnyType> iterator( ) { return new ArrayListIterator( ); } private class ArrayListIterator implements java.util.Iterator<AnyType> { private int current = 0; public boolean hasNext( ) { return current < size( ); } public AnyType next( ) { if( !hasNext( ) ) throw new java.util.NoSuchElementException( ); return theItems[ current++ ]; } public void remove( ) { MyArrayList.this.remove( --current ); } } } Figure 3.16 MyArrayList class (Part 2 of 2)

3.4 Implementation of ArrayList method simply returns an instance of ArrayListIterator, which is a class that implements the Iterator interface. The ArrayListIterator stores the notion of a current position, and provides implementations of hasNext, next, and remove. The current position represents the (array index of the) next element that is to be viewed, so initially the current position is 0. 3.4.2 The Iterator and Java Nested and Inner Classes The ArrayListIterator class uses a tricky Java construct known as the inner class. Clearly the class is declared inside of the MyArrayList class, a feature that is supported by many languages. However, an inner class in Java has a more subtle property. To see how an inner class works, Figure 3.17 sketches the iterator idea (however, the code is ﬂawed), making ArrayListIterator a top-level class. We focus only on the data ﬁelds of MyArrayList, the iterator method in MyArrayList, and the ArrayListIterator (but not its remove method). In Figure 3.17, ArrayListIterator is generic, it stores a current position, and the code attempts to use the current position in next to index the array and then advance. Note that if arr is an array, arr[idx++] uses idx to the array, and then advances idx. The positioning of the ++ matters. The form we used is called the postﬁx ++ operator, in which the ++ is after idx. But in the preﬁx ++ operator, arr[++idx] advances idx and then uses the new idx to index the array. The problem with Figure 3.17 is that theItems[current++] is illegal, because theItems is not part of the ArrayListIterator class; it is part of the MyArrayList. Thus the code doesn’t make sense at all. public class MyArrayList<AnyType> implements Iterable<AnyType> { private int theSize; private AnyType [ ] theItems; ... public java.util.Iterator<AnyType> iterator( ) { return new ArrayListIterator<AnyType>( ); } } class ArrayListIterator<AnyType> implements java.util.Iterator<AnyType> { private int current = 0; ... public boolean hasNext( ) { return current < size( ); } public AnyType next( ) { return theItems[ current++ ]; } } Figure 3.17 Iterator Version #1 (doesn’t work): The iterator is a top-level class and stores the current position. It doesn’t work because theItems and size() are not part of the ArrayListIterator class

Chapter 3 Lists, Stacks, and Queues public class MyArrayList<AnyType> implements Iterable<AnyType> { private int theSize; private AnyType [ ] theItems; ... public java.util.Iterator<AnyType> iterator( ) { return new ArrayListIterator<AnyType>( this ); } } class ArrayListIterator<AnyType> implements java.util.Iterator<AnyType> { private int current = 0; private MyArrayList<AnyType> theList; ... public ArrayListIterator( MyArrayList<AnyType> list ) { theList = list; } public boolean hasNext( ) { return current < theList.size( ); } public AnyType next( ) { return theList.theItems[ current++ ]; } } Figure 3.18 Iterator Version #2 (almost works): The iterator is a top-level class and stores the current position and a link to the MyArrayList. It doesn’t work because theItems is private in the MyArrayList class The simplest solution is shown in Figure 3.18, which is unfortunately also ﬂawed, but in a more minor way. In Figure 3.18, we solve the problem of not having the array in the iterator by having the iterator store a reference to the MyArrayList that it is iterating over. This reference is a second data ﬁeld and is initialized by a new one-parameter constructor for ArrayListIterator. Now that we have a reference to MyArrayList, we can access the array ﬁeld that is contained in MyArrayList (and also get the size of the MyArrayList, which is needed in hasNext). The ﬂaw in Figure 3.18 is that theItems is a private ﬁeld in MyArrayList, and since ArrayListIterator is a different class, it is illegal to access theItems in the next method. The simplest ﬁx would be to change the visibility of theItems in MyArrayList from private to something less restrictive (such as public, or the default which is known as package visibility). But this violates basic principles of good object-oriented programming, which requires data to be as hidden as possible. Instead, Figure 3.19 shows a solution that works: Make the ArrayListIterator class a nested class. When we make ArrayListIterator a nested class, it is placed inside of another class (in this case MyArrayList) which is the outer class. We must use the word static to signify that it is nested; without static we will get an inner class, which is sometimes good and sometimes bad. The nested class is the type of class that is typical of many programming languages. Observe that the nested class can be made private, which is nice

3.4 Implementation of ArrayList public class MyArrayList<AnyType> implements Iterable<AnyType> { private int theSize; private AnyType [ ] theItems; ... public java.util.Iterator<AnyType> iterator( ) { return new ArrayListIterator<AnyType>( this ); } private static class ArrayListIterator<AnyType> implements java.util.Iterator<AnyType> { private int current = 0; private MyArrayList<AnyType> theList; ... public ArrayListIterator( MyArrayList<AnyType> list ) { theList = list; } public boolean hasNext( ) { return current < theList.size( ); } public AnyType next( ) { return theList.theItems[ current++ ]; } } } Figure 3.19 Iterator Version #3 (works): The iterator is a nested class and stores the current position and a link to the MyArrayList. It works because the nested class is considered part of the MyArrayList class because then it is inaccessible except by the outer class MyArrayList. More importantly, because the nested class is considered to be part of the outer class, there are no visibility issues that arise: theItems is a visible member of class MyArrayList, because next is part of MyArrayList. Now that we have a nested class, we can discuss the inner class. The problem with the nested class is that in our original design, when we wrote theItems without referring to MyArrayList that it was contained in, the code looked nice, and kind of made sense, but was illegal because it was impossible for the compiler to deduce which MyArrayList was being referred to. It would be nice not to have to keep track of this ourselves. This is exactly what an inner class does for you. When you declare an inner class, the compiler adds an implicit reference to the outer class object that caused the inner class object’s construction. If the name of the outer class is Outer, then the implicit reference is Outer.this. Thus if ArrayListIterator is declared as an inner class, without the static, then MyArrayList.this and theList would both be referencing the same MyArrayList. Thus theList would be redundant and could be removed.

Chapter 3 Lists, Stacks, and Queues lst itr1 items: 3, 5, 2 theSize: 3 MyArrayList.this current = 3 MyArrayList.this current = 0 itr2 Figure 3.20 Iterator/container with inner classes The inner class is useful in a situation in which each inner class object is associated with exactly one instance of an outer class object. In such a case, the inner class object can never exist without having an outer class object with which to be associated. In the case of the MyArrayList and its iterator, Figure 3.20 shows the relationship between the iterator class and MyArrayList class, when inner classes are used to implement the iterator. The use of theList.theItems could be replaced with MyArrayList.this.theItems. This is hardly an improvement, but a further simpliﬁcation is possible. Just as this.data can be written simply as data (provided there is no other variable named data that could clash), MyArrayList.this.theItems can be written simply as theItems. Figure 3.21 shows the simpliﬁcation of the ArrayListIterator. public class MyArrayList<AnyType> implements Iterable<AnyType> { private int theSize; private AnyType [ ] theItems; ... public java.util.Iterator<AnyType> iterator( ) { return new ArrayListIterator( ); } private class ArrayListIterator implements java.util.Iterator<AnyType> { private int current = 0; public boolean hasNext( ) { return current < size( ); } public AnyType next( ) { return theItems[ current++ ]; } public void remove( ) { MyArrayList.this.remove( --current ); } } } Figure 3.21 Iterator Version #4 (works): The iterator is an inner class and stores the current position and an implicit link to the MyArrayList

3.5 Implementation of LinkedList First, the ArrayListIterator is implicitly generic, since it is now tied to MyArrayList, which is generic; we don’t have to say so. Second, theList is gone, and we use size() and theItems[current++] as shorthands for MyArrayList.this.size() and MyArrayList.this.theItems[current++]. The removal of theList as a data member also removes the associated constructor, so the code reverts back to the style in Version #1. We can implement the iterator’s remove by calling MyArrayList’s remove. Since MyArrayList’s remove would conﬂict with ArrayListIterator’s remove, we have to use MyArrayList.this.remove. Note that after the item is removed, elements shift, so for current to be viewing the same element, it must also shift. Hence the use of --, rather than -1. Inner classes are a syntactical convenience for Java programmers. They are not needed to write any Java code, but their presence in the language allows the Java programmer to write code in the style that was natural (like Version #1), with the compiler writing the extra code required to associate the inner class object with the outer class object. 3.5 Implementation of LinkedList In this section, we provide the implementation of a usable LinkedList generic class. As in the case of the ArrayList class, our list class will be named MyLinkedList to avoid ambiguities with the library class. Recall that the LinkedList class will be implemented as a doubly linked list, and that we will need to maintain references to both ends of the list. Doing so allows us to maintain constant time cost per operation, so long as the operation occurs at a known position. The known position can be either end, or at a position speciﬁed by an iterator (however, we do not implement a ListIterator, thus leaving some code for the reader). In considering the design, we will need to provide three classes: 1. The MyLinkedList class itself, which contains links to both ends, the size of the list, and a host of methods. 2. The Node class, which is likely to be a private nested class. A node contains the data and links to the previous and next nodes, along with appropriate constructors. 3. The LinkedListIterator class, which abstracts the notion of a position and is a private inner class, implementing the Iterator interface. It provides implementations of next, hasNext, and remove. Because the iterator classes store a reference to the “current node,” and the end marker is a valid position, it makes sense to create an extra node at the end of the list to represent the end marker. Further, we can create an extra node at the front of the list, logically representing the beginning marker. These extra nodes are sometimes known as sentinel nodes; speciﬁcally, the node at the front is sometimes known as a header node, and the node at the end is sometimes known as a tail node.

Chapter 3 Lists, Stacks, and Queues head tail a b Figure 3.22 A doubly linked list with header and tail nodes head tail Figure 3.23 An empty doubly linked list with header and tail nodes The advantage of using these extra nodes is that they greatly simplify the coding by removing a host of special cases. For instance, if we do not use a header node, then removing the ﬁrst node becomes a special case, because we must reset the list’s link to the ﬁrst node during the remove, and also because the remove algorithm in general needs to access the node prior to the node being removed (and without a header node, the ﬁrst node does not have a node prior to it). Figure 3.22 shows a doubly linked list with header and tail nodes. Figure 3.23 shows an empty list. Figure 3.24 shows the outline and partial implementation of the MyLinkedList class. We can see at line 3 the beginning of the declaration of the private nested Node class. Figure 3.25 shows the Node class, consisting of the stored item, links to the previous and next Node, and a constructor. All the data members are public. Recall that in a class, the data members are normally private. However, members in a nested class are visible even in the outer class. Since the Node class is private, the visibility of the data members in the Node class is irrelevant; the MyLinkedList methods can see all Node data members, and classes outside of MyLinkedList cannot see the Node class at all. Back in Figure 3.24, lines 46 to 49 contain the data members for MyLinkedList, namely the reference to the header and tail nodes. We also keep track of the size in a data member, so that the size method can be implemented in constant time. At line 47, we have one additional data ﬁeld that is used to help the iterator detect changes in the collection. modCount represents the number of changes to the linked list since construction. Each call to add or remove will update modCount. The idea is that when an iterator is created, it will store the modCount of the collection. Each call to an iterator method (next or remove) will check the stored modCount in the iterator with the current modCount in the linked list and will throw a ConcurrentModificationException if these two counts don’t match. The rest of the MyLinkedList class consists of the constructor, the implementation of the iterator, and a host of methods. Many of the methods are one-liners.

public class MyLinkedList<AnyType> implements Iterable<AnyType> { private static class Node<AnyType> { /* Figure 3.25 */ } public MyLinkedList( ) { doClear( ); } public void clear( ) { /* Figure 3.26 */ } public int size( ) { return theSize; } public boolean isEmpty( ) { return size( ) == 0; } public boolean add( AnyType x ) { add( size( ), x ); return true; } public void add( int idx, AnyType x ) { addBefore( getNode( idx, 0, size( ) ), x ); } public AnyType get( int idx ) { return getNode( idx ).data; } public AnyType set( int idx, AnyType newVal ) { Node<AnyType> p = getNode( idx ); AnyType oldVal = p.data; p.data = newVal; return oldVal; } public AnyType remove( int idx ) { return remove( getNode( idx ) ); } private void addBefore( Node<AnyType> p, AnyType x ) { /* Figure 3.28 */ } private AnyType remove( Node<AnyType> p ) { /* Figure 3.30 */ } private Node<AnyType> getNode( int idx ) { /* Figure 3.31 */ } private Node<AnyType> getNode( int idx, int lower, int upper ) { /* Figure 3.31 */ } public java.util.Iterator<AnyType> iterator( ) { return new LinkedListIterator( ); } private class LinkedListIterator implements java.util.Iterator<AnyType> { /* Figure 3.32 */ } private int theSize; private int modCount = 0; private Node<AnyType> beginMarker; private Node<AnyType> endMarker; } Figure 3.24 MyLinkedList class

Chapter 3 Lists, Stacks, and Queues private static class Node<AnyType> { public Node( AnyType d, Node<AnyType> p, Node<AnyType> n ) { data = d; prev = p; next = n; } public AnyType data; public Node<AnyType> prev; public Node<AnyType> next; } Figure 3.25 Nested Node class for MyLinkedList class public void clear( ) { doClear( ); } private void doClear( ) { beginMarker = new Node<AnyType>( null, null, null ); endMarker = new Node<AnyType>( null, beginMarker, null ); beginMarker.next = endMarker; theSize = 0; modCount++; } Figure 3.26 clear routine for MyLinkedList class, which invokes private doClear The doClear method in Figure 3.26 is invoked by the constructor. It creates and connects the header and tail nodes and then sets the size to 0. In Figure 3.24, at line 43 we see the beginning of the declaration of the private inner LinkedListIterator class. We’ll discuss those details when we see the actual implementations later. Figure 3.27 illustrates how a new node containing x is spliced in between a node referenced by p and p.prev. The assignment to the node links can be described as follows: Node newNode = new Node( x, p.prev, p ); // Steps 1 and 2 p.prev.next = newNode; // Step 3 p.prev = newNode; // Step 4 Steps 3 and 4 can be combined, yielding only two lines: Node newNode = new Node( x, p.prev, p ); // Steps 1 and 2 p.prev = p.prev.next = newNode; // Steps 3 and 4 But then these two lines can also be combined, yielding: p.prev = p.prev.next = new Node( x, p.prev, p );

3.5 Implementation of LinkedList prev x p ... ... Figure 3.27 Insertion in a doubly linked list by getting new node and then changing pointers in the order indicated This makes the addBefore routine in Figure 3.28 short. Figure 3.29 shows the logic of removing a node. If p references the node being removed, only two links change before the node is disconnected and eligible to be reclaimed by the Virtual Machine: p.prev.next = p.next; p.next.prev = p.prev; /** * Adds an item to this collection, at specified position p. * Items at or after that position are slid one position higher. * @param p Node to add before. * @param x any object. * @throws IndexOutOfBoundsException if idx is not between 0 and size(),. */ private void addBefore( Node<AnyType> p, AnyType x ) { Node<AnyType> newNode = new Node<>( x, p.prev, p ); newNode.prev.next = newNode; p.prev = newNode; theSize++; modCount++; } Figure 3.28 add routine for MyLinkedList class p . . . . . . Figure 3.29 Removing node speciﬁed by p from a doubly linked list

Chapter 3 Lists, Stacks, and Queues /** * Removes the object contained in Node p. * @param p the Node containing the object. * @return the item was removed from the collection. */ private AnyType remove( Node<AnyType> p ) { p.next.prev = p.prev; p.prev.next = p.next; theSize--; modCount++; return p.data; } Figure 3.30 remove routine for MyLinkedList class Figure 3.30 shows the basic private remove routine that contains the two lines of code shown above. Figure 3.31 has the previously mentioned private getNode methods. If the index represents a node in the ﬁrst half of the list, then at lines 29 to 34 we step through the linked list, in the forward direction. Otherwise, we go backward, starting at the end, as shown in lines 37 to 39. The LinkedListIterator, shown in Figure 3.32, has logic that is similar to the ArrayListIterator but incorporates signiﬁcant error checking. The iterator maintains a current position, shown at line 3. current represents the node containing the item that is to be returned by a call to next. Observe that when current is positioned at the endMarker, a call to next is illegal. In order to detect a situation in which the collection has been modiﬁed during the iteration, at line 4 the iterator stores in the data ﬁeld expectedModCount the modCount of the linked list at the time the iterator is constructed. At line 5, the Boolean data ﬁeld okToRemove is true if a next has been performed, without a subsequent remove. Thus okToRemove is initially false, set to true in next, and set to false in remove. hasNext is fairly routine. As in java.util.LinkedList’s iterator, it does not check for modiﬁcation of the linked list. The next method advances current (line 18) after getting the value in the node (line 17) that is to be returned (line 20). okToRemove is updated at line 19. Finally, the iterator’s remove method is shown at lines 23 to 33. It is mostly error checking (which is why we avoided the error checks in the ArrayListIterator). The actual remove at line 30 mimics the logic in the ArrayListIterator. But here, current remains unchanged, because the node that current is viewing is unaffected by the removal of the prior node (in the ArrayListIterator, items shifted, requiring an update of current).

3.5 Implementation of LinkedList /** * Gets the Node at position idx, which must range from 0 to size( ) - 1. * @param idx index to search at. * @return internal node corresponding to idx. * @throws IndexOutOfBoundsException if idx is not * between 0 and size( ) - 1, inclusive. */ private Node<AnyType> getNode( int idx ) { return getNode( idx, 0, size( ) - 1 ); } /** * Gets the Node at position idx, which must range from lower to upper. * @param idx index to search at. * @param lower lowest valid index. * @param upper highest valid index. * @return internal node corresponding to idx. * @throws IndexOutOfBoundsException if idx is not * between lower and upper, inclusive. */ private Node<AnyType> getNode( int idx, int lower, int upper ) { Node<AnyType> p; if( idx < lower || idx > upper ) throw new IndexOutOfBoundsException( ); if( idx < size( ) / 2 ) { p = beginMarker.next; for( int i = 0; i < idx; i++ ) p = p.next; } else { p = endMarker; for( int i = size( ); i > idx; i-- ) p = p.prev; } return p; } Figure 3.31 Private getNode routine for MyLinkedList class

Chapter 3 Lists, Stacks, and Queues private class LinkedListIterator implements java.util.Iterator<AnyType> { private Node<AnyType> current = beginMarker.next; private int expectedModCount = modCount; private boolean okToRemove = false; public boolean hasNext( ) { return current != endMarker; } public AnyType next( ) { if( modCount != expectedModCount ) throw new java.util.ConcurrentModificationException( ); if( !hasNext( ) ) throw new java.util.NoSuchElementException( ); AnyType nextItem = current.data; current = current.next; okToRemove = true; return nextItem; } public void remove( ) { if( modCount != expectedModCount ) throw new java.util.ConcurrentModificationException( ); if( !okToRemove ) throw new IllegalStateException( ); MyLinkedList.false.remove( current.prev ); expectedModCount++; okToRemove = false; } } Figure 3.32 Inner Iterator class for MyList class 3.6 The Stack ADT 3.6.1 Stack Model A stack is a list with the restriction that insertions and deletions can be performed in only one position, namely, the end of the list, called the top. The fundamental operations on a stack are push, which is equivalent to an insert, and pop, which deletes the most recently

3.6 The Stack ADT push Stack pop top Figure 3.33 Stack model: input to a stack is by push, output is by pop and top top Figure 3.34 Stack model: Only the top element is accessible inserted element. The most recently inserted element can be examined prior to performing a pop by use of the top routine. A pop or top on an empty stack is generally considered an error in the stack ADT. On the other hand, running out of space when performing a push is an implementation limit but not an ADT error. Stacks are sometimes known as LIFO (last in, ﬁrst out) lists. The model depicted in Figure 3.33 signiﬁes only that pushes are input operations and pops and tops are output. The usual operations to make empty stacks and test for emptiness are part of the repertoire, but essentially all that you can do to a stack is push and pop. Figure 3.34 shows an abstract stack after several operations. The general model is that there is some element that is at the top of the stack, and it is the only element that is visible. 3.6.2 Implementation of Stacks Since a stack is a list, any list implementation will do. Clearly ArrayList and LinkedList support stack operations; 99% of the time they are the most reasonable choice. Occasionally it can be faster to design a special-purpose implementation (for instance, if the items being placed on the stack are a primitive type). Because stack operations are constant-time operations, this is unlikely to yield any discernable improvement except under very unique circumstances. For these special times, we will give two popular implementations. One

Chapter 3 Lists, Stacks, and Queues uses a linked structure and the other uses an array, and both simplify the logic in ArrayList and LinkedList, so we do not provide code. Linked List Implementation of Stacks The ﬁrst implementation of a stack uses a singly linked list. We perform a push by inserting at the front of the list. We perform a pop by deleting the element at the front of the list. A top operation merely examines the element at the front of the list, returning its value. Sometimes the pop and top operations are combined into one. Array Implementation of Stacks An alternative implementation avoids links and is probably the more popular solution. Mimicking the ArrayList add operation, the implementation is trivial. Associated with each stack is theArray and topOfStack, which is −1 for an empty stack (this is how an empty stack is initialized). To push some element x onto the stack, we increment topOfStack and then set theArray[topOfStack] = x. To pop, we set the return value to theArray[topOfStack] and then decrement topOfStack. Notice that these operations are performed in not only constant time, but very fast constant time. On some machines, pushes and pops (of integers) can be written in one machine instruction, operating on a register with auto-increment and auto-decrement addressing. The fact that most modern machines have stack operations as part of the instruction set enforces the idea that the stack is probably the most fundamental data structure in computer science, after the array. 3.6.3 Applications It should come as no surprise that if we restrict the operations allowed on a list, those operations can be performed very quickly. The big surprise, however, is that the small number of operations left are so powerful and important. We give three of the many applications of stacks. The third application gives a deep insight into how programs are organized. Balancing Symbols Compilers check your programs for syntax errors, but frequently a lack of one symbol (such as a missing brace or comment starter) will cause the compiler to spill out a hundred lines of diagnostics without identifying the real error. (Fortunately, most Java compilers are pretty good about this. But not all languages and compilers are as responsible.) A useful tool in this situation is a program that checks whether everything is balanced. Thus, every right brace, bracket, and parenthesis must correspond to its left counterpart. The sequence [()] is legal, but [(]) is wrong. Obviously, it is not worthwhile writing a huge program for this, but it turns out that it is easy to check these things. For simplicity, we will just check for balancing of parentheses, brackets, and braces and ignore any other character that appears. The simple algorithm uses a stack and is as follows: Make an empty stack. Read characters until end of ﬁle. If the character is an opening symbol, push it onto the stack. If it is a closing symbol, then if the stack is empty report

3.6 The Stack ADT an error. Otherwise, pop the stack. If the symbol popped is not the corresponding opening symbol, then report an error. At end of ﬁle, if the stack is not empty report an error. You should be able to convince yourself that this algorithm works. It is clearly linear and actually makes only one pass through the input. It is thus online and quite fast. Extra work can be done to attempt to decide what to do when an error is reported—such as identifying the likely cause. Postﬁx Expressions Suppose we have a pocket calculator and would like to compute the cost of a shopping trip. To do so, we add a list of numbers and multiply the result by 1.06; this computes the purchase price of some items with local sales tax added. If the items are 4.99, 5.99, and 6.99, then a natural way to enter this would be the sequence 4.99 + 5.99 + 6.99 ∗1.06 = Depending on the calculator, this produces either the intended answer, 19.05, or the scientiﬁc answer, 18.39. Most simple four-function calculators will give the ﬁrst answer, but many advanced calculators know that multiplication has higher precedence than addition. On the other hand, some items are taxable and some are not, so if only the ﬁrst and last items were actually taxable, then the sequence 4.99 ∗1.06 + 5.99 + 6.99 ∗1.06 = would give the correct answer (18.69) on a scientiﬁc calculator and the wrong answer (19.37) on a simple calculator. A scientiﬁc calculator generally comes with parentheses, so we can always get the right answer by parenthesizing, but with a simple calculator we need to remember intermediate results. A typical evaluation sequence for this example might be to multiply 4.99 and 1.06, saving this answer as A1. We then add 5.99 and A1, saving the result in A1. We multiply 6.99 and 1.06, saving the answer in A2, and ﬁnish by adding A1 and A2, leaving the ﬁnal answer in A1. We can write this sequence of operations as follows: 4.99 1.06 ∗5.99 + 6.99 1.06 ∗+ This notation is known as postﬁx or reverse Polish notation and is evaluated exactly as we have described above. The easiest way to do this is to use a stack. When a number is seen, it is pushed onto the stack; when an operator is seen, the operator is applied to the two numbers (symbols) that are popped from the stack, and the result is pushed onto the stack. For instance, the postﬁx expression 6 5 2 3 + 8 ∗+ 3 + ∗ is evaluated as follows: The ﬁrst four symbols are placed on the stack. The resulting stack is

Chapter 3 Lists, Stacks, and Queues topOfStack → Next a ‘+’ is read, so 3 and 2 are popped from the stack and their sum, 5, is pushed. topOfStack → Next 8 is pushed. topOfStack → Now a ‘∗’ is seen, so 8 and 5 are popped and 5 ∗8 = 40 is pushed. topOfStack → Next a ‘+’ is seen, so 40 and 5 are popped and 5 + 40 = 45 is pushed. topOfStack →

3.6 The Stack ADT Now, 3 is pushed. topOfStack → Next ‘+’ pops 3 and 45 and pushes 45 + 3 = 48. topOfStack → Finally, a ‘∗’ is seen and 48 and 6 are popped; the result, 6 ∗48 = 288, is pushed. topOfStack → The time to evaluate a postﬁx expression is O(N), because processing each element in the input consists of stack operations and thus takes constant time. The algorithm to do so is very simple. Notice that when an expression is given in postﬁx notation, there is no need to know any precedence rules; this is an obvious advantage. Inﬁx to Postﬁx Conversion Not only can a stack be used to evaluate a postﬁx expression, but we can also use a stack to convert an expression in standard form (otherwise known as inﬁx) into postﬁx. We will concentrate on a small version of the general problem by allowing only the operators +, *, (, ), and insisting on the usual precedence rules. We will further assume that the expression is legal. Suppose we want to convert the inﬁx expression a + b * c + ( d * e + f ) * g into postﬁx. A correct answer is a b c * + d e * f + g * +. When an operand is read, it is immediately placed onto the output. Operators are not immediately output, so they must be saved somewhere. The correct thing to do is to place operators that have been seen, but not placed on the output, onto the stack. We will also stack left parentheses when they are encountered. We start with an initially empty stack.

Chapter 3 Lists, Stacks, and Queues If we see a right parenthesis, then we pop the stack, writing symbols until we encounter a (corresponding) left parenthesis, which is popped but not output. If we see any other symbol (+, *, (), then we pop entries from the stack until we ﬁnd an entry of lower priority. One exception is that we never remove a ( from the stack except when processing a ). For the purposes of this operation, + has lowest priority and ( highest. When the popping is done, we push the operator onto the stack. Finally, if we read the end of input, we pop the stack until it is empty, writing symbols onto the output. The idea of this algorithm is that when an operator is seen, it is placed on the stack. The stack represents pending operators. However, some of the operators on the stack that have high precedence are now known to be completed and should be popped, as they will no longer be pending. Thus prior to placing the operator on the stack, operators that are on the stack and are to be completed prior to the current operator, are popped. This is illustrated in the following table: Stack When Third Expression Operator Is Processed Action a*b-c+d - - is completed; + is pushed a/b+c*d + Nothing is completed; * is pushed a-b*c/d - * * is completed; / is pushed a-b*c+d - * * and - are completed; + is pushed Parentheses simply add an additional complication. We can view a left parenthesis as a high-precedence operator when it is an input symbol (so that pending operators remain pending), and a low-precedence operator when it is on the stack (so that it is not accidentally removed by an operator). Right parentheses are treated as the special case. To see how this algorithm performs, we will convert the long inﬁx expression above into its postﬁx form. First, the symbol a is read, so it is passed through to the output. Then + is read and pushed onto the stack. Next b is read and passed through to the output. The state of affairs at this juncture is as follows: a b Output Stack + Next a * is read. The top entry on the operator stack has lower precedence than *, so nothing is output and * is put on the stack. Next, c is read and output. Thus far, we have a b c Output Stack + *

3.6 The Stack ADT The next symbol is a +. Checking the stack, we ﬁnd that we will pop a * and place it on the output; pop the other +, which is not of lower but equal priority, on the stack; and then push the +. a b c * + Output Stack + The next symbol read is a (, which, being of highest precedence, is placed on the stack. Then d is read and output. a b c * + d Output Stack + ( We continue by reading a *. Since open parentheses do not get removed except when a closed parenthesis is being processed, there is no output. Next, e is read and output. a b c * + d e Output Stack + ( * The next symbol read is a +. We pop and output * and then push +. Then we read and output f. a b c * + d e * f Output Stack + ( + Now we read a ), so the stack is emptied back to the (. We output a +. a b c * + d e * f + Output Stack + We read a * next; it is pushed onto the stack. Then g is read and output.

Chapter 3 Lists, Stacks, and Queues a b c * + d e * f + g Output Stack + * The input is now empty, so we pop and output symbols from the stack until it is empty. a b c * + d e * f + g * + Output Stack As before, this conversion requires only O(N) time and works in one pass through the input. We can add subtraction and division to this repertoire by assigning subtraction and addition equal priority and multiplication and division equal priority. A subtle point is that the expression a-b-c will be converted to ab-c- and not abc--. Our algorithm does the right thing, because these operators associate from left to right. This is not necessarily the case in general, since exponentiation associates right to left: 223 = 28 = 256, not 43 = 64. We leave as an exercise the problem of adding exponentiation to the repertoire of operators. Method Calls The algorithm to check balanced symbols suggests a way to implement method calls in compiled procedural and object-oriented languages.1 The problem here is that when a call is made to a new method, all the variables local to the calling routine need to be saved by the system, since otherwise the new method will overwrite the memory used by the calling routine’s variables. Furthermore, the current location in the routine must be saved so that the new method knows where to go after it is done. The variables have generally been assigned by the compiler to machine registers, and there are certain to be conﬂicts (usually all methods get some variables assigned to register #1), especially if recursion is involved. The reason that this problem is similar to balancing symbols is that a method call and method return are essentially the same as an open parenthesis and closed parenthesis, so the same ideas should work. When there is a method call, all the important information that needs to be saved, such as register values (corresponding to variable names) and the return address (which can be obtained from the program counter, which is typically in a register), is saved “on a piece of paper” in an abstract way and put at the top of a pile. Then the control is transferred to the new method, which is free to replace the registers with its values. If it makes other method calls, it follows the same procedure. When the method wants to return, it looks at the “paper” at the top of the pile and restores all the registers. It then makes the return jump. 1 Since Java is interpreted, rather than compiled, some details in this section may not apply to Java, but the general concepts still do in Java and many other languages.

3.6 The Stack ADT /** * Print container from itr. */ public static <AnyType> void printList( Iterator<AnyType> itr ) { if( !itr.hasNext( ) ) return; System.out.println( itr.next( ) ); printList( itr ); } Figure 3.35 A bad use of recursion: printing a linked list Clearly, all of this work can be done using a stack, and that is exactly what happens in virtually every programming language that implements recursion. The information saved is called either an activation record or stack frame. Typically, a slight adjustment is made: The current environment is represented at the top of the stack. Thus, a return gives the previous environment (without copying). The stack in a real computer frequently grows from the high end of your memory partition downward, and on many non-Java systems there is no checking for overﬂow. There is always the possibility that you will run out of stack space by having too many simultaneously active methods. Needless to say, running out of stack space is always a fatal error. In languages and systems that do not check for stack overﬂow, programs crash without an explicit explanation. In Java, an exception is thrown. In normal events, you should not run out of stack space; doing so is usually an indication of runaway recursion (forgetting a base case). On the other hand, some perfectly legal and seemingly innocuous programs can cause you to run out of stack space. The routine in Figure 3.35, which prints out a collection, is perfectly legal and actually correct. It properly handles the base case of an empty collection, and the recursion is ﬁne. This program can be proven correct. Unfortunately, if the collection contains 20,000 elements to print, there will be a stack of 20,000 activation records representing the nested calls of line 10. Activation records are typically large because of all the information they contain, so this program is likely to run out of stack space. (If 20,000 elements are not enough to make the program crash, replace the number with a larger one.) This program is an example of an extremely bad use of recursion known as tail recursion. Tail recursion refers to a recursive call at the last line. Tail recursion can be mechanically eliminated by enclosing the body in a while loop and replacing the recursive call with one assignment per method argument. This simulates the recursive call because nothing needs to be saved; after the recursive call ﬁnishes, there is really no need to know the saved values. Because of this, we can just go to the top of the method with the values that would have been used in a recursive call. The method in Figure 3.36 shows the mechanically improved version. Removal of tail recursion is so simple that some compilers do it automatically. Even so, it is best not to ﬁnd out that yours does not.

Chapter 3 Lists, Stacks, and Queues /** * Print container from itr. */ public static <AnyType> void printList( Iterator<AnyType> itr ) { while( true ) { if( !itr.hasNext( ) ) return; System.out.println( itr.next( ) ); } } Figure 3.36 Printing a list without recursion; a compiler might do this Recursion can always be completely removed (compilers do so in converting to assembly language), but doing so can be quite tedious. The general strategy requires using a stack and is worthwhile only if you can manage to put the bare minimum on the stack. We will not dwell on this further, except to point out that although nonrecursive programs are certainly generally faster than equivalent recursive programs, the speed advantage rarely justiﬁes the lack of clarity that results from removing the recursion. 3.7 The Queue ADT Like stacks, queues are lists. With a queue, however, insertion is done at one end, whereas deletion is performed at the other end. 3.7.1 Queue Model The basic operations on a queue are enqueue, which inserts an element at the end of the list (called the rear), and dequeue, which deletes (and returns) the element at the start of the list (known as the front). Figure 3.37 shows the abstract model of a queue. 3.7.2 Array Implementation of Queues As with stacks, any list implementation is legal for queues. Like stacks, both the linked list and array implementations give fast O(1) running times for every operation. The linked list implementation is straightforward and left as an exercise. We will now discuss an array implementation of queues. For each queue data structure, we keep an array, theArray, and the positions front and back, which represent the ends of the queue. We also keep track of the number of elements

3.7 The Queue ADT   enqueue   Queue    dequeue Figure 3.37 Model of a queue that are actually in the queue, currentSize. The following ﬁgure shows a queue in some intermediate state. front back ↑ ↑ The operations should be clear. To enqueue an element x, we increment currentSize and back, then set theArray[back]=x. To dequeue an element, we set the return value to theArray[front], decrement currentSize, and then increment front. Other strategies are possible (this is discussed later). We will comment on checking for errors presently. There is one potential problem with this implementation. After 10 enqueues, the queue appears to be full, since back is now at the last array index, and the next enqueue would be in a nonexistent position. However, there might only be a few elements in the queue, because several elements may have already been dequeued. Queues, like stacks, frequently stay small even in the presence of a lot of operations. The simple solution is that whenever front or back gets to the end of the array, it is wrapped around to the beginning. The following ﬁgures show the queue during some operations. This is known as a circular array implementation. front back Initial State ↑ ↑ front back After enqueue(1) ↑ ↑

Chapter 3 Lists, Stacks, and Queues front back After enqueue(3) ↑ ↑ front back After dequeue, Which Returns 2 ↑ ↑ front back After dequeue, Which Returns 4 ↑ ↑ back front After dequeue, Which Returns 1 ↑ front back After dequeue, Which Returns 3 and Makes the Queue Empty ↑ ↑ The extra code required to implement the wraparound is minimal (although it probably doubles the running time). If incrementing either back or front causes it to go past the array, the value is reset to the ﬁrst position in the array. Some programmers use different ways of representing the front and back of a queue. For instance, some do not use an entry to keep track of the size, because they rely on the base case that when the queue is empty, back = front-1. The size is computed implicitly by comparing back and front. This is a very tricky way to go, because there are some special cases, so be very careful if you need to modify code written this way. If the currentSize is not maintained as an explicit data ﬁeld, then the queue is full when there are theArray.length-1 elements, since only theArray.length different sizes can be differentiated, and one of these is 0. Pick any style you like and make sure that all your routines

3.7 The Queue ADT are consistent. Since there are a few options for implementation, it is probably worth a comment or two in the code, if you don’t use the currentSize ﬁeld. In applications where you are sure that the number of enqueues is not larger than the capacity of the queue, the wraparound is not necessary. As with stacks, dequeues are rarely performed unless the calling routines are certain that the queue is not empty. Thus error checks are frequently skipped for this operation, except in critical code. This is generally not justiﬁable, because the time savings that you are likely to achieve are minimal. 3.7.3 Applications of Queues There are many algorithms that use queues to give efﬁcient running times. Several of these are found in graph theory, and we will discuss them in Chapter 9. For now, we will give some simple examples of queue usage. When jobs are submitted to a printer, they are arranged in order of arrival. Thus, essentially, jobs sent to a line printer are placed on a queue.2 Virtually every real-life line is (supposed to be) a queue. For instance, lines at ticket counters are queues, because service is ﬁrst-come ﬁrst-served. Another example concerns computer networks. There are many network setups of personal computers in which the disk is attached to one machine, known as the ﬁle server. Users on other machines are given access to ﬁles on a ﬁrst-come ﬁrst-served basis, so the data structure is a queue. Further examples include the following: r Calls to large companies are generally placed on a queue when all operators are busy. r In large universities, where resources are limited, students must sign a waiting list if all terminals are occupied. The student who has been at a terminal the longest is forced off ﬁrst, and the student who has been waiting the longest is the next user to be allowed on. A whole branch of mathematics, known as queuing theory, deals with computing, probabilistically, how long users expect to wait on a line, how long the line gets, and other such questions. The answer depends on how frequently users arrive to the line and how long it takes to process a user once the user is served. Both of these parameters are given as probability distribution functions. In simple cases, an answer can be computed analytically. An example of an easy case would be a phone line with one operator. If the operator is busy, callers are placed on a waiting line (up to some maximum limit). This problem is important for businesses, because studies have shown that people are quick to hang up the phone. If there are k operators, then this problem is much more difﬁcult to solve. Problems that are difﬁcult to solve analytically are often solved by a simulation. In our case, we would need to use a queue to perform the simulation. If k is large, we also need other data structures to do this efﬁciently. We shall see how to do this simulation in Chapter 6. We 2 We say essentially because jobs can be killed. This amounts to a deletion from the middle of the queue, which is a violation of the strict deﬁnition.

Chapter 3 Lists, Stacks, and Queues could then run the simulation for several values of k and choose the minimum k that gives a reasonable waiting time. Additional uses for queues abound, and as with stacks, it is staggering that such a simple data structure can be so important.

C H A P T E R 4 Trees For large amounts of input, the linear access time of linked lists is prohibitive. In this chapter we look at a simple data structure for which the running time of most operations is O(log N) on average. We also sketch a conceptually simple modiﬁcation to this data structure that guarantees the above time bound in the worst case and discuss a second modiﬁcation that essentially gives an O(log N) running time per operation for a long sequence of instructions. The data structure that we are referring to is known as a binary search tree. The binary search tree is the basis for the implementation of two library collections classes, TreeSet and TreeMap, which are used in many applications. Trees in general are very useful abstractions in computer science, so we will discuss their use in other, more general applications. In this chapter, we will r See how trees are used to implement the ﬁle system of several popular operating systems. r See how trees can be used to evaluate arithmetic expressions. r Show how to use trees to support searching operations in O(log N) average time, and how to reﬁne these ideas to obtain O(log N) worst-case bounds. We will also see how to implement these operations when the data are stored on a disk. r Discuss and use the TreeSet and TreeMap classes. 4.1 Preliminaries A tree can be deﬁned in several ways. One natural way to deﬁne a tree is recursively. A tree is a collection of nodes. The collection can be empty; otherwise, a tree consists of a distinguished node r, called the root, and zero or more nonempty (sub)trees T1, T2, . . . , Tk, each of whose roots are connected by a directed edge from r. The root of each subtree is said to be a child of r, and r is the parent of each subtree root. Figure 4.1 shows a typical tree using the recursive deﬁnition. From the recursive deﬁnition, we ﬁnd that a tree is a collection of N nodes, one of which is the root, and N −1 edges. That there are N −1 edges follows from the fact that each edge connects some node to its parent, and every node except the root has one parent (see Figure 4.2).

Chapter 4 Trees root . . . T1 T10 T2 T4 T3 Figure 4.1 Generic tree A B C D E F G H I J K L M N P Q Figure 4.2 A tree In the tree of Figure 4.2, the root is A. Node F has A as a parent and K, L, and M as children. Each node may have an arbitrary number of children, possibly zero. Nodes with no children are known as leaves; the leaves in the tree above are B, C, H, I, P, Q, K, L, M, and N. Nodes with the same parent are siblings; thus K, L, and M are all siblings. Grandparent and grandchild relations can be deﬁned in a similar manner. A path from node n1 to nk is deﬁned as a sequence of nodes n1, n2, . . . , nk such that ni is the parent of ni+1 for 1 ≤i < k. The length of this path is the number of edges on the path, namely k −1. There is a path of length zero from every node to itself. Notice that in a tree there is exactly one path from the root to each node. For any node ni, the depth of ni is the length of the unique path from the root to ni. Thus, the root is at depth 0. The height of ni is the length of the longest path from ni to a leaf. Thus all leaves are at height 0. The height of a tree is equal to the height of the root. For the tree in Figure 4.2, E is at depth 1 and height 2; F is at depth 1 and height 1; the height of the tree is 3. The depth of a tree is equal to the depth of the deepest leaf; this is always equal to the height of the tree. If there is a path from n1 to n2, then n1 is an ancestor of n2 and n2 is a descendant of n1. If n1 ̸= n2, then n1 is a proper ancestor of n2 and n2 is a proper descendant of n1. 4.1.1 Implementation of Trees One way to implement a tree would be to have in each node, besides its data, a link to each child of the node. However, since the number of children per node can vary so greatly and is not known in advance, it might be infeasible to make the children direct links in the data

4.1 Preliminaries class TreeNode { Object element; TreeNode firstChild; TreeNode nextSibling; } Figure 4.3 Node declarations for trees A B C D E F G H I J K L M N P Q Figure 4.4 First child/next sibling representation of the tree shown in Figure 4.2 structure, because there would be too much wasted space. The solution is simple: Keep the children of each node in a linked list of tree nodes. The declaration in Figure 4.3 is typical. Figure 4.4 shows how a tree might be represented in this implementation. Arrows that point downward are firstChild links. Horizontal arrows are nextSibling links. Null links are not drawn, because there are too many. In the tree of Figure 4.4, node E has both a link to a sibling (F) and a link to a child (I), while some nodes have neither. 4.1.2 Tree Traversals with an Application There are many applications for trees. One of the popular uses is the directory structure in many common operating systems, including UNIX and DOS. Figure 4.5 is a typical directory in the UNIX ﬁle system. The root of this directory is /usr. (The asterisk next to the name indicates that /usr is itself a directory.) /usr has three children, mark, alex, and bill, which are themselves directories. Thus, /usr contains three directories and no regular ﬁles. The ﬁlename /usr/mark/book/ch1.r is obtained by following the leftmost child three times. Each / after the ﬁrst indicates an edge; the result is the full pathname. This hierarchical ﬁle system is very popular, because it allows users to organize their data logically. Furthermore, two ﬁles in different directories can share the same name, because they must have different paths from the root and thus have different pathnames. A directory in the UNIX ﬁle system is just a ﬁle with a list of all its children, so the directories are structured almost exactly in accordance

Chapter 4 Trees ch1.r ch2.r ch3.r book* mark* course*         junk cop3530* fall* fall* fall* spr* sum* syl.r syl.r syl.r /usr* alex* bill*   junk         work* course* cop3212* grades prog1.r prog2.r grades prog1.r prog2.r Figure 4.5 UNIX directory private void listAll( int depth ) { printName( depth ); // Print the name of the object if( isDirectory( ) ) for each file c in this directory (for each child) c.listAll( depth + 1 ); } public void listAll( ) { listAll( 0 ); } Figure 4.6 Pseudocode to list a directory in a hierarchical ﬁle system with the type declaration above.1 Indeed, on some versions of UNIX, if the normal command to print a ﬁle is applied to a directory, then the names of the ﬁles in the directory can be seen in the output (along with other non-ASCII information). Suppose we would like to list the names of all of the ﬁles in the directory. Our output format will be that ﬁles that are depth di will have their names indented by di tabs. Our algorithm is given in Figure 4.6 as pseudocode.2 The heart of the algorithm is the recursive method listAll. This routine needs to be started with a depth of 0, to signify no indenting for the root. This depth is an internal bookkeeping variable and is hardly a parameter that a calling routine should be expected to know about. Thus the driver routine is used to interface the recursive routine to the outside world. 1 Each directory in the UNIX ﬁle system also has one entry that points to itself and another entry that points to the parent of the directory. Thus, technically, the UNIX ﬁle system is not a tree, but is treelike. 2 The Java code to implement this is provided in the ﬁle FileSystem.java online. It uses Java features that have not been discussed in the text.

4.1 Preliminaries /usr mark book ch1.r ch2.r ch3.r course cop3530 fall syl.r spr syl.r sum syl.r junk alex junk bill work course cop3212 fall grades prog1.r prog2.r fall prog2.r prog1.r grades Figure 4.7 The (preorder) directory listing The logic of the algorithm is simple to follow. The name of the ﬁle object is printed out with the appropriate number of tabs. If the entry is a directory, then we process all children recursively, one by one. These children are one level deeper and thus need to be indented an extra space. The output is in Figure 4.7. This traversal strategy is known as a preorder traversal. In a preorder traversal, work at a node is performed before (pre) its children are processed. When this program is run, it is clear that line 1 is executed exactly once per node, since each name is output once. Since line 1 is executed at most once per node, line 2 must also be executed once per node. Furthermore, line 4 can be executed at most once for each child of each node. But the number of children is exactly one less than the number of nodes. Finally, the for loop iterates once per execution of line 4, plus once each time the loop ends. Thus, the total amount of work is constant per node. If there are N ﬁle names to be output, then the running time is O(N).

Chapter 4 Trees ch1.r(3) ch2.r(2) ch3.r(4) book*(1) mark*(1) course*(1) junk (6) cop3530*(1) fall*(1) spr*(1) sum*(1) syl.r(1) syl.r(5) syl.r(2) /usr*(1) alex*(1) bill*(1) junk (8) work*(1) course*(1) cop3212*(1) fall*(1) fall*(1) grades(3) prog1.r(4) prog2.r(1) grades(9) prog1.r(7) prog2.r(2) Figure 4.8 UNIX directory with ﬁle sizes obtained via postorder traversal public int size( ) { int totalSize = sizeOfThisFile( ); if( isDirectory( ) ) for each file c in this directory (for each child) totalSize += c.size( ); return totalSize; } Figure 4.9 Pseudocode to calculate the size of a directory Another common method of traversing a tree is the postorder traversal. In a postorder traversal, the work at a node is performed after (post) its children are evaluated. As an example, Figure 4.8 represents the same directory structure as before, with the numbers in parentheses representing the number of disk blocks taken up by each ﬁle. Since the directories are themselves ﬁles, they have sizes too. Suppose we would like to calculate the total number of blocks used by all the ﬁles in the tree. The most natural way to do this would be to ﬁnd the number of blocks contained in the subdirectories /usr/mark (30), /usr/alex (9), and /usr/bill (32). The total number of blocks is then the total in the subdirectories (71) plus the one block used by /usr, for a total of 72. The pseudocode method size in Figure 4.9 implements this strategy. If the current object is not a directory, then size merely returns the number of blocks it uses. Otherwise, the number of blocks used by the directory is added to the number of blocks (recursively) found in all the children. To see the difference between the postorder traversal strategy and the preorder traversal strategy, Figure 4.10 shows how the size of each directory or ﬁle is produced by the algorithm.

4.2 Binary Trees ch1.r ch2.r ch3.r book syl.r fall syl.r spr syl.r sum cop3530 course junk mark junk alex work grades prog1.r prog2.r fall prog2.r prog1.r grades fall cop3212 course bill /usr Figure 4.10 Trace of the size function 4.2 Binary Trees A binary tree is a tree in which no node can have more than two children. Figure 4.11 shows that a binary tree consists of a root and two subtrees, TL and TR, both of which could possibly be empty. A property of a binary tree that is sometimes important is that the depth of an average binary tree is considerably smaller than N. An analysis shows that the average depth is O( √ N), and that for a special type of binary tree, namely the binary search tree, the average value of the depth is O(log N). Unfortunately, the depth can be as large as N −1, as the example in Figure 4.12 shows.

Chapter 4 Trees root TL TR Figure 4.11 Generic binary tree A B C D E Figure 4.12 Worst-case binary tree 4.2.1 Implementation Because a binary tree node has at most two children, we can keep direct links to them. The declaration of tree nodes is similar in structure to that for doubly linked lists in that a node is a structure consisting of the element information plus two references (left and right) to other nodes (see Figure 4.13). We could draw the binary trees using the rectangular boxes that are customary for linked lists, but trees are generally drawn as circles connected by lines, because they are class BinaryNode { // Friendly data; accessible by other package routines Object element; // The data in the node BinaryNode left; // Left child BinaryNode right; // Right child } Figure 4.13 Binary tree node class

4.2 Binary Trees actually graphs. We also do not explicitly draw null links when referring to trees, because every binary tree with N nodes would require N + 1 null links. Binary trees have many important uses not associated with searching. One of the principal uses of binary trees is in the area of compiler design, which we will now explore. 4.2.2 An Example: Expression Trees Figure 4.14 shows an example of an expression tree. The leaves of an expression tree are operands, such as constants or variable names, and the other nodes contain operators. This particular tree happens to be binary, because all the operators are binary, and although this is the simplest case, it is possible for nodes to have more than two children. It is also possible for a node to have only one child, as is the case with the unary minus operator. We can evaluate an expression tree, T, by applying the operator at the root to the values obtained by recursively evaluating the left and right subtrees. In our example, the left subtree evaluates to a + (b * c) and the right subtree evaluates to ((d * e) + f) * g. The entire tree therefore represents (a + (b * c)) + (((d * e) + f) * g). We can produce an (overly parenthesized) inﬁx expression by recursively producing a parenthesized left expression, then printing out the operator at the root, and ﬁnally recursively producing a parenthesized right expression. This general strategy (left, node, right) is known as an inorder traversal; it is easy to remember because of the type of expression it produces. An alternate traversal strategy is to recursively print out the left subtree, the right subtree, and then the operator. If we apply this strategy to our tree above, the output is a b c * + d e * f + g * +, which is easily seen to be the postﬁx representation of Section 3.6.3. This traversal strategy is generally known as a postorder traversal. We have seen this traversal strategy earlier in Section 4.1. A third traversal strategy is to print out the operator ﬁrst and then recursively print out the left and right subtrees. The resulting expression, + + a * b c * + * d e f g, is the less useful preﬁx notation and the traversal strategy is a preorder traversal, which we have also seen earlier in Section 4.1. We will return to these traversal strategies later in the chapter. a + b * c + d * e + f * g Figure 4.14 Expression tree for (a + b * c) + ((d * e + f ) * g)

Chapter 4 Trees Constructing an Expression Tree We now give an algorithm to convert a postﬁx expression into an expression tree. Since we already have an algorithm to convert inﬁx to postﬁx, we can generate expression trees from the two common types of input. The method we describe strongly resembles the postﬁx evaluation algorithm of Section 3.6.3. We read our expression one symbol at a time. If the symbol is an operand, we create a one-node tree and push it onto a stack. If the symbol is an operator, we pop two trees T1 and T2 from the stack (T1 is popped ﬁrst) and form a new tree whose root is the operator and whose left and right children are T2 and T1, respectively. This new tree is then pushed onto the stack. As an example, suppose the input is a b + c d e + * * The ﬁrst two symbols are operands, so we create one-node trees and push them onto a stack.3 a b Next, a + is read, so two trees are popped, a new tree is formed, and it is pushed onto the stack. + a b Next, c, d, and e are read, and for each a one-node tree is created and the corresponding tree is pushed onto the stack. 3 For convenience, we will have the stack grow from left to right in the diagrams.

4.2 Binary Trees + a b c d e Now a + is read, so two trees are merged. + a b c + d e Continuing, a * is read, so we pop two trees and form a new tree with a * as root. + a b * c + d e Finally, the last symbol is read, two trees are merged, and the ﬁnal tree is left on the stack.

Chapter 4 Trees * + a b * c + d e 4.3 The Search Tree ADT—Binary Search Trees An important application of binary trees is their use in searching. Let us assume that each node in the tree stores an item. In our examples, we will assume for simplicity that these are integers, although arbitrarily complex items are easily handled in Java. We will also assume that all the items are distinct and deal with duplicates later. The property that makes a binary tree into a binary search tree is that for every node, X, in the tree, the values of all the items in its left subtree are smaller than the item in X, and the values of all the items in its right subtree are larger than the item in X. Notice that this implies that all the elements in the tree can be ordered in some consistent manner. In Figure 4.15, the tree on the left is a binary search tree, but the tree on the right is not. The tree on the right has a node with item 7 in the left subtree of a node with item 6 (which happens to be the root). We now give brief descriptions of the operations that are usually performed on binary search trees. Note that because of the recursive deﬁnition of trees, it is common to write these routines recursively. Because the average depth of a binary search tree turns out to be O(log N), we generally do not need to worry about running out of stack space. The binary search tree requires that all the items can be ordered. To write a generic class, we need to provide an interface type that represents this property. This interface is Comparable, as described in Chapter 1. The interface tells us that two items in the tree can always be compared using a compareTo method. From this, we can determine all other possible relationships. Speciﬁcally, we do not use the equals method. Instead, two items are equal if and only if the compareTo method returns 0. An alternative, described in Section 4.3.1, is to allow a function object. Figure 4.16 also shows the BinaryNode class that, like the node class in the linked list class, is a nested class.

4.3 The Search Tree ADT—Binary Search Trees Figure 4.15 Two binary trees (only the left tree is a search tree) private static class BinaryNode<AnyType> { // Constructors BinaryNode( AnyType theElement ) { this( theElement, null, null ); } BinaryNode( AnyType theElement, BinaryNode<AnyType> lt, BinaryNode<AnyType> rt ) { element = theElement; left = lt; right = rt; } AnyType element; // The data in the node BinaryNode<AnyType> left; // Left child BinaryNode<AnyType> right; // Right child } Figure 4.16 The BinaryNode class Figure 4.17 shows the BinarySearchTree class skeleton. The single data ﬁeld is a reference to the root node; this reference is null for empty trees. The public methods use the general technique of calling private recursive methods. We can now describe some of the private methods. 4.3.1 contains This operation requires returning true if there is a node in tree T that has item X, or false if there is no such node. The structure of the tree makes this simple. If T is empty, then we can just return false. Otherwise, if the item stored at T is X, we can return true. Otherwise, we make a recursive call on a subtree of T, either left or right, depending on the relationship of X to the item stored in T. The code in Figure 4.18 is an implementation of this strategy. Notice the order of the tests. It is crucial that the test for an empty tree be performed ﬁrst, since otherwise, we would generate a NullPointerException attempting to access a data ﬁeld through a null reference. The remaining tests are arranged with the least likely case

public class BinarySearchTree<AnyType extends Comparable<? super AnyType>> { private static class BinaryNode<AnyType> { /* Figure 4.16 */ } private BinaryNode<AnyType> root; public BinarySearchTree( ) { root = null; } public void makeEmpty( ) { root = null; } public boolean isEmpty( ) { return root == null; } public boolean contains( AnyType x ) { return contains( x, root ); } public AnyType findMin( ) { if( isEmpty( ) ) throw new UnderflowException( ); return findMin( root ).element; } public AnyType findMax( ) { if( isEmpty( ) ) throw new UnderflowException( ); return findMax( root ).element; } public void insert( AnyType x ) { root = insert( x, root ); } public void remove( AnyType x ) { root = remove( x, root ); } public void printTree( ) { /* Figure 4.56 */ } private boolean contains( AnyType x, BinaryNode<AnyType> t ) { /* Figure 4.18 */ } private BinaryNode<AnyType> findMin( BinaryNode<AnyType> t ) { /* Figure 4.20 */ } private BinaryNode<AnyType> findMax( BinaryNode<AnyType> t ) { /* Figure 4.20 */ } private BinaryNode<AnyType> insert( AnyType x, BinaryNode<AnyType> t ) { /* Figure 4.22 */ } private BinaryNode<AnyType> remove( AnyType x, BinaryNode<AnyType> t ) { /* Figure 4.25 */ } private void printTree( BinaryNode<AnyType> t ) { /* Figure 4.56 */ } } Figure 4.17 Binary search tree class skeleton

4.3 The Search Tree ADT—Binary Search Trees /** * Internal method to find an item in a subtree. * @param x is item to search for. * @param t the node that roots the subtree. * @return true if the item is found; false otherwise. */ private boolean contains( AnyType x, BinaryNode<AnyType> t ) { if( t == null ) return false; int compareResult = x.compareTo( t.element ); if( compareResult < 0 ) return contains( x, t.left ); else if( compareResult > 0 ) return contains( x, t.right ); else return true; // Match } Figure 4.18 contains operation for binary search trees last. Also note that both recursive calls are actually tail recursions and can be easily removed with a while loop. The use of tail recursion is justiﬁable here because the simplicity of algorithmic expression compensates for the decrease in speed, and the amount of stack space used is expected to be only O(log N). Figure 4.19 shows the trivial changes required to use a function object rather than requiring that the items be Comparable. This mimics the idioms in Section 1.6. 4.3.2 findMin and findMax These private routines return a reference to the node containing the smallest and largest elements in the tree, respectively. To perform a findMin, start at the root and go left as long as there is a left child. The stopping point is the smallest element. The findMax routine is the same, except that branching is to the right child. This is so easy that many programmers do not bother using recursion. We will code the routines both ways by doing findMin recursively and findMax nonrecursively (see Figure 4.20). Notice how we carefully handle the degenerate case of an empty tree. Although this is always important to do, it is especially crucial in recursive programs. Also notice that it is safe to change t in findMax, since we are only working with a copy of a reference. Always be extremely careful, however, because a statement such as t.right = t.right.right will make changes.

Chapter 4 Trees public class BinarySearchTree<AnyType> { private BinaryNode<AnyType> root; private Comparator<? super AnyType> cmp; public BinarySearchTree( ) { this( null ); } public BinarySearchTree( Comparator<? super AnyType> c ) { root = null; cmp = c; } private int myCompare( AnyType lhs, AnyType rhs ) { if( cmp != null ) return cmp.compare( lhs, rhs ); else return ((Comparable)lhs).compareTo( rhs ); } private boolean contains( AnyType x, BinaryNode<AnyType> t ) { if( t == null ) return false; int compareResult = myCompare( x, t.element ); if( compareResult < 0 ) return contains( x, t.left ); else if( compareResult > 0 ) return contains( x, t.right ); else return true; // Match } // Remainder of class is similar with calls to compareTo replaced by myCompare } Figure 4.19 Illustrates use of a function object to implement binary search tree 4.3.3 insert The insertion routine is conceptually simple. To insert X into tree T, proceed down the tree as you would with a contains. If X is found, do nothing (or “update” something). Otherwise, insert X at the last spot on the path traversed. Figure 4.21 shows what happens.

4.3 The Search Tree ADT—Binary Search Trees /** * Internal method to find the smallest item in a subtree. * @param t the node that roots the subtree. * @return node containing the smallest item. */ private BinaryNode<AnyType> findMin( BinaryNode<AnyType> t ) { if( t == null ) return null; else if( t.left == null ) return t; return findMin( t.left ); } /** * Internal method to find the largest item in a subtree. * @param t the node that roots the subtree. * @return node containing the largest item. */ private BinaryNode<AnyType> findMax( BinaryNode<AnyType> t ) { if( t != null ) while( t.right != null ) t = t.right; return t; } Figure 4.20 Recursive implementation of findMin and nonrecursive implementation of findMax for binary search trees To insert 5, we traverse the tree as though a contains were occurring. At the node with item 4, we need to go right, but there is no subtree, so 5 is not in the tree, and this is the correct spot. Duplicates can be handled by keeping an extra ﬁeld in the node record indicating the frequency of occurrence. This adds some extra space to the entire tree but is better than putting duplicates in the tree (which tends to make the tree very deep). Of course, this strategy does not work if the key that guides the compareTo method is only part of a larger structure. If that is the case, then we can keep all of the structures that have the same key in an auxiliary data structure, such as a list or another search tree. Figure 4.22 shows the code for the insertion routine. Since t references the root of the tree, and the root changes on the ﬁrst insertion, insert is written as a method that returns a reference to the root of the new tree. Lines 15 and 17 recursively insert and attach x into the appropriate subtree.

Chapter 4 Trees Figure 4.21 Binary search trees before and after inserting 5 /** * Internal method to insert into a subtree. * @param x the item to insert. * @param t the node that roots the subtree. * @return the new root of the subtree. */ private BinaryNode<AnyType> insert( AnyType x, BinaryNode<AnyType> t ) { if( t == null ) return new BinaryNode<>( x, null, null ); int compareResult = x.compareTo( t.element ); if( compareResult < 0 ) t.left = insert( x, t.left ); else if( compareResult > 0 ) t.right = insert( x, t.right ); else ; // Duplicate; do nothing return t; } Figure 4.22 Insertion into a binary search tree 4.3.4 remove As is common with many data structures, the hardest operation is deletion. Once we have found the node to be deleted, we need to consider several possibilities. If the node is a leaf, it can be deleted immediately. If the node has one child, the node can be deleted after its parent adjusts a link to bypass the node (we will draw the link directions explicitly for clarity). See Figure 4.23.

4.3 The Search Tree ADT—Binary Search Trees Figure 4.23 Deletion of a node (4) with one child, before and after Figure 4.24 Deletion of a node (2) with two children, before and after The complicated case deals with a node with two children. The general strategy is to replace the data of this node with the smallest data of the right subtree (which is easily found) and recursively delete that node (which is now empty). Because the smallest node in the right subtree cannot have a left child, the second remove is an easy one. Figure 4.24 shows an initial tree and the result of a deletion. The node to be deleted is the left child of the root; the key value is 2. It is replaced with the smallest data in its right subtree (3), and then that node is deleted as before. The code in Figure 4.25 performs deletion. It is inefﬁcient, because it makes two passes down the tree to ﬁnd and delete the smallest node in the right subtree when this is appropriate. It is easy to remove this inefﬁciency by writing a special removeMin method, and we have left it in only for simplicity. If the number of deletions is expected to be small, then a popular strategy to use is lazy deletion: When an element is to be deleted, it is left in the tree and merely marked

Chapter 4 Trees /** * Internal method to remove from a subtree. * @param x the item to remove. * @param t the node that roots the subtree. * @return the new root of the subtree. */ private BinaryNode<AnyType> remove( AnyType x, BinaryNode<AnyType> t ) { if( t == null ) return t; // Item not found; do nothing int compareResult = x.compareTo( t.element ); if( compareResult < 0 ) t.left = remove( x, t.left ); else if( compareResult > 0 ) t.right = remove( x, t.right ); else if( t.left != null && t.right != null ) // Two children { t.element = findMin( t.right ).element; t.right = remove( t.element, t.right ); } else t = ( t.left != null ) ? t.left : t.right; return t; } Figure 4.25 Deletion routine for binary search trees as being deleted. This is especially popular if duplicate items are present, because then the ﬁeld that keeps count of the frequency of appearance can be decremented. If the number of real nodes in the tree is the same as the number of “deleted” nodes, then the depth of the tree is only expected to go up by a small constant (why?), so there is a very small time penalty associated with lazy deletion. Also, if a deleted item is reinserted, the overhead of allocating a new cell is avoided. 4.3.5 Average-Case Analysis Intuitively, we expect that all of the operations of the previous section should take O(log N) time, because in constant time we descend a level in the tree, thus operating on a tree that is now roughly half as large. Indeed, the running time of all the operations is O(d), where d is the depth of the node containing the accessed item (in the case of remove this may be the replacement node in the two-child case). We prove in this section that the average depth over all nodes in a tree is O(log N) on the assumption that all insertion sequences are equally likely.

4.3 The Search Tree ADT—Binary Search Trees The sum of the depths of all nodes in a tree is known as the internal path length. We will now calculate the average internal path length of a binary search tree, where the average is taken over all possible insertion sequences into binary search trees. Let D(N) be the internal path length for some tree T of N nodes. D(1) = 0. An N-node tree consists of an i-node left subtree and an (N −i −1)-node right subtree, plus a root at depth zero for 0 ≤i < N. D(i) is the internal path length of the left subtree with respect to its root. In the main tree, all these nodes are one level deeper. The same holds for the right subtree. Thus, we get the recurrence D(N) = D(i) + D(N −i −1) + N −1 If all subtree sizes are equally likely, which is true for binary search trees (since the subtree size depends only on the relative rank of the ﬁrst element inserted into the tree), but not binary trees, then the average value of both D(i) and D(N−i−1) is (1/N) N−1 j=0 D(j). This yields D(N) = 2 N ⎡ ⎣ N−1  j=0 D(j) ⎤ ⎦+ N −1 This recurrence will be encountered and solved in Chapter 7, obtaining an average value of D(N) = O(N log N). Thus, the expected depth of any node is O(log N). As an example, the randomly generated 500-node tree shown in Figure 4.26 has nodes at expected depth 9.98. It is tempting to say immediately that this result implies that the average running time of all the operations discussed in the previous section is O(log N), but this is not entirely Figure 4.26 A randomly generated binary search tree

Chapter 4 Trees Figure 4.27 Binary search tree after (N2) insert/remove pairs true. The reason for this is that because of deletions, it is not clear that all binary search trees are equally likely. In particular, the deletion algorithm described above favors making the left subtrees deeper than the right, because we are always replacing a deleted node with a node from the right subtree. The exact effect of this strategy is still unknown, but it seems only to be a theoretical novelty. It has been shown that if we alternate insertions and deletions (N2) times, then the trees will have an expected depth of ( √ N). After a quarter-million random insert/remove pairs, the tree that was somewhat right-heavy in Figure 4.26 looks decidedly unbalanced (average depth equals 12.51). See Figure 4.27. We could try to eliminate the problem by randomly choosing between the smallest element in the right subtree and the largest in the left when replacing the deleted element. This apparently eliminates the bias and should keep the trees balanced, but nobody has actually proved this. In any event, this phenomenon appears to be mostly a theoretical novelty, because the effect does not show up at all for small trees, and stranger still, if o(N2) insert/remove pairs are used, then the tree seems to gain balance! The main point of this discussion is that deciding what “average” means is generally extremely difﬁcult and can require assumptions that may or may not be valid. In the absence of deletions, or when lazy deletion is used, we can conclude that the average running times of the operations above are O(log N). Except for strange cases like the one discussed above, this result is very consistent with observed behavior. If the input comes into a tree presorted, then a series of inserts will take quadratic time and give a very expensive implementation of a linked list, since the tree will consist only of nodes with no left children. One solution to the problem is to insist on an extra structural condition called balance: No node is allowed to get too deep. There are quite a few general algorithms to implement balanced trees. Most are quite a bit more complicated than a standard binary search tree, and all take longer on average for updates. They do, however, provide protection against the embarrassingly simple cases. Below, we will sketch one of the oldest forms of balanced search trees, the AVL tree.

4.4 AVL Trees A second, newer method is to forgo the balance condition and allow the tree to be arbitrarily deep, but after every operation, a restructuring rule is applied that tends to make future operations efﬁcient. These types of data structures are generally classiﬁed as self-adjusting. In the case of a binary search tree, we can no longer guarantee an O(log N) bound on any single operation but can show that any sequence of M operations takes total time O(M log N) in the worst case. This is generally sufﬁcient protection against a bad worst case. The data structure we will discuss is known as a splay tree; its analysis is fairly intricate and is discussed in Chapter 11. 4.4 AVL Trees An AVL (Adelson-Velskii and Landis) tree is a binary search tree with a balance condition. The balance condition must be easy to maintain, and it ensures that the depth of the tree is O(log N). The simplest idea is to require that the left and right subtrees have the same height. As Figure 4.28 shows, this idea does not force the tree to be shallow. Another balance condition would insist that every node must have left and right subtrees of the same height. If the height of an empty subtree is deﬁned to be −1 (as is usual), then only perfectly balanced trees of 2k −1 nodes would satisfy this criterion. Thus, although this guarantees trees of small depth, the balance condition is too rigid to be useful and needs to be relaxed. An AVL tree is identical to a binary search tree, except that for every node in the tree, the height of the left and right subtrees can differ by at most 1. (The height of an empty tree is deﬁned to be −1.) In Figure 4.29 the tree on the left is an AVL tree, but the tree on the right is not. Height information is kept for each node (in the node structure). It can be shown that the height of an AVL tree is at most roughly 1.44 log(N + 2) −1.328, but in practice it is only slightly more than log N. As an example, the AVL tree of height 9 with the fewest nodes (143) is shown in Figure 4.30. This tree has as a left subtree an AVL tree of height 7 of minimum size. The right subtree is an AVL tree of height 8 of minimum size. This tells us that the minimum number of nodes, S(h), in an AVL tree of height h is given by S(h) = S(h −1) + S(h −2) + 1. For h = 0, S(h) = 1. For h = 1, S(h) = 2. The function Figure 4.28 A bad binary tree. Requiring balance at the root is not enough

Chapter 4 Trees Figure 4.29 Two binary search trees. Only the left tree is AVL Figure 4.30 Smallest AVL tree of height 9 S(h) is closely related to the Fibonacci numbers, from which the bound claimed above on the height of an AVL tree follows. Thus, all the tree operations can be performed in O(log N) time, except possibly insertion (we will assume lazy deletion). When we do an insertion, we need to update all the balancing information for the nodes on the path back to the root, but the reason that insertion is potentially difﬁcult is that inserting a node could violate the AVL tree property. (For instance, inserting 6 into the AVL tree in Figure 4.29 would destroy the balance condition

4.4 AVL Trees at the node with key 8.) If this is the case, then the property has to be restored before the insertion step is considered over. It turns out that this can always be done with a simple modiﬁcation to the tree, known as a rotation. After an insertion, only nodes that are on the path from the insertion point to the root might have their balance altered because only those nodes have their subtrees altered. As we follow the path up to the root and update the balancing information, we may ﬁnd a node whose new balance violates the AVL condition. We will show how to rebalance the tree at the ﬁrst (i.e., deepest) such node, and we will prove that this rebalancing guarantees that the entire tree satisﬁes the AVL property. Let us call the node that must be rebalanced α. Since any node has at most two children, and a height imbalance requires that α’s two subtrees’ height differ by two, it is easy to see that a violation might occur in four cases: 1. An insertion into the left subtree of the left child of α. 2. An insertion into the right subtree of the left child of α. 3. An insertion into the left subtree of the right child of α. 4. An insertion into the right subtree of the right child of α. Cases 1 and 4 are mirror image symmetries with respect to α, as are cases 2 and 3. Consequently, as a matter of theory, there are two basic cases. From a programming perspective, of course, there are still four cases. The ﬁrst case, in which the insertion occurs on the “outside” (i.e., left–left or right– right), is ﬁxed by a single rotation of the tree. The second case, in which the insertion occurs on the “inside” (i.e., left–right or right–left) is handled by the slightly more complex double rotation. These are fundamental operations on the tree that we’ll see used several times in balanced-tree algorithms. The remainder of this section describes these rotations, proves that they sufﬁce to maintain balance, and gives a casual implementation of the AVL tree. Chapter 12 describes other balanced-tree methods with an eye toward a more careful implementation. 4.4.1 Single Rotation Figure 4.31 shows the single rotation that ﬁxes case 1. The before picture is on the left, and the after is on the right. Let us analyze carefully what is going on. Node k2 violates the AVL balance property because its left subtree is two levels deeper than its right subtree (the dashed lines in the middle of the diagram mark the levels). The situation depicted is the only possible case 1 scenario that allows k2 to satisfy the AVL property before an insertion but violate it afterwards. Subtree X has grown to an extra level, causing it to be exactly two levels deeper than Z. Y cannot be at the same level as the new X because then k2 would have been out of balance before the insertion, and Y cannot be at the same level as Z because then k1 would be the ﬁrst node on the path toward the root that was in violation of the AVL balancing condition. To ideally rebalance the tree, we would like to move X up a level and Z down a level. Note that this is actually more than the AVL property would require. To do this, we rearrange

Chapter 4 Trees k 2 k 1 Z Y X k 1 k 2 X Z Y Figure 4.31 Single rotation to ﬁx case 1 nodes into an equivalent tree as shown in the second part of Figure 4.31. Here is an abstract scenario: Visualize the tree as being ﬂexible, grab the child node k1, close your eyes, and shake it, letting gravity take hold. The result is that k1 will be the new root. The binary search tree property tells us that in the original tree k2 > k1, so k2 becomes the right child of k1 in the new tree. X and Z remain as the left child of k1 and right child of k2, respectively. Subtree Y, which holds items that are between k1 and k2 in the original tree, can be placed as k2’s left child in the new tree and satisfy all the ordering requirements. As a result of this work, which requires only a few link changes, we have another binary search tree that is an AVL tree. This happens because X moves up one level, Y stays at the same level, and Z moves down one level. k2 and k1 not only satisfy the AVL requirements, but they also have subtrees that are exactly the same height. Furthermore, the new height of the entire subtree is exactly the same as the height of the original subtree prior to the insertion that caused X to grow. Thus no further updating of heights on the path to the root is needed, and consequently no further rotations are needed. Figure 4.32 shows that after the insertion of 6 into the original AVL tree on the left, node 8 becomes unbalanced. Thus, we do a single rotation between 7 and 8, obtaining the tree on the right. As we mentioned earlier, case 4 represents a symmetric case. Figure 4.33 shows how a single rotation is applied. Let us work through a rather long example. Suppose we start with an initially empty AVL tree and insert the items 3, 2, 1, and then 4 through 7 in sequential Figure 4.32 AVL property destroyed by insertion of 6, then ﬁxed by a single rotation

4.4 AVL Trees k 2 k 1 Z Y X k 1 k 2 X Z Y Figure 4.33 Single rotation ﬁxes case 4 order. The ﬁrst problem occurs when it is time to insert 1 because the AVL property is violated at the root. We perform a single rotation between the root and its left child to ﬁx the problem. Here are the before and after trees: before after A dashed line joins the two nodes that are the subject of the rotation. Next we insert 4, which causes no problems, but the insertion of 5 creates a violation at node 3 that is ﬁxed by a single rotation. Besides the local change caused by the rotation, the programmer must remember that the rest of the tree has to be informed of this change. Here this means that 2’s right child must be reset to link to 4 instead of 3. Forgetting to do so is easy and would destroy the tree (4 would be inaccessible). before after Next we insert 6. This causes a balance problem at the root, since its left subtree is of height 0 and its right subtree would be height 2. Therefore, we perform a single rotation at the root between 2 and 4.

Chapter 4 Trees before after The rotation is performed by making 2 a child of 4 and 4’s original left subtree the new right subtree of 2. Every item in this subtree must lie between 2 and 4, so this transformation makes sense. The next item we insert is 7, which causes another rotation: before after 4.4.2 Double Rotation The algorithm described above has one problem: As Figure 4.34 shows, it does not work for cases 2 or 3. The problem is that subtree Y is too deep, and a single rotation does not make it any less deep. The double rotation that solves the problem is shown in Figure 4.35. The fact that subtree Y in Figure 4.34 has had an item inserted into it guarantees that it is nonempty. Thus, we may assume that it has a root and two subtrees. Consequently, the tree may be viewed as four subtrees connected by three nodes. As the diagram suggests, k2 k1 Z Y X k1 k2 Z Y X Figure 4.34 Single rotation fails to ﬁx case 2

4.4 AVL Trees D C B k2 k1 k1 k2 k3 k3 A A D B C Figure 4.35 Left–right double rotation to ﬁx case 2 k1 k3 k2 A B C k2 k1 k3 A D D B C Figure 4.36 Right–left double rotation to ﬁx case 3 exactly one of tree B or C is two levels deeper than D (unless all are empty), but we cannot be sure which one. It turns out not to matter; in Figure 4.35, both B and C are drawn at 1 1 2 levels below D. To rebalance, we see that we cannot leave k3 as the root, and a rotation between k3 and k1 was shown in Figure 4.34 to not work, so the only alternative is to place k2 as the new root. This forces k1 to be k2’s left child and k3 to be its right child, and it also completely determines the resulting locations of the four subtrees. It is easy to see that the resulting tree satisﬁes the AVL tree property, and as was the case with the single rotation, it restores the height to what it was before the insertion, thus guaranteeing that all rebalancing and height updating is complete. Figure 4.36 shows that the symmetric case 3 can also be ﬁxed by a double rotation. In both cases the effect is the same as rotating between α’s child and grandchild, and then between α and its new child. We will continue our previous example by inserting 10 through 16 in reverse order, followed by 8 and then 9. Inserting 16 is easy, since it does not destroy the balance property, but inserting 15 causes a height imbalance at node 7. This is case 3, which is solved by a right–left double rotation. In our example, the right–left double rotation will involve 7, 16, and 15. In this case, k1 is the node with item 7, k3 is the node with item 16, and k2 is the node with item 15. Subtrees A, B, C, and D are empty.

Chapter 4 Trees k1 k3 k2 k1 k2 k3 before after Next we insert 14, which also requires a double rotation. Here the double rotation that will restore the tree is again a right–left double rotation that will involve 6, 15, and 7. In this case, k1 is the node with item 6, k2 is the node with item 7, and k3 is the node with item 15. Subtree A is the tree rooted at the node with item 5; subtree B is the empty subtree that was originally the left child of the node with item 7, subtree C is the tree rooted at the node with item 14, and ﬁnally, subtree D is the tree rooted at the node with item 16. k1 k2 k3 before after k1 k3 k2 If 13 is now inserted, there is an imbalance at the root. Since 13 is not between 4 and 7, we know that the single rotation will work. before after

4.4 AVL Trees Insertion of 12 will also require a single rotation: before after To insert 11, a single rotation needs to be performed, and the same is true for the subsequent insertion of 10. We insert 8 without a rotation creating an almost perfectly balanced tree: before Finally, we will insert 9 to show the symmetric case of the double rotation. Notice that 9 causes the node containing 10 to become unbalanced. Since 9 is between 10 and 8

Chapter 4 Trees (which is 10’s child on the path to 9), a double rotation needs to be performed, yielding the following tree: after Let us summarize what happens. The programming details are fairly straightforward except that there are several cases. To insert a new node with item X into an AVL tree T, we recursively insert X into the appropriate subtree of T (let us call this TLR). If the height of TLR does not change, then we are done. Otherwise, if a height imbalance appears in T, we do the appropriate single or double rotation depending on X and the items in T and TLR, update the heights (making the connection from the rest of the tree above), and are done. Since one rotation always sufﬁces, a carefully coded nonrecursive version generally turns out to be faster than the recursive version, but on modern compilers the difference is not as signiﬁcant as in the past. However, nonrecursive versions are quite difﬁcult to code correctly, whereas a casual recursive implementation is easily readable. Another efﬁciency issue concerns storage of the height information. Since all that is really required is the difference in height, which is guaranteed to be small, we could get by with two bits (to represent +1, 0, −1) if we really try. Doing so will avoid repetitive calculation of balance factors but results in some loss of clarity. The resulting code is somewhat more complicated than if the height were stored at each node. If a recursive routine is written, then speed is probably not the main consideration. In this case, the slight speed advantage obtained by storing balance factors hardly seems worth the loss of clarity and relative simplicity. Furthermore, since most machines will align this to at least an 8-bit boundary anyway, there is not likely to be any difference in the amount of space used. An eight-bit byte will allow us to store absolute heights of up to 127. Since the tree is balanced, it is inconceivable that this would be insufﬁcient (see the exercises). With all this, we are ready to write the AVL routines. We show some of the code here; the rest is online. First, we need the AvlNode class. This is given in Figure 4.37. We also

4.4 AVL Trees private static class AvlNode<AnyType> { // Constructors AvlNode( AnyType theElement ) { this( theElement, null, null ); } AvlNode( AnyType theElement, AvlNode<AnyType> lt, AvlNode<AnyType> rt ) { element = theElement; left = lt; right = rt; height = 0; } AnyType element; // The data in the node AvlNode<AnyType> left; // Left child AvlNode<AnyType> right; // Right child int height; // Height } Figure 4.37 Node declaration for AVL trees /** * Return the height of node t, or -1, if null. */ private int height( AvlNode<AnyType> t ) { return t == null ? -1 : t.height; } Figure 4.38 Method to compute height of an AVL node need a quick method to return the height of a node. This method is necessary to handle the annoying case of a null reference. This is shown in Figure 4.38. The basic insertion routine is easy to write (see Figure 4.39): It adds only a single line at the end that invokes a balancing method. The balancing method applies a single or double rotation if needed, updates the height, and returns the resulting tree. For the trees in Figure 4.40, rotateWithLeftChild converts the tree on the left to the tree on the right, returning a reference to the new root. rotateWithRightChild is symmetric. The code is shown in Figure 4.41. Similarly, the double rotation pictured in Figure 4.42 can be implemented by the code shown in Figure 4.43. Since deletion in a binary search tree is somewhat more complicated than insertion, one can assume that deletion in an AVL tree is also more complicated. In a perfect world, one would hope that the deletion routine in Figure 4.25 could easily be modiﬁed by changing the last line to return after calling the balance method, as was done for insertion. This would yield the code in Figure 4.44. This change works! A deletion could cause one side of the tree to become two levels shallower than the other side. The case-by-case analysis is similar to the imbalances that are caused by insertion, but not exactly the same. For

/** * Internal method to insert into a subtree. * @param x the item to insert. * @param t the node that roots the subtree. * @return the new root of the subtree. */ private AvlNode<AnyType> insert( AnyType x, AvlNode<AnyType> t ) { if( t == null ) return new AvlNode<>( x, null, null ); int compareResult = x.compareTo( t.element ); if( compareResult < 0 ) t.left = insert( x, t.left ); else if( compareResult > 0 ) t.right = insert( x, t.right ); else ; // Duplicate; do nothing return balance( t ); } private static final int ALLOWED_IMBALANCE = 1; // Assume t is either balanced or within one of being balanced private AvlNode<AnyType> balance( AvlNode<AnyType> t ) { if( t == null ) return t; if( height( t.left ) - height( t.right ) > ALLOWED_IMBALANCE ) if( height( t.left.left ) >= height( t.left.right ) ) t = rotateWithLeftChild( t ); else t = doubleWithLeftChild( t ); else if( height( t.right ) - height( t.left ) > ALLOWED_IMBALANCE ) if( height( t.right.right ) >= height( t.right.left ) ) t = rotateWithRightChild( t ); else t = doubleWithRightChild( t ); t.height = Math.max( height( t.left ), height( t.right ) ) + 1; return t; } Figure 4.39 Insertion into an AVL tree

4.4 AVL Trees k 2 Z k 1 X Y k 1 k 2 Y X Z Figure 4.40 Single rotation /** * Rotate binary tree node with left child. * For AVL trees, this is a single rotation for case 1. * Update heights, then return new root. */ private AvlNode<AnyType> rotateWithLeftChild( AvlNode<AnyType> k2 ) { AvlNode<AnyType> k1 = k2.left; k2.left = k1.right; k1.right = k2; k2.height = Math.max( height( k2.left ), height( k2.right ) ) + 1; k1.height = Math.max( height( k1.left ), k2.height ) + 1; return k1; } Figure 4.41 Routine to perform single rotation k 3 D k 1 k 2 A C B k 2 k 1 k 3 A B C D Figure 4.42 Double rotation instance, case 1 in Figure 4.31, which would now reﬂect a deletion from tree Z (rather than an insertion into X), must be augmented with the possiblity that tree Y could be as deep as tree X. Even so, it is easy to see that the rotation rebalances this case and the symmetric case 4 in Figure 4.33. Thus the code for balance in Figure 4.39 lines 32 and 38 uses >= instead of > speciﬁcally to ensure that single rotations are done in these cases, rather than double rotations. We leave veriﬁcation of the remaining cases as an exercise.

Chapter 4 Trees /** * Double rotate binary tree node: first left child * with its right child; then node k3 with new left child. * For AVL trees, this is a double rotation for case 2. * Update heights, then return new root. */ private AvlNode<AnyType> doubleWithLeftChild( AvlNode<AnyType> k3 ) { k3.left = rotateWithRightChild( k3.left ); return rotateWithLeftChild( k3 ); } Figure 4.43 Routine to perform double rotation /** * Internal method to remove from a subtree. * @param x the item to remove. * @param t the node that roots the subtree. * @return the new root of the subtree. */ private AvlNode<AnyType> remove( AnyType x, AvlNode<AnyType> t ) { if( t == null ) return t; // Item not found; do nothing int compareResult = x.compareTo( t.element ); if( compareResult < 0 ) t.left = remove( x, t.left ); else if( compareResult > 0 ) t.right = remove( x, t.right ); else if( t.left != null && t.right != null ) // Two children { t.element = findMin( t.right ).element; t.right = remove( t.element, t.right ); } else t = ( t.left != null ) ? t.left : t.right; return balance( t ); } Figure 4.44 Deletion in an AVL tree

4.5 Splay Trees 4.5 Splay Trees We now describe a relatively simple data structure, known as a splay tree, that guarantees that any M consecutive tree operations starting from an empty tree take at most O(M log N) time. Although this guarantee does not preclude the possibility that any single operation might take θ(N) time, and thus the bound is not as strong as an O(log N) worst-case bound per operation, the net effect is the same: There are no bad input sequences. Generally, when a sequence of M operations has total worst-case running time of O(Mf(N)), we say that the amortized running time is O(f(N)). Thus, a splay tree has an O(log N) amortized cost per operation. Over a long sequence of operations, some may take more, some less. Splay trees are based on the fact that the O(N) worst-case time per operation for binary search trees is not bad, as long as it occurs relatively infrequently. Any one access, even if it takes θ(N), is still likely to be extremely fast. The problem with binary search trees is that it is possible, and not uncommon, for a whole sequence of bad accesses to take place. The cumulative running time then becomes noticeable. A search tree data structure with O(N) worst-case time, but a guarantee of at most O(M log N) for any M consecutive operations, is certainly satisfactory, because there are no bad sequences. If any particular operation is allowed to have an O(N) worst-case time bound, and we still want an O(log N) amortized time bound, then it is clear that whenever a node is accessed, it must be moved. Otherwise, once we ﬁnd a deep node, we could keep performing accesses on it. If the node does not change location, and each access costs θ(N), then a sequence of M accesses will cost θ(M · N). The basic idea of the splay tree is that after a node is accessed, it is pushed to the root by a series of AVL tree rotations. Notice that if a node is deep, there are many nodes on the path that are also relatively deep, and by restructuring we can make future accesses cheaper on all these nodes. Thus, if the node is unduly deep, then we want this restructuring to have the side effect of balancing the tree (to some extent). Besides giving a good time bound in theory, this method is likely to have practical utility, because in many applications, when a node is accessed, it is likely to be accessed again in the near future. Studies have shown that this happens much more often than one would expect. Splay trees also do not require the maintenance of height or balance information, thus saving space and simplifying the code to some extent (especially when careful implementations are written). 4.5.1 A Simple Idea (That Does Not Work) One way of performing the restructuring described above is to perform single rotations, bottom up. This means that we rotate every node on the access path with its parent. As an example, consider what happens after an access (a find) on k1 in the following tree.

Chapter 4 Trees k 3 D k 2 k 1 A C B k 4 E k 5 F The access path is dashed. First, we would perform a single rotation between k1 and its parent, obtaining the following tree. k 3 D k 1 k 2 C B A k 4 E k 5 F Then, we rotate between k1 and k3, obtaining the next tree. k 3 D C k 1 k 2 B A k 4 E k 5 F Then two more rotations are performed until we reach the root.

4.5 Splay Trees k 3 D C k 1 k 2 B A k 4 k 5 E F k 3 D C k 1 k 2 B A k 4 E k 5 F These rotations have the effect of pushing k1 all the way to the root, so that future accesses on k1 are easy (for a while). Unfortunately, it has pushed another node (k3) almost as deep as k1 used to be. An access on that node will then push another node deep, and so on. Although this strategy makes future accesses of k1 cheaper, it has not signiﬁcantly improved the situation for the other nodes on the (original) access path. It turns out that it is possible to prove that using this strategy, there is a sequence of M operations requiring (M · N) time, so this idea is not quite good enough. The simplest way to show this is to consider the tree formed by inserting keys 1, 2, 3, . . . , N into an initially empty tree (work this example out). This gives a tree consisting of only left children. This is not necessarily bad, though, since the time to build this tree is O(N) total. The bad part is that accessing the node with key 1 takes N −1 units of time. After the rotations are complete, an access of the node with key 2 takes N −2 units of time. The total for accessing all the keys in order is N−1 i=1 i −(N2). After they are accessed, the tree reverts to its original state, and we can repeat the sequence. 4.5.2 Splaying The splaying strategy is similar to the rotation idea above, except that we are a little more selective about how rotations are performed. We will still rotate bottom up along the access

Chapter 4 Trees G D P X A C B X P G A B C D Figure 4.45 Zig-zag X B A P C G D X A B G C D P Figure 4.46 Zig-zig path. Let X be a (nonroot) node on the access path at which we are rotating. If the parent of X is the root of the tree, we merely rotate X and the root. This is the last rotation along the access path. Otherwise, X has both a parent (P) and a grandparent (G), and there are two cases, plus symmetries, to consider. The ﬁrst case is the zig-zag case (see Figure 4.45). Here, X is a right child and P is a left child (or vice versa). If this is the case, we perform a double rotation, exactly like an AVL double rotation. Otherwise, we have a zig-zig case: X and P are both left children (or, in the symmetric case, both right children). In that case, we transform the tree on the left of Figure 4.46 to the tree on the right. As an example, consider the tree from the last example, with a contains on k1: k 2 k 1 A k 4 k 3 E D k 5 F C B The ﬁrst splay step is at k1 and is clearly a zig-zag, so we perform a standard AVL double rotation using k1, k2, and k3. The resulting tree follows.

4.5 Splay Trees k 3 D k 2 k 1 A C B k 4 E k 5 F The next splay step at k1 is a zig-zig, so we do the zig-zig rotation with k1, k4, and k5, obtaining the ﬁnal tree. k 3 D k 2 k 1 A C B k 4 k 5 E F Although it is hard to see from small examples, splaying not only moves the accessed node to the root but also has the effect of roughly halving the depth of most nodes on the access path (some shallow nodes are pushed down at most two levels). To see the difference that splaying makes over simple rotation, consider again the effect of inserting items 1, 2, 3, . . . , N into an initially empty tree. This takes a total of O(N), as before, and yields the same tree as simple rotations. Figure 4.47 shows the result of splaying Figure 4.47 Result of splaying at node 1

Chapter 4 Trees at the node with item 1. The difference is that after an access of the node with item 1, which takes N−1 units, the access on the node with item 2 will only take about N/2 units instead of N −2 units; there are no nodes quite as deep as before. An access on the node with item 2 will bring nodes to within N/4 of the root, and this is repeated until the depth becomes roughly log N (an example with N = 7 is too small to see the effect well). Figures 4.48 to 4.56 show the result of accessing items 1 through 9 in Figure 4.48 Result of splaying at node 1 a tree of all left children Figure 4.49 Result of splaying the previous tree at node 2

4.5 Splay Trees Figure 4.50 Result of splaying the previous tree at node 3 Figure 4.51 Result of splaying the previous tree at node 4 Figure 4.52 Result of splaying the previous tree at node 5 a 32-node tree that originally contains only left children. Thus we do not get the same bad behavior from splay trees that is prevalent in the simple rotation strategy. (Actually, this turns out to be a very good case. A rather complicated proof shows that for this example, the N accesses take a total of O(N) time.) These ﬁgures highlight the fundamental and crucial property of splay trees. When access paths are long, thus leading to a longer-than-normal search time, the rotations tend to be good for future operations. When accesses are cheap, the rotations are not as good and can be bad. The extreme case is the initial tree formed by the insertions. All the insertions were constant-time operations leading to a bad initial tree. At that point in time, we had a very bad tree, but we were running ahead of schedule and had the compensation of less total running time. Then a couple of really horrible accesses left a nearly balanced tree,

Chapter 4 Trees Figure 4.53 Result of splaying the previous tree at node 6 Figure 4.54 Result of splaying the previous tree at node 7 Figure 4.55 Result of splaying the previous tree at node 8 but the cost was that we had to give back some of the time that had been saved. The main theorem, which we will prove in Chapter 11, is that we never fall behind a pace of O(log N) per operation: We are always on schedule, even though there are occasionally bad operations. We can perform deletion by accessing the node to be deleted. This puts the node at the root. If it is deleted, we get two subtrees TL and TR (left and right). If we ﬁnd the largest element in TL (which is easy), then this element is rotated to the root of TL, and TL will now have a root with no right child. We can ﬁnish the deletion by making TR the right child.

4.6 Tree Traversals (Revisited) Figure 4.56 Result of splaying the previous tree at node 9 The analysis of splay trees is difﬁcult, because it must take into account the ever-changing structure of the tree. On the other hand, splay trees are much simpler to program than AVL trees, since there are fewer cases to consider and no balance information to maintain. Some empirical evidence suggests that this translates into faster code in practice, although the case for this is far from complete. Finally, we point out that there are several variations of splay trees that can perform even better in practice. One variation is completely coded in Chapter 12. 4.6 Tree Traversals (Revisited) Because of the ordering information in a binary search tree, it is simple to list all the items in sorted order. The recursive method in Figure 4.57 does the real work. Convince yourself that this method works. As we have seen before, this kind of routine when applied to trees is known as an inorder traversal (which makes sense, since it lists the items in order). The general strategy of an inorder traversal is to process the left subtree ﬁrst, then perform processing at the current node, and ﬁnally process the right subtree. The interesting part about this algorithm, aside from its simplicity, is that the total running time is O(N). This is because there is constant work being performed at every node in the tree. Each node is visited once, and the work performed at each node is testing against null, setting up two method calls, and doing a println. Since there is constant work per node and N nodes, the running time is O(N). Sometimes we need to process both subtrees ﬁrst before we can process a node. For instance, to compute the height of a node, we need to know the height of the subtrees ﬁrst. The code in Figure 4.58 computes this. Since it is always a good idea to check the special cases—and crucial when recursion is involved—notice that the routine will declare the height of a leaf to be zero, which is correct. This general order of traversal, which we have also seen before, is known as a postorder traversal. Again, the total running time is O(N), because constant work is performed at each node.

Chapter 4 Trees /** * Print the tree contents in sorted order. */ public void printTree( ) { if( isEmpty( ) ) System.out.println( "Empty tree" ); else printTree( root ); } /** * Internal method to print a subtree in sorted order. * @param t the node that roots the subtree. */ private void printTree( BinaryNode<AnyType> t ) { if( t != null ) { printTree( t.left ); System.out.println( t.element ); printTree( t.right ); } } Figure 4.57 Routine to print a binary search tree in order /** * Internal method to compute height of a subtree. * @param t the node that roots the subtree. */ private int height( BinaryNode<AnyType> t ) { if( t == null ) return -1; else return 1 + Math.max( height( t.left ), height( t.right ) ); } Figure 4.58 Routine to compute the height of a tree using a postorder traversal The third popular traversal scheme that we have seen is preorder traversal. Here, the node is processed before the children. This could be useful, for example, if you wanted to label each node with its depth.

4.7 B-Trees The common idea in all these routines is that you handle the null case ﬁrst, and then the rest. Notice the lack of extraneous variables. These routines pass only the reference to the node that roots the subtree and do not declare or pass any extra variables. The more compact the code, the less likely that a silly bug will turn up. A fourth, less often used, traversal (which we have not seen yet) is level-order traversal. In a level-order traversal, all nodes at depth d are processed before any node at depth d + 1. Level-order traversal differs from the other traversals in that it is not done recursively; a queue is used, instead of the implied stack of recursion. 4.7 B-Trees Thus far, we have assumed that we can store an entire data structure in the main memory of a computer. Suppose, however, that we have more data than can ﬁt in main memory, meaning that we must have the data structure reside on disk. When this happens, the rules of the game change because the Big-Oh model is no longer meaningful. The problem is that a Big-Oh analysis assumes that all operations are equal. However, this is not true, especially when disk I/O is involved. Modern computers execute billions of instructions per second. That is pretty fast, mainly because the speed depends largely on electrical properties. On the other hand, a disk is mechanical. Its speed depends largely on the time it takes to spin the disk and to move a disk head. Many disks spin at 7,200 RPM. Thus in 1 min, it makes 7,200 revolutions; hence, one revolution occurs in 1/120 of a second, or 8.3 ms. On average, we might expect that we have to spin a disk halfway to ﬁnd what we are looking for, but this is compensated by the time to move the disk head, so we get an access time of 8.3 ms. (This is a very charitable estimate; 9–11 ms access times are more common.) Consequently, we can do approximately 120 disk accesses per second. This sounds pretty good, until we compare it with the processor speed. What we have is billions of instructions equal to 120 disk accesses. Of course, everything here is a rough calculation, but the relative speeds are pretty clear: Disk accesses are incredibly expensive. Furthermore, processor speeds are increasing at a much faster rate than disk speeds (it is disk sizes that are increasing quite quickly). So we are willing to do lots of calculations just to save a disk access. In almost all cases, it is the number of disk accesses that will dominate the running time. Thus, if we halve the number of disk accesses, the running time will also halve. Here is how the typical search tree performs on disk. Suppose we want to access the driving records for citizens in the State of Florida. We assume that we have 10 million items, that each key is 32 bytes (representing a name), and that a record is 256 bytes. We assume this does not ﬁt in main memory and that we are 1 of 20 users on a system (so we have 1/20 of the resources). Thus, in 1 sec, we can execute billions of instructions or perform six disk accesses. The unbalanced binary search tree is a disaster. In the worst case, it has linear depth and thus could require 10 million disk accesses. On average, a successful search would require 1.38 log N disk accesses, and since log 10000000 ≈24, an average search would require 32 disk accesses, or 5 sec. In a typical randomly constructed tree, we would expect

Chapter 4 Trees Figure 4.59 5-ary tree of 31 nodes has only three levels that a few nodes are three times deeper; these would require about 100 disk accesses, or 16 sec. An AVL tree is somewhat better. The worst case of 1.44 log N is unlikely to occur, and the typical case is very close to log N. Thus an AVL tree would use about 25 disk accesses on average, requiring 4 sec. We want to reduce the number of disk accesses to a very small constant, such as three or four; and we are willing to write complicated code to do this because machine instructions are essentially free, as long as we are not ridiculously unreasonable. It should probably be clear that a binary search tree will not work, since the typical AVL tree is close to optimal height. We cannot go below log N using a binary search tree. The solution is intuitively simple: If we have more branching, we have less height. Thus, while a perfect binary tree of 31 nodes has ﬁve levels, a 5-ary tree of 31 nodes has only three levels, as shown in Figure 4.59. An M-ary search tree allows M-way branching. As branching increases, the depth decreases. Whereas a complete binary tree has height that is roughly log2 N, a complete M-ary tree has height that is roughly logM N. We can create an M-ary search tree in much the same way as a binary search tree. In a binary search tree, we need one key to decide which of two branches to take. In an M-ary search tree, we need M −1 keys to decide which branch to take. To make this scheme efﬁcient in the worst case, we need to ensure that the M-ary search tree is balanced in some way. Otherwise, like a binary search tree, it could degenerate into a linked list. Actually, we want an even more restrictive balancing condition. That is, we do not want an M-ary search tree to degenerate to even a binary search tree, because then we would be stuck with log N accesses. One way to implement this is to use a B-tree. The basic B-tree4 is described here. Many variations and improvements are possible, and an implementation is somewhat complex because there are quite a few cases. However, it is easy to see that, in principle, a B-tree guarantees only a few disk accesses. A B-tree of order M is an M-ary tree with the following properties:5 1. The data items are stored at leaves. 2. The nonleaf nodes store up to M −1 keys to guide the searching; key i represents the smallest key in subtree i + 1. 3. The root is either a leaf or has between two and M children. 4 What is described is popularly known as a B+ tree. 5 Rules 3 and 5 must be relaxed for the ﬁrst L insertions.

4.7 B-Trees 4. All nonleaf nodes (except the root) have between ⌈M/2⌉and M children. 5. All leaves are at the same depth and have between ⌈L/2⌉and L data items, for some L (the determination of L is described shortly). An example of a B-tree of order 5 is shown in Figure 4.60. Notice that all nonleaf nodes have between three and ﬁve children (and thus between two and four keys); the root could possibly have only two children. Here, we have L = 5. (It happens that L and M are the same in this example, but this is not necessary.) Since L is 5, each leaf has between three and ﬁve data items. Requiring nodes to be half full guarantees that the B-tree does not degenerate into a simple binary tree. Although there are various deﬁnitions of B-trees that change this structure, mostly in minor ways, this deﬁnition is one of the popular forms. Each node represents a disk block, so we choose M and L on the basis of the size of the items that are being stored. As an example, suppose one block holds 8,192 bytes. In our Florida example, each key uses 32 bytes. In a B-tree of order M, we would have M−1 keys, for a total of 32M −32 bytes, plus M branches. Since each branch is essentially a number of another disk block, we can assume that a branch is 4 bytes. Thus the branches use 4M bytes. The total memory requirement for a nonleaf node is thus 36M−32. The largest value of M for which this is no more than 8,192 is 228. Thus we would choose M = 228. Since each data record is 256 bytes, we would be able to ﬁt 32 records in a block. Thus we would choose L = 32. We are guaranteed that each leaf has between 16 and 32 data records and that each internal node (except the root) branches in at least 114 ways. Since there are 10 million records, there are at most 625,000 leaves. Consequently, in the worst case, leaves would be on level 4. In more concrete terms, the worst-case number of accesses is given by approximately logM/2 N, give or take 1 (for example, the root and the next level could be cached in main memory, so that, over the long run, disk accesses would be needed only for level 3 and deeper). The remaining issue is how to add and remove items from the B-tree; the ideas involved are sketched next. Note that many of the themes seen before recur. We begin by examining insertion. Suppose we want to insert 57 into the B-tree in Figure 4.60. A search down the tree reveals that it is not already in the tree. We can then add it to the leaf as a ﬁfth item. Note that we may have to reorganize all the data in the leaf Figure 4.60 B-tree of order 5

Chapter 4 Trees Figure 4.61 B-tree after insertion of 57 into the tree in Figure 4.60 Figure 4.62 Insertion of 55 into the B-tree in Figure 4.61 causes a split into two leaves to do this. However, the cost of doing this is negligible when compared to that of the disk access, which in this case also includes a disk write. Of course, that was relatively painless because the leaf was not already full. Suppose we now want to insert 55. Figure 4.61 shows a problem: The leaf where 55 wants to go is already full. The solution is simple, however: Since we now have L+1 items, we split them into two leaves, both guaranteed to have the minimum number of data records needed. We form two leaves with three items each. Two disk accesses are required to write these leaves, and a third disk access is required to update the parent. Note that in the parent, both keys and branches change, but they do so in a controlled way that is easily calculated. The resulting B-tree is shown in Figure 4.62. Although splitting nodes is time-consuming because it requires at least two additional disk writes, it is a relatively rare occurrence. If L is 32, for example, then when a node is split, two leaves with 16 and 17 items, respectively, are created. For the leaf with 17 items, we can perform 15 more insertions without another split. Put another way, for every split, there are roughly L/2 nonsplits. The node splitting in the previous example worked because the parent did not have its full complement of children. But what would happen if it did? Suppose, for example, that we were to insert 40 into the B-tree in Figure 4.62. We would then have to split the leaf containing the keys 35 through 39, and now 40, into two leaves. But doing this would give

4.7 B-Trees Figure 4.63 Insertion of 40 into the B-tree in Figure 4.62 causes a split into two leaves and then a split of the parent node the parent six children, and it is allowed only ﬁve. Hence, the solution is to split the parent. The result of this is shown in Figure 4.63. When the parent is split, we must update the values of the keys and also the parent’s parent, thus incurring an additional two disk writes (so this insertion costs ﬁve disk writes). However, once again, the keys change in a very controlled manner, although the code is certainly not simple because of a host of cases. When a nonleaf node is split, as is the case here, its parent gains a child. What if the parent already has reached its limit of children? In that case, we continue splitting nodes up the tree until either we ﬁnd a parent that does not need to be split or we reach the root. If we split the root, then we have two roots. Obviously, this is unacceptable, but we can create a new root that has the split roots as its two children. This is why the root is granted the special two-child minimum exemption. It also is the only way that a B-tree gains height. Needless to say, splitting all the way up to the root is an exceptionally rare event, because a tree with four levels indicates that the root has been split three times throughout the entire sequence of insertions (assuming no deletions have occurred). In fact, splitting of any nonleaf node is also quite rare. There are other ways to handle the overﬂowing of children. One technique is to put a child up for adoption should a neighbor have room. To insert 29 into the B-tree in Figure 4.63, for example, we could make room by moving 32 to the next leaf. This technique requires a modiﬁcation of the parent because the keys are affected. However, it tends to keep nodes fuller and thus saves space in the long run. We can perform deletion by ﬁnding the item that needs to be removed and then removing it. The problem is that if the leaf it was in had the minimum number of data items, then it is now below the minimum. We can rectify this situation by adopting a neighboring item, if the neighbor is not itself at its minimum. If it is, then we can combine with the neighbor to form a full leaf. Unfortunately, this means that the parent has lost a child. If this loss causes the parent to fall below its minimum, then it follows the same strategy. This process could percolate all the way up to the root. The root cannot have just one child (and even if this were allowed, it would be silly). If a root is left with one child as a result of the adoption process, then we remove the root and make its child the new root of the tree. This is the only way for a B-tree to lose height. For example, suppose we want to remove 99 from the B-tree in Figure 4.63. Since the leaf has only two items, and its neighbor is already at its minimum of three, we combine the items into a new leaf of ﬁve items. As a

Chapter 4 Trees Figure 4.64 B-tree after the deletion of 99 from the B-tree in Figure 4.63 result, the parent has only two children. However, it can adopt from a neighbor because the neighbor has four children. As a result, both have three children. The result is shown in Figure 4.64. 4.8 Sets and Maps in the Standard Library The List containers discussed in Chapter 3, namely ArrayList and LinkedList, are inefﬁcient for searching. Consequently, the Collections API provides two additional containers, Set and Map, that provide efﬁcient implementations for basic operations such as insertion, deletion, and searching. 4.8.1 Sets The Set interface represents a Collection that does not allow duplicates. A special kind of Set, given by the SortedSet interface, guarantees that the items are maintained in sorted order. Because a Set IS-A Collection, the idioms used to access items in a List, which are inherited from Collection, also work for a Set. The print method described in Figure 3.6 will work if passed a Set. The unique operations required by the Set are the abilities to insert, remove, and perform a basic search (efﬁciently). For a Set, the add method returns true if the add succeeds and false if it fails because the item being added is already present. The implementation of Set that maintains items in sorted order is a TreeSet. Basic operations in a TreeSet take logarithmic worst-case time. By default, ordering assumes that the items in the TreeSet implement the Comparable interface. An alternative ordering can be speciﬁed by instantiating the TreeSet with a Comparator. For instance, we can create a TreeSet that stores String objects, ignoring case distinctions by using the CaseInsensitiveCompare function object coded in Figure 1.18. In the following code, the Set s has size 1. Set<String> s = new TreeSet<>( new CaseInsensitiveCompare( ) ); s.add( "Hello" ); s.add( "HeLLo" ); System.out.println( "The size is: " + s.size( ) );

4.8 Sets and Maps in the Standard Library 4.8.2 Maps A Map is an interface that represents a collection of entries that consists of keys and their values. Keys must be unique, but several keys can map to the same values. Thus values need not be unique. In a SortedMap, the keys in the map are maintained in logically sorted order. An implementation of SortedMap is the TreeMap. The basic operations for a Map include methods such as isEmpty, clear, size, and most importantly, the following: boolean containsKey( KeyType key ) ValueType get( KeyType key ) ValueType put( KeyType key, ValueType value ) get returns the value associated with key in the Map, or null if key is not present. If there are no null values in the Map, the value returned by get can be used to determine if key is in the Map. However, if there are null values, you have to use containsKey. Method put places a key/value pair into the Map, returning either null or the old value associated with key. Iterating through a Map is trickier than a Collection because the Map does not provide an iterator. Instead, three methods are provided that return the view of a Map as a Collection. Since the views are themselves Collections, the views can be iterated. The three methods are: Set<KeyType> keySet( ) Collection<ValueType> values( ) Set<Map.Entry<KeyType,ValueType>> entrySet( ) Methods keySet and values return simple collections (the keys contain no duplicates, thus the keys are returned in a Set). The entrySet is returned as a Set of entries (there are no duplicate entries, since the keys are unique). Each entry is represented by the nested interface Map.Entry. For an object of type Map.Entry, the available methods include accessing the key, the value, and changing the value: KeyType getKey( ) ValueType getValue( ) ValueType setValue( ValueType newValue ) 4.8.3 Implementation of TreeSet and TreeMap Java requires that TreeSet and TreeMap support the basic add, remove, and contains operations in logarithmic worst-case time. Consequently, the underlying implementation is a balanced binary search tree. Typically, an AVL tree is not used; instead, top-down red-black trees, which are discussed in Section 12.2, are often used. An important issue in implementing TreeSet and TreeMap is providing support for the iterator classes. Of course, internally, the iterator maintains a link to the “current” node

Chapter 4 Trees in the iteration. The hard part is efﬁciently advancing to the next node. There are several possible solutions, some of which are listed here: 1. When the iterator is constructed, have each iterator store as its data an array containing the TreeSet items. This is lame, because we might as well use toArray and have no need for an iterator. 2. Have the iterator maintain a stack storing nodes on the path to the current node. With this information, one can deduce the next node in the iteration, which is either the node in the current node’s right subtree that contains the minimum item, or the nearest ancestor that contains the current node in its left subtree. This makes the iterator somewhat large, and makes the iterator code clumsy. 3. Have each node in the search tree store its parent in addition to the children. The iterator is not as large, but there is now extra memory required in each node, and the code to iterate is still clumsy. 4. Have each node maintain extra links: one to the next smaller, and one to the next larger node. This takes space, but the iteration is very simple to do, and it is easy to maintain these links. 5. Maintain the extra links only for nodes that have null left or right links, by using extra Boolean variables to allow the routines to tell if a left link is being used as a standard binary search tree left link or a link to the next smaller node, and similarly for the right link (Exercise 4.50). This idea is called a threaded tree, and is used in many balanced binary search tree implementations. 4.8.4 An Example That Uses Several Maps Many words are similar to other words. For instance, by changing the ﬁrst letter, the word wine can become dine, fine, line, mine, nine, pine, or vine. By changing the third letter, wine can become wide, wife, wipe, or wire, among others. By changing the fourth letter, wine can become wind, wing, wink, or wins, among others. This gives 15 different words that can be obtained by changing only one letter in wine. In fact, there are over 20 different words, some more obscure. We would like to write a program to ﬁnd all words that can be changed into at least 15 other words by a single one-character substitution. We assume that we have a dictionary consisting of approximately 89,000 different words of varying lengths. Most words are between 6 and 11 characters. The distribution includes 8,205 six-letter words, 11,989 seven-letter words, 13,672 eight-letter words, 13,014 nine-letter words, 11,297 ten-letter words, and 8,617 eleven-letter words. (In reality, the most changeable words are three-, four- and ﬁve-letter words, but the longer words are the time-consuming ones to check.) The most straightforward strategy is to use a Map in which the keys are words and the values are lists containing the words that can be changed from the key with a one-character substitution. The routine in Figure 4.65 shows how the Map that is eventually produced (we have yet to write code for that part) can be used to print the required answers. The code obtains the entry set and uses the enhanced for loop to step through the entry set and view entries that are pairs consisting of a word and a list of words.

4.8 Sets and Maps in the Standard Library public static void printHighChangeables( Map<String,List<String>> adjWords, int minWords ) { for( Map.Entry<String,List<String>> entry : adjWords.entrySet( ) ) { List<String> words = entry.getValue( ); if( words.size( ) >= minWords ) { System.out.print( entry.getKey( ) + " (" ); System.out.print( words.size( ) + "):" ); for( String w : words ) System.out.print( " " + w ); System.out.println( ); } } } Figure 4.65 Given a map containing words as keys and a list of words that differ in only one character as values, output words that have minWords or more words obtainable by a one-character substitution // Returns true if word1 and word2 are the same length // and differ in only one character. private static boolean oneCharOff( String word1, String word2 ) { if( word1.length( ) != word2.length( ) ) return false; int diffs = 0; for( int i = 0; i < word1.length( ); i++ ) if( word1.charAt( i ) != word2.charAt( i ) ) if( ++diffs > 1 ) return false; return diffs == 1; } Figure 4.66 Routine to check if two words differ in only one character The main issue is how to construct the Map from an array that contains the 89,000 words. The routine in Figure 4.66 is a straightforward function to test if two words are identical except for a one-character substitution. We can use the routine to provide the simplest algorithm for the Map construction, which is a brute-force test of all pairs of words. This algorithm is shown in Figure 4.67.

Chapter 4 Trees To step through the collection of words, we could use an iterator, but because we are stepping through it with a nested loop (i.e., several times), we dump the collection into an array using toArray (lines 9 and 11). Among other things, this avoids repeated calls to cast from Object to String, which occur behind the scenes if generics are used. Instead, we are simply indexing a String[]. // Computes a map in which the keys are words and values are Lists of words // that differ in only one character from the corresponding key. // Uses a quadratic algorithm (with appropriate Map). public static Map<String,List<String>> computeAdjacentWords( List<String> theWords ) { Map<String,List<String>> adjWords = new TreeMap<>( ); String [ ] words = new String[ theWords.size( ) ]; theWords.toArray( words ); for( int i = 0; i < words.length; i++ ) for( int j = i + 1; j < words.length; j++ ) if( oneCharOff( words[ i ], words[ j ] ) ) { update( adjWords, words[ i ], words[ j ] ); update( adjWords, words[ j ], words[ i ] ); } return adjWords; } private static <KeyType> void update( Map<KeyType,List<String>> m, KeyType key, String value ) { List<String> lst = m.get( key ); if( lst == null ) { lst = new ArrayList<>( ); m.put( key, lst ); } lst.add( value ); } Figure 4.67 Function to compute a map containing words as keys and a list of words that differ in only one character as values. This version runs in 75 seconds on an 89,000-word dictionary

4.8 Sets and Maps in the Standard Library If we ﬁnd a pair of words that differ in only one character, we can update the Map at lines 16 and 17. In the private update method, at line 26 we see if there is already a list of words associated with the key. If we have previously seen key, because lst is not null, then it is in the Map, and we need only add the new word to the List in the Map, and we do this by calling add at line 33. If we have never seen key before, then lines 29 and 30 place it in the Map, with a List of size 0, so the add updates the List to be size 1. All in all, this is a standard idiom for maintaining a Map, in which the value is a collection. The problem with this algorithm is that it is slow, and takes 75 seconds on our computer. An obvious improvement is to avoid comparing words of different lengths. We can do this by grouping words by their length, and then running the previous algorithm on each of the separate groups. To do this, we can use a second map! Here the key is an integer representing a word length, and the value is a collection of all the words of that length. We can use a List to store each collection, and the same idiom applies. The code is shown in Figure 4.68. Line 9 shows the declaration for the second Map, lines 12 and 13 populate the Map, and then an extra loop is used to iterate over each group of words. Compared to the ﬁrst algorithm, the second algorithm is only marginally more difﬁcult to code and runs in 16 seconds, or about ﬁve times as fast. Our third algorithm is more complex, and uses additional maps! As before, we group the words by word length, and then work on each group separately. To see how this algorithm works, suppose we are working on words of length 4. Then ﬁrst we want to ﬁnd word pairs such as wine and nine that are identical except for the ﬁrst letter. One way to do this, for each word of length 4, is to remove the ﬁrst character, leaving a three-character word representative. Form a Map in which the key is the representative, and the value is a List of all words that have that representative. For instance, in considering the ﬁrst character of the four-letter word group, representative "ine" corresponds to "dine", "fine", "wine", "nine", "mine", "vine", "pine", "line". Representative "oot" corresponds to "boot", "foot", "hoot", "loot", "soot", "zoot". Each individual List that is a value in this latest Map forms a clique of words in which any word can be changed to any other word by a onecharacter substitution, so after this latest Map is constructed, it is easy to traverse it and add entries to the original Map that is being computed. We would then proceed to the second character of the four-letter word group, with a new Map. And then the third character, and ﬁnally the fourth character. The general outline is: for each group g, containing words of length len for each position p (ranging from 0 to len-1) { Make an empty Map<String,List<String> > repsToWords for each word w { Obtain w’s representative by removing position p Update repsToWords } Use cliques in repsToWords to update adjWords map }

Chapter 4 Trees Figure 4.69 contains an implementation of this algorithm. The running time improves to one second. It is interesting to note that although the use of the additional Maps makes the algorithm faster, and the syntax is relatively clean, the code makes no use of the fact that the keys of the Map are maintained in sorted order. As such, it is possible that a data structure that supports the Map operations but does not guarantee sorted order can perform better, since it is being asked to do less. Chapter 5 explores this possibility and discusses the ideas behind the alternative Map implementation, known as a HashMap. A HashMap reduces the running time of the implementation from one second to roughly 0.8 seconds. // Computes a map in which the keys are words and values are Lists of words // that differ in only one character from the corresponding key. // Uses a quadratic algorithm (with appropriate Map), but speeds things by // maintaining an additional map that groups words by their length. public static Map<String,List<String>> computeAdjacentWords( List<String> theWords ) { Map<String,List<String>> adjWords = new TreeMap<>( ); Map<Integer,List<String>> wordsByLength = new TreeMap<>( ); // Group the words by their length for( String w : theWords ) update( wordsByLength, w.length( ), w ); // Work on each group separately for( List<String> groupsWords : wordsByLength.values( ) ) { String [ ] words = new String[ groupsWords.size( ) ]; groupsWords.toArray( words ); for( int i = 0; i < words.length; i++ ) for( int j = i + 1; j < words.length; j++ ) if( oneCharOff( words[ i ], words[ j ] ) ) { update( adjWords, words[ i ], words[ j ] ); update( adjWords, words[ j ], words[ i ] ); } } return adjWords; } Figure 4.68 Function to compute a map containing words as keys and a list of words that differ in only one character as values. Splits words into groups by word length. This version runs in 16 seconds on an 89,000-word dictionary

// Computes a map in which the keys are words and values are Lists of words // that differ in only one character from the corresponding key. // Uses an efficient algorithm that is O(N log N) with a TreeMap. public static Map<String,List<String>> computeAdjacentWords( List<String> words ) { Map<String,List<String>> adjWords = new TreeMap<>( ); Map<Integer,List<String>> wordsByLength = new TreeMap<>( ); // Group the words by their length for( String w : words ) update( wordsByLength, w.length( ), w ); // Work on each group separately for( Map.Entry<Integer,List<String>> entry : wordsByLength.entrySet( ) ) { List<String> groupsWords = entry.getValue( ); int groupNum = entry.getKey( ); // Work on each position in each group for( int i = 0; i < groupNum; i++ ) { // Remove one character in specified position, computing // representative. Words with same representative are // adjacent, so first populate a map ... Map<String,List<String>> repToWord = new TreeMap<>( ); for( String str : groupsWords ) { String rep = str.substring( 0, i ) + str.substring( i + 1 ); update( repToWord, rep, str ); } // and then look for map values with more than one string for( List<String> wordClique : repToWord.values( ) ) if( wordClique.size( ) >= 2 ) for( String s1 : wordClique ) for( String s2 : wordClique ) if( s1 != s2 ) update( adjWords, s1, s2 ); } } return adjWords; } Figure 4.69 Function to compute a map containing words as keys and a list of words that differ in only one character as values. Runs in 1 second on an 89,000-word dictionary

C H A P T E R 5 Hashing In Chapter 4, we discussed the search tree ADT, which allowed various operations on a set of elements. In this chapter, we discuss the hash table ADT, which supports only a subset of the operations allowed by binary search trees. The implementation of hash tables is frequently called hashing. Hashing is a technique used for performing insertions, deletions, and searches in constant average time. Tree operations that require any ordering information among the elements are not supported efﬁciently. Thus, operations such as findMin, findMax, and the printing of the entire table in sorted order in linear time are not supported. The central data structure in this chapter is the hash table. We will r See several methods of implementing the hash table. r Compare these methods analytically. r Show numerous applications of hashing. r Compare hash tables with binary search trees. 5.1 General Idea The ideal hash table data structure is merely an array of some ﬁxed size, containing the items. As discussed in Chapter 4, generally a search is performed on some part (that is, data ﬁeld) of the item. This is called the key. For instance, an item could consist of a string (that serves as the key) and additional data ﬁelds (for instance, a name that is part of a large employee structure). We will refer to the table size as TableSize, with the understanding that this is part of a hash data structure and not merely some variable ﬂoating around globally. The common convention is to have the table run from 0 to TableSize −1; we will see why shortly. Each key is mapped into some number in the range 0 to TableSize −1 and placed in the appropriate cell. The mapping is called a hash function, which ideally should be simple to compute and should ensure that any two distinct keys get different cells. Since there are a ﬁnite number of cells and a virtually inexhaustible supply of keys, this is clearly impossible, and thus we seek a hash function that distributes the keys evenly among the cells. Figure 5.1 is typical of a perfect situation. In this example, john hashes to 3, phil hashes to 4, dave hashes to 6, and mary hashes to 7.

Chapter 5 Hashing john 25000 phil 31250 dave 27500 mary 28200 Figure 5.1 An ideal hash table This is the basic idea of hashing. The only remaining problems deal with choosing a function, deciding what to do when two keys hash to the same value (this is known as a collision), and deciding on the table size. 5.2 Hash Function If the input keys are integers, then simply returning Key mod TableSize is generally a reasonable strategy, unless Key happens to have some undesirable properties. In this case, the choice of hash function needs to be carefully considered. For instance, if the table size is 10 and the keys all end in zero, then the standard hash function is a bad choice. For reasons we shall see later, and to avoid situations like the one above, it is often a good idea to ensure that the table size is prime. When the input keys are random integers, then this function is not only very simple to compute but also distributes the keys evenly. Usually, the keys are strings; in this case, the hash function needs to be chosen carefully. One option is to add up the ASCII (or Unicode) values of the characters in the string. The routine in Figure 5.2 implements this strategy. The hash function depicted in Figure 5.2 is simple to implement and computes an answer quickly. However, if the table size is large, the function does not distribute the keys well. For instance, suppose that TableSize = 10,007 (10,007 is a prime number). Suppose all the keys are eight or fewer characters long. Since an ASCII character has an integer value that is always at most 127, the hash function typically can only assume values between 0 and 1,016, which is 127 ∗8. This is clearly not an equitable distribution! Another hash function is shown in Figure 5.3. This hash function assumes that Key has at least three characters. The value 27 represents the number of letters in the English alphabet, plus the blank, and 729 is 272. This function examines only the ﬁrst three characters,

5.2 Hash Function public static int hash( String key, int tableSize ) { int hashVal = 0; for( int i = 0; i < key.length( ); i++ ) hashVal += key.charAt( i ); return hashVal % tableSize; } Figure 5.2 A simple hash function public static int hash( String key, int tableSize ) { return ( key.charAt( 0 ) + 27 * key.charAt( 1 ) + 729 * key.charAt( 2 ) ) % tableSize; } Figure 5.3 Another possible hash function—not too good but if these are random and the table size is 10,007, as before, then we would expect a reasonably equitable distribution. Unfortunately, English is not random. Although there are 263 = 17,576 possible combinations of three characters (ignoring blanks), a check of a reasonably large online dictionary reveals that the number of different combinations is actually only 2,851. Even if none of these combinations collide, only 28 percent of the table can actually be hashed to. Thus this function, although easily computable, is also not appropriate if the hash table is reasonably large. Figure 5.4 shows a third attempt at a hash function. This hash function involves all characters in the key and can generally be expected to distribute well (it computes KeySize−1 i=0 Key[KeySize −i −1] · 37i and brings the result into proper range). The code computes a polynomial function (of 37) by use of Horner’s rule. For instance, another way of computing hk = k0 + 37k1 + 372k2 is by the formula hk = ((k2) ∗37 + k1) ∗37 + k0. Horner’s rule extends this to an nth degree polynomial. The hash function takes advantage of the fact that overﬂow is allowed. This may introduce a negative number; thus the extra test at the end. The hash function described in Figure 5.4 is not necessarily the best with respect to table distribution but does have the merit of extreme simplicity and is reasonably fast. If the keys are very long, the hash function will take too long to compute. A common practice in this case is not to use all the characters. The length and properties of the keys would then inﬂuence the choice. For instance, the keys could be a complete street address. The hash function might include a couple of characters from the street address and perhaps a couple of characters from the city name and ZIP code. Some programmers implement their hash function by using only the characters in the odd spaces, with the idea that the time saved computing the hash function will make up for a slightly less evenly distributed function.

Chapter 5 Hashing /** * A hash routine for String objects. * @param key the String to hash. * @param tableSize the size of the hash table. * @return the hash value. */ public static int hash( String key, int tableSize ) { int hashVal = 0; for( int i = 0; i < key.length( ); i++ ) hashVal = 37 * hashVal + key.charAt( i ); hashVal %= tableSize; if( hashVal < 0 ) hashVal += tableSize; return hashVal; } Figure 5.4 A good hash function The main programming detail left is collision resolution. If, when an element is inserted, it hashes to the same value as an already inserted element, then we have a collision and need to resolve it. There are several methods for dealing with this. We will discuss two of the simplest: separate chaining and open addressing; then we will look at some more recently discovered alternatives. 5.3 Separate Chaining The ﬁrst strategy, commonly known as separate chaining, is to keep a list of all elements that hash to the same value. We can use the standard library list implementations. If space is tight, it might be preferable to avoid their use (since those lists are doubly linked and waste space). We assume, for this section, that the keys are the ﬁrst 10 perfect squares and that the hashing function is simply hash(x) = x mod 10. (The table size is not prime but is used here for simplicity.) Figure 5.5 should make this clear. To perform a search, we use the hash function to determine which list to traverse. We then search the appropriate list. To perform an insert, we check the appropriate list to see whether the element is already in place (if duplicates are expected, an extra ﬁeld is usually kept, and this ﬁeld would be incremented in the event of a match). If the element turns out to be new, it is inserted at the front of the list, since it is convenient and also because frequently it happens that recently inserted elements are the most likely to be accessed in the near future. The class skeleton required to implement separate chaining is shown in Figure 5.6. The hash table stores an array of linked lists, which are allocated in the constructor.

5.3 Separate Chaining Figure 5.5 A separate chaining hash table public class SeparateChainingHashTable<AnyType> { public SeparateChainingHashTable( ) { /* Figure 5.9 */ } public SeparateChainingHashTable( int size ) { /* Figure 5.9 */ } public void insert( AnyType x ) { /* Figure 5.10 */ } public void remove( AnyType x ) { /* Figure 5.10 */ } public boolean contains( AnyType x ) { /* Figure 5.10 */ } public void makeEmpty( ) { /* Figure 5.9 */ } private static final int DEFAULT_TABLE_SIZE = 101; private List<AnyType> [ ] theLists; private int currentSize; private void rehash( ) { /* Figure 5.22 */ } private int myhash( AnyType x ) { /* Figure 5.7 */ } private static int nextPrime( int n ) { /* See online code */ } private static boolean isPrime( int n ) { /* See online code */ } } Figure 5.6 Class skeleton for separate chaining hash table

Chapter 5 Hashing private int myhash( AnyType x ) { int hashVal = x.hashCode( ); hashVal %= theLists.length; if( hashVal < 0 ) hashVal += theLists.length; return hashVal; } Figure 5.7 myHash method for hash tables public class Employee { public boolean equals( Object rhs ) { return rhs instanceof Employee && name.equals( ((Employee)rhs).name ); } public int hashCode( ) { return name.hashCode( ); } private String name; private double salary; private int seniority; // Additional fields and methods } Figure 5.8 Example of Employee class that can be in a hash table Just as the binary search tree works only for objects that are Comparable, the hash tables in this chapter work only for objects that follow a certain protocol. In Java such objects must provide an appropriate equals method and a hashCode method that returns an int. The hash table can then scale this int into a suitable array index via myHash, as shown in Figure 5.7. Figure 5.8 illustrates an Employee class that can be stored in a hash table. The Employee class provides an equals method and a hashCode method based on the Employee’s name. The hashCode for the Employee class works by using the hashCode deﬁned in the Standard String class. That hashCode is basically the code in Figure 5.4 with lines 14–16 removed. Figure 5.9 shows the constructors and makeEmpty. The code to implement contains, insert, and remove is shown in Figure 5.10.

5.3 Separate Chaining /** * Construct the hash table. */ public SeparateChainingHashTable( ) { this( DEFAULT_TABLE_SIZE ); } /** * Construct the hash table. * @param size approximate table size. */ public SeparateChainingHashTable( int size ) { theLists = new LinkedList[ nextPrime( size ) ]; for( int i = 0; i < theLists.length; i++ ) theLists[ i ] = new LinkedList<>( ); } /** * Make the hash table logically empty. */ public void makeEmpty( ) { for( int i = 0; i < theLists.length; i++ ) theLists[ i ].clear( ); currentSize = 0; } Figure 5.9 Constructors and makeEmpty for separate chaining hash table In the insertion routine, if the item to be inserted is already present, then we do nothing; otherwise, we place it in the list. The element can be placed anywhere in the list; using add is most convenient in our case. Any scheme could be used besides linked lists to resolve the collisions; a binary search tree or even another hash table would work, but we expect that if the table is large and the hash function is good, all the lists should be short, so basic separate chaining makes no attempt to try anything complicated. We deﬁne the load factor, λ, of a hash table to be the ratio of the number of elements in the hash table to the table size. In the example above, λ = 1.0. The average length of a list is λ. The effort required to perform a search is the constant time required to evaluate the hash function plus the time to traverse the list. In an unsuccessful search, the number of nodes to examine is λ on average. A successful search requires that about 1+(λ/2) links be traversed. To see this, notice that the list that is being searched contains the one node that stores the match plus zero or more other nodes. The expected number of “other nodes”

Chapter 5 Hashing /** * Find an item in the hash table. * @param x the item to search for. * @return true if x is not found. */ public boolean contains( AnyType x ) { List<AnyType> whichList = theLists[ myhash( x ) ]; return whichList.contains( x ); } /** * Insert into the hash table. If the item is * already present, then do nothing. * @param x the item to insert. */ public void insert( AnyType x ) { List<AnyType> whichList = theLists[ myhash( x ) ]; if( !whichList.contains( x ) ) { whichList.add( x ); // Rehash; see Section 5.5 if( ++currentSize > theLists.length ) rehash( ); } } /** * Remove from the hash table. * @param x the item to remove. */ public void remove( AnyType x ) { List<AnyType> whichList = theLists[ myhash( x ) ]; if( whichList.contains( x ) ) { whichList.remove( x ); currentSize--; } } Figure 5.10 contains, insert, and remove routines for separate chaining hash table

5.4 Hash Tables Without Linked Lists in a table of N elements and M lists is (N −1)/M = λ −1/M, which is essentially λ, since M is presumed to be large. On average, half the “other nodes” are searched, so combined with the matching node, we obtain an average search cost of 1 + λ/2 nodes. This analysis shows that the table size is not really important, but the load factor is. The general rule for separate chaining hashing is to make the table size about as large as the number of elements expected (in other words, let λ ≈1). In the code in Figure 5.10, if the load factor exceeds 1, we expand the table size by calling rehash at line 26. rehash is discussed in Section 5.5. It is also a good idea, as mentioned before, to keep the table size prime to ensure a good distribution. 5.4 Hash Tables Without Linked Lists Separate chaining hashing has the disadvantage of using linked lists. This could slow the algorithm down a bit because of the time required to allocate new cells (especially in other languages), and also essentially requires the implementation of a second data structure. An alternative to resolving collisions with linked lists is to try alternative cells until an empty cell is found. More formally, cells h0(x), h1(x), h2(x), . . . are tried in succession, where hi(x) = (hash(x)+f(i)) mod TableSize, with f(0) = 0. The function, f, is the collision resolution strategy. Because all the data go inside the table, a bigger table is needed in such a scheme than for separate chaining hashing. Generally, the load factor should be below λ = 0.5 for a hash table that doesn’t use separate chaining. We call such tables probing hash tables. We now look at three common collision resolution strategies. 5.4.1 Linear Probing In linear probing, f is a linear function of i, typically f(i) = i. This amounts to trying cells sequentially (with wraparound) in search of an empty cell. Figure 5.11 shows the result of inserting keys {89, 18, 49, 58, 69} into a hash table using the same hash function as before and the collision resolution strategy, f(i) = i. The ﬁrst collision occurs when 49 is inserted; it is put in the next available spot, namely, spot 0, which is open. The key 58 collides with 18, 89, and then 49 before an empty cell is found three away. The collision for 69 is handled in a similar manner. As long as the table is big enough, a free cell can always be found, but the time to do so can get quite large. Worse, even if the table is relatively empty, blocks of occupied cells start forming. This effect, known as primary clustering, means that any key that hashes into the cluster will require several attempts to resolve the collision, and then it will add to the cluster. Although we will not perform the calculations here, it can be shown that the expected number of probes using linear probing is roughly 1 2(1 + 1/(1 −λ)2) for insertions and unsuccessful searches, and 1 2(1 + 1/(1 −λ)) for successful searches. The calculations are somewhat involved. It is easy to see from the code that insertions and unsuccessful searches require the same number of probes. A moment’s thought suggests that, on average, successful searches should take less time than unsuccessful searches. The corresponding formulas, if clustering is not a problem, are fairly easy to derive. We will assume a very large table and that each probe is independent of the previous probes.

Chapter 5 Hashing Empty Table After 89 After 18 After 49 After 58 After 69 Figure 5.11 Hash table with linear probing, after each insertion These assumptions are satisﬁed by a random collision resolution strategy and are reasonable unless λ is very close to 1. First, we derive the expected number of probes in an unsuccessful search. This is just the expected number of probes until we ﬁnd an empty cell. Since the fraction of empty cells is 1 −λ, the number of cells we expect to probe is 1/(1 −λ). The number of probes for a successful search is equal to the number of probes required when the particular element was inserted. When an element is inserted, it is done as a result of an unsuccessful search. Thus, we can use the cost of an unsuccessful search to compute the average cost of a successful search. The caveat is that λ changes from 0 to its current value, so that earlier insertions are cheaper and should bring the average down. For instance, in the table in Figure 5.11, λ = 0.5, but the cost of accessing 18 is determined when 18 is inserted. At that point, λ = 0.2. Since 18 was inserted into a relatively empty table, accessing it should be easier than accessing a recently inserted element such as 69. We can estimate the average by using an integral to calculate the mean value of the insertion time, obtaining I(λ) = 1 λ  λ 1 −xdx = 1 λ ln 1 −λ These formulas are clearly better than the corresponding formulas for linear probing. Clustering is not only a theoretical problem but actually occurs in real implementations. Figure 5.12 compares the performance of linear probing (dashed curves) with what would be expected from more random collision resolution. Successful searches are indicated by an S, and unsuccessful searches and insertions are marked with U and I, respectively. If λ = 0.75, then the formula above indicates that 8.5 probes are expected for an insertion in linear probing. If λ = 0.9, then 50 probes are expected, which is unreasonable. This compares with 4 and 10 probes for the respective load factors if clustering were not a problem. We see from these formulas that linear probing can be a bad idea if the table is expected to be more than half full. If λ = 0.5, however, only 2.5 probes are required on average for insertion, and only 1.5 probes are required, on average, for a successful search.

5.4 Hash Tables Without Linked Lists 0.0 3.0 6.0 9.0 12.0 15.0 .10 .15 .20 .25 .30 .35 .40 .45 .50 .55 .60 .65 .70 .75 .80 .85 .90 .95  U,I U,I S S Figure 5.12 Number of probes plotted against load factor for linear probing (dashed) and random strategy (S is successful search, U is unsuccessful search, and I is insertion) Empty Table After 89 After 18 After 49 After 58 After 69 Figure 5.13 Hash table with quadratic probing, after each insertion 5.4.2 Quadratic Probing Quadratic probing is a collision resolution method that eliminates the primary clustering problem of linear probing. Quadratic probing is what you would expect—the collision function is quadratic. The popular choice is f(i) = i2. Figure 5.13 shows the resulting hash table with this collision function on the same input used in the linear probing example. When 49 collides with 89, the next position attempted is one cell away. This cell is empty, so 49 is placed there. Next 58 collides at position 8. Then the cell one away is tried,

Chapter 5 Hashing but another collision occurs. A vacant cell is found at the next cell tried, which is 22 = 4 away. The key 58 is thus placed in cell 2. The same thing happens for 69. For linear probing it is a bad idea to let the hash table get nearly full, because performance degrades. For quadratic probing, the situation is even more drastic: There is no guarantee of ﬁnding an empty cell once the table gets more than half full, or even before the table gets half full if the table size is not prime. This is because at most half of the table can be used as alternative locations to resolve collisions. Indeed, we prove now that if the table is half empty and the table size is prime, then we are always guaranteed to be able to insert a new element. Theorem 5.1. If quadratic probing is used, and the table size is prime, then a new element can always be inserted if the table is at least half empty. Proof. Let the table size, TableSize, be an (odd) prime greater than 3. We show that the ﬁrst ⌈TableSize/2⌉alternative locations (including the initial location h0(x)) are all distinct. Two of these locations are h(x)+i2 (mod TableSize) and h(x)+j2 (mod TableSize), where 0 ≤i, j ≤⌊TableSize/2⌋. Suppose, for the sake of contradiction, that these locations are the same, but i ̸= j. Then h(x) + i2 = h(x) + j2 (mod TableSize) i2 = j2 (mod TableSize) i2 −j2 = 0 (mod TableSize) (i −j)(i + j) = 0 (mod TableSize) Since TableSize is prime, it follows that either (i −j) or (i + j) is equal to 0 (mod TableSize). Since i and j are distinct, the ﬁrst option is not possible. Since 0 ≤i, j ≤ ⌊TableSize/2⌋, the second option is also impossible. Thus, the ﬁrst ⌈TableSize/2⌉alternative locations are distinct. If at most ⌊TableSize/2⌋positions are taken, then an empty spot can always be found. If the table is even one more than half full, the insertion could fail (although this is extremely unlikely). Therefore, it is important to keep this in mind. It is also crucial that the table size be prime.1 If the table size is not prime, the number of alternative locations can be severely reduced. As an example, if the table size were 16, then the only alternative locations would be at distances 1, 4, or 9 away. Standard deletion cannot be performed in a probing hash table, because the cell might have caused a collision to go past it. For instance, if we remove 89, then virtually all the remaining contains operations will fail. Thus, probing hash tables require lazy deletion, although in this case there really is no laziness implied. 1 If the table size is a prime of the form 4k + 3, and the quadratic collision resolution strategy F(i) = ±i2 is used, then the entire table can be probed. The cost is a slightly more complicated routine.

5.4 Hash Tables Without Linked Lists The class skeleton required to implement probing hash tables is shown in Figure 5.14. Instead of an array of lists, we have an array of hash table entry cells, which are also shown in Figure 5.14. Each entry in the array of HashEntry references is either 1. null. 2. Not null, and the entry is active (isActive is true). 3. Not null, and the entry is marked deleted (isActive is false). Constructing the table (Figure 5.15) consists of allocating space and then setting each HashEntry reference to null. contains(x), shown in Figure 5.16 (on page 186), invokes private methods isActive and findPos. The private method findPos performs the collision resolution. We ensure in the insert routine that the hash table is at least twice as large as the number of elements in the table, so quadratic resolution will always work. In the implementation in Figure 5.16, elements that are marked as deleted count as being in the table. This can cause problems, because the table can get too full prematurely. We shall discuss this item presently. Lines 25 through 28 represent the fast way of doing quadratic resolution. From the deﬁnition of the quadratic resolution function, f(i) = f(i −1) + 2i −1, so the next cell to try is a distance from the previous cell tried and this distance increases by 2 on successive probes. If the new location is past the array, it can be put back in range by subtracting TableSize. This is faster than the obvious method, because it avoids the multiplication and division that seem to be required. An important warning: The order of testing at lines 22 and 23 is important. Don’t switch it! The ﬁnal routine is insertion. As with separate chaining hashing, we do nothing if x is already present. It is a simple modiﬁcation to do something else. Otherwise, we place it at the spot suggested by the findPos routine. The code is shown in Figure 5.17 (on page 187). If the load factor exceeds 0.5, the table is full and we enlarge the hash table. This is called rehashing, and is discussed in Section 5.5. Although quadratic probing eliminates primary clustering, elements that hash to the same position will probe the same alternative cells. This is known as secondary clustering. Secondary clustering is a slight theoretical blemish. Simulation results suggest that it generally causes less than an extra half probe per search. The following technique eliminates this, but does so at the cost of computing an extra hash function. 5.4.3 Double Hashing The last collision resolution method we will examine is double hashing. For double hashing, one popular choice is f(i) = i·hash2(x). This formula says that we apply a second hash function to x and probe at a distance hash2(x), 2hash2(x), . . . , and so on. A poor choice of hash2(x) would be disastrous. For instance, the obvious choice hash2(x) = x mod 9 would not help if 99 were inserted into the input in the previous examples. Thus, the function must never evaluate to zero. It is also important to make sure all cells can be probed (this is not possible in the example below, because the table size is not prime). A function such as hash2(x) = R −(x mod R), with R a prime smaller than TableSize, will work well. If we choose R = 7, then Figure 5.18 shows the results of inserting the same keys as before.

Chapter 5 Hashing public class QuadraticProbingHashTable<AnyType> { public QuadraticProbingHashTable( ) { /* Figure 5.15 */ } public QuadraticProbingHashTable( int size ) { /* Figure 5.15 */ } public void makeEmpty( ) { /* Figure 5.15 */ } public boolean contains( AnyType x ) { /* Figure 5.16 */ } public void insert( AnyType x ) { /* Figure 5.17 */ } public void remove( AnyType x ) { /* Figure 5.17 */ } private static class HashEntry<AnyType> { public AnyType element; // the element public boolean isActive; // false if marked deleted public HashEntry( AnyType e ) { this( e, true ); } public HashEntry( AnyType e, boolean i ) { element = e; isActive = i; } } private static final int DEFAULT_TABLE_SIZE = 11; private HashEntry<AnyType> [ ] array; // The array of elements private int currentSize; // The number of occupied cells private void allocateArray( int arraySize ) { /* Figure 5.15 */ } private boolean isActive( int currentPos ) { /* Figure 5.16 */ } private int findPos( AnyType x ) { /* Figure 5.16 */ } private void rehash( ) { /* Figure 5.22 */ } private int myhash( AnyType x ) { /* See online code */ } private static int nextPrime( int n ) { /* See online code */ } private static boolean isPrime( int n ) { /* See online code */ } } Figure 5.14 Class skeleton for hash tables using probing strategies, including the nested HashEntry class

5.4 Hash Tables Without Linked Lists /** * Construct the hash table. */ public QuadraticProbingHashTable( ) { this( DEFAULT_TABLE_SIZE ); } /** * Construct the hash table. * @param size the approximate initial size. */ public QuadraticProbingHashTable( int size ) { allocateArray( size ); makeEmpty( ); } /** * Make the hash table logically empty. */ public void makeEmpty( ) { currentSize = 0; for( int i = 0; i < array.length; i++ ) array[ i ] = null; } /** * Internal method to allocate array. * @param arraySize the size of the array. */ private void allocateArray( int arraySize ) { array = new HashEntry[ nextPrime( arraySize ) ]; } Figure 5.15 Routines to initialize hash table The ﬁrst collision occurs when 49 is inserted. hash2(49) = 7−0 = 7, so 49 is inserted in position 6. hash2(58) = 7 −2 = 5, so 58 is inserted at location 3. Finally, 69 collides and is inserted at a distance hash2(69) = 7 −6 = 1 away. If we tried to insert 60 in position 0, we would have a collision. Since hash2(60) = 7 −4 = 3, we would then try positions 3, 6, 9, and then 2 until an empty spot is found. It is generally possible to ﬁnd some bad case, but there are not too many here.

Chapter 5 Hashing /** * Find an item in the hash table. * @param x the item to search for. * @return the matching item. */ public boolean contains( AnyType x ) { int currentPos = findPos( x ); return isActive( currentPos ); } /** * Method that performs quadratic probing resolution in half-empty table. * @param x the item to search for. * @return the position where the search terminates. */ private int findPos( AnyType x ) { int offset = 1; int currentPos = myhash( x ); while( array[ currentPos ] != null && !array[ currentPos ].element.equals( x ) ) { currentPos += offset; // Compute ith probe offset += 2; if( currentPos >= array.length ) currentPos -= array.length; } return currentPos; } /** * Return true if currentPos exists and is active. * @param currentPos the result of a call to findPos. * @return true if currentPos is active. */ private boolean isActive( int currentPos ) { return array[ currentPos ] != null && array[ currentPos ].isActive; } Figure 5.16 contains routine (and private helpers) for hashing with quadratic probing

5.4 Hash Tables Without Linked Lists /** * Insert into the hash table. If the item is * already present, do nothing. * @param x the item to insert. */ public void insert( AnyType x ) { // Insert x as active int currentPos = findPos( x ); if( isActive( currentPos ) ) return; array[ currentPos ] = new HashEntry<>( x, true ); // Rehash; see Section 5.5 if( ++currentSize > array.length / 2 ) rehash( ); } /** * Remove from the hash table. * @param x the item to remove. */ public void remove( AnyType x ) { int currentPos = findPos( x ); if( isActive( currentPos ) ) array[ currentPos ].isActive = false; } Figure 5.17 insert routine for hash tables with quadratic probing As we have said before, the size of our sample hash table is not prime. We have done this for convenience in computing the hash function, but it is worth seeing why it is important to make sure the table size is prime when double hashing is used. If we attempt to insert 23 into the table, it would collide with 58. Since hash2(23) = 7 −2 = 5, and the table size is 10, we essentially have only one alternative location, and it is already taken. Thus, if the table size is not prime, it is possible to run out of alternative locations prematurely. However, if double hashing is correctly implemented, simulations imply that the expected number of probes is almost the same as for a random collision resolution strategy. This makes double hashing theoretically interesting. Quadratic probing, however, does not require the use of a second hash function and is thus likely to be simpler and faster in practice, especially for keys like strings whose hash functions are expensive to compute.

Chapter 5 Hashing Empty Table After 89 After 18 After 49 After 58 After 69 Figure 5.18 Hash table with double hashing, after each insertion 5.5 Rehashing If the table gets too full, the running time for the operations will start taking too long and insertions might fail for open addressing hashing with quadratic resolution. This can happen if there are too many removals intermixed with insertions. A solution, then, is to build another table that is about twice as big (with an associated new hash function) and scan down the entire original hash table, computing the new hash value for each (nondeleted) element and inserting it in the new table. As an example, suppose the elements 13, 15, 24, and 6 are inserted into a linear probing hash table of size 7. The hash function is h(x) = x mod 7. Suppose linear probing is used to resolve collisions. The resulting hash table appears in Figure 5.19. If 23 is inserted into the table, the resulting table in Figure 5.20 will be over 70 percent full. Because the table is so full, a new table is created. The size of this table is 17, because this is the ﬁrst prime that is twice as large as the old table size. The new hash function is then h(x) = x mod 17. The old table is scanned, and elements 6, 15, 23, 24, and 13 are inserted into the new table. The resulting table appears in Figure 5.21. This entire operation is called rehashing. This is obviously a very expensive operation; the running time is O(N), since there are N elements to rehash and the table size is roughly 2N, but it is actually not all that bad, because it happens very infrequently. In particular, there must have been N/2 insertions prior to the last rehash, so it essentially adds a constant cost to each insertion.2 If this data structure is part of the program, the effect is not noticeable. On the other hand, if the hashing is performed as part of an interactive system, then the unfortunate user whose insertion caused a rehash could see a slowdown. Rehashing can be implemented in several ways with quadratic probing. One alternative is to rehash as soon as the table is half full. The other extreme is to rehash only when an 2 This is why the new table is made twice as large as the old table.

5.6 Hash Tables in the Standard Library Figure 5.19 Hash table with linear probing with input 13, 15, 6, 24 Figure 5.20 Hash table with linear probing after 23 is inserted insertion fails. A third, middle-of-the-road strategy is to rehash when the table reaches a certain load factor. Since performance does degrade as the load factor increases, the third strategy, implemented with a good cutoff, could be best. Rehashing for separate chaining hash tables is similar. Figure 5.22 shows that rehashing is simple to implement, and provides an implementation for separate chaining rehashing also. 5.6 Hash Tables in the Standard Library The Standard Library includes hash table implementations of Set and Map, namely HashSet and HashMap. The items in the HashSet (or the keys in the HashMap) must provide an equals and hashCode method, as described earlier in Section 5.3. The HashSet and HashMap are currently implemented using separate chaining hashing. These classes can be used if it is not important for the entries to be viewable in sorted order. For instance, in the word-changing example in Section 4.8, there were three maps:

Chapter 5 Hashing Figure 5.21 Linear probing hash table after rehashing 1. A map in which the key is a word length, and the value is a collection of all words of that word length. 2. A map in which the key is a representative, and the value is a collection of all words with that representative. 3. A map in which the key is a word, and the value is a collection of all words that differ in only one character from that word. Because the order in which word lengths are processed does not matter, the ﬁrst map can be a HashMap. Because the representatives are not even needed after the second map is built, the second map can be a HashMap. The third map can also be a HashMap, unless we want printHighChangeables to alphabetically list the subset of words that can be changed into a large number of other words. The performance of a HashMap can often be superior to a TreeMap, but it is hard to know for sure without writing the code both ways. Thus, in cases where either a HashMap or TreeMap is acceptable, it is preferable to declare variables using the interface type Map and then change the instantiation from a TreeMap to a HashMap, and perform timing tests.

5.6 Hash Tables in the Standard Library /** * Rehashing for quadratic probing hash table. */ private void rehash( ) { HashEntry<AnyType> [ ] oldArray = array; // Create a new double-sized, empty table allocateArray( nextPrime( 2 * oldArray.length ) ); currentSize = 0; // Copy table over for( int i = 0; i < oldArray.length; i++ ) if( oldArray[ i ] != null && oldArray[ i ].isActive ) insert( oldArray[ i ].element ); } /** * Rehashing for separate chaining hash table. */ private void rehash( ) { List<AnyType> [ ] oldLists = theLists; // Create new double-sized, empty table theLists = new List[ nextPrime( 2 * theLists.length ) ]; for( int j = 0; j < theLists.length; j++ ) theLists[ j ] = new LinkedList<>( ); // Copy table over currentSize = 0; for( int i = 0; i < oldLists.length; i++ ) for( AnyType item : oldLists[ i ] ) insert( item ); } Figure 5.22 Rehashing for both separate chaining and probing hash tables In Java, library types that can be reasonably inserted into a HashSet or as keys into a HashMap already have equals and hashCode deﬁned. In particular the String class has a hashCode that is essentially the code in Figure 5.4 with lines 14–16 removed, and 37 replaced with 31. Because the expensive part of the hash table operations is computing the hashCode, the hashCode method in the String class contains an important optimization: Each String object stores internally the value of its hashCode. Initially it is 0, but if hashCode is invoked, the value is remembered. Thus if hashCode is computed on the same String

Chapter 5 Hashing public final class String { public int hashCode( ) { if( hash != 0 ) return hash; for( int i = 0; i < length( ); i++ ) hash = hash * 31 + (int) charAt( i ); return hash; } private int hash = 0; } Figure 5.23 Excerpt of String class hashCode object a second time, we can avoid the expensive recomputation. This technique is called caching the hash code, and represents another classic time-space tradeoff. Figure 5.23 shows an implementation of the String class that caches the hash code. Caching the hash code works only because Strings are immutable: If the String were allowed to change, it would invalidate the hashCode, and the hashCode would have to be reset back to 0. Although two String objects with the same state must have their hash codes computed independently, there are many situations in which the same String object keeps having its hash code queried. One situation where caching the hash code helps occurs during rehashing, because all the Strings involved in the rehashing have already had their hash codes cached. On the other hand, caching the hash code does not help in the representative map for the word changing example. Each of the representatives is a different String computed by removing a character from a larger String, and thus each individual String has to have its hash code computed separately. However, in the third map, caching the hash code does help, because the keys are only Strings that were stored in the original array of Strings. 5.7 Hash Tables with Worst-Case O(1) Access The hash tables that we have examined so far all have the property that with reasonable load factors, and appropriate hash functions, we can expect O(1) cost on average for insertions, removes, and searching. But what is the expected worst case for a search assuming a reasonably well-behaved hash function? For separate chaining, assuming a load factor of 1, this is one version of the classic balls and bins problem: Given N balls placed randomly (uniformly) in N bins, what is the expected number of balls in the most occupied bin? The answer is well known to be Θ(log N/ log log N), meaning that on average, we expect some queries to take nearly

5.7 Hash Tables with Worst-Case O(1) Access logarithmic time. Similar types of bounds are observed (or provable) for the length of the longest expected probe sequence in a probing hash table. We would like to obtain O(1) worst-case cost. In some applications, such as hardware implementations of lookup tables for routers and memory caches, it is especially important that the search have a deﬁnite (i.e., constant) amount of completion time. Let us assume that N is known in advance, so no rehashing is needed. If we are allowed to rearrange items as they are inserted, then O(1) worst-case cost is achievable for searches. In the remainder of this section we describe the earliest solution to this problem, namely perfect hashing, and then two more recent approaches that appear to offer promising alternatives to the classic hashing schemes that have been prevalent for many years. 5.7.1 Perfect Hashing Suppose, for purposes of simpliﬁcation, that all N items are known in advance. If a separate chaining implementation could guarantee that each list had at most a constant number of items, we would be done. We know that as we make more lists, the lists will on average be shorter, so theoretically if we have enough lists, then with a reasonably high probability we might expect to have no collisions at all! But there are two fundamental problems with this approach: First, the number of lists might be unreasonably large; second, even with lots of lists, we might still get unlucky. The second problem is relatively easy to address in principle. Suppose we choose the number of lists to be M (i.e., TableSize is M), which is sufﬁciently large to guarantee that with probability at least 1 2, there will be no collisions. Then if a collision is detected, we simply clear out the table and try again using a different hash function that is independent of the ﬁrst. If we still get a collision, we try a third hash function, and so on. The expected number of trials will be at most 2 (since the success probability is 1 2), and this is all folded into the insertion cost. Section 5.8 discusses the crucial issue of how to produce additional hash functions. So we are left with determining how large M, the number of lists, needs to be. Unfortunately, M needs to be quite large; speciﬁcally M = (N2). However, if M = N2, we can show that the table is collision free with probability at least 1 2, and this result can be used to make a workable modiﬁcation to our basic approach. Theorem 5.2. If N balls are placed into M = N2 bins, the probability that no bin has more than one ball is less than 1 2. Proof. If a pair (i, j) of balls are placed in the same bin, we call that a collision. Let Ci,j be the expected number of collisions produced by any two balls (i, j). Clearly the probability that any two speciﬁed balls collide is 1/M, and thus Ci,j is 1/M, since the number of collisions that involve the pair (i, j) is either 0 or 1. Thus the expected number of collisions in the entire table is  (i,j), i<j Ci,j. Since there are N(N −1)/2 pairs, this sum is N(N −1)/(2M) = N(N −1)/(2N2) < 1 2. Since the expected number of collisions is below 1 2, the probability that there is even one collision must also be below 1 2.

Chapter 5 Hashing 22 = 4 22 = 4 32 = 9 Figure 5.24 Perfect hashing table using secondary hash tables Of course, using N2 lists is impractical. However, the preceding analysis suggests the following alternative: Use only N bins, but resolve the collisions in each bin by using hash tables instead of linked lists. The idea is that because the bins are expected to have only a few items each, the hash table that is used for each bin can be quadratic in the bin size. Figure 5.24 shows the basic structure. Here, the primary hash table has ten bins. Bins 1, 3, 5, and 7 are all empty. Bins 0, 4, and 8 have one item, so they are resolved by a secondary hash table with one position. Bins 2 and 6 have two items, so they will be resolved into a secondary hash table with four (22) positions. And bin 9 has three items, so it is resolved into a secondary hash table with nine (32) positions. As with the original idea, each secondary hash table will be constructed using a different hash function until it is collision free. The primary hash table can also be constructed several times if the number of collisions that are produced is higher than required. This scheme is known as perfect hashing. All that remains to be shown is that the total size of the secondary hash tables is indeed expected to be linear. Theorem 5.3. If N items are placed into a primary hash table containing N bins, then the total size of the secondary hash tables has expected value at most 2N. Proof. Using the same logic as in the proof of Theorem 5.2, the expected number of pairwise collisions is at most N(N −1)/2N, or (N −1)/2. Let bi be the number of items that hash to position i in the primary hash table; observe that b2 i space is used for this cell in the secondary hash table, and that this accounts for bi (bi −1)/2 pairwise collisions, which we will call ci. Thus the amount of space used for the ith secondary hash table is 2ci + bi. The total space is then 2  ci +  bi. The total number of collisions is

5.7 Hash Tables with Worst-Case O(1) Access (N −1)/2 (from the ﬁrst sentence of this proof); the total number of items is of course N, so we obtain a total secondary space requirement of 2(N −1)/2 + N < 2N. Thus the probability that the total secondary space requirement is more than 4N is at most 1 2 (since, otherwise the expected value would be higher than 2N), so we can keep choosing hash functions for the primary table until we generate the appropriate secondary space requirement. Once that is done, each secondary hash table will itself require only an average of two trials to be collision free. After the tables are built, any lookup can be done in two probes. Perfect hashing works if the items are all known in advance. There are dynamic schemes that allow insertions and deletions (dynamic perfect hashing), but instead we will investigate two newer alternatives that are relatively easy to code and appear to be competitive in practice with the classic hashing algorithms. 5.7.2 Cuckoo Hashing From our previous discussion, we know that in the balls and bins problem, if N items are randomly tossed into N bins, the size of the largest bin is expected to be Θ(log N/ log log N). Since this bound has been known for a long time, and the problem has been well studied by mathematicians, it was surprising when in the mid 1990s, it was shown that if, at each toss, two bins were randomly chosen and the item was tossed into the more empty bin (at the time), then the size of the largest bin would only be Θ(log log N), a signiﬁcantly lower number. Quickly, a host of potential algorithms and data structures arose out of this new concept of the “power of two choices.” One of the ideas is cuckoo hashing. In cuckoo hashing, suppose we have N items. We maintain two tables each more than half empty, and we have two independent has functions that can assign each item to a position in each table. Cuckoo hashing maintains the invariant that an item is always stored in one of these two locations. As an example, Figure 5.25 shows a potential cuckoo hash table for six items, with two tables of size 5 (These tables are too small, but serve well as an example). Based on the randomly chosen hash functions, item A can be at either position 0 in Table 1, or position 2 in Table 2. Item F can be at either position 3 in Table 1, or position 4 in Table 2, and so on. Immediately, this implies that a search in a cuckoo hash table requires at most two Table 1 B C E Table 2 D A F A: 0, 2 B: 0, 0 C: 1, 4 D: 1, 0 E: 3, 2 F: 3, 4 Figure 5.25 Potential cuckoo hash table. Hash functions are shown on the right. For these six items, there are only three valid positions in Table 1 and three valid positions in Table 2, so it is not clear that this arrangement can easily be found

Chapter 5 Hashing table accesses, and a remove is trivial, once the item is located (lazy deletion is not needed now!). But there is an important detail: How is the table built? For instance, in Figure 5.25, there are only three available locations in the ﬁrst table for the six items, and there are only three available locations in the second table for the six items. So there are only six available locations for these six items, and thus we must ﬁnd an ideal matching of slots for our six items. Clearly if there were a seventh item G with locations 1 for Table 1 and 2 for Table 2, it could not be inserted into the table by any algorithm (the seven items would be competing for six table locations). One could argue that this means that the table would simply be too loaded (G would yield a 0.70 load factor), but at the same time, if the table had thousands of items, and were lightly loaded, but we had A, B, C, D, E, F, G with these hash positions, it would still be impossible to insert all seven of those items. So it is not at all obvious that this scheme can be made to work. The answer in this situation would be to pick another hash function, and this can be ﬁne as long as it is unlikely that this situation occurs. The cuckoo hashing algorithm itself is simple: To insert a new item x, ﬁrst make sure it is not already there. We can then use the ﬁrst hash function and if the (ﬁrst) table location is empty, the item can be placed. So Figure 5.26 shows the result of inserting A into an empty hash table. Suppose now we want to insert B, which has hash locations 0 in Table 1 and 0 in Table 2. For the remainder of the algorithm we will use (h1, h2) to specify the two locations, so B’s locations are given by (0, 0). Table 1 is already occupied in position 0. At this point there are two options: One is to look in Table 2. The problem is that position 0 in Table 2 could also be occupied. It happens that in this case it is not, but the algorithm that the standard cuckoo hash table uses does not bother to look. Instead, it preemptively places the new item B in Table 1. In order to do so, it must displace A, so A moves to Table 2, using its Table 2 hash location, which is position 2. The result is shown in Figure 5.27. It is easy to insert C, and this is shown in Figure 5.28. Next we want to insert D, with hash locations (1, 0). But the Table 1 location (position 1) is already taken. Note also, that the Table 2 location is not already taken, but we don’t look there. Instead, we have D replace C, and then C goes into Table 2 at position 4, as suggested by its second hash function. The resulting tables are shown in Figure 5.29. After this is done, E can be easily inserted. So far, so good, but can we now insert F? Figures 5.30 to 5.33 show that this algorithm successfully inserts F, by displacing E, then A, and then B. Table 1 A Table 2 A: 0, 2 Figure 5.26 Cuckoo hash table after insertion of A

5.7 Hash Tables with Worst-Case O(1) Access Table 1 B Table 2 A A: 0, 2 B: 0, 0 Figure 5.27 Cuckoo hash table after insertion of B Table 1 B C Table 2 A A: 0, 2 B: 0, 0 C: 1, 4 Figure 5.28 Cuckoo hash table after insertion of C Table 1 B D E Table 2 A C A: 0, 2 B: 0, 0 C: 1, 4 D: 1, 0 E: 3, 2 Figure 5.29 Cuckoo hash table after insertion of D Table 1 B D F Table 2 A C A: 0, 2 B: 0, 0 C: 1, 4 D: 1, 0 E: 3, 2 F: 3, 4 Figure 5.30 Cuckoo hash table starting the insertion of F into the table in Figure 5.29. First, F displaces E Clearly as we mentioned before, we cannot successfully insert G, with hash locations (1, 2). If we were to try, we would displace D, then B, then A, E, F, and C, and then C would try to go back into Table 1, position 1, displacing G which was placed there at the start. This would get us to Figure 5.34. So now G would try its alternate in Table 2 (location 2) and then displace A, which would displace B, which would displace D, which

Chapter 5 Hashing Table 1 B D F Table 2 E C A: 0, 2 B: 0, 0 C: 1, 4 D: 1, 0 E: 3, 2 F: 3, 4 Figure 5.31 Continuing the insertion of F into the table in Figure 5.29. Next, E displaces A Table 1 A D F Table 2 E C A: 0, 2 B: 0, 0 C: 1, 4 D: 1, 0 E: 3, 2 F: 3, 4 Figure 5.32 Continuing the insertion of F into the table in Figure 5.29. Next, A displaces B Table 1 A D F Table 2 B E C A: 0, 2 B: 0, 0 C: 1, 4 D: 1, 0 E: 3, 2 F: 3, 4 Figure 5.33 Completing the insertion of F into the table in Figure 5.29. Miraculously (?), B ﬁnds an empty position in Table 2 would displace C, which would displace F, which would displace E, which would now displace G from position 2. At this point, G would be in a cycle. The central issue then concerns questions such as what is the probability of there being a cycle that prevents the insertion from completing, and what is the expected number of displacements required for a successful insertion? Fortunately, if the table’s load factor is below 0.5, an analysis shows that the probability of a cycle is very low, that the expected number of displacements is a small constant, and that it is extremely unlikely that a successful insertion would require more than O(log N) displacements. As such, we can simply rebuild the tables with new hash functions after a certain number of displacements are detected. More precisely, the probability that a single insertion would require a new set of hash functions can be made to be O(1/N2); the new hash functions themselves generate N more insertions to rebuild the table, but even so, this means the rebuilding cost is minimal.

5.7 Hash Tables with Worst-Case O(1) Access Table 1 B C E Table 2 D A F A: 0, 2 B: 0, 0 C: 1, 4 D: 1, 0 E: 3, 2 F: 3, 4 G: 1, 2 Figure 5.34 Inserting G into the table in Figure 5.33. G displaces D, which displaces B, which displaces A, which displaces E, which displaces F, which displaces C, which displaces G. It is not yet hopeless since when G is displaced, we would now try the other hash table, at position 2. However, while that could be successful in general, in this case there is a cycle and the insertion will not terminate 1 item per cell 2 items per cell 4 items per cell 2 hash functions 0.49 0.86 0.93 3 hash functions 0.91 0.97 0.98 4 hash functions 0.97 0.99 0.999 Figure 5.35 Maximum load factors for cuckoo hashing variations However, if the table’s load factor is at 0.5 or higher, then the probability of a cycle becomes drastically higher, and this scheme is unlikely to work well at all. After the publication of cuckoo hashing, numerous extensions were proposed. For instance, instead of two tables, we can use a higher number of tables, such as 3 or 4. While this increases the cost of a lookup, it also drastically increases the theoretical space utilization. In some applications the lookups through separate hash functions can be done in parallel and thus cost little to no additional time. Another extension is to allow each table to store multiple keys; again this can increase space utilization and also make it easier to do insertions and can be more cache-friendly. Various combinations are possible, as shown in Figure 5.35. And ﬁnally, often cuckoo hash tables are implemented as one giant table with two (or more) hash functions that probe the entire table, and some variations attempt to place an item in the second hash table immediately if there is an available spot, rather than starting a sequence of displacements. Cuckoo Hash Table Implementation Implementing cuckoo hashing requires a collection of hash functions; simply using hashCode to generate the collection of hash functions makes no sense, since any hashCode collisions will result in collisions in all the hash functions. Figure 5.36 shows a simple interface that can be used to send families of hash functions to the cuckoo hash table. Figure 5.37 provides the class skeleton for cuckoo hashing. We will code a variant that will allow an arbitrary number of hash functions (speciﬁed by the HashFamily object that constructs the hash table) which uses a single array that is addressed by all the

Chapter 5 Hashing public interface HashFamily<AnyType> { int hash( AnyType x, int which ); int getNumberOfFunctions( ); void generateNewFunctions( ); } Figure 5.36 Generic HashFamily interface for cuckoo hashing hash functions. Thus our implementation differs from the classic notion of two separately addressable hash tables. We can implement the classic version by making relatively minor changes to the code; however, this version provided in this section seems to perform better in tests using simple hash functions. In Figure 5.37, we specify that the maximum load for the table is 0.4; if the load factor of the table is about to exceed this limit, an automatic table expansion is performed. We also deﬁne ALLOWED_REHASHES, which speciﬁes how many rehashes we will perform if evictions take too long. In theory, ALLOWED_REHASHES can be inﬁnite, since we expect only a small constant number of rehashes are needed; in practice, depending on several factors such as the number of hash functions, the quality of the hash functions, and the load factor, the rehashes could signiﬁcantly slow things down, and it might be worthwhile to expand the table, even though this will cost space. The data representation for the cuckoo hash table is straightforward: We store a simple array, the current size, and the collections of hash functions, represented in a HashFamily instance. We also maintain the number of hash functions, even though that is always obtainable from the HashFamily instance. Figure 5.38 shows the constructor and doClear methods, and these are straightforward. Figure 5.39 shows a pair of private methods. The ﬁrst, myHash, is used to select the appropriate hash function and then scale it into a valid array index. The second, findPos, consults all the hash functions to return the index containing item x, or −1 if x is not found. findPos is then used by contains and remove in Figures 5.40 and 5.41, respectively, and we can see that those methods are easy to implement. The difﬁcult routine is insertion. In Figure 5.42, we can see that the basic plan is to check to see if the item is already present, returning if so. Otherwise, we check to see if the table is fully loaded, and if so, we expand it. Finally we call a helper routine to do all the dirty work. The helper routine for insertion is shown in Figure 5.43. We declare a variable rehashes to keep track of how many attempts have been made to rehash in this insertion. Our insertion routine is mutually recursive: If needed, insert eventually calls rehash, which eventually calls back into insert. Thus rehashes is declared in an outer scope for code simplicity. Our basic logic is different from the classic scheme. We have already tested that the item to insert is not already present. At lines 15–25, we check to see if any of the valid

public class CuckooHashTable<AnyType> { public CuckooHashTable( HashFamily<? super AnyType> hf ) { /* Figure 5.38 */ } public CuckooHashTable( HashFamily<? super AnyType> hf, int size ); { /* Figure 5.38 */ } public void makeEmpty( ) { doClear( ); } public boolean contains( AnyType x ) { /* Figure 5.40 */ } private int myhash( AnyType x, int which ) { /* Figure 5.39 */ } private int findPos( AnyType x ) { /* Figure 5.39 */ } public boolean remove( AnyType x ) { /* Figure 5.41 */ } public boolean insert( AnyType x ) { /* Figure 5.42 */ } private void expand( ) { /* Figure 5.44 */ } private void rehash( ) { /* Figure 5.44 */ } private void doClear( ) { /* Figure 5.38 */ } private void allocateArray( int arraySize ) { array = (AnyType[]) new Object[ arraySize ]; } private static final double MAX_LOAD = 0.4; private static final int ALLOWED_REHASHES = 1; private static final int DEFAULT_TABLE_SIZE = 101; private final HashFamily<? super AnyType> hashFunctions; private final int numHashFunctions; private AnyType [ ] array; private int currentSize; } Figure 5.37 Class skeleton for cuckoo hashing

Chapter 5 Hashing /** * Construct the hash table. * @param hf the hash family */ public CuckooHashTable( HashFamily<? super AnyType> hf ) { this( hf, DEFAULT_TABLE_SIZE ); } /** * Construct the hash table. * @param hf the hash family * @param size the approximate initial size. */ public CuckooHashTable( HashFamily<? super AnyType> hf, int size ) { allocateArray( nextPrime( size ) ); doClear( ); hashFunctions = hf; numHashFunctions = hf.getNumberOfFunctions( ); } private void doClear( ) { currentSize = 0; for( int i = 0; i < array.length; i++ ) array[ i ] = null; } Figure 5.38 Routines to initialize the cuckoo hash table positions are empty; if so, we place our item in the ﬁrst available position and we are done. Otherwise, we evict one of the existing items. However, there are some tricky issues: r Evicting the ﬁrst item did not perform well in experiments. r Evicting the last item did not perform well in experiments. r Evicting the items in sequence (i.e., the ﬁrst eviction uses hash function 0, the next uses hash function 1, etc.) did not perform well in experiments. r Evicting the item purely randomly did not perform well in experiments: In particular, with only two hash functions, it tended to create cycles. To alleviate the last problem, we maintain the last position that was evicted and if our random item was the last evicted item, we select a new random item. This will loop forever if used with two hash functions, and both hash functions happen to probe to the same

5.7 Hash Tables with Worst-Case O(1) Access /** * Compute the hash code for x using specified hash function * @param x the item * @param which the hash function * @return the hash code */ private int myhash( AnyType x, int which ) { int hashVal = hashFunctions.hash( x, which ); hashVal %= array.length; if( hashVal < 0 ) hashVal += array.length; return hashVal; } /** * Method that searches all hash function places. * @param x the item to search for. * @return the position where the search terminates, or -1 if not found. */ private int findPos( AnyType x ) { for( int i = 0; i < numHashFunctions; i++ ) { int pos = myhash( x, i ); if( array[ pos ] != null && array[ pos ].equals( x ) ) return pos; } return -1; } Figure 5.39 Routines to ﬁnd the location of an item in the cuckoo hash table and to compute the hash code for a given table location, and that location was a prior eviction, so we limit the loop to ﬁve iterations (deliberately using an odd number). The code for expand and rehash is shown in Figure 5.44. expand creates a larger array but keeps the same hash functions. The zero-parameter rehash leaves the array size unchanged but creates a new array that is populated with newly chosen hash functions. Finally, Figure 5.45 shows the StringHashFamily class that provides a set of simple hash functions for strings. These hash functions replace the constant 37 in Figure 5.4 with randomly chosen numbers (not necessarily prime).

Chapter 5 Hashing /** * Find an item in the hash table. * @param x the item to search for. * @return true if item is found. */ public boolean contains( AnyType x ) { return findPos( x ) != -1; } Figure 5.40 Routine to search a cuckoo hash table /** * Remove from the hash table. * @param x the item to remove. * @return true if item was found and removed */ public boolean remove( AnyType x ) { int pos = findPos( x ); if( pos != -1 ) { array[ pos ] = null; currentSize--; } return pos != -1; } Figure 5.41 Routine to remove from a cuckoo hash table The beneﬁts of cuckoo hashing include the worst-case constant lookup and deletion times, the avoidance of lazy deletion and extra data, and the potential for parallelism. However, cuckoo hashing is extremely sensitive to the choice of hash functions; the inventors of the cuckoo hash table reported that many of the standard hash functions that they attempted performed poorly in tests. Furthermore, although the insertion time is expected to be constant time as long as the load factor is below 1 2, the bound that has been shown for the expected insertion cost for classic cuckoo hashing with two separate tables (both with load factor λ) is roughly 1/(1 −(4 λ2)1/3), which deteriorates rapidly as the load factor gets close to 1 2 (the formula itself makes no sense when λ equals or exceeds 1 2). Using lower load factors or more than two hash functions seems like a reasonable alternative.

5.7 Hash Tables with Worst-Case O(1) Access /** * Insert into the hash table. If the item is * already present, return false. * @param x the item to insert. */ public boolean insert( AnyType x ) { if( contains( x ) ) return false; if( currentSize >= array.length * MAX_LOAD ) expand( ); return insertHelper1( x ); } Figure 5.42 Public insert routine for cuckoo hashing 5.7.3 Hopscotch Hashing Hopscotch hashing is a new algorithm that tries to improve on the classic linear probing algorithm. Recall that in linear probing, cells are tried in sequential order, starting from the hash location. Because of primary and secondary clustering, this sequence can be long on average as the table gets loaded, and thus many improvements such as quadratic probing, double hashing, and so forth, have been proposed to reduce the number of collisions. However, on some modern architectures, the locality produced by probing adjacent cells is a more signiﬁcant factor than the extra probes, and linear probing can still be practical or even a best choice. The idea of hopscotch hashing is to bound the maximal length of the probe sequence by a predetermined constant that is optimized to the underlying computer’s architecture. Doing so would give constant-time lookups in the worst case, and like cuckoo hashing, the lookup could be parallelized to simultaneously check the bounded set of possible locations. If an insertion would place a new item too far from its hash location, then we efﬁciently go backward toward the hash location, evicting potential items. If we are careful, the evictions can be done quickly and guarantee that those evicted are not placed too far from their hash locations. The algorithm is deterministic in that given a hash function, either the items can be evicted or they can’t. The latter case implies that the table is likely too crowded, and a rehash is in order; but this would happen only at extremely high load factors, exceeding 0.9. For a table with a load factor of 1 2, the failure probability is almost zero (Exercise 5.23). Let MAX_DIST be the chosen bound on the maximum probe sequence. This means that item x must be found somewhere in the MAX_DIST positions listed in hash(x), hash(x) + 1, . . . , hash(x) + (MAX_DIST −1). In order to efﬁciently process evictions, we maintain information that tells for each position x, whether the item in the alternate position is occupied by an element that hashes to position x.

private int rehashes = 0; private Random r = new Random( ); private boolean insertHelper1( AnyType x ) { final int COUNT_LIMIT = 100; while( true ) { int lastPos = -1; int pos; for( int count = 0; count < COUNT_LIMIT; count++ ) { for( int i = 0; i < numHashFunctions; i++ ) { pos = myhash( x, i ); if( array[ pos ] == null ) { array[ pos ] = x; currentSize++; return true; } } // none of the spots are available. Evict out a random one int i = 0; do { pos = myhash( x, r.nextInt( numHashFunctions ) ) ; } while( pos == lastPos && i++ < 5 ); AnyType tmp = array[ lastPos = pos ]; array[ pos ] = x; x = tmp; } if( ++rehashes > ALLOWED_REHASHES ) { expand( ); // Make the table bigger rehashes = 0; // Reset the # of rehashes } else rehash( ); // Same table size, new hash functions } } Figure 5.43 Insertion routine for cuckoo hashing uses a different algorithm that chooses the item to evict randomly, attempting not to re-evict the last item. The table will attempt to select new hash functions (rehash) if there are too many evictions and will expand if there are too many rehashes

5.7 Hash Tables with Worst-Case O(1) Access private void expand( ) { rehash( (int) ( array.length / MAX_LOAD ) ); } private void rehash( ) { hashFunctions.generateNewFunctions( ); rehash( array.length ); } private void rehash( int newLength ) { AnyType [ ] oldArray = array; allocateArray( nextPrime( newLength ) ); currentSize = 0; // Copy table over for( AnyType str : oldArray ) if( str != null ) insert( str ); } Figure 5.44 Rehashing and expanding code for cuckoo hash tables As an example, Figure 5.46 shows a fairly crowded hopscotch hash table, using MAX_DIST = 4. The bit array for position 6 shows that only position 6 has an item (C) with hash value 6: Only the ﬁrst bit of Hop[6] is set. Hop[7] has the ﬁrst two bits set, indicating that positions 7 and 8 (A and D) are occupied with items whose hash value is 7. And Hop[8] has only the third bit set, indicating that the item in position 10 (E) has hash value 8. If MAX_DIST is no more than 32, the Hop array is essentially an array of 32-bit integers, so the additional space requirement is not substantial. If Hop[pos] contains all 1’s for some pos, then an attempt to insert an item whose hash value is pos will clearly fail, since there would now be MAX_DIST + 1 items trying to reside within MAX_DIST positions of pos—an impossibility. Continuing the example, suppose we now insert item H with hash value 9. Our normal linear probing would try to place it in position 13, but that is too far from the hash value of 9. So instead, we look to evict an item and relocate it to position 13. The only candidates to go into position 13 would be items with hash value of 10, 11, 12, or 13. If we examine Hop[10], we see that there are no candidates with hash value 10. But Hop[11] produces a candidate, G, with value 11 that can be placed into position 13. Since position 11 is now close enough to the hash value of H, we can now insert H. These steps, along with the changes to the Hop information, are shown in Figure 5.47. Finally, we will attempt to insert I whose hash value is 6. Linear probing suggests position 14, but of course that is too far away. Thus we look for in Hop[11], and it tells

Chapter 5 Hashing public class StringHashFamily implements HashFamily<String> { private final int [ ] MULTIPLIERS; private final java.util.Random r = new java.util.Random( ); public StringHashFamily( int d ) { MULTIPLIERS = new int[ d ]; generateNewFunctions( ); } public int getNumberOfFunctions( ) { return MULTIPLIERS.length; } public void generateNewFunctions( ) { for( int i = 0; i < MULTIPLIERS.length; i++ ) MULTIPLIERS[ i ] = r.nextInt( ); } public int hash( String x, int which ) { final int multiplier = MULTIPLIERS[ which ]; int hashVal = 0; for( int i = 0; i < x.length( ); i++ ) hashVal = multiplier * hashVal + x.charAt( i ); return hashVal; } } Figure 5.45 Casual string hashing for cuckoo hashing; these hash functions do not provably satisfy the requirements needed for cuckoo hashing but offer decent performance if the table is not highly loaded and the alternate insertion routine in Figure 5.43 is used us that G can move down, freeing up position 13. Now that 13 is vacant, we can look in Hop[10] to ﬁnd another element to evict. But Hop[10] has all zeros in the ﬁrst three positions, so there are no items with hash value 10 that can be moved. So we examine Hop[11]. There we ﬁnd all zeros in the ﬁrst two positions. So we try Hop[12], where we need the ﬁrst position to be 1, which it is. Thus F can move down. These two steps are shown in Figure 5.48. Notice, that if this were not the case—for instance if hash(F) were 9 instead of 12—we would be stuck and have to rehash.

5.7 Hash Tables with Worst-Case O(1) Access Item Hop . . . C A D B E G F . . . A: 7 B: 9 C: 6 D: 7 E: 8 F: 12 G: 11 Figure 5.46 Hopscotch hashing table. The hops tell which of the positions in the block are occupied with cells containing this hash value. Thus Hop[8] = 0010 indicates that only position 10 currently contains items whose hash value is 8, while positions 8, 9, and 11 do not Item Hop . . . C A D B E G F . . . → Item Hop . . . C A D B E F G . . . → Item Hop . . . C A D B E H F G . . . A: 7 B: 9 C: 6 D: 7 E: 8 F: 12 G: 11 H: 9 Figure 5.47 Hopscotch hashing table. Attempting to insert H. Linear probing suggests location 13, but that is too far, so we evict G from position 11 to ﬁnd a closer position However, that is not a problem with our algorithm; instead there would simply be no way to place all of C, I, A, D, E, B, H, and F (if F’s hash value were 9); these items would all have hash values between 6 and 9, and would thus need to be placed in the seven spots between 6 and 12. But that would be eight items in seven spots—an impossibility. However, since this is not the case for our example, and we have evicted an item from position 12, we can now continue, and Figure 5.49 shows the remaining eviction from position 9, and subsequent placement of I.

Chapter 5 Hashing Item Hop . . . C A D B E H F G . . . → Item Hop . . . C A D B E H F G . . . → Item Hop . . . C A D B E H F G . . . A: 7 B: 9 C: 6 D: 7 E: 8 F: 12 G: 11 H: 9 I: 6 Figure 5.48 Hopscotch hashing table. Attempting to insert I. Linear probing suggests location 14, but that is too far; consulting Hop[11], we see that G can move down, leaving position 13 open. Consulting Hop[10] gives no suggestions. Hop[11] does not help either (why?), so Hop[12] suggests moving F Item Hop . . . C A D B E H F G . . . → Item Hop . . . C A D E H B F G . . . → Item Hop . . . C A D I E H B F G . . . A: 7 B: 9 C: 6 D: 7 E: 8 F: 12 G: 11 H: 9 I: 6 Figure 5.49 Hopscotch hashing table. Insertion of I continues: Next B is evicted, and ﬁnally we have a spot that is close enough to the hash value and can insert I Hopscotch hashing is a relatively new algorithm, but the initial experimental results are very promising, especially for applications that make use of multiple processors and require signiﬁcant parallelism and concurrency. It remains to be seen if either cuckoo hashing or hopscotch hashing emerge as a practical alternative to the classic separate chaining and linear/quadratic probing schemes.

5.8 Universal Hashing 5.8 Universal Hashing Although hash tables are very efﬁcient and have average cost per operation, assuming appropriate load factors, their analysis and performance depend on the hash function having two fundamental properties: 1. The hash function must be computable in constant time (i.e., independent of the number of items in the hash table). 2. The hash function must distribute its items uniformly among the array slots. In particular, if the hash function is poor, then all bets are off, and the cost per operation can be linear. In this section, we discuss universal hash functions, which allow us to choose the hash function randomly in such a way that condition 2 above is satisﬁed. As in Section 5.7, we use M to represent Tablesize. Although a strong motivation for the use of universal hash functions is to provide theoretical justiﬁcation for the assumptions used in the classic hash table analyses, these functions can also be used in applications that require a high level of robustness, in which worst-case (or even substantially degraded) performance, perhaps based on inputs generated by a saboteur or hacker, simply cannot be tolerated. As in Section 5.7, we use M to represent TableSize. Deﬁnition 5.1. A family H of hash functions is universal, if for any x ̸= y, the number of hash functions h in H for which h(x) = h(y) is at most |H|/M. Notice that this deﬁnition holds for each pair of items, rather than being averaged over all pairs of items. The deﬁnition above means that if we choose a hash function randomly from a universal family H, then the probability of a collision between any two distinct items is at most 1/M, and when adding into a table with N items, the probability of a collision at the initial point is at most N/M, or the load factor. The use of a universal hash function for separate chaining or hopscotch hashing would be sufﬁcient to meet the assumptions used in the analysis of those data structures. However, it is not sufﬁcient for cuckoo hashing, which requires a stronger notion of independence. In cuckoo hashing, we ﬁrst see if there is a vacant location; if there is not, and we do an eviction, a different item is now involved in looking for a vacant location. This repeats until we ﬁnd the vacant location, or decide to rehash [generally within O(log N) steps]. In order for the analysis to work, each step must have a collision probability of N/M independently, with a different item x being subject to the hash function. We can formalize this independence requirement in the following deﬁnition.

Chapter 5 Hashing Deﬁnition 5.2. A family H of hash functions is k-universal, if for any x1 ̸= y1, x2 ̸= y2, . . . , xk ̸= yk, the number of hash functions h in H for which h(x1) = h(y1), h(x2) = h(y2), . . ., and h(xk) = h(yk) is at most |H|/Mk. With this deﬁnition, we see that the analysis of cuckoo hashing requires an O(log N)- universal hash function (after that many evictions, we give up and rehash). In this section we look only at universal hash functions. To design a simple universal hash function, we will assume ﬁrst that we are mapping very large integers into smaller integers ranging from 0 to M −1. Let p be a prime larger than the largest input key. Our universal family H will consist of the following set of functions, where a and b are chosen randomly: H = {Ha,b(x) = ((ax + b) mod p) mod M, where 1 ≤a ≤p −1, 0 ≤b ≤p −1} For example, in this family, three of the possible random choices of (a, b) yield three different hash functions: H3,7(x) = ((3x + 7) mod p) mod M H4,1(x) = ((4x + 1) mod p) mod M H8,0(x) = ((8x) mod p) mod M Observe that there are p(p −1) possible hash functions that can be chosen. Theorem 5.4. The hash family H = {Ha,b(x) = ((ax + b) mod p) mod M, where 1 ≤a ≤p −1, 0 ≤b ≤p −1} is universal. Proof. Let x and y be distinct values, with x > y, such that Ha,b(x) = Ha,b(y). Clearly if (ax + b) mod p is equal to (ay + b) mod p, then we will have a collision. However, this cannot happen: Subtracting equations yields a(x −y) ≡0 (mod p), which would mean that p divides a or p divides x −y, since p is prime. But neither can happen, since both a and x −y are between 1 and p −1. So let r = (ax + b) mod p and let s = (ay + b) mod p, and by the above argument, r ̸= s. Thus there are p possible values for r, and for each r, there are p −1 possible values for s, for a total of p(p −1) possible (r, s) pairs. Notice that the number of (a, b) pairs and the number of (r, s) pairs is identical; thus each (r, s) pair will correspond to exactly one (a, b) pair if we can solve for (a, b) in terms of r and s. But that is easy: As before, subtracting equations yields a(x −y) ≡(r −s) (mod p), which means that by multiplying both sides by the unique multiplicative inverse of (x −y) (which must exist, since x −y is not zero and p is prime), we obtain a, in terms of r and s. Then b follows. Finally, this means that the probability that x and y collide is equal to the probability that r ≡s (mod M), and the above analysis allows us to assume that r and s are chosen randomly, rather than a and b. Immediate intuition would place this probability at 1/M, but that would only be true if p were an exact multiple of M, and all possible

5.8 Universal Hashing (r, s) pairs were equally likely. Since p is prime, and r ̸= s, that is not exactly true, so a more careful analysis is needed. For a given r, the number of values of s that can collide mod M is at most ⌈p/M⌉−1 (the −1 is because r ̸= s). It is easy to see that this is at most (p −1)/M. Thus the probability that r and s will generate a collision is at most 1/M (we divide by p −1, because as mentioned earlier in the proof, there are only p −1 choices for s given r). This implies that the hash family is universal. Implementation of this hash function would seem to require two mod operations: one mod p and the second mod M. Figure 5.50 shows a simple implementation in Java, assuming that M is signiﬁcantly less than the 231 −1 limit of a Java integer. Because the computations must now be exactly as speciﬁed, and thus overﬂow is no longer acceptable, we promote to 64-bit long computations. However, we are allowed to choose any prime p, as long as it is larger than M. Hence, it makes sense to choose a prime that is most favorable for computations. One such prime is p = 231 −1. Prime numbers of this form are known as Mersenne primes; other Mersenne primes include 25 −1, 261 −1 and 289 −1. Just as a multiplication by a Mersenne prime such as 31 can be implemented by a bit shift and a subtract, a mod operation involving a Mersenne prime can also be implemented by a bit shift and an addition: Suppose r ≡y (mod p). If we divide y by (p + 1), then y = q′(p + 1) + r′, where q′ and r′ are the quotient and remainder, respectively. Thus, r ≡q′(p + 1) + r′ (mod p). And since (p + 1) ≡1 (mod p), we obtain r ≡q′ + r′ (mod p). Figure 5.51 implements this idea, which is known as the Carter-Wegman trick. On line 8, the bit shift computes the quotient and the bitwise and computes the remainder public static int universalHash( int x, int A, int B, int P, int M ) { return (int) ( ( ( (long) A * x ) + B ) % P ) % M; } Figure 5.50 Simple implementation of universal hashing public static final int DIGS = 31; public static final int mersennep = (1<<DIGS) - 1; public static int universalHash( int x, int A, int B, int M ) { long hashVal = (long) A * x + B; hashVal = ( ( hashVal >> DIGS ) + ( hashVal & mersennep ) ); if( hashVal >= mersennep ) hashVal -= mersennep; return (int) hashVal % M; } Figure 5.51 Simple implementation of universal hashing

Chapter 5 Hashing when dividing by (p + 1); these bitwise operations work because (p + 1) is an exact power of two. Since the remainder could be almost as large as p, the resulting sum might be larger than p, so we scale it back down at lines 9 and 10. Universal hash functions exist for strings also. First, choose any prime p, larger than M (and larger than the largest character code). Then use our standard string hashing function, choosing the multiplier randomly between 1 and p−1 and returning an intermediate hash value between 0 and p −1, inclusive. Finally, apply a universal hash function to generate the ﬁnal hash value between 0 and M −1. 5.9 Extendible Hashing Our last topic in this chapter deals with the case where the amount of data is too large to ﬁt in main memory. As we saw in Chapter 4, the main consideration then is the number of disk accesses required to retrieve data. As before, we assume that at any point we have N records to store; the value of N changes over time. Furthermore, at most M records ﬁt in one disk block. We will use M = 4 in this section. If either probing hashing or separate chaining hashing is used, the major problem is that collisions could cause several blocks to be examined during a search, even for a well-distributed hash table. Furthermore, when the table gets too full, an extremely expensive rehashing step must be performed, which requires O(N) disk accesses. A clever alternative, known as extendible hashing, allows a search to be performed in two disk accesses. Insertions also require few disk accesses. We recall from Chapter 4 that a B-tree has depth O(logM/2 N). As M increases, the depth of a B-tree decreases. We could in theory choose M to be so large that the depth of the B-tree would be 1. Then any search after the ﬁrst would take one disk access, since, presumably, the root node could be stored in main memory. The problem with this strategy is that the branching factor is so high that it would take considerable processing to determine which leaf the data were in. If the time to perform this step could be reduced, then we would have a practical scheme. This is exactly the strategy used by extendible hashing. Let us suppose, for the moment, that our data consist of several six-bit integers. Figure 5.52 shows an extendible hashing scheme for these data. The root of the “tree” contains four links determined by the leading two bits of the data. Each leaf has up to M = 4 elements. It happens that in each leaf the ﬁrst two bits are identical; this is indicated by the number in parentheses. To be more formal, D will represent the number of bits used by the root, which is sometimes known as the directory. The number of entries in the directory is thus 2D. dL is the number of leading bits that all the elements of some leaf L have in common. dL will depend on the particular leaf, and dL ≤D. Suppose that we want to insert the key 100100. This would go into the third leaf, but as the third leaf is already full, there is no room. We thus split this leaf into two leaves, which are now determined by the ﬁrst three bits. This requires increasing the directory size to 3. These changes are reﬂected in Figure 5.53.

5.9 Extendible Hashing (2) (2) (2) (2) Figure 5.52 Extendible hashing: original data (2) (2) (3) (3) (2) Figure 5.53 Extendible hashing: after insertion of 100100 and directory split Notice that all the leaves not involved in the split are now pointed to by two adjacent directory entries. Thus, although an entire directory is rewritten, none of the other leaves is actually accessed. If the key 000000 is now inserted, then the ﬁrst leaf is split, generating two leaves with dL = 3. Since D = 3, the only change required in the directory is the updating of the 000 and 001 links. See Figure 5.54.

Chapter 5 Hashing (3) (3) (2) (3) (3) (2) Figure 5.54 Extendible hashing: after insertion of 000000 and leaf split This very simple strategy provides quick access times for insert and search operations on large databases. There are a few important details we have not considered. First, it is possible that several directory splits will be required if the elements in a leaf agree in more than D + 1 leading bits. For instance, starting at the original example, with D = 2, if 111010, 111011, and ﬁnally 111100 are inserted, the directory size must be increased to 4 to distinguish between the ﬁve keys. This is an easy detail to take care of, but must not be forgotten. Second, there is the possibility of duplicate keys; if there are more than M duplicates, then this algorithm does not work at all. In this case, some other arrangements need to be made. These possibilities suggest that it is important for the bits to be fairly random. This can be accomplished by hashing the keys into a reasonably long integer—hence the name. We close by mentioning some of the performance properties of extendible hashing, which are derived after a very difﬁcult analysis. These results are based on the reasonable assumption that the bit patterns are uniformly distributed. The expected number of leaves is (N/M) log2 e. Thus the average leaf is ln 2 = 0.69 full. This is the same as for B-trees, which is not entirely surprising, since for both data structures new nodes are created when the (M + 1)th entry is added. The more surprising result is that the expected size of the directory (in other words, 2D) is O(N1+1/M/M). If M is very small, then the directory can get unduly large. In this case, we can have the leaves contain links to the records instead of the actual records, thus increasing the value of M. This adds a second disk access to each search operation in order to maintain a smaller directory. If the directory is too large to ﬁt in main memory, the second disk access would be needed anyway.

C H A P T E R 6 Priority Queues (Heaps) Although jobs sent to a printer are generally placed on a queue, this might not always be the best thing to do. For instance, one job might be particularly important, so it might be desirable to allow that job to be run as soon as the printer is available. Conversely, if, when the printer becomes available, there are several 1-page jobs and one 100-page job, it might be reasonable to make the long job go last, even if it is not the last job submitted. (Unfortunately, most systems do not do this, which can be particularly annoying at times.) Similarly, in a multiuser environment, the operating system scheduler must decide which of several processes to run. Generally a process is allowed to run only for a ﬁxed period of time. One algorithm uses a queue. Jobs are initially placed at the end of the queue. The scheduler will repeatedly take the ﬁrst job on the queue, run it until either it ﬁnishes or its time limit is up, and place it at the end of the queue if it does not ﬁnish. This strategy is generally not appropriate, because very short jobs will seem to take a long time because of the wait involved to run. Generally, it is important that short jobs ﬁnish as fast as possible, so these jobs should have precedence over jobs that have already been running. Furthermore, some jobs that are not short are still very important and should also have precedence. This particular application seems to require a special kind of queue, known as a priority queue. In this chapter, we will discuss r Efﬁcient implementation of the priority queue ADT. r Uses of priority queues. r Advanced implementations of priority queues. The data structures we will see are among the most elegant in computer science. 6.1 Model A priority queue is a data structure that allows at least the following two operations: insert, which does the obvious thing; and deleteMin, which ﬁnds, returns, and removes the minimum element in the priority queue. The insert operation is the equivalent of enqueue, and deleteMin is the priority queue equivalent of the queue’s dequeue operation. As with most data structures, it is sometimes possible to add other operations, but these are extensions and not part of the basic model depicted in Figure 6.1. Priority queues have many applications besides operating systems. In Chapter 7, we will see how priority queues are used for external sorting. Priority queues are also

Chapter 6 Priority Queues (Heaps) insert Priority Queue deleteMin Figure 6.1 Basic model of a priority queue important in the implementation of greedy algorithms, which operate by repeatedly ﬁnding a minimum; we will see speciﬁc examples in Chapters 9 and 10. In this chapter we will see a use of priority queues in discrete event simulation. 6.2 Simple Implementations There are several obvious ways to implement a priority queue. We could use a simple linked list, performing insertions at the front in O(1) and traversing the list, which requires O(N) time, to delete the minimum. Alternatively, we could insist that the list be kept always sorted; this makes insertions expensive (O(N)) and deleteMins cheap (O(1)). The former is probably the better idea of the two, based on the fact that there are never more deleteMins than insertions. Another way of implementing priority queues would be to use a binary search tree. This gives an O(log N) average running time for both operations. This is true in spite of the fact that although the insertions are random, the deletions are not. Recall that the only element we ever delete is the minimum. Repeatedly removing a node that is in the left subtree would seem to hurt the balance of the tree by making the right subtree heavy. However, the right subtree is random. In the worst case, where the deleteMins have depleted the left subtree, the right subtree would have at most twice as many elements as it should. This adds only a small constant to its expected depth. Notice that the bound can be made into a worst-case bound by using a balanced tree; this protects one against bad insertion sequences. Using a search tree could be overkill because it supports a host of operations that are not required. The basic data structure we will use will not require links and will support both operations in O(log N) worst-case time. Insertion will actually take constant time on average, and our implementation will allow building a priority queue of N items in linear time, if no deletions intervene. We will then discuss how to implement priority queues to support efﬁcient merging. This additional operation seems to complicate matters a bit and apparently requires the use of a linked structure. 6.3 Binary Heap The implementation we will use is known as a binary heap. Its use is so common for priority queue implementations that, in the context of priority queues, when the word heap is used without a qualiﬁer, it is generally assumed to be referring to this implementation

6.3 Binary Heap of the data structure. In this section, we will refer to binary heaps merely as heaps. Like binary search trees, heaps have two properties, namely, a structure property and a heaporder property. As with AVL trees, an operation on a heap can destroy one of the properties, so a heap operation must not terminate until all heap properties are in order. This turns out to be simple to do. 6.3.1 Structure Property A heap is a binary tree that is completely ﬁlled, with the possible exception of the bottom level, which is ﬁlled from left to right. Such a tree is known as a complete binary tree. Figure 6.2 shows an example. It is easy to show that a complete binary tree of height h has between 2h and 2h+1 −1 nodes. This implies that the height of a complete binary tree is ⌊log N⌋, which is clearly O(log N). An important observation is that because a complete binary tree is so regular, it can be represented in an array and no links are necessary. The array in Figure 6.3 corresponds to the heap in Figure 6.2. For any element in array position i, the left child is in position 2i, the right child is in the cell after the left child (2i + 1), and the parent is in position ⌊i/2⌋. Thus not only are links not required, but the operations required to traverse the tree are extremely simple A B C F G E D H J I Figure 6.2 A complete binary tree A B C D E F G H I J Figure 6.3 Array implementation of complete binary tree

Chapter 6 Priority Queues (Heaps) public class BinaryHeap<AnyType extends Comparable<? super AnyType>> { public BinaryHeap( ) { /* See online code */ } public BinaryHeap( int capacity ) { /* See online code */ } public BinaryHeap( AnyType [ ] items ) { /* Figure 6.14 */ } public void insert( AnyType x ) { /* Figure 6.8 */ } public AnyType findMin( ) { /* See online code */ } public AnyType deleteMin( ) { /* Figure 6.12 */ } public boolean isEmpty( ) { /* See online code */ } public void makeEmpty( ) { /* See online code */ } private static final int DEFAULT_CAPACITY = 10; private int currentSize; // Number of elements in heap private AnyType [ ] array; // The heap array private void percolateDown( int hole ) { /* Figure 6.12 */ } private void buildHeap( ) { /* Figure 6.14 */ } private void enlargeArray( int newSize ) { /* See online code */ } } Figure 6.4 Class skeleton for priority queue and likely to be very fast on most computers. The only problem with this implementation is that an estimate of the maximum heap size is required in advance, but typically this is not a problem (and we can resize if necessary). In Figure 6.3, the limit on the heap size is 13 elements. The array has a position 0; more on this later. A heap data structure will, then, consist of an array (of Comparable objects) and an integer representing the current heap size. Figure 6.4 shows a priority queue skeleton. Throughout this chapter, we shall draw the heaps as trees, with the implication that an actual implementation will use simple arrays.

6.3 Binary Heap Figure 6.5 Two complete trees (only the left tree is a heap) 6.3.2 Heap-Order Property The property that allows operations to be performed quickly is the heap-order property. Since we want to be able to ﬁnd the minimum quickly, it makes sense that the smallest element should be at the root. If we consider that any subtree should also be a heap, then any node should be smaller than all of its descendants. Applying this logic, we arrive at the heap-order property. In a heap, for every node X, the key in the parent of X is smaller than (or equal to) the key in X, with the exception of the root (which has no parent).1 In Figure 6.5 the tree on the left is a heap, but the tree on the right is not (the dashed line shows the violation of heap order). By the heap-order property, the minimum element can always be found at the root. Thus, we get the extra operation, findMin, in constant time. 6.3.3 Basic Heap Operations It is easy (both conceptually and practically) to perform the two required operations. All the work involves ensuring that the heap-order property is maintained. insert To insert an element X into the heap, we create a hole in the next available location, since otherwise the tree will not be complete. If X can be placed in the hole without violating heap order, then we do so and are done. Otherwise we slide the element that is in the hole’s parent node into the hole, thus bubbling the hole up toward the root. We continue this process until X can be placed in the hole. Figure 6.6 shows that to insert 14, we create a hole in the next available heap location. Inserting 14 in the hole would violate the heaporder property, so 31 is slid down into the hole. This strategy is continued in Figure 6.7 until the correct location for 14 is found. 1 Analogously, we can declare a (max) heap, which enables us to efﬁciently ﬁnd and remove the maximum element, by changing the heap-order property. Thus, a priority queue can be used to ﬁnd either a minimum or a maximum, but this needs to be decided ahead of time.

Chapter 6 Priority Queues (Heaps) Figure 6.6 Attempt to insert 14: creating the hole and bubbling the hole up Figure 6.7 The remaining two steps to insert 14 in previous heap This general strategy is known as a percolate up; the new element is percolated up the heap until the correct location is found. Insertion is easily implemented with the code shown in Figure 6.8. We could have implemented the percolation in the insert routine by performing repeated swaps until the correct order was established, but a swap requires three assignment statements. If an element is percolated up d levels, the number of assignments performed by the swaps would be 3d. Our method uses d + 1 assignments. If the element to be inserted is the new minimum, it will be pushed all the way to the top. At some point, hole will be 1 and we will want to break out of the loop. We could do this with an explicit test, or we can put a reference to the inserted item in position 0 in order to make the loop terminate. We elect to place x into position 0 in our implementation. The time to do the insertion could be as much as O(log N), if the element to be inserted is the new minimum and is percolated all the way to the root. On average, the percolation terminates early; it has been shown that 2.607 comparisons are required on average to perform an insert, so the average insert moves an element up 1.607 levels.

6.3 Binary Heap /** * Insert into the priority queue, maintaining heap order. * Duplicates are allowed. * @param x the item to insert. */ public void insert( AnyType x ) { if( currentSize == array.length - 1 ) enlargeArray( array.length * 2 + 1 ); // Percolate up int hole = ++currentSize; for( array[ 0 ] = x; x.compareTo( array[ hole / 2 ] ) < 0; hole /= 2 ) array[ hole ] = array[ hole / 2 ]; array[ hole ] = x; } Figure 6.8 Procedure to insert into a binary heap Figure 6.9 Creation of the hole at the root deleteMin deleteMins are handled in a similar manner as insertions. Finding the minimum is easy; the hard part is removing it. When the minimum is removed, a hole is created at the root. Since the heap now becomes one smaller, it follows that the last element X in the heap must move somewhere in the heap. If X can be placed in the hole, then we are done. This is unlikely, so we slide the smaller of the hole’s children into the hole, thus pushing the hole down one level. We repeat this step until X can be placed in the hole. Thus, our action is to place X in its correct spot along a path from the root containing minimum children. In Figure 6.9 the left ﬁgure shows a heap prior to the deleteMin. After 13 is removed, we must now try to place 31 in the heap. The value 31 cannot be placed in the hole, because this would violate heap order. Thus, we place the smaller child (14) in the hole, sliding the hole down one level (see Figure 6.10). We repeat this again, and since 31 is larger than 19,

Chapter 6 Priority Queues (Heaps) Figure 6.10 Next two steps in deleteMin Figure 6.11 Last two steps in deleteMin we place 19 into the hole and create a new hole one level deeper. We then place 26 in the hole and create a new hole on the bottom level since once again, 31 is too large. Finally, we are able to place 31 in the hole (Figure 6.11). This general strategy is known as a percolate down. We use the same technique as in the insert routine to avoid the use of swaps in this routine. A frequent implementation error in heaps occurs when there are an even number of elements in the heap, and the one node that has only one child is encountered. You must make sure not to assume that there are always two children, so this usually involves an extra test. In the code depicted in Figure 6.12, we’ve done this test at line 29. One extremely tricky solution is always to ensure that your algorithm thinks every node has two children. Do this by placing a sentinel, of value higher than any in the heap, at the spot after the heap ends, at the start of each percolate down when the heap size is even. You should think very carefully before attempting this, and you must put in a prominent comment if you do use this technique. Although this eliminates the need to test for the presence of a right child, you cannot eliminate the requirement that you test when you reach the bottom, because this would require a sentinel for every leaf. The worst-case running time for this operation is O(log N). On average, the element that is placed at the root is percolated almost to the bottom of the heap (which is the level it came from), so the average running time is O(log N).

6.3 Binary Heap /** * Remove the smallest item from the priority queue. * @return the smallest item, or throw UnderflowException, if empty. */ public AnyType deleteMin( ) { if( isEmpty( ) ) throw new UnderflowException( ); AnyType minItem = findMin( ); array[ 1 ] = array[ currentSize-- ]; percolateDown( 1 ); return minItem; } /** * Internal method to percolate down in the heap. * @param hole the index at which the percolate begins. */ private void percolateDown( int hole ) { int child; AnyType tmp = array[ hole ]; for( ; hole * 2 <= currentSize; hole = child ) { child = hole * 2; if( child != currentSize && array[ child + 1 ].compareTo( array[ child ] ) < 0 ) child++; if( array[ child ].compareTo( tmp ) < 0 ) array[ hole ] = array[ child ]; else break; } array[ hole ] = tmp; } Figure 6.12 Method to perform deleteMin in a binary heap

Chapter 6 Priority Queues (Heaps) Figure 6.13 A very large complete binary tree 6.3.4 Other Heap Operations Notice that although ﬁnding the minimum can be performed in constant time, a heap designed to ﬁnd the minimum element (also known as a (min)heap) is of no help whatsoever in ﬁnding the maximum element. In fact, a heap has very little ordering information, so there is no way to ﬁnd any particular element without a linear scan through the entire heap. To see this, consider the large heap structure (the elements are not shown) in Figure 6.13, where we see that the only information known about the maximum element is that it is at one of the leaves. Half the elements, though, are contained in leaves, so this is practically useless information. For this reason, if it is important to know where elements are, some other data structure, such as a hash table, must be used in addition to the heap. (Recall that the model does not allow looking inside the heap.) If we assume that the position of every element is known by some other method, then several other operations become cheap. The ﬁrst three operations below all run in logarithmic worst-case time. decreaseKey The decreaseKey(p, ) operation lowers the value of the item at position p by a positive amount  . Since this might violate the heap order, it must be ﬁxed by a percolate up. This operation could be useful to system administrators: They can make their programs run with highest priority. increaseKey The increaseKey(p, ) operation increases the value of the item at position p by a positive amount  . This is done with a percolate down. Many schedulers automatically drop the priority of a process that is consuming excessive CPU time.

6.3 Binary Heap delete The delete(p) operation removes the node at position p from the heap. This is done by ﬁrst performing decreaseKey(p,∞) and then performing deleteMin(). When a process is terminated by a user (instead of ﬁnishing normally), it must be removed from the priority queue. buildHeap The binary heap is sometimes constructed from an initial collection of items. This constructor takes as input N items and places them into a heap. Obviously, this can be done with N successive inserts. Since each insert will take O(1) average and O(log N) worstcase time, the total running time of this algorithm would be O(N) average but O(N log N) worst-case. Since this is a special instruction and there are no other operations intervening, and we already know that the instruction can be performed in linear average time, it is reasonable to expect that with reasonable care a linear time bound can be guaranteed. The general algorithm is to place the N items into the tree in any order, maintaining the structure property. Then, if percolateDown(i) percolates down from node i, the buildHeap routine in Figure 6.14 can be used by the constructor to create a heap-ordered tree. The ﬁrst tree in Figure 6.15 is the unordered tree. The seven remaining trees in Figures 6.15 through 6.18 show the result of each of the seven percolateDowns. Each dashed line corresponds to two comparisons: one to ﬁnd the smaller child and one to compare the smaller child with the node. Notice that there are only 10 dashed lines in the entire algorithm (there could have been an 11th—where?) corresponding to 20 comparisons. To bound the running time of buildHeap, we must bound the number of dashed lines. This can be done by computing the sum of the heights of all the nodes in the heap, which is the maximum number of dashed lines. What we would like to show is that this sum is O(N). Theorem 6.1. For the perfect binary tree of height h containing 2h+1−1 nodes, the sum of the heights of the nodes is 2h+1 −1 −(h + 1). Proof. It is easy to see that this tree consists of 1 node at height h, 2 nodes at height h −1, 22 nodes at height h −2, and in general 2i nodes at height h −i. The sum of the heights of all the nodes is then S = h  i=0 2i(h −i) = h + 2(h −1) + 4(h −2) + 8(h −3) + 16(h −4) + · · · + 2h−1(1) (6.1) Multiplying by 2 gives the equation 2S = 2h + 4(h −1) + 8(h −2) + 16(h −3) + · · · + 2h(1) (6.2) We subtract these two equations and obtain Equation (6.3). We ﬁnd that certain terms almost cancel. For instance, we have 2h −2(h −1) = 2, 4(h −1) −4(h −2) = 4, and so on. The last term in Equation (6.2), 2h, does not appear in Equation (6.1);

Chapter 6 Priority Queues (Heaps) /** * Construct the binary heap given an array of items. */ public BinaryHeap( AnyType [ ] items ) { currentSize = items.length; array = (AnyType[]) new Comparable[ ( currentSize + 2 ) * 11 / 10 ]; int i = 1; for( AnyType item : items ) array[ i++ ] = item; buildHeap( ); } /** * Establish heap order property from an arbitrary * arrangement of items. Runs in linear time. */ private void buildHeap( ) { for( int i = currentSize / 2; i > 0; i-- ) percolateDown( i ); } Figure 6.14 Sketch of buildHeap 120 140 120 140 Figure 6.15 Left: initial heap; right: after percolateDown(7) thus, it appears in Equation (6.3). The ﬁrst term in Equation (6.1), h, does not appear in Equation (6.2); thus, −h appears in Equation (6.3). We obtain S = −h + 2 + 4 + 8 + · · · + 2h−1 + 2h = (2h+1 −1) −(h + 1) (6.3) which proves the theorem.

6.3 Binary Heap 120 140 120 140 Figure 6.16 Left: after percolateDown(6); right: after percolateDown(5) 120 140 120 140 Figure 6.17 Left: after percolateDown(4); right: after percolateDown(3) 120 140 120 140 Figure 6.18 Left: after percolateDown(2); right: after percolateDown(1) A complete tree is not a perfect binary tree, but the result we have obtained is an upper bound on the sum of the heights of the nodes in a complete tree. Since a complete tree has between 2h and 2h+1 nodes, this theorem implies that this sum is O(N), where N is the number of nodes. Although the result we have obtained is sufﬁcient to show that buildHeap is linear, the bound on the sum of the heights is not as strong as possible. For a complete tree with N = 2h nodes, the bound we have obtained is roughly 2N. The sum of the heights can be shown by induction to be N −b(N), where b(N) is the number of 1s in the binary representation of N.

Chapter 6 Priority Queues (Heaps) 6.4 Applications of Priority Queues We have already mentioned how priority queues are used in operating systems design. In Chapter 9, we will see how priority queues are used to implement several graph algorithms efﬁciently. Here we will show how to use priority queues to obtain solutions to two problems. 6.4.1 The Selection Problem The ﬁrst problem we will examine is the selection problem from Chapter 1. Recall that the input is a list of N elements, which can be totally ordered, and an integer k. The selection problem is to ﬁnd the kth largest element. Two algorithms were given in Chapter 1, but neither is very efﬁcient. The ﬁrst algorithm, which we shall call algorithm 1A, is to read the elements into an array and sort them, returning the appropriate element. Assuming a simple sorting algorithm, the running time is O(N2). The alternative algorithm, 1B, is to read k elements into an array and sort them. The smallest of these is in the kth position. We process the remaining elements one by one. As an element arrives, it is compared with the kth element in the array. If it is larger, then the kth element is removed, and the new element is placed in the correct place among the remaining k −1 elements. When the algorithm ends, the element in the kth position is the answer. The running time is O(N·k) (why?). If k = ⌈N/2⌉, then both algorithms are O(N2). Notice that for any k, we can solve the symmetric problem of ﬁnding the (N −k + 1)th smallest element, so k = ⌈N/2⌉is really the hardest case for these algorithms. This also happens to be the most interesting case, since this value of k is known as the median. We give two algorithms here, both of which run in O(N log N) in the extreme case of k = ⌈N/2⌉, which is a distinct improvement. Algorithm 6A For simplicity, we assume that we are interested in ﬁnding the kth smallest element. The algorithm is simple. We read the N elements into an array. We then apply the buildHeap algorithm to this array. Finally, we perform k deleteMin operations. The last element extracted from the heap is our answer. It should be clear that by changing the heap-order property, we could solve the original problem of ﬁnding the kth largest element. The correctness of the algorithm should be clear. The worst-case timing is O(N) to construct the heap, if buildHeap is used, and O(log N) for each deleteMin. Since there are k deleteMins, we obtain a total running time of O(N + k log N). If k = O(N/log N), then the running time is dominated by the buildHeap operation and is O(N). For larger values of k, the running time is O(k log N). If k = ⌈N/2⌉, then the running time is (N log N). Notice that if we run this program for k = N and record the values as they leave the heap, we will have essentially sorted the input ﬁle in O(N log N) time. In Chapter 7, we will reﬁne this idea to obtain a fast sorting algorithm known as heapsort.

6.4 Applications of Priority Queues Algorithm 6B For the second algorithm, we return to the original problem and ﬁnd the kth largest element. We use the idea from algorithm 1B. At any point in time we will maintain a set S of the k largest elements. After the ﬁrst k elements are read, when a new element is read it is compared with the kth largest element, which we denote by Sk. Notice that Sk is the smallest element in S. If the new element is larger, then it replaces Sk in S. S will then have a new smallest element, which may or may not be the newly added element. At the end of the input, we ﬁnd the smallest element in S and return it as the answer. This is essentially the same algorithm described in Chapter 1. Here, however, we will use a heap to implement S. The ﬁrst k elements are placed into the heap in total time O(k) with a call to buildHeap. The time to process each of the remaining elements is O(1), to test if the element goes into S, plus O(log k), to delete Sk and insert the new element if this is necessary. Thus, the total time is O(k + (N −k) log k) = O(N log k). This algorithm also gives a bound of (N log N) for ﬁnding the median. In Chapter 7, we will see how to solve this problem in O(N) average time. In Chapter 10, we will see an elegant, albeit impractical, algorithm to solve this problem in O(N) worst-case time. 6.4.2 Event Simulation In Section 3.7.3, we described an important queuing problem. Recall that we have a system, such as a bank, where customers arrive and wait in a line until one of k tellers is available. Customer arrival is governed by a probability distribution function, as is the service time (the amount of time to be served once a teller is available). We are interested in statistics such as how long on average a customer has to wait or how long the line might be. With certain probability distributions and values of k, these answers can be computed exactly. However, as k gets larger, the analysis becomes considerably more difﬁcult, so it is appealing to use a computer to simulate the operation of the bank. In this way, the bank ofﬁcers can determine how many tellers are needed to ensure reasonably smooth service. A simulation consists of processing events. The two events here are (a) a customer arriving and (b) a customer departing, thus freeing up a teller. We can use the probability functions to generate an input stream consisting of ordered pairs of arrival time and service time for each customer, sorted by arrival time. We do not need to use the exact time of day. Rather, we can use a quantum unit, which we will refer to as a tick. One way to do this simulation is to start a simulation clock at zero ticks. We then advance the clock one tick at a time, checking to see if there is an event. If there is, then we process the event(s) and compile statistics. When there are no customers left in the input stream and all the tellers are free, then the simulation is over. The problem with this simulation strategy is that its running time does not depend on the number of customers or events (there are two events per customer), but instead depends on the number of ticks, which is not really part of the input. To see why this is important, suppose we changed the clock units to milliticks and multiplied all the times in the input by 1,000. The result would be that the simulation would take 1,000 times longer!

Chapter 6 Priority Queues (Heaps) The key to avoiding this problem is to advance the clock to the next event time at each stage. This is conceptually easy to do. At any point, the next event that can occur is either (a) the next customer in the input ﬁle arrives or (b) one of the customers at a teller leaves. Since all the times when the events will happen are available, we just need to ﬁnd the event that happens nearest in the future and process that event. If the event is a departure, processing includes gathering statistics for the departing customer and checking the line (queue) to see whether there is another customer waiting. If so, we add that customer, process whatever statistics are required, compute the time when that customer will leave, and add that departure to the set of events waiting to happen. If the event is an arrival, we check for an available teller. If there is none, we place the arrival on the line (queue); otherwise we give the customer a teller, compute the customer’s departure time, and add the departure to the set of events waiting to happen. The waiting line for customers can be implemented as a queue. Since we need to ﬁnd the event nearest in the future, it is appropriate that the set of departures waiting to happen be organized in a priority queue. The next event is thus the next arrival or next departure (whichever is sooner); both are easily available. It is then straightforward, although possibly time-consuming, to write the simulation routines. If there are C customers (and thus 2C events) and k tellers, then the running time of the simulation would be O(C log(k + 1)) because computing and processing each event takes O(log H), where H = k + 1 is the size of the heap.2 6.5 d-Heaps Binary heaps are so simple that they are almost always used when priority queues are needed. A simple generalization is a d-heap, which is exactly like a binary heap except that all nodes have d children (thus, a binary heap is a 2-heap). Figure 6.19 shows a 3-heap. Notice that a d-heap is much shallower than a binary heap, improving the running time of inserts to O(logd N). However, for large d, the deleteMin operation is more expensive, because even though the tree is shallower, the minimum of d children must be found, which takes d −1 comparisons using a standard algorithm. This raises the time for this operation to O(d logd N). If d is a constant, both running times are, of course, O(log N). Although an array can still be used, the multiplications and divisions to ﬁnd children and parents are now by d, which, unless d is a power of 2, seriously increases the running time, because we can no longer implement division by a bit shift. d-heaps are interesting in theory, because there are many algorithms where the number of insertions is much greater than the number of deleteMins (and thus a theoretical speedup is possible). They are also of interest when the priority queue is too large to ﬁt entirely in main memory. In this case, a d-heap can be advantageous in much the same way as B-trees. Finally, there is evidence suggesting that 4-heaps may outperform binary heaps in practice. The most glaring weakness of the heap implementation, aside from the inability to perform finds, is that combining two heaps into one is a hard operation. This extra operation 2 We use O(C log(k + 1)) instead of O(C log k) to avoid confusion for the k = 1 case.

6.6 Leftist Heaps Figure 6.19 A d-heap is known as a merge. There are quite a few ways of implementing heaps so that the running time of a merge is O(log N). We will now discuss three data structures, of various complexity, that support the merge operation efﬁciently. We will defer any complicated analysis until Chapter 11. 6.6 Leftist Heaps It seems difﬁcult to design a data structure that efﬁciently supports merging (that is, processes a merge in o(N) time) and uses only an array, as in a binary heap. The reason for this is that merging would seem to require copying one array into another, which would take (N) time for equal-sized heaps. For this reason, all the advanced data structures that support efﬁcient merging require the use of a linked data structure. In practice, we can expect that this will make all the other operations slower. Like a binary heap, a leftist heap has both a structural property and an ordering property. Indeed, a leftist heap, like virtually all heaps used, has the same heap-order property we have already seen. Furthermore, a leftist heap is also a binary tree. The only difference between a leftist heap and a binary heap is that leftist heaps are not perfectly balanced but actually attempt to be very unbalanced. 6.6.1 Leftist Heap Property We deﬁne the null path length, npl(X), of any node X to be the length of the shortest path from X to a node without two children. Thus, the npl of a node with zero or one child is 0, while npl(null) = −1. In the tree in Figure 6.20, the null path lengths are indicated inside the tree nodes. Notice that the null path length of any node is 1 more than the minimum of the null path lengths of its children. This applies to nodes with less than two children because the null path length of null is −1. The leftist heap property is that for every node X in the heap, the null path length of the left child is at least as large as that of the right child. This property is satisﬁed by only one of the trees in Figure 6.20, namely, the tree on the left. This property actually goes

Chapter 6 Priority Queues (Heaps) 1* Figure 6.20 Null path lengths for two trees; only the left tree is leftist out of its way to ensure that the tree is unbalanced, because it clearly biases the tree to get deep toward the left. Indeed, a tree consisting of a long path of left nodes is possible (and actually preferable to facilitate merging)—hence the name leftist heap. Because leftist heaps tend to have deep left paths, it follows that the right path ought to be short. Indeed, the right path down a leftist heap is as short as any in the heap. Otherwise, there would be a path that goes through some node X and takes the left child. Then X would violate the leftist property. Theorem 6.2. A leftist tree with r nodes on the right path must have at least 2r −1 nodes. Proof. The proof is by induction. If r = 1, there must be at least one tree node. Otherwise, suppose that the theorem is true for 1, 2, . . . , r. Consider a leftist tree with r + 1 nodes on the right path. Then the root has a right subtree with r nodes on the right path, and a left subtree with at least r nodes on the right path (otherwise it would not be leftist). Applying the inductive hypothesis to these subtrees yields a minimum of 2r −1 nodes in each subtree. This plus the root gives at least 2r+1 −1 nodes in the tree, proving the theorem. From this theorem, it follows immediately that a leftist tree of N nodes has a right path containing at most ⌊log(N + 1)⌋nodes. The general idea for the leftist heap operations is to perform all the work on the right path, which is guaranteed to be short. The only tricky part is that performing inserts and merges on the right path could destroy the leftist heap property. It turns out to be extremely easy to restore the property. 6.6.2 Leftist Heap Operations The fundamental operation on leftist heaps is merging. Notice that insertion is merely a special case of merging, since we may view an insertion as a merge of a one-node heap with a larger heap. We will ﬁrst give a simple recursive solution and then show how this might

6.6 Leftist Heaps H1 H2 Figure 6.21 Two leftist heaps H1 and H2 Figure 6.22 Result of merging H2 with H1’s right subheap be done nonrecursively. Our input is the two leftist heaps, H1 and H2, in Figure 6.21. You should check that these heaps really are leftist. Notice that the smallest elements are at the roots. In addition to space for the data and left and right references, each node will have an entry that indicates the null path length. If either of the two heaps is empty, then we can return the other heap. Otherwise, to merge the two heaps, we compare their roots. First, we recursively merge the heap with the larger root with the right subheap of the heap with the smaller root. In our example, this means we recursively merge H2 with the subheap of H1 rooted at 8, obtaining the heap in Figure 6.22. Since this tree is formed recursively, and we have not yet ﬁnished the description of the algorithm, we cannot at this point show how this heap was obtained. However, it is reasonable to assume that the resulting tree is a leftist heap, because it was obtained via a recursive step. This is much like the inductive hypothesis in a proof by induction. Since we

Chapter 6 Priority Queues (Heaps) can handle the base case (which occurs when one tree is empty), we can assume that the recursive step works as long as we can ﬁnish the merge; this is rule 3 of recursion, which we discussed in Chapter 1. We now make this new heap the right child of the root of H1 (see Figure 6.23). Although the resulting heap satisﬁes the heap-order property, it is not leftist because the left subtree of the root has a null path length of 1 whereas the right subtree has a null path length of 2. Thus, the leftist property is violated at the root. However, it is easy to see that the remainder of the tree must be leftist. The right subtree of the root is leftist, because of the recursive step. The left subtree of the root has not been changed, so it too must still be leftist. Thus, we need only to ﬁx the root. We can make the entire tree leftist by merely swapping the root’s left and right children (Figure 6.24) and updating the null path length—the new null path length is 1 plus the null path length of the new right child—completing the merge. Notice that if the null path length is not updated, then all null path lengths will be 0, and the heap will not be leftist but merely random. In this case, the algorithm will work, but the time bound we will claim will no longer be valid. The description of the algorithm translates directly into code. The node class (Figure 6.25) is the same as the binary tree, except that it is augmented with the npl (null path length) ﬁeld. The leftist heap stores a reference to the root as its data member. We have seen in Chapter 4 that when an element is inserted into an empty binary tree, the node referenced by the root will need to change. We use the usual technique of implementing private recursive methods to do the merging. The class skeleton is also shown in Figure 6.25. The two merge routines (Figure 6.26) are drivers designed to remove special cases and ensure that H1 has the smaller root. The actual merging is performed in merge1 (Figure 6.27). The public merge method merges rhs into the controlling heap. rhs becomes empty. The alias test in the public method disallows h.merge(h). The time to perform the merge is proportional to the sum of the length of the right paths, because constant work is performed at each node visited during the recursive calls. Thus we obtain an O(log N) time bound to merge two leftist heaps. We can also perform this operation nonrecursively by essentially performing two passes. In the ﬁrst pass, we create a new tree by merging the right paths of both heaps. To do this, we arrange the nodes on the right paths of H1 and H2 in sorted order, keeping their respective left children. In our example, the new right path is 3, 6, 7, 8, 18 and the resulting tree is shown in Figure 6.28. A second pass is made up the heap, and child swaps are performed at nodes that violate the leftist heap property. In Figure 6.28, there is a swap at nodes 7 and 3, and the same tree as before is obtained. The nonrecursive version is simpler to visualize but harder to code. We leave it to the reader to show that the recursive and nonrecursive procedures do the same thing. As mentioned above, we can carry out insertions by making the item to be inserted a one-node heap and performing a merge. To perform a deleteMin, we merely destroy the root, creating two heaps, which can then be merged. Thus, the time to perform a deleteMin is O(log N). These two routines are coded in Figure 6.29 and Figure 6.30.

6.6 Leftist Heaps Figure 6.23 Result of attaching leftist heap of previous ﬁgure as H1’s right child Figure 6.24 Result of swapping children of H1’s root

Chapter 6 Priority Queues (Heaps) public class LeftistHeap<AnyType extends Comparable<? super AnyType>> { public LeftistHeap( ) { root = null; } public void merge( LeftistHeap<AnyType> rhs ) { /* Figure 6.26 */ } public void insert( AnyType x ) { /* Figure 6.29 */ } public AnyType findMin( ) { /* See online code */ } public AnyType deleteMin( ) { /* Figure 6.30 */ } public boolean isEmpty( ) { return root == null; } public void makeEmpty( ) { root = null; } private static class Node<AnyType> { // Constructors Node( AnyType theElement ) { this( theElement, null, null ); } Node( AnyType theElement, Node<AnyType> lt, Node<AnyType> rt ) { element = theElement; left = lt; right = rt; npl = 0; } AnyType element; // The data in the node Node<AnyType> left; // Left child Node<AnyType> right; // Right child int npl; // null path length } private Node<AnyType> root; // root private Node<AnyType> merge( Node<AnyType> h1, Node<AnyType> h2 ) { /* Figure 6.26 */ } private Node<AnyType> merge1( Node<AnyType> h1, Node<AnyType> h2 ) { /* Figure 6.27 */ } private void swapChildren( Node<AnyType> t ) { /* See online code */ } } Figure 6.25 Leftist heap type declarations

6.6 Leftist Heaps /** * Merge rhs into the priority queue. * rhs becomes empty. rhs must be different from this. * @param rhs the other leftist heap. */ public void merge( LeftistHeap<AnyType> rhs ) { if( this == rhs ) // Avoid aliasing problems return; root = merge( root, rhs.root ); rhs.root = null; } /** * Internal method to merge two roots. * Deals with deviant cases and calls recursive merge1. */ private Node<AnyType> merge( Node<AnyType> h1, Node<AnyType> h2 ) { if( h1 == null ) return h2; if( h2 == null ) return h1; if( h1.element.compareTo( h2.element ) < 0 ) return merge1( h1, h2 ); else return merge1( h2, h1 ); } Figure 6.26 Driving routines for merging leftist heaps Finally, we can build a leftist heap in O(N) time by building a binary heap (obviously using a linked implementation). Although a binary heap is clearly leftist, this is not necessarily the best solution, because the heap we obtain is the worst possible leftist heap. Furthermore, traversing the tree in reverse-level order is not as easy with links. The buildHeap effect can be obtained by recursively building the left and right subtrees and then percolating the root down. The exercises contain an alternative solution.

Chapter 6 Priority Queues (Heaps) /** * Internal method to merge two roots. * Assumes trees are not empty, and h1’s root contains smallest item. */ private Node<AnyType> merge1( Node<AnyType> h1, Node<AnyType> h2 ) { if( h1.left == null ) // Single node h1.left = h2; // Other fields in h1 already accurate else { h1.right = merge( h1.right, h2 ); if( h1.left.npl < h1.right.npl ) swapChildren( h1 ); h1.npl = h1.right.npl + 1; } return h1; } Figure 6.27 Actual routine to merge leftist heaps Figure 6.28 Result of merging right paths of H1 and H2

6.7 Skew Heaps /** * Insert into the priority queue, maintaining heap order. * @param x the item to insert. */ public void insert( AnyType x ) { root = merge( new Node<>( x ), root ); } Figure 6.29 Insertion routine for leftist heaps /** * Remove the smallest item from the priority queue. * @return the smallest item, or throw UnderflowException if empty. */ public AnyType deleteMin( ) { if( isEmpty( ) ) throw new UnderflowException( ); AnyType minItem = root.element; root = merge( root.left, root.right ); return minItem; } Figure 6.30 deleteMin routine for leftist heaps 6.7 Skew Heaps A skew heap is a self-adjusting version of a leftist heap that is incredibly simple to implement. The relationship of skew heaps to leftist heaps is analogous to the relation between splay trees and AVL trees. Skew heaps are binary trees with heap order, but there is no structural constraint on these trees. Unlike leftist heaps, no information is maintained about the null path length of any node. The right path of a skew heap can be arbitrarily long at any time, so the worst-case running time of all operations is O(N). However, as with splay trees, it can be shown (see Chapter 11) that for any M consecutive operations, the total worst-case running time is O(M log N). Thus, skew heaps have O(log N) amortized cost per operation. As with leftist heaps, the fundamental operation on skew heaps is merging. The merge routine is once again recursive, and we perform the exact same operations as before, with

Chapter 6 Priority Queues (Heaps) one exception. The difference is that for leftist heaps, we check to see whether the left and right children satisfy the leftist heap structure property and swap them if they do not. For skew heaps, the swap is unconditional; we always do it, with the one exception that the largest of all the nodes on the right paths does not have its children swapped. This one exception is what happens in the natural recursive implementation, so it is not really a special case at all. Furthermore, it is not necessary to prove the bounds, but since this node is guaranteed not to have a right child, it would be silly to perform the swap and give it one. (In our example, there are no children of this node, so we do not worry about it.) Again, suppose our input is the same two heaps as before, Figure 6.31. If we recursively merge H2 with the subheap of H1 rooted at 8, we will get the heap in Figure 6.32. Again, this is done recursively, so by the third rule of recursion (Section 1.3) we need not worry about how it was obtained. This heap happens to be leftist, but there is no H1 H2 Figure 6.31 Two skew heaps H1 and H2 Figure 6.32 Result of merging H2 with H1’s right subheap

6.7 Skew Heaps Figure 6.33 Result of merging skew heaps H1 and H2 guarantee that this is always the case. We make this heap the new left child of H1, and the old left child of H1 becomes the new right child (see Figure 6.33). The entire tree is leftist, but it is easy to see that that is not always true: Inserting 15 into this new heap would destroy the leftist property. We can perform all operations nonrecursively, as with leftist heaps, by merging the right paths and swapping left and right children for every node on the right path, with the exception of the last. After a few examples, it becomes clear that since all but the last node on the right path have their children swapped, the net effect is that this becomes the new left path (see the preceding example to convince yourself). This makes it very easy to merge two skew heaps visually.3 The implementation of skew heaps is left as a (trivial) exercise. Note that because a right path could be long, a recursive implementation could fail because of lack of stack space, even though performance would otherwise be acceptable. Skew heaps have the advantage that no extra space is required to maintain path lengths and no tests are required to determine when to swap children. It is an open problem to determine precisely the expected right path length of both leftist and skew heaps (the latter is undoubtedly more difﬁcult). Such a comparison would make it easier to determine whether the slight loss of balance information is compensated by the lack of testing. 3 This is not exactly the same as the recursive implementation (but yields the same time bounds). If we only swap children for nodes on the right path that are above the point where the merging of right paths terminated due to exhaustion of one heap’s right path, we get the same result as the recursive version.

Chapter 6 Priority Queues (Heaps) B 3 B 2 B 1 B 0 B 4 Figure 6.34 Binomial trees B0, B1, B2, B3, and B4 6.8 Binomial Queues Although both leftist and skew heaps support merging, insertion, and deleteMin all effectively in O(log N) time per operation, there is room for improvement because we know that binary heaps support insertion in constant average time per operation. Binomial queues support all three operations in O(log N) worst-case time per operation, but insertions take constant time on average. 6.8.1 Binomial Queue Structure Binomial queues differ from all the priority queue implementations that we have seen in that a binomial queue is not a heap-ordered tree but rather a collection of heap-ordered trees, known as a forest. Each of the heap-ordered trees is of a constrained form known as a binomial tree (the reason for the name will be obvious later). There is at most one binomial tree of every height. A binomial tree of height 0 is a one-node tree; a binomial tree, Bk, of height k is formed by attaching a binomial tree, Bk−1, to the root of another binomial tree, Bk−1. Figure 6.34 shows binomial trees B0, B1, B2, B3, and B4. From the diagram we see that a binomial tree, Bk, consists of a root with children B0, B1, . . . , Bk−1. Binomial trees of height k have exactly 2k nodes, and the number of nodes at depth d is the binomial coefﬁcient k d  . If we impose heap order on the binomial

6.8 Binomial Queues H 1: Figure 6.35 Binomial queue H1 with six elements trees and allow at most one binomial tree of any height, we can represent a priority queue of any size by a collection of binomial trees. For instance, a priority queue of size 13 could be represented by the forest B3, B2, B0. We might write this representation as 1101, which not only represents 13 in binary but also represents the fact that B3, B2, and B0 are present in the representation and B1 is not. As an example, a priority queue of six elements could be represented as in Figure 6.35. 6.8.2 Binomial Queue Operations The minimum element can then be found by scanning the roots of all the trees. Since there are at most log N different trees, the minimum can be found in O(log N) time. Alternatively, we can maintain knowledge of the minimum and perform the operation in O(1) time, if we remember to update the minimum when it changes during other operations. Merging two binomial queues is a conceptually easy operation, which we will describe by example. Consider the two binomial queues, H1 and H2, with six and seven elements, respectively, pictured in Figure 6.36. The merge is performed by essentially adding the two queues together. Let H3 be the new binomial queue. Since H1 has no binomial tree of height 0 and H2 does, we can just use the binomial tree of height 0 in H2 as part of H3. Next, we add binomial trees of height 1. Since both H1 and H2 have binomial trees of height 1, we merge them by making the larger root a subtree of the smaller, creating a binomial tree of height 2, shown in Figure 6.37. Thus, H3 will not have a binomial tree of height 1. There are now three binomial trees of height 2, namely, the original trees of H1 and H2 plus the tree formed : 13 : H1 H2 Figure 6.36 Two binomial queues H1 and H2

Chapter 6 Priority Queues (Heaps) Figure 6.37 Merge of the two B1 trees in H1 and H2 H 3: 13 Figure 6.38 Binomial queue H3: the result of merging H1 and H2 by the previous step. We keep one binomial tree of height 2 in H3 and merge the other two, creating a binomial tree of height 3. Since H1 and H2 have no trees of height 3, this tree becomes part of H3 and we are ﬁnished. The resulting binomial queue is shown in Figure 6.38. Since merging two binomial trees takes constant time with almost any reasonable implementation, and there are O(log N) binomial trees, the merge takes O(log N) time in the worst case. To make this operation efﬁcient, we need to keep the trees in the binomial queue sorted by height, which is certainly a simple thing to do. Insertion is just a special case of merging, since we merely create a one-node tree and perform a merge. The worst-case time of this operation is likewise O(log N). More precisely, if the priority queue into which the element is being inserted has the property that the smallest nonexistent binomial tree is Bi, the running time is proportional to i + 1. For example, H3 (Figure 6.38) is missing a binomial tree of height 1, so the insertion will terminate in two steps. Since each tree in a binomial queue is present with probability 2, it follows that we expect an insertion to terminate in two steps, so the average time is constant. Furthermore, an analysis will show that performing N inserts on an initially empty binomial queue will take O(N) worst-case time. Indeed, it is possible to do this operation using only N −1 comparisons; we leave this as an exercise. As an example, we show in Figures 6.39 through 6.45 the binomial queues that are formed by inserting 1 through 7 in order. Inserting 4 shows off a bad case. We merge 4 with B0, obtaining a new tree of height 1. We then merge this tree with B1, obtaining a tree of height 2, which is the new priority queue. We count this as three steps (two tree merges plus the stopping case). The next insertion after 7 is inserted is another bad case and would require three tree merges. Figure 6.39 After 1 is inserted

6.8 Binomial Queues Figure 6.40 After 2 is inserted Figure 6.41 After 3 is inserted Figure 6.42 After 4 is inserted Figure 6.43 After 5 is inserted Figure 6.44 After 6 is inserted A deleteMin can be performed by ﬁrst ﬁnding the binomial tree with the smallest root. Let this tree be Bk, and let the original priority queue be H. We remove the binomial tree Bk from the forest of trees in H, forming the new binomial queue H′. We also remove the root of Bk, creating binomial trees B0, B1, . . . , Bk−1, which collectively form priority queue H′′. We ﬁnish the operation by merging H′ and H′′. As an example, suppose we perform a deleteMin on H3, which is shown again in Figure 6.46. The minimum root is 12, so we obtain the two priority queues H′ and H′′ in Figure 6.47 and Figure 6.48. The binomial queue that results from merging H′ and H′′ is the ﬁnal answer and is shown in Figure 6.49.

Chapter 6 Priority Queues (Heaps) Figure 6.45 After 7 is inserted H 3: 13 Figure 6.46 Binomial queue H3 H′: Figure 6.47 Binomial queue H′, containing all the binomial trees in H3 except B3 H′′: Figure 6.48 Binomial queue H′′: B3 with 12 removed For the analysis, note ﬁrst that the deleteMin operation breaks the original binomial queue into two. It takes O(log N) time to ﬁnd the tree containing the minimum element and to create the queues H′ and H′′. Merging these two queues takes O(log N) time, so the entire deleteMin operation takes O(log N) time. 6.8.3 Implementation of Binomial Queues The deleteMin operation requires the ability to ﬁnd all the subtrees of the root quickly, so the standard representation of general trees is required: The children of each node are kept

6.8 Binomial Queues Figure 6.49 Result of applying deleteMin to H3 H 3: 13 Figure 6.50 Binomial queue H3 drawn as a forest Figure 6.51 Representation of binomial queue H3 in a linked list, and each node has a reference to its ﬁrst child (if any). This operation also requires that the children be ordered by the size of their subtrees. We also need to make sure that it is easy to merge two trees. When two trees are merged, one of the trees is added as a child to the other. Since this new tree will be the largest subtree, it makes sense to maintain the subtrees in decreasing sizes. Only then will we be able to merge two binomial trees, and thus two binomial queues, efﬁciently. The binomial queue will be an array of binomial trees. To summarize, then, each node in a binomial tree will contain the data, ﬁrst child, and right sibling. The children in a binomial tree are arranged in decreasing rank. Figure 6.51 shows how the binomial queue in Figure 6.50 is represented. Figure 6.52 shows the type declarations for a node in the binomial tree, and the binomial queue class skeleton.

public class BinomialQueue<AnyType extends Comparable<? super AnyType>> { public BinomialQueue( ) { /* See online code */ } public BinomialQueue( AnyType item ) { /* See online code */ } public void merge( BinomialQueue<AnyType> rhs ) { /* Figure 6.55 */ } public void insert( AnyType x ) { merge( new BinomialQueue<>( x ) ); } public AnyType findMin( ) { /* See online code */ } public AnyType deleteMin( ) { /* Figure 6.56 */ } public boolean isEmpty( ) { return currentSize == 0; } public void makeEmpty( ) { /* See online code */ } private static class Node<AnyType> { // Constructors Node( AnyType theElement ) { this( theElement, null, null ); } Node( AnyType theElement, Node<AnyType> lt, Node<AnyType> nt ) { element = theElement; leftChild = lt; nextSibling = nt; } AnyType element; // The data in the node Node<AnyType> leftChild; // Left child Node<AnyType> nextSibling; // Right child } private static final int DEFAULT_TREES = 1; private int currentSize; // # items in priority queue private Node<AnyType> [ ] theTrees; // An array of tree roots private void expandTheTrees( int newNumTrees ) { /* See online code */ } private Node<AnyType> combineTrees( Node<AnyType> t1, Node<AnyType> t2 ) { /* Figure 6.54 */ } private int capacity( ) { return ( 1 << theTrees.length ) - 1; } private int findMinIndex( ) { /* See online code */ } } Figure 6.52 Binomial queue class skeleton and node deﬁnition

6.8 Binomial Queues Figure 6.53 Merging two binomial trees /** * Return the result of merging equal-sized t1 and t2. */ private Node<AnyType> combineTrees( Node<AnyType> t1, Node<AnyType> t2 ) { if( t1.element.compareTo( t2.element ) > 0 ) return combineTrees( t2, t1 ); t2.nextSibling = t1.leftChild; t1.leftChild = t2; return t1; } Figure 6.54 Routine to merge two equal-sized binomial trees In order to merge two binomial queues, we need a routine to merge two binomial trees of the same size. Figure 6.53 shows how the links change when two binomial trees are merged. The code to do this is simple and is shown in Figure 6.54. We provide a simple implementation of the merge routine. H1 is represented by the current object and H2 is represented by rhs. The routine combines H1 and H2, placing the result in H1 and making H2 empty. At any point we are dealing with trees of rank i. t1 and t2 are the trees in H1 and H2, respectively, and carry is the tree carried from a previous step (it might be null). Depending on each of the eight possible cases, the tree that results for rank i and the carry tree of rank i + 1 is formed. This process proceeds from rank 0 to the last rank in the resulting binomial queue. The code is shown in Figure 6.55. Improvements to the code are suggested in Exercise 6.35. The deleteMin routine for binomial queues is given in Figure 6.56. We can extend binomial queues to support some of the nonstandard operations that binary heaps allow, such as decreaseKey and delete, when the position of the affected element is known. A decreaseKey is a percolateUp, which can be performed in O(log N) time if we add a ﬁeld to each node that stores a parent link. An arbitrary delete can be performed by a combination of decreaseKey and deleteMin in O(log N) time.

C H A P T E R 7 Sorting In this chapter we discuss the problem of sorting an array of elements. To simplify matters, we will assume in our examples that the array contains only integers, although our code will once again allow more general objects. For most of this chapter, we will also assume that the entire sort can be done in main memory, so that the number of elements is relatively small (less than a few million). Sorts that cannot be performed in main memory and must be done on disk or tape are also quite important. This type of sorting, known as external sorting, will be discussed at the end of the chapter. Our investigation of internal sorting will show that r There are several easy algorithms to sort in O(N2), such as insertion sort. r There is an algorithm, Shellsort, that is very simple to code, runs in o(N2), and is efﬁcient in practice. r There are slightly more complicated O(N log N) sorting algorithms. r Any general-purpose sorting algorithm requires (N log N) comparisons. The rest of this chapter will describe and analyze the various sorting algorithms. These algorithms contain interesting and important ideas for code optimization as well as algorithm design. Sorting is also an example where the analysis can be precisely performed. Be forewarned that where appropriate, we will do as much analysis as possible. 7.1 Preliminaries The algorithms we describe will all be interchangeable. Each will be passed an array containing the elements; we assume all array positions contain data to be sorted. We will assume that N is the number of elements passed to our sorting routines. The objects being sorted are of type Comparable, as described in Section 1.4. We thus use the compareTo method to place a consistent ordering on the input. Besides (reference) assignments, this is the only operation allowed on the input data. Sorting under these conditions is known as comparison-based sorting. The sorting algorithms are easily rewritten to use Comparators, in the event that the default ordering is unavailable or unacceptable.

Chapter 7 Sorting Original Positions Moved After p = 1 After p = 2 After p = 3 After p = 4 After p = 5 Figure 7.1 Insertion sort after each pass 7.2 Insertion Sort 7.2.1 The Algorithm One of the simplest sorting algorithms is the insertion sort. Insertion sort consists of N −1 passes. For pass p = 1 through N −1, insertion sort ensures that the elements in positions 0 through p are in sorted order. Insertion sort makes use of the fact that elements in positions 0 through p −1 are already known to be in sorted order. Figure 7.1 shows a sample array after each pass of insertion sort. Figure 7.1 shows the general strategy. In pass p, we move the element in position p left until its correct place is found among the ﬁrst p + 1 elements. The code in Figure 7.2 implements this strategy. Lines 12 through 15 implement that data movement without the explicit use of swaps. The element in position p is saved in tmp, and all larger elements (prior to position p) are moved one spot to the right. Then tmp is placed in the correct spot. This is the same technique that was used in the implementation of binary heaps. 7.2.2 Analysis of Insertion Sort Because of the nested loops, each of which can take N iterations, insertion sort is O(N2). Furthermore, this bound is tight, because input in reverse order can achieve this bound. A precise calculation shows that the number of tests in the inner loop in Figure 7.2 is at most p + 1 times for each value of p. Summing over all p gives a total of N  i=2 i = 2 + 3 + 4 + · · · + N = (N2) On the other hand, if the input is presorted, the running time is O(N), because the test in the inner for loop always fails immediately. Indeed, if the input is almost sorted (this term will be more rigorously deﬁned in the next section), insertion sort will run quickly. Because of this wide variation, it is worth analyzing the average-case behavior of this algorithm. It turns out that the average case is (N2) for insertion sort, as well as for a variety of other sorting algorithms, as the next section shows.

7.3 A Lower Bound for Simple Sorting Algorithms /** * Simple insertion sort. * @param a an array of Comparable items. */ public static <AnyType extends Comparable<? super AnyType>> void insertionSort( AnyType [ ] a ) { int j; for( int p = 1; p < a.length; p++ ) { AnyType tmp = a[ p ]; for( j = p; j > 0 && tmp.compareTo( a[ j - 1 ] ) < 0; j-- ) a[ j ] = a[ j - 1 ]; a[ j ] = tmp; } } Figure 7.2 Insertion sort routine 7.3 A Lower Bound for Simple Sorting Algorithms An inversion in an array of numbers is any ordered pair (i, j) having the property that i < j but a[i] > a[j]. In the example of the last section, the input list 34, 8, 64, 51, 32, 21 had nine inversions, namely (34, 8), (34, 32), (34, 21), (64, 51), (64, 32), (64, 21), (51, 32), (51, 21), and (32, 21). Notice that this is exactly the number of swaps that needed to be (implicitly) performed by insertion sort. This is always the case, because swapping two adjacent elements that are out of place removes exactly one inversion, and a sorted array has no inversions. Since there is O(N) other work involved in the algorithm, the running time of insertion sort is O(I + N), where I is the number of inversions in the original array. Thus, insertion sort runs in linear time if the number of inversions is O(N). We can compute precise bounds on the average running time of insertion sort by computing the average number of inversions in a permutation. As usual, deﬁning average is a difﬁcult proposition. We will assume that there are no duplicate elements (if we allow duplicates, it is not even clear what the average number of duplicates is). Using this assumption, we can assume that the input is some permutation of the ﬁrst N integers (since only relative ordering is important) and that all are equally likely. Under these assumptions, we have the following theorem: Theorem 7.1. The average number of inversions in an array of N distinct elements is N(N −1)/4.

Chapter 7 Sorting Proof. For any list, L, of elements, consider Lr, the list in reverse order. The reverse list of the example is 21, 32, 51, 64, 8, 34. Consider any pair of two elements in the list (x, y), with y > x. Clearly, in exactly one of L and Lr this ordered pair represents an inversion. The total number of these pairs in a list L and its reverse Lr is N(N −1)/2. Thus, an average list has half this amount, or N(N −1)/4 inversions. This theorem implies that insertion sort is quadratic on average. It also provides a very strong lower bound about any algorithm that only exchanges adjacent elements. Theorem 7.2. Any algorithm that sorts by exchanging adjacent elements requires (N2) time on average. Proof. The average number of inversions is initially N(N−1)/4 = (N2). Each swap removes only one inversion, so (N2) swaps are required. This is an example of a lower-bound proof. It is valid not only for insertion sort, which performs adjacent exchanges implicitly, but also for other simple algorithms such as bubble sort and selection sort, which we will not describe here. In fact, it is valid over an entire class of sorting algorithms, including those undiscovered, that perform only adjacent exchanges. Because of this, this proof cannot be conﬁrmed empirically. Although this lower-bound proof is rather simple, in general proving lower bounds is much more complicated than proving upper bounds and in some cases resembles magic. This lower bound shows us that in order for a sorting algorithm to run in subquadratic, or o(N2), time, it must do comparisons and, in particular, exchanges between elements that are far apart. A sorting algorithm makes progress by eliminating inversions, and to run efﬁciently, it must eliminate more than just one inversion per exchange. 7.4 Shellsort Shellsort, named after its inventor, Donald Shell, was one of the ﬁrst algorithms to break the quadratic time barrier, although it was not until several years after its initial discovery that a subquadratic time bound was proven. As suggested in the previous section, it works by comparing elements that are distant; the distance between comparisons decreases as the algorithm runs until the last phase, in which adjacent elements are compared. For this reason, Shellsort is sometimes referred to as diminishing increment sort. Shellsort uses a sequence, h1, h2, . . . , ht, called the increment sequence. Any increment sequence will do as long as h1 = 1, but some choices are better than others (we will discuss that issue later). After a phase, using some increment hk, for every i, we have a[i] ≤a[i + hk] (where this makes sense); all elements spaced hk apart are sorted. The ﬁle is then said to be hkhkhk-sorted. For example, Figure 7.3 shows an array after several phases of Shellsort. An important property of Shellsort (which we state without proof) is that an hk-sorted ﬁle that is then hk−1-sorted remains hk-sorted. If this were not the case, the

7.4 Shellsort Original After 5-sort After 3-sort After 1-sort Figure 7.3 Shellsort after each pass, using {1, 3, 5} as the increment sequence /** * Shellsort, using Shell’s (poor) increments. * @param a an array of Comparable items. */ public static <AnyType extends Comparable<? super AnyType>> void shellsort( AnyType [ ] a ) { int j; for( int gap = a.length / 2; gap > 0; gap /= 2 ) for( int i = gap; i < a.length; i++ ) { AnyType tmp = a[ i ]; for( j = i; j >= gap && tmp.compareTo( a[ j - gap ] ) < 0; j -= gap ) a[ j ] = a[ j - gap ]; a[ j ] = tmp; } } Figure 7.4 Shellsort routine using Shell’s increments (better increments are possible) algorithm would likely be of little value, since work done by early phases would be undone by later phases. The general strategy to hk-sort is for each position, i, in hk, hk + 1, . . . , N −1, place the element in the correct spot among i, i −hk, i −2hk, and so on. Although this does not affect the implementation, a careful examination shows that the action of an hk-sort is to perform an insertion sort on hk independent subarrays. This observation will be important when we analyze the running time of Shellsort. A popular (but poor) choice for increment sequence is to use the sequence suggested by Shell: ht = ⌊N/2⌋, and hk = ⌊hk+1/2⌋(This is not the sequence used in the example in Figure 7.3). Figure 7.4 contains a method that implements Shellsort using this sequence. We shall see later that there are increment sequences that give a signiﬁcant improvement in the algorithm’s running time; even a minor change can drastically affect performance (Exercise 7.10).

Chapter 7 Sorting The program in Figure 7.4 avoids the explicit use of swaps in the same manner as our implementation of insertion sort. 7.4.1 Worst-Case Analysis of Shellsort Although Shellsort is simple to code, the analysis of its running time is quite another story. The running time of Shellsort depends on the choice of increment sequence, and the proofs can be rather involved. The average-case analysis of Shellsort is a long-standing open problem, except for the most trivial increment sequences. We will prove tight worst-case bounds for two particular increment sequences. Theorem 7.3. The worst-case running time of Shellsort, using Shell’s increments, is (N2). Proof. The proof requires showing not only an upper bound on the worst-case running time but also showing that there exists some input that actually takes  (N2) time to run. We prove the lower bound ﬁrst, by constructing a bad case. First, we choose N to be a power of 2. This makes all the increments even, except for the last increment, which is 1. Now, we will give as input an array with the N/2 largest numbers in the even positions and the N/2 smallest numbers in the odd positions (for this proof, the ﬁrst position is position 1). As all the increments except the last are even, when we come to the last pass, the N/2 largest numbers are still all in even positions and the N/2 smallest numbers are still all in odd positions. The ith smallest number (i ≤N/2) is thus in position 2i −1 before the beginning of the last pass. Restoring the ith element to its correct place requires moving it i−1 spaces in the array. Thus, to merely place the N/2 smallest elements in the correct place requires at least N/2 i=1 i−1 =  (N2) work. As an example, Figure 7.5 shows a bad (but not the worst) input when N = 16. The number of inversions remaining after the 2-sort is exactly 1 + 2 + 3 + 4 + 5 + 6 + 7 = 28; thus, the last pass will take considerable time. To ﬁnish the proof, we show the upper bound of O(N2). As we have observed before, a pass with increment hk consists of hk insertion sorts of about N/hk elements. Since insertion sort is quadratic, the total cost of a pass is O(hk(N/hk)2) = O(N2/hk). Summing over all passes gives a total bound of O(t i=1 N2/hi) = O(N2 t i=1 1/hi). Because the increments form a geometric series with common ratio 2, and the largest term in the series is h1 = 1, t i=1 1/hi < 2. Thus we obtain a total bound of O(N2). Start After 8-sort After 4-sort After 2-sort After 1-sort Figure 7.5 Bad case for Shellsort with Shell’s increments (positions are numbered 1 to 16)

7.4 Shellsort The problem with Shell’s increments is that pairs of increments are not necessarily relatively prime, and thus the smaller increment can have little effect. Hibbard suggested a slightly different increment sequence, which gives better results in practice (and theoretically). His increments are of the form 1, 3, 7, . . . , 2k −1. Although these increments are almost identical, the key difference is that consecutive increments have no common factors. We now analyze the worst-case running time of Shellsort for this increment sequence. The proof is rather complicated. Theorem 7.4. The worst-case running time of Shellsort using Hibbard’s increments is (N3/2). Proof. We will prove only the upper bound and leave the proof of the lower bound as an exercise. The proof requires some well-known results from additive number theory. References to these results are provided at the end of the chapter. For the upper bound, as before, we bound the running time of each pass and sum over all passes. For increments hk > N1/2, we will use the bound O(N2/hk) from the previous theorem. Although this bound holds for the other increments, it is too large to be useful. Intuitively, we must take advantage of the fact that this increment sequence is special. What we need to show is that for any element a[p] in position p, when it is time to perform an hk-sort, there are only a few elements to the left of position p that are larger than a[p]. When we come to hk-sort the input array, we know that it has already been hk+1and hk+2-sorted. Prior to the hk-sort, consider elements in positions p and p −i, i ≤p. If i is a multiple of hk+1 or hk+2, then clearly a[p −i] < a[p]. We can say more, however. If i is expressible as a linear combination (in nonnegative integers) of hk+1 and hk+2, then a[p −i] < a[p]. As an example, when we come to 3-sort, the ﬁle is already 7- and 15-sorted. 52 is expressible as a linear combination of 7 and 15, because 52 = 1 ∗7 + 3 ∗15. Thus, a[100] cannot be larger than a[152] because a[100] ≤a[107] ≤a[122] ≤a[137] ≤a[152]. Now, hk+2 = 2hk+1 + 1, so hk+1 and hk+2 cannot share a common factor. In this case, it is possible to show that all integers that are at least as large as (hk+1 −1) (hk+2 −1) = 8h2 k +4hk can be expressed as a linear combination of hk+1 and hk+2 (see the reference at the end of the chapter). This tells us that the body of the innermost for loop can be executed at most 8hk + 4 = O(hk) times for each of the N −hk positions. This gives a bound of O(Nhk) per pass. Using the fact that about half the increments satisfy hk < √ N, and assuming that t is even, the total running time is then O ⎛ ⎝ t/2  k=1 Nhk + t  k=t/2+1 N2/hk ⎞ ⎠= O ⎛ ⎝N t/2  k=1 hk + N2 t  k=t/2+1 1/hk ⎞ ⎠

Chapter 7 Sorting Because both sums are geometric series, and since ht/2 = ( √ N), this simpliﬁes to = O  Nht/2  + O  N2 ht/2 = O(N3/2) The average-case running time of Shellsort, using Hibbard’s increments, is thought to be O(N5/4), based on simulations, but nobody has been able to prove this. Pratt has shown that the (N3/2) bound applies to a wide range of increment sequences. Sedgewick has proposed several increment sequences that give an O(N4/3) worstcase running time (also achievable). The average running time is conjectured to be O(N7/6) for these increment sequences. Empirical studies show that these sequences perform signiﬁcantly better in practice than Hibbard’s. The best of these is the sequence {1, 5, 19, 41, 109, . . .}, in which the terms are either of the form 9 · 4i −9 · 2i + 1 or 4i −3 · 2i + 1. This is most easily implemented by placing these values in an array. This increment sequence is the best known in practice, although there is a lingering possibility that some increment sequence might exist that could give a signiﬁcant improvement in the running time of Shellsort. There are several other results on Shellsort that (generally) require difﬁcult theorems from number theory and combinatorics and are mainly of theoretical interest. Shellsort is a ﬁne example of a very simple algorithm with an extremely complex analysis. The performance of Shellsort is quite acceptable in practice, even for N in the tens of thousands. The simplicity of the code makes it the algorithm of choice for sorting up to moderately large input. 7.5 Heapsort As mentioned in Chapter 6, priority queues can be used to sort in O(N log N) time. The algorithm based on this idea is known as heapsort and gives the best Big-Oh running time we have seen so far. Recall, from Chapter 6, that the basic strategy is to build a binary heap of N elements. This stage takes O(N) time. We then perform N deleteMin operations. The elements leave the heap smallest ﬁrst, in sorted order. By recording these elements in a second array and then copying the array back, we sort N elements. Since each deleteMin takes O(log N) time, the total running time is O(N log N). The main problem with this algorithm is that it uses an extra array. Thus, the memory requirement is doubled. This could be a problem in some instances. Notice that the extra time spent copying the second array back to the ﬁrst is only O(N), so that this is not likely to affect the running time signiﬁcantly. The problem is space. A clever way to avoid using a second array makes use of the fact that after each deleteMin, the heap shrinks by 1. Thus the cell that was last in the heap can be used to store the element that was just deleted. As an example, suppose we have a heap with six elements. The ﬁrst deleteMin produces a1. Now the heap has only ﬁve elements, so we can place a1 in position 6. The next deleteMin produces a2. Since the heap will now only have four elements, we can place a2 in position 5.

7.5 Heapsort Figure 7.6 (Max) heap after buildHeap phase Using this strategy, after the last deleteMin the array will contain the elements in decreasing sorted order. If we want the elements in the more typical increasing sorted order, we can change the ordering property so that the parent has a larger key than the child. Thus we have a (max)heap. In our implementation, we will use a (max)heap but avoid the actual ADT for the purposes of speed. As usual, everything is done in an array. The ﬁrst step builds the heap in linear time. We then perform N −1 deleteMaxes by swapping the last element in the heap with the ﬁrst, decrementing the heap size, and percolating down. When the algorithm terminates, the array contains the elements in sorted order. For instance, consider the input sequence 31, 41, 59, 26, 53, 58, 97. The resulting heap is shown in Figure 7.6. Figure 7.7 shows the heap that results after the ﬁrst deleteMax. As the ﬁgures imply, the last element in the heap is 31; 97 has been placed in a part of the heap array that is technically no longer part of the heap. After 5 more deleteMax operations, the heap will actually have only one element, but the elements left in the heap array will be in sorted order. The code to perform heapsort is given in Figure 7.8. The slight complication is that, unlike the binary heap, where the data begin at array index 1, the array for heapsort contains data in position 0. Thus the code is a little different from the binary heap code. The changes are minor. 7.5.1 Analysis of Heapsort As we saw in Chapter 6, the ﬁrst phase, which constitutes the building of the heap, uses less than 2N comparisons. In the second phase, the ith deleteMax uses at most less than 2⌊log (N −i + 1)⌋comparisons, for a total of at most 2N log N −O(N) comparisons (assuming N ≥2). Consequently, in the worst case, at most 2N log N −O(N) comparisons are used by heapsort. Exercise 7.13 asks you to show that it is possible for all of the deleteMax operations to achieve their worst case simultaneously.

Chapter 7 Sorting Figure 7.7 Heap after ﬁrst deleteMax Experiments have shown that the performance of heapsort is extremely consistent: On average it uses only slightly fewer comparisons than the worst-case bound suggests. For many years, nobody had been able to show nontrivial bounds on heapsort’s average running time. The problem, it seems, is that successive deleteMax operations destroy the heap’s randomness, making the probability arguments very complex. Eventually another approach proved successful. Theorem 7.5. The average number of comparisons used to heapsort a random permutation of N distinct items is 2N log N −O(N log log N). Proof. The heap construction phase uses (N) comparisons on average, and so we only need to prove the bound for the second phase. We assume a permutation of {1, 2, . . . , N}. Suppose the ith deleteMax pushes the root element down di levels. Then it uses 2di comparisons. For heapsort on any input, there is a cost sequence D : d1, d2, . . . , dN that deﬁnes the cost of phase 2. That cost is given by MD = N i=1 di; the number of comparisons used is thus 2MD. Let f(N) be the number of heaps of N items. One can show (Exercise 7.58) that f(N) > (N/(4e))N (where e = 2.71828 . . .). We will show that only an exponentially small fraction of these heaps (in particular (N/16)N) have a cost smaller than M = N(log N −log log N −4). When this is shown, it follows that the average value of MD is at least M minus a term that is o(1), and thus the average number of comparisons is at least 2M. Consequently, our basic goal is to show that there are very few heaps that have small cost sequences.

/** * Internal method for heapsort. * @param i the index of an item in the heap. * @return the index of the left child. */ private static int leftChild( int i ) { return 2 * i + 1; } /** * Internal method for heapsort that is used in deleteMax and buildHeap. * @param a an array of Comparable items. * @int i the position from which to percolate down. * @int n the logical size of the binary heap. */ private static <AnyType extends Comparable<? super AnyType>> void percDown( AnyType [ ] a, int i, int n ) { int child; AnyType tmp; for( tmp = a[ i ]; leftChild( i ) < n; i = child ) { child = leftChild( i ); if( child != n - 1 && a[ child ].compareTo( a[ child + 1 ] ) < 0 ) child++; if( tmp.compareTo( a[ child ] ) < 0 ) a[ i ] = a[ child ]; else break; } a[ i ] = tmp; } /** * Standard heapsort. * @param a an array of Comparable items. */ public static <AnyType extends Comparable<? super AnyType>> void heapsort( AnyType [ ] a ) { for( int i = a.length / 2 - 1; i >= 0; i-- ) /* buildHeap */ percDown( a, i, a.length ); for( int i = a.length - 1; i > 0; i-- ) { swapReferences( a, 0, i ); /* deleteMax */ percDown( a, 0, i ); } } Figure 7.8 Heapsort

Chapter 7 Sorting Because level di has at most 2di nodes, there are 2di possible places that the root element can go for any di. Consequently, for any sequence D, the number of distinct corresponding deleteMax sequences is at most SD = 2d12d2 · · · 2dN A simple algebraic manipulation shows that for a given sequence D SD = 2MD Because each di can assume any value between 1 and ⌊log N⌋, there are at most (log N)N possible sequences D. It follows that the number of distinct deleteMax sequences that require cost exactly equal to M is at most the number of cost sequences of total cost M times the number of deleteMax sequences for each of these cost sequences. A bound of (log N)N2M follows immediately. The total number of heaps with cost sequence less than M is at most M−1  i=1 (log N)N2i < (log N)N2M If we choose M = N(log N −log log N −4), then the number of heaps that have cost sequence less than M is at most (N/16)N, and the theorem follows from our earlier comments. Using a more complex argument, it can be shown that heapsort always uses at least N log N −O(N) comparisons, and that there are inputs that can achieve this bound. The average case analysis also can be improved to 2N log N −O(N) comparisons (rather than the nonlinear second term in Theorem 7.5). 7.6 Mergesort We now turn our attention to mergesort. Mergesort runs in O(N log N) worst-case running time, and the number of comparisons used is nearly optimal. It is a ﬁne example of a recursive algorithm. The fundamental operation in this algorithm is merging two sorted lists. Because the lists are sorted, this can be done in one pass through the input, if the output is put in a third list. The basic merging algorithm takes two input arrays A and B, an output array C, and three counters, Actr, Bctr, and Cctr, which are initially set to the beginning of their respective arrays. The smaller of A[Actr] and B[Bctr] is copied to the next entry in C, and the appropriate counters are advanced. When either input list is exhausted, the remainder of the other list is copied to C. An example of how the merge routine works is provided for the following input. Actr ↑ ↑ ↑ Bctr Cctr

7.6 Mergesort If the array A contains 1, 13, 24, 26, and B contains 2, 15, 27, 38, then the algorithm proceeds as follows: First, a comparison is done between 1 and 2. 1 is added to C, and then 13 and 2 are compared. Actr ↑ ↑ ↑ Bctr Cctr 2 is added to C, and then 13 and 15 are compared. Actr ↑ ↑ ↑ Bctr Cctr 13 is added to C, and then 24 and 15 are compared. This proceeds until 26 and 27 are compared. Actr ↑ ↑ ↑ Bctr Cctr ↑ ↑ ↑ Actr Bctr Cctr ↑ ↑ ↑ Actr Bctr Cctr 26 is added to C, and the A array is exhausted. Actr ↑ ↑ ↑ Bctr Cctr

Chapter 7 Sorting The remainder of the B array is then copied to C. Actr ↑ ↑ ↑ Bctr Cctr The time to merge two sorted lists is clearly linear, because at most N −1 comparisons are made, where N is the total number of elements. To see this, note that every comparison adds an element to C, except the last comparison, which adds at least two. The mergesort algorithm is therefore easy to describe. If N = 1, there is only one element to sort, and the answer is at hand. Otherwise, recursively mergesort the ﬁrst half and the second half. This gives two sorted halves, which can then be merged together using the merging algorithm described above. For instance, to sort the eight-element array 24, 13, 26, 1, 2, 27, 38, 15, we recursively sort the ﬁrst four and last four elements, obtaining 1, 13, 24, 26, 2, 15, 27, 38. Then we merge the two halves as above, obtaining the ﬁnal list 1, 2, 13, 15, 24, 26, 27, 38. This algorithm is a classic divide-and-conquer strategy. The problem is divided into smaller problems and solved recursively. The conquering phase consists of patching together the answers. Divide-and-conquer is a very powerful use of recursion that we will see many times. An implementation of mergesort is provided in Figure 7.9. The public mergeSort is just a driver for the private recursive method mergeSort. The merge routine is subtle. If a temporary array is declared locally for each recursive call of merge, then there could be log N temporary arrays active at any point. A close examination shows that since merge is the last line of mergeSort, there only needs to be one temporary array active at any point, and that the temporary array can be created in the public mergeSort driver. Further, we can use any part of the temporary array; we will use the same portion as the input array a. This allows the improvement described at the end of this section. Figure 7.10 implements the merge routine. 7.6.1 Analysis of Mergesort Mergesort is a classic example of the techniques used to analyze recursive routines: we have to write a recurrence relation for the running time. We will assume that N is a power of 2, so that we always split into even halves. For N = 1, the time to mergesort is constant, which we will denote by 1. Otherwise, the time to mergesort N numbers is equal to the time to do two recursive mergesorts of size N/2, plus the time to merge, which is linear. The following equations say this exactly: T(1) = 1 T(N) = 2T(N/2) + N This is a standard recurrence relation, which can be solved several ways. We will show two methods. The ﬁrst idea is to divide the recurrence relation through by N. The reason for doing this will become apparent soon. This yields T(N) N = T(N/2) N/2 + 1

7.6 Mergesort /** * Internal method that makes recursive calls. * @param a an array of Comparable items. * @param tmpArray an array to place the merged result. * @param left the left-most index of the subarray. * @param right the right-most index of the subarray. */ private static <AnyType extends Comparable<? super AnyType>> void mergeSort( AnyType [ ] a, AnyType [ ] tmpArray, int left, int right ) { if( left < right ) { int center = ( left + right ) / 2; mergeSort( a, tmpArray, left, center ); mergeSort( a, tmpArray, center + 1, right ); merge( a, tmpArray, left, center + 1, right ); } } /** * Mergesort algorithm. * @param a an array of Comparable items. */ public static <AnyType extends Comparable<? super AnyType>> void mergeSort( AnyType [ ] a ) { AnyType [ ] tmpArray = (AnyType[]) new Comparable[ a.length ]; mergeSort( a, tmpArray, 0, a.length - 1 ); } Figure 7.9 Mergesort routines This equation is valid for any N that is a power of 2, so we may also write T(N/2) N/2 = T(N/4) N/4 + 1 and T(N/4) N/4 = T(N/8) N/8 + 1 ... T(2) = T(1) + 1

Chapter 7 Sorting /** * Internal method that merges two sorted halves of a subarray. * @param a an array of Comparable items. * @param tmpArray an array to place the merged result. * @param leftPos the left-most index of the subarray. * @param rightPos the index of the start of the second half. * @param rightEnd the right-most index of the subarray. */ private static <AnyType extends Comparable<? super AnyType>> void merge( AnyType [ ] a, AnyType [ ] tmpArray, int leftPos, int rightPos, int rightEnd ) { int leftEnd = rightPos - 1; int tmpPos = leftPos; int numElements = rightEnd - leftPos + 1; // Main loop while( leftPos <= leftEnd && rightPos <= rightEnd ) if( a[ leftPos ].compareTo( a[ rightPos ] ) <= 0 ) tmpArray[ tmpPos++ ] = a[ leftPos++ ]; else tmpArray[ tmpPos++ ] = a[ rightPos++ ]; while( leftPos <= leftEnd ) // Copy rest of first half tmpArray[ tmpPos++ ] = a[ leftPos++ ]; while( rightPos <= rightEnd ) // Copy rest of right half tmpArray[ tmpPos++ ] = a[ rightPos++ ]; // Copy tmpArray back for( int i = 0; i < numElements; i++, rightEnd-- ) a[ rightEnd ] = tmpArray[ rightEnd ]; } Figure 7.10 merge routine Now add up all the equations. This means that we add all of the terms on the left-hand side and set the result equal to the sum of all of the terms on the right-hand side. Observe that the term T(N/2)/(N/2) appears on both sides and thus cancels. In fact, virtually all the terms appear on both sides and cancel. This is called telescoping a sum. After everything is added, the ﬁnal result is T(N) N = T(1) + log N

7.6 Mergesort because all of the other terms cancel and there are log N equations, and so all the 1’s at the end of these equations add up to log N. Multiplying through by N gives the ﬁnal answer. T(N) = N log N + N = O(N log N) Notice that if we did not divide through by N at the start of the solutions, the sum would not telescope. This is why it was necessary to divide through by N. An alternative method is to substitute the recurrence relation continually on the righthand side. We have T(N) = 2T(N/2) + N Since we can substitute N/2 into the main equation, 2T(N/2) = 2(2(T(N/4)) + N/2) = 4T(N/4) + N we have T(N) = 4T(N/4) + 2N Again, by substituting N/4 into the main equation, we see that 4T(N/4) = 4(2T(N/8) + N/4) = 8T(N/8) + N So we have T(N) = 8T(N/8) + 3N Continuing in this manner, we obtain T(N) = 2kT(N/2k) + k · N Using k = log N, we obtain T(N) = NT(1) + N log N = N log N + N The choice of which method to use is a matter of taste. The ﬁrst method tends to produce scrap work that ﬁts better on a standard, 81/2 ×11 sheet of paper, leading to fewer mathematical errors, but it requires a certain amount of experience to apply. The second method is more of a brute-force approach. Recall that we have assumed N = 2k. The analysis can be reﬁned to handle cases when N is not a power of 2. The answer turns out to be almost identical (this is usually the case). Although mergesort’s running time is O(N log N), it has the signiﬁcant problem that merging two sorted lists uses linear extra memory.1 The additional work involved in copying to the temporary array and back, throughout the algorithm, slows the sort considerably. This copying can be avoided by judiciously switching the roles of a and tmpArray at alternate levels of the recursion. A variant of mergesort can also be implemented nonrecursively (Exercise 7.16). 1 It is theoretically possible to use less extra memory, but the resulting algorithm is complex and impractical.

Chapter 7 Sorting The running time of mergesort, when compared with other O(N log N) alternatives, depends heavily on the relative costs of comparing elements and moving elements in the array (and the temporary array). These costs are language dependent. For instance, in Java, when performing a generic sort (using a Comparator), an element comparison can be expensive (because comparisons might not be easily inlined, and thus the overhead of dynamic dispatch could slow things down), but moving elements is cheap (because they are reference assignments, rather than copies of large objects). Mergesort uses the lowest number of comparisons of all the popular sorting algorithms, and thus is a good candidate for general-purpose sorting in Java. In fact, it is the algorithm used in the standard Java library for generic sorting. On the other hand, in C++, in a generic sort, copying objects can be expensive if the objects are large, while comparing objects often is relatively cheap because of the ability of the compiler to aggressively perform inline optimization. In this scenario, it might be reasonable to have an algorithm use a few more comparisons, if we can also use signiﬁcantly fewer data movements. Quicksort, which we discuss in the next section, achieves this tradeoff, and is the sorting routine commonly used in C++ libraries. In Java, quicksort is also used as the standard library sort for primitive types. Here, the costs of comparisons and data moves are similar, so using signiﬁcantly fewer data movements more than compensates for a few extra comparisons. 7.7 Quicksort As its name implies, quicksort is a fast sorting algorithm in practice and is especially useful in C++, or for sorting primitive types in Java. Its average running time is O(N log N). It is very fast, mainly due to a very tight and highly optimized inner loop. It has O(N2) worst-case performance, but this can be made exponentially unlikely with a little effort. By combining quicksort with heapsort, we can achieve quicksort’s fast running time on almost all inputs, with heapsort’s O(N log N) worst-case running time. Exercise 7.27 describes this approach. The quicksort algorithm is simple to understand and prove correct, although for many years it had the reputation of being an algorithm that could in theory be highly optimized but in practice was impossible to code correctly. Like mergesort, quicksort is a divide-andconquer recursive algorithm. Let us begin with the following simple sorting algorithm to sort a list. Arbitrarily choose any item, and then form three groups: those smaller than the chosen item, those equal to the chosen item, and those larger than the chosen item. Recursively sort the ﬁrst and third groups, and then concatenate the three groups. The result is guaranteed by the basic principles of recursion to be a sorted arrangement of the original list. A direct implementation of this algorithm is shown in Figure 7.11, and its performance, is generally speaking, quite respectable on most inputs. In fact, if the list contains large numbers of duplicates with relatively few distinct items, as is sometimes the case, then the performance is extremely good. The algorithm we have described forms the basis of the quicksort. However, by making the extra lists, and doing so recursively, it is hard to see how we have improved upon

7.7 Quicksort public static void sort( List<Integer> items ) { if( items.size( ) > 1 ) { List<Integer> smaller = new ArrayList<>( ); List<Integer> same = new ArrayList<>( ); List<Integer> larger = new ArrayList<>( ); Integer chosenItem = items.get( items.size( ) / 2 ); for( Integer i : items ) { if( i < chosenItem ) smaller.add( i ); else if( i > chosenItem ) larger.add( i ); else same.add( i ); } sort( smaller ); // Recursive call! sort( larger ); // Recursive call! items.clear( ); items.addAll( smaller ); items.addAll( same ); items.addAll( larger ); } } Figure 7.11 Simple recursive sorting algorithm mergesort. In fact, so far, we really haven’t. In order to do better, we must avoid using signiﬁcant extra memory and have inner loops that are clean. Thus quicksort is commonly written in a manner that avoids creating the second group (the equal items), and the algorithm has numerous subtle details that affect the performance; therein lies the complications. We now describe the most common implementation of quicksort—“classic quicksort,” in which the input is an array, and in which no extra arrays are created by the algorithm. The classic quicksort algorithm to sort an array S consists of the following four easy steps: 1. If the number of elements in S is 0 or 1, then return. 2. Pick any element v in S. This is called the pivot.

Chapter 7 Sorting 3. Partition S −{v} (the remaining elements in S) into two disjoint groups: S1 = {x ∈ S −{v}|x ≤v}, and S2 = {x ∈S −{v}|x ≥v}. 4. Return {quicksort(S1) followed by v followed by quicksort(S2)}. Since the partition step ambiguously describes what to do with elements equal to the pivot, this becomes a design decision. Part of a good implementation is handling this case as efﬁciently as possible. Intuitively, we would hope that about half the keys that are equal to the pivot go into S1 and the other half into S2, much as we like binary search trees to be balanced. Figure 7.12 shows the action of quicksort on a set of numbers. The pivot is chosen (by chance) to be 65. The remaining elements in the set are partitioned into two smaller sets. Recursively sorting the set of smaller numbers yields 0, 13, 26, 31, 43, 57 (by rule 3 of recursion). The set of large numbers is similarly sorted. The sorted arrangement of the entire set is then trivially obtained. It should be clear that this algorithm works, but it is not clear why it is any faster than mergesort. Like mergesort, it recursively solves two subproblems and requires linear additional work (step 3), but, unlike mergesort, the subproblems are not guaranteed to be of equal size, which is potentially bad. The reason that quicksort is faster is that the partitioning step can actually be performed in place and very efﬁciently. This efﬁciency can more than make up for the lack of equal-sized recursive calls. The algorithm as described so far lacks quite a few details, which we now ﬁll in. There are many ways to implement steps 2 and 3; the method presented here is the result of extensive analysis and empirical study and represents a very efﬁcient way to implement quicksort. Even the slightest deviations from this method can cause surprisingly bad results. 7.7.1 Picking the Pivot Although the algorithm as described works no matter which element is chosen as the pivot, some choices are obviously better than others. A Wrong Way The popular, uninformed choice is to use the ﬁrst element as the pivot. This is acceptable if the input is random, but if the input is presorted or in reverse order, then the pivot provides a poor partition, because either all the elements go into S1 or they go into S2. Worse, this happens consistently throughout the recursive calls. The practical effect is that if the ﬁrst element is used as the pivot and the input is presorted, then quicksort will take quadratic time to do essentially nothing at all, which is quite embarrassing. Moreover, presorted input (or input with a large presorted section) is quite frequent, so using the ﬁrst element as the pivot is an absolutely horrible idea and should be discarded immediately. An alternative is choosing the larger of the ﬁrst two distinct elements as the pivot, but this has the same bad properties as merely choosing the ﬁrst element. Do not use that pivoting strategy either.

7.7 Quicksort select pivot partition quicksort large quicksort small Figure 7.12 The steps of quicksort illustrated by example A Safe Maneuver A safe course is merely to choose the pivot randomly. This strategy is generally perfectly safe, unless the random number generator has a ﬂaw (which is not as uncommon as you might think), since it is very unlikely that a random pivot would consistently provide a poor partition. On the other hand, random number generation is generally an expensive commodity and does not reduce the average running time of the rest of the algorithm at all.

Chapter 7 Sorting Median-of-Three Partitioning The median of a group of N numbers is the ⌈N/2⌉th largest number. The best choice of pivot would be the median of the array. Unfortunately, this is hard to calculate and would slow down quicksort considerably. A good estimate can be obtained by picking three elements randomly and using the median of these three as the pivot. The randomness turns out not to help much, so the common course is to use as the pivot the median of the left, right, and center elements. For instance, with input 8, 1, 4, 9, 6, 3, 5, 2, 7, 0 as before, the left element is 8, the right element is 0, and the center (in position ⌊(left + right)/2⌋) element is 6. Thus, the pivot would be v = 6. Using median-of-three partitioning clearly eliminates the bad case for sorted input (the partitions become equal in this case) and actually reduces the number of comparisons by 14 percent. 7.7.2 Partitioning Strategy There are several partitioning strategies used in practice, but the one described here is known to give good results. It is very easy, as we shall see, to do this wrong or inefﬁciently, but it is safe to use a known method. The ﬁrst step is to get the pivot element out of the way by swapping it with the last element. i starts at the ﬁrst element and j starts at the next-to-last element. If the original input was the same as before, the following ﬁgure shows the current situation. ↑ ↑ i j For now we will assume that all the elements are distinct. Later on we will worry about what to do in the presence of duplicates. As a limiting case, our algorithm must do the proper thing if all of the elements are identical. It is surprising how easy it is to do the wrong thing. What our partitioning stage wants to do is to move all the small elements to the left part of the array and all the large elements to the right part. “Small” and “large” are, of course, relative to the pivot. While i is to the left of j, we move i right, skipping over elements that are smaller than the pivot. We move j left, skipping over elements that are larger than the pivot. When i and j have stopped, i is pointing at a large element and j is pointing at a small element. If i is to the left of j, those elements are swapped. The effect is to push a large element to the right and a small element to the left. In the example above, i would not move and j would slide over one place. The situation is as follows. ↑ ↑ i j

7.7 Quicksort We then swap the elements pointed to by i and j and repeat the process until i and j cross. After First Swap ↑ ↑ i j Before Second Swap ↑ ↑ i j After Second Swap ↑ ↑ i j Before Third Swap ↑ ↑ j i At this stage, i and j have crossed, so no swap is performed. The ﬁnal part of the partitioning is to swap the pivot element with the element pointed to by i. After Swap with Pivot ↑ ↑ i pivot When the pivot is swapped with i in the last step, we know that every element in a position p < i must be small. This is because either position p contained a small element to start with, or the large element originally in position p was replaced during a swap. A similar argument shows that elements in positions p > i must be large. One important detail we must consider is how to handle elements that are equal to the pivot. The questions are whether or not i should stop when it sees an element equal to the pivot and whether or not j should stop when it sees an element equal to the pivot. Intuitively, i and j ought to do the same thing, since otherwise the partitioning step is biased. For instance, if i stops and j does not, then all elements that are equal to the pivot will wind up in S2.

Chapter 7 Sorting To get an idea of what might be good, we consider the case where all the elements in the array are identical. If both i and j stop, there will be many swaps between identical elements. Although this seems useless, the positive effect is that i and j will cross in the middle, so when the pivot is replaced, the partition creates two nearly equal subarrays. The mergesort analysis tells us that the total running time would then be O(N log N). If neither i nor j stops, and code is present to prevent them from running off the end of the array, no swaps will be performed. Although this seems good, a correct implementation would then swap the pivot into the last spot that i touched, which would be the next-tolast position (or last, depending on the exact implementation). This would create very uneven subarrays. If all the elements are identical, the running time is O(N2). The effect is the same as using the ﬁrst element as a pivot for presorted input. It takes quadratic time to do nothing! Thus, we ﬁnd that it is better to do the unnecessary swaps and create even subarrays than to risk wildly uneven subarrays. Therefore, we will have both i and j stop if they encounter an element equal to the pivot. This turns out to be the only one of the four possibilities that does not take quadratic time for this input. At ﬁrst glance it may seem that worrying about an array of identical elements is silly. After all, why would anyone want to sort 50,000 identical elements? However, recall that quicksort is recursive. Suppose there are 1,000,000 elements, of which 50,000 are identical (or, more likely, complex elements whose sort keys are identical). Eventually, quicksort will make the recursive call on only these 50,000 elements. Then it really will be important to make sure that 50,000 identical elements can be sorted efﬁciently. 7.7.3 Small Arrays For very small arrays (N ≤20), quicksort does not perform as well as insertion sort. Furthermore, because quicksort is recursive, these cases will occur frequently. A common solution is not to use quicksort recursively for small arrays, but instead use a sorting algorithm that is efﬁcient for small arrays, such as insertion sort. Using this strategy can actually save about 15 percent in the running time (over doing no cutoff at all). A good cutoff range is N = 10, although any cutoff between 5 and 20 is likely to produce similar results. This also saves nasty degenerate cases, such as taking the median of three elements when there are only one or two. 7.7.4 Actual Quicksort Routines The driver for quicksort is shown in Figure 7.13. The general form of the routines will be to pass the array and the range of the array (left and right) to be sorted. The ﬁrst routine to deal with is pivot selection. The easiest way to do this is to sort a[left], a[right], and a[center] in place. This has the extra advantage that the smallest of the three winds up in a[left], which is where the partitioning step would put it anyway. The largest winds up in a[right], which is also the correct place, since it is larger than the pivot. Therefore, we can place the pivot in a[right-1] and initialize i and j to left+1 and right-2 in the partition phase. Yet another beneﬁt is that because a[left] is smaller than the pivot, it will act as a sentinel for j. Thus, we do not need

7.7 Quicksort /** * Quicksort algorithm. * @param a an array of Comparable items. */ public static <AnyType extends Comparable<? super AnyType>> void quicksort( AnyType [ ] a ) { quicksort( a, 0, a.length - 1 ); } Figure 7.13 Driver for quicksort /** * Return median of left, center, and right. * Order these and hide the pivot. */ private static <AnyType extends Comparable<? super AnyType>> AnyType median3( AnyType [ ] a, int left, int right ) { int center = ( left + right ) / 2; if( a[ center ].compareTo( a[ left ] ) < 0 ) swapReferences( a, left, center ); if( a[ right ].compareTo( a[ left ] ) < 0 ) swapReferences( a, left, right ); if( a[ right ].compareTo( a[ center ] ) < 0 ) swapReferences( a, center, right ); // Place pivot at position right - 1 swapReferences( a, center, right - 1 ); return a[ right - 1 ]; } Figure 7.14 Code to perform median-of-three partitioning to worry about j running past the end. Since i will stop on elements equal to the pivot, storing the pivot in a[right-1] provides a sentinel for i. The code in Figure 7.14 does the median-of-three partitioning with all the side effects described. It may seem that it is only slightly inefﬁcient to compute the pivot by a method that does not actually sort a[left], a[center], and a[right], but, surprisingly, this produces bad results (see Exercise 7.51). The real heart of the quicksort routine is in Figure 7.15. It includes the partitioning and recursive calls. There are several things worth noting in this implementation. Line 16 initializes i and j to 1 past their correct values, so that there are no special cases to consider. This initialization depends on the fact that median-of-three partitioning has some side

Chapter 7 Sorting /** * Internal quicksort method that makes recursive calls. * Uses median-of-three partitioning and a cutoff of 10. * @param a an array of Comparable items. * @param left the left-most index of the subarray. * @param right the right-most index of the subarray. */ private static <AnyType extends Comparable<? super AnyType>> void quicksort( AnyType [ ] a, int left, int right ) { if( left + CUTOFF <= right ) { AnyType pivot = median3( a, left, right ); // Begin partitioning int i = left, j = right - 1; for( ; ; ) { while( a[ ++i ].compareTo( pivot ) < 0 ) { } while( a[ --j ].compareTo( pivot ) > 0 ) { } if( i < j ) swapReferences( a, i, j ); else break; } swapReferences( a, i, right - 1 ); // Restore pivot quicksort( a, left, i - 1 ); // Sort small elements quicksort( a, i + 1, right ); // Sort large elements } else // Do an insertion sort on the subarray insertionSort( a, left, right ); } Figure 7.15 Main quicksort routine effects; this program will not work if you try to use it without change with a simple pivoting strategy, because i and j start in the wrong place and there is no longer a sentinel for j. The swapping action at line 22 is sometimes written explicitly, for speed purposes. For the algorithm to be fast, it may be necessary to force the compiler to compile this code inline. Many compilers will do this automatically if swapReferences is a final method, but for those that do not, the difference can be signiﬁcant.

7.7 Quicksort int i = left + 1, j = right ; for( ; ; ) { while( a[ i ].compareTo( pivot ) < 0 ) i++; while( a[ j ].compareTo( pivot ) > 0 ) j-- if( i < j ) swapReferences( a, i, j ); else break; } Figure 7.16 A small change to quicksort, which breaks the algorithm Finally, lines 19 and 20 show why quicksort is so fast. The inner loop of the algorithm consists of an increment/decrement (by 1, which is fast), a test, and a jump. There is no extra juggling as there is in mergesort. This code is still surprisingly tricky. It is tempting to replace lines 16 through 25 with the statements in Figure 7.16. This does not work, because there would be an inﬁnite loop if a[i] = a[j] = pivot. 7.7.5 Analysis of Quicksort Like mergesort, quicksort is recursive, and hence, its analysis requires solving a recurrence formula. We will do the analysis for a quicksort, assuming a random pivot (no medianof-three partitioning) and no cutoff for small arrays. We will take T(0) = T(1) = 1, as in mergesort. The running time of quicksort is equal to the running time of the two recursive calls plus the linear time spent in the partition (the pivot selection takes only constant time). This gives the basic quicksort relation T(N) = T(i) + T(N −i −1) + cN (7.1) where i = |S1| is the number of elements in S1. We will look at three cases. Worst-Case Analysis The pivot is the smallest element, all the time. Then i = 0 and if we ignore T(0) = 1, which is insigniﬁcant, the recurrence is T(N) = T(N −1) + cN, N > 1 (7.2) We telescope, using Equation (7.2) repeatedly. Thus T(N −1) = T(N −2) + c(N −1) (7.3) T(N −2) = T(N −3) + c(N −2) (7.4) ... T(2) = T(1) + c(2) (7.5)

Chapter 7 Sorting Adding up all these equations yields T(N) = T(1) + c N  i=2 i = (N2) (7.6) as claimed earlier. To see that this is the worst possible case, note that the total cost of all the partitions in recursive calls at depth d must be at most N. Since the recursion depth is at most N, this gives as O(N2) worst-case bound for quicksort. Best-Case Analysis In the best case, the pivot is in the middle. To simplify the math, we assume that the two subarrays are each exactly half the size of the original, and although this gives a slight overestimate, this is acceptable because we are only interested in a Big-Oh answer. T(N) = 2T(N/2) + cN (7.7) Divide both sides of Equation (7.7) by N. T(N) N = T(N/2) N/2 + c (7.8) We will telescope using this equation. T(N/2) N/2 = T(N/4) N/4 + c (7.9) T(N/4) N/4 = T(N/8) N/8 + c (7.10) ... T(2) = T(1) + c (7.11) We add all the equations from (7.8) to (7.11) and note that there are log N of them: T(N) N = T(1) + c log N (7.12) which yields T(N) = cN log N + N = (N log N) (7.13) Notice that this is the exact same analysis as mergesort, hence we get the same answer. That this is the best case is implied by results in Section 7.8. Average-Case Analysis This is the most difﬁcult part. For the average case, we assume that each of the sizes for S1 is equally likely, and hence has probability 1/N. This assumption is actually valid for our pivoting and partitioning strategy, but it is not valid for some others. Partitioning strategies that do not preserve the randomness of the subarrays cannot use this analysis. Interestingly, these strategies seem to result in programs that take longer to run in practice.

7.7 Quicksort With this assumption, the average value of T(i), and hence T(N −i −1), is (1/N) N−1 j=0 T(j). Equation (7.1) then becomes T(N) = 2 N ⎡ ⎣ N−1  j=0 T(j) ⎤ ⎦+ cN (7.14) If Equation (7.14) is multiplied by N, it becomes NT(N) = 2 ⎡ ⎣ N−1  j=0 T(j) ⎤ ⎦+ cN2 (7.15) We need to remove the summation sign to simplify matters. We note that we can telescope with one more equation. (N −1)T(N −1) = 2 ⎡ ⎣ N−2  j=0 T(j) ⎤ ⎦+ c(N −1)2 (7.16) If we subtract Equation (7.16) from Equation (7.15), we obtain NT(N) −(N −1)T(N −1) = 2T(N −1) + 2cN −c (7.17) We rearrange terms and drop the insigniﬁcant −c on the right, obtaining NT(N) = (N + 1)T(N −1) + 2cN (7.18) We now have a formula for T(N) in terms of T(N −1) only. Again the idea is to telescope, but Equation (7.18) is in the wrong form. Divide Equation (7.18) by N (N + 1): T(N) N + 1 = T(N −1) N + 2c N + 1 (7.19) Now we can telescope. T(N −1) N = T(N −2) N −1 + 2c N (7.20) T(N −2) N −1 = T(N −3) N −2 + 2c N −1 (7.21) ... T(2) = T(1) + 2c (7.22)

Chapter 7 Sorting Adding Equations (7.19) through (7.22) yields T(N) N + 1 = T(1) + 2c N+1  i=3 i (7.23) The sum is about loge(N + 1) + γ −3 2, where γ ≈0.577 is known as Euler’s constant, so T(N) N + 1 = O(log N) (7.24) And so T(N) = O(N log N) (7.25) Although this analysis seems complicated, it really is not—the steps are natural once you have seen some recurrence relations. The analysis can actually be taken further. The highly optimized version that was described above has also been analyzed, and this result gets extremely difﬁcult, involving complicated recurrences and advanced mathematics. The effect of equal elements has also been analyzed in detail, and it turns out that the code presented does the right thing. 7.7.6 A Linear-Expected-Time Algorithm for Selection Quicksort can be modiﬁed to solve the selection problem, which we have seen in Chapters 1 and 6. Recall that by using a priority queue, we can ﬁnd the kth largest (or smallest) element in O(N + k log N). For the special case of ﬁnding the median, this gives an O(N log N) algorithm. Since we can sort the array in O(N log N) time, one might expect to obtain a better time bound for selection. The algorithm we present to ﬁnd the kth smallest element in a set S is almost identical to quicksort. In fact, the ﬁrst three steps are the same. We will call this algorithm quickselect. Let |Si| denote the number of elements in Si. The steps of quickselect are 1. If |S| = 1, then k = 1 and return the element in S as the answer. If a cutoff for small arrays is being used and |S| ≤CUTOFF, then sort S and return the kth smallest element. 2. Pick a pivot element, v ∈S. 3. Partition S −{v} into S1 and S2, as was done with quicksort. 4. If k ≤|S1|, then the kth smallest element must be in S1. In this case, return quickselect (S1, k). If k = 1+|S1|, then the pivot is the kth smallest element and we can return it as the answer. Otherwise, the kth smallest element lies in S2, and it is the (k −|S1| −1)st smallest element in S2. We make a recursive call and return quickselect (S2, k−|S1|−1). In contrast to quicksort, quickselect makes only one recursive call instead of two. The worst case of quickselect is identical to that of quicksort and is O(N2). Intuitively, this is because quicksort’s worst case is when one of S1 and S2 is empty; thus, quickselect is not really saving a recursive call. The average running time, however, is O(N). The analysis is similar to quicksort’s and is left as an exercise.

7.7 Quicksort The implementation of quickselect is even simpler than the abstract description might imply. The code to do this is shown in Figure 7.17. When the algorithm terminates, the kth smallest element is in position k −1 (because arrays start at index 0). This destroys the original ordering; if this is not desirable, then a copy must be made. /** * Internal selection method that makes recursive calls. * Uses median-of-three partitioning and a cutoff of 10. * Places the kth smallest item in a[k-1]. * @param a an array of Comparable items. * @param left the left-most index of the subarray. * @param right the right-most index of the subarray. * @param k the desired index (1 is minimum) in the entire array. */ private static <AnyType extends Comparable<? super AnyType>> void quickSelect( AnyType [ ] a, int left, int right, int k ) { if( left + CUTOFF <= right ) { AnyType pivot = median3( a, left, right ); // Begin partitioning int i = left, j = right - 1; for( ; ; ) { while( a[ ++i ].compareTo( pivot ) < 0 ) { } while( a[ --j ].compareTo( pivot ) > 0 ) { } if( i < j ) swapReferences( a, i, j ); else break; } swapReferences( a, i, right - 1 ); // Restore pivot if( k <= i ) quickSelect( a, left, i - 1, k ); else if( k > i + 1 ) quickSelect( a, i + 1, right, k ); } else // Do an insertion sort on the subarray insertionSort( a, left, right ); } Figure 7.17 Main quickselect routine

Chapter 7 Sorting Using a median-of-three pivoting strategy makes the chance of the worst case occurring almost negligible. By carefully choosing the pivot, however, we can eliminate the quadratic worst case and ensure an O(N) algorithm. The overhead involved in doing this is considerable, so the resulting algorithm is mostly of theoretical interest. In Chapter 10, we will examine the linear-time worst-case algorithm for selection, and we shall also see an interesting technique of choosing the pivot that results in a somewhat faster selection algorithm in practice. 7.8 A General Lower Bound for Sorting Although we have O(N log N) algorithms for sorting, it is not clear that this is as good as we can do. In this section, we prove that any algorithm for sorting that uses only comparisons requires  (N log N) comparisons (and hence time) in the worst case, so that mergesort and heapsort are optimal to within a constant factor. The proof can be extended to show that  (N log N) comparisons are required, even on average, for any sorting algorithm that uses only comparisons, which means that quicksort is optimal on average to within a constant factor. Speciﬁcally, we will prove the following result: Any sorting algorithm that uses only comparisons requires ⌈log(N!)⌉comparisons in the worst case and log(N!) comparisons on average. We will assume that all N elements are distinct, since any sorting algorithm must work for this case. 7.8.1 Decision Trees A decision tree is an abstraction used to prove lower bounds. In our context, a decision tree is a binary tree. Each node represents a set of possible orderings, consistent with comparisons that have been made, among the elements. The results of the comparisons are the tree edges. The decision tree in Figure 7.18 represents an algorithm that sorts the three elements a, b, and c. The initial state of the algorithm is at the root. (We will use the terms state and node interchangeably.) No comparisons have been done, so all orderings are legal. The ﬁrst comparison that this particular algorithm performs compares a and b. The two results lead to two possible states. If a < b, then only three possibilities remain. If the algorithm reaches node 2, then it will compare a and c. Other algorithms might do different things; a different algorithm would have a different decision tree. If a > c, the algorithm enters state 5. Since there is only one ordering that is consistent, the algorithm can terminate and report that it has completed the sort. If a < c, the algorithm cannot do this, because there are two possible orderings and it cannot possibly be sure which is correct. In this case, the algorithm will require one more comparison. Every algorithm that sorts by using only comparisons can be represented by a decision tree. Of course, it is only feasible to draw the tree for extremely small input sizes. The number of comparisons used by the sorting algorithm is equal to the depth of the deepest leaf. In our case, this algorithm uses three comparisons in the worst case. The average

7.8 A General Lower Bound for Sorting a < b < c a < b a < c c < a c < b b < a b < c a < c < b a < b < c a < c < b a < b < c b < c a < b < c a < c < b c < b a < c < b b < a < c b < c < a b < a < c c < b < a b < c < a b < a < c a < c b < a < c b < c < a c < a b < c < a c < a < b c < a < b c < a < b c < b < a c < b < a Figure 7.18 A decision tree for three-element sort number of comparisons used is equal to the average depth of the leaves. Since a decision tree is large, it follows that there must be some long paths. To prove the lower bounds, all that needs to be shown are some basic tree properties. Lemma 7.1. Let T be a binary tree of depth d. Then T has at most 2d leaves. Proof. The proof is by induction. If d = 0, then there is at most one leaf, so the basis is true. Otherwise, we have a root, which cannot be a leaf, and a left and right subtree, each of depth at most d −1. By the induction hypothesis, they can each have at most 2d−1 leaves, giving a total of at most 2d leaves. This proves the lemma. Lemma 7.2. A binary tree with L leaves must have depth at least ⌈log L⌉. Proof. Immediate from the preceding lemma.

Chapter 7 Sorting Theorem 7.6. Any sorting algorithm that uses only comparisons between elements requires at least ⌈log(N!)⌉comparisons in the worst case. Proof. A decision tree to sort N elements must have N! leaves. The theorem follows from the preceding lemma. Theorem 7.7. Any sorting algorithm that uses only comparisons between elements requires  (N log N) comparisons. Proof. From the previous theorem, log(N!) comparisons are required. log(N!) = log(N(N −1)(N −2) · · · (2)(1)) = log N + log(N −1) + log(N −2) + · · · + log 2 + log 1 ≥log N + log(N −1) + log(N −2) + · · · + log(N/2) ≥N 2 log N ≥N 2 log N −N =  (N log N) This type of lower-bound argument, when used to prove a worst-case result, is sometimes known as an information-theoretic lower bound. The general theorem says that if there are P different possible cases to distinguish, and the questions are of the form YES/NO, then ⌈log P⌉questions are always required in some case by any algorithm to solve the problem. It is possible to prove a similar result for the average-case running time of any comparison-based sorting algorithm. This result is implied by the following lemma, which is left as an exercise: Any binary tree with L leaves has an average depth of at least log L. Note that log(N!) is roughly N log N −O(N) (Exercise 7.34). 7.9 Decision-Tree Lower Bounds for Selection Problems Section 7.8 employed a decision tree argument to show the fundamental lower bound that any comparison-based sorting algorithm must use roughly N log N comparisons. In this section we show additional lower bounds for selection in an N-element collection, speciﬁcally 1. N −1 comparisons are necessary to ﬁnd the smallest item 2. N + ⌈log N⌉−2 comparisons are necessary to ﬁnd the two smallest items 3. ⌈3N/2⌉−O(log N) comparisons are necessary to ﬁnd the median

7.9 Decision-Tree Lower Bounds for Selection Problems The lower bounds for all these problems, with the exception of ﬁnding the median, are tight: Algorithms exist that use exactly the speciﬁed number of comparisons. In all our proofs, we assume all items are unique. Lemma 7.3. If all the leaves in a decision tree are at depth d or higher, the decision tree must have at least 2d leaves. Proof. Note that all nonleaf nodes in a decision tree have two children. The proof is by induction and follows Lemma 7.1. The ﬁrst lower bound, for ﬁnding the smallest item, is the easiest and most trivial to show. Theorem 7.8. Any comparison-based algorithm to ﬁnd the smallest element must use at least N −1 comparisons. Proof. Every element, x, except the smallest element, must be involved in a comparison with some other element y, in which x is declared larger than y. Otherwise, if there were two different elements that had not been declared larger than any other elements, then either could be the smallest. Lemma 7.4. The decision tree for ﬁnding the smallest of N elements must have at least 2N −1 leaves. Proof. By Theorem 7.8, all leaves in this decision tree are at depth N −1 or higher. Then this lemma follows from Lemma 7.3. The bound for selection is a little more complicated and requires looking at the structure of the decision tree. It will allow us to prove lower bounds for problems 2 and 3 on our list. Lemma 7.5. The decision tree for ﬁnding the kth smallest of N elements must have at least  N k −1 2N −k leaves. Proof. Observe that any algorithm that correctly identiﬁes the kth smallest element t must be able to prove that all other elements x are either larger than or smaller than t. Otherwise, it would be giving the same answer regardless of whether x was larger or smaller than t, and the answer cannot be the same in both circumstances. Thus each

Chapter 7 Sorting b < f yes no TY TN TY TREE T TREE T  ′ Figure 7.19 Smallest three elements are S = { a, b, c }; largest four elements are R = { d, e, f, g }; the comparison between b and f for this choice of R and S can be eliminated when forming tree T′ leaf in the tree, in addition to representing the kth smallest element, also represents the k −1 smallest items that have been identiﬁed. Let T be the decision tree, and consider two sets: S = { x1, x2, . . . , xk −1 }, representing the k −1 smallest items, and R which are the remaining items, including the kth smallest. Form a new decision tree T′, by purging any comparisons in T between an element in S and an element in R. Since any element in S is smaller than an element in R, the comparison tree node and its right subtree may be removed from T, without any loss of information. Figure 7.19 shows how nodes can be pruned. Any permutation of R that is fed into T′ follows the same path of nodes and leads to the same leaf as a corresponding sequence consisting of a permutation of S followed by the same permutation of R. Since T identiﬁes the overall kth smallest element, and the smallest element in R is that element, it follows that T′ identiﬁes the smallest element in R. Thus T′ must have at least 2|R|−1 = 2N −k leaves. These leaves in T′ directly correspond to 2N −k leaves representing S. Since there are  N k −1 choices for S, there must be at least  N k −1 2N −k leaves in T. A direct application of Lemma 7.5 allows us to prove the lower bounds for ﬁnding the second smallest element, and also the median. Theorem 7.9. Any comparison-based algorithm to ﬁnd the kth smallest element must use at least N −k +  log  N k −1 	 comparisons. Proof. Immediate from Lemma 7.5 and Lemma 7.2.

7.10 Adversary Lower Bounds Theorem 7.10. Any comparison-based algorithm to ﬁnd the second smallest element must use at least N + ⌈log N⌉−2 comparisons. Proof. Applying Theorem 7.9, with k = 2 yields N −2 + ⌈log N⌉. Theorem 7.11. Any comparison-based algorithm to ﬁnd the median must use at least ⌈3N/2⌉− O(log N) comparisons. Proof. Apply Theorem 7.9, with k = ⌈N/2⌉. The lower bound for selection is not tight, nor is it the best known; see the references for details. 7.10 Adversary Lower Bounds Although decision-tree arguments allowed us to show lower bounds for sorting and some selection problems, generally the bounds that result are not that tight, and sometimes are trivial. For instance, consider the problem of ﬁnding the minimum item. Since there are N possible choices for the minimum, the information theory lower bound that is produced by a decision-tree argument is only log N. In Theorem 7.8, we were able to show the N−1 bound by using what is essentially an adversary argument. In this section, we expand on this argument and use it to prove the following lower bound: 4. ⌈3N/2⌉−2 comparisons are necessary to ﬁnd both the smallest and largest item Recall our proof that any algorithm to ﬁnd the smallest item requires at least N −1 comparisons: Every element, x, except the smallest element, must be involved in a comparison with some other element y, in which x is declared larger than y. Otherwise, if there were two different elements that had not been declared larger than any other elements, then either could be the smallest. This is the underlying idea of an adversary argument which has some basic steps: 1. Establish that some basic amount of information must be obtained by any algorithm that solves a problem 2. In each step of the algorithm, the adversary will maintain an input that is consistent with all the answers that have been provided by the algorithm thus far 3. Argue that with insufﬁcient steps, there are multiple consistent inputs that would provide different answers to the algorithm; hence the algorithm has not done enough steps, because if the algorithm were to provide an answer at that point, the adversary would be able to show an input for which the answer is wrong.

Chapter 7 Sorting To see how this works, we will reprove the lower bound for ﬁnding the smallest element using this proof template. Theorem 7.8. (restated) Any comparison-based algorithm to ﬁnd the smallest element must use at least N −1 comparisons. New Proof. Begin by marking each item as U (for unknown). When an item is declared larger than another item, we will change its marking to E (for eliminated). This change represents one unit of information. Initially each unknown item has a value of 0, but there have been no comparisons, so this ordering is consistent with prior answers. A comparison between two items is either between two unknowns, or it involves at least one item eliminated from being the minimum. Figure 7.20 shows how our adversary will construct the input values, based on the questioning. If the comparison is between two unknowns, the ﬁrst is declared the smaller, and the second is automatically eliminated, providing one unit of information. We then assign it (irrevocably) a number larger than 0; the most convenient is the number of eliminated items. If a comparison is between an eliminated number and an unknown, the eliminated number (which is larger than 0 by the prior sentence) will be declared larger, and there will be no changes, no eliminations, and no information obtained. If two eliminated numbers are compared, then they will be different, and a consistent answer can be provided, again with no changes, and no information provided. At the end, we need to obtain N−1 units of information, and each comparison provides only 1 unit at the most; hence, at least N −1 comparisons are necessary. Lower Bound for Finding the Minimum and Maximum We can use this same technique to establish a lower bound for ﬁnding both the minimum and maximum item. Observe that all but one item must be eliminated from being the smallest, and all but one item must be eliminated from being the largest; thus the total information that any algorithm must acquire is 2N −2. However, a comparison x < y, eliminates both x from being the maximum and y from being the minimum; thus a comparison can provide two units of information. Consequently, this argument yields only the x y Answer Information New x New y Mark y as E U U x < y No change Change y to #elim All others Consistently No change No change Figure 7.20 Adversary constructs input for ﬁnding the minimum as algorithm runs

7.10 Adversary Lower Bounds trivial N −1 lower bound. Our adversary needs to do more work to ensure that it does not give out two units of information more than it needs to. To achieve this, each item will initially be unmarked. If it “wins” a comparison (i.e., it is declared larger than some item), it obtains a W. If it “loses” a comparison (i.e., it is declared smaller than some item), it obtains an L. At the end, all but two items will be WL. Our adversary will ensure that it only hands out two units of information if it is comparing two unmarked items. That can happen only ⌊N/2⌋times; then the remaining information has to be obtained one unit at a time, which will establish the bound. Theorem 7.12. Any comparison-based algorithm to ﬁnd the minimum and maximum must use at least ⌈3N/2⌉−2 comparisons. Proof. The basic idea if that if two items are unmarked, the adversary must give out two pieces of information. Otherwise, one of the items has either a W or an L (perhaps both). In that case, with reasonable care, the adversary should be able to avoid giving out two units of information. For instance, if one item, x, has a W and the other item, y, is unmarked, the adversary lets x win again by saying x > y. This gives one unit of information for y, but no new information for x. It is easy to see that in principle, there is no reason that the adversary should have to give more than one unit of information out if there is at least one unmarked item involved in the comparison. It remains to show that the adversary can maintain values that are consistent with its answers. If both items are unmarked, then obviously they can be safely assigned values consistent with the comparison answer; this case yields two units of information. Otherwise, if one of the items involved in a comparison is unmarked, it can be assigned a value the ﬁrst time, consistent with the other item in the comparison. This case yields one unit of information. Otherwise both items involved in the comparison are marked. If both are WL, then we can answer consistently with the current assignment, yielding no information.2 Otherwise at least one of the items has only an L or only a W. We will allow that item to compare redundantly (if it is an L then it loses again, if it is a W then it wins again), and its value can be easily adjusted if needed, based on the other item in the comparison (an L can be lowered as needed; a W can be raised as needed). This yields at most one unit of information for the other item in the comparison, possibly zero. Figure 7.21 summarizes the action of the adversary, making y the primary element whose value changes in all cases. At most ⌊N/2⌋comparisons yield two units of information, meaning that the remaining information, namely 2N −2 −2⌊N/2⌋units, must each be obtained one comparison at a time. Thus the total number of comparisons that are needed is at least 2N −2 −⌊N/2⌋= ⌈3N/2⌉−2. 2 It is possible that the current assignment for both items has the same number; in such a case we can increase all items whose current value is larger than y by 2, and then add 1 to y to break the tie.

Chapter 7 Sorting x y Answer Information New x New y – – x < y L W L – x < y L W unchanged x + 1 W or WL – x > y W or WL L unchanged x −1 W or WL W x < y 1 or 0 WL W unchanged max(x + 1, y) L or W or L x > y 1 or 0 or 0 WL or W or WL L WL unchanged min(x −1, y) WL WL consistent unchanged unchanged – W – WL – L SYMMETRIC TO AN ABOVE CASE L W L WL W WL Figure 7.21 Adversary constructs input for ﬁnding the maximum and minimum as algorithm runs It is easy to see that this bound is achievable. Pair up the elements, and perform a comparison between each pair. Then ﬁnd the maximum among the winners, and the minimum amoung the losers. 7.11 Linear-Time Sorts: Bucket Sort and Radix Sort Although we proved in Section 7.8 that any general sorting algorithm that uses only comparisons requires (N log N) time in the worst case, recall that it is still possible to sort in linear time in some special cases. A simple example is bucket sort. For bucket sort to work, extra information must be available. The input A1, A2, . . . , AN must consist of only positive integers smaller than M. (Obviously extensions to this are possible.) If this is the case, then the algorithm is simple: Keep an array called count, of size M, which is initialized to all 0’s. Thus, count has M cells, or buckets, which are initially empty. When Ai is read, increment count[Ai] by 1. After all the input is read, scan the count array, printing out a representation of the

7.11 Linear-Time Sorts: Bucket Sort and Radix Sort sorted list. This algorithm takes O(M + N); the proof is left as an exercise. If M is O(N), then the total is O(N). Although this algorithm seems to violate the lower bound, it turns out that it does not because it uses a more powerful operation than simple comparisons. By incrementing the appropriate bucket, the algorithm essentially performs an M-way comparison in unit time. This is similar to the strategy used in extendible hashing (Section 5.9). This is clearly not in the model for which the lower bound was proven. This algorithm does, however, question the validity of the model used in proving the lower bound. The model actually is a strong model, because a general-purpose sorting algorithm cannot make assumptions about the type of input it can expect to see, but must make decisions based on ordering information only. Naturally, if there is extra information available, we should expect to ﬁnd a more efﬁcient algorithm, since otherwise the extra information would be wasted. Although bucket sort seems like much too trivial an algorithm to be useful, it turns out that there are many cases where the input is only small integers, so that using a method like quicksort is really overkill. One such example is radix sort. Radix sort is sometimes known as card sort because it was used until the advent of modern computers to sort old-style punch cards. Suppose we have 10 numbers in the range 0 to 999 that we would like to sort. In general this is N numbers in the range 0 to b p−1 for some constant p. Obviously we cannot use bucket sort; there would be too many buckets. The trick is to use several passes of bucket sort. The natural algorithm would be to bucket sort by the most signiﬁcant “digit” (digit is taken to base b), then next most signiﬁcant, and so on. But a simpler idea is to perform bucket sorts in the reverse order, starting with the least signiﬁcant “digit” ﬁrst. Of course, more than one number could fall into the same bucket, and unlike the original bucket sort, these numbers could be different, so we keep them in a list. Each pass is stable: Items that agree in the current digit retain the ordering determined in prior passes. The trace in Figure 7.22 shows the result of sorting 64, 8, 216, 512, 27, 729, 0, 1, 343, 125, which is the ﬁrst ten cubes arranged randomly (we use 0’s to make clear the tens and hundreds digits). After the ﬁrst pass, the items are sorted and in general, after the kth pass, the items are sorted on the k least signiﬁcant digits. So at the end, the items are completely sorted. To see that the algorithm works, notice that the only possible failure would occur if two numbers came out of the same bucket in the wrong order. But the previous passes ensure that when several numbers enter a bucket, they enter in sorted order. The running time is O (p(N + b)) where p is the number of passes, N is the number of elements to sort, and b is the number of buckets. One application of radix sort is sorting strings. If all the strings have the same length L, then by using buckets for each character, we can implement a radix sort in O (NL) INITIAL ITEMS: 064, 008, 216, 512, 027, 729, 000, 001, 343, 125 SORTED BY 1’s digit: 000, 001, 512, 343, 064, 125, 216, 027, 008, 729 SORTED BY 10’s digit: 000, 001, 008, 512, 216, 125, 027, 729, 343, 064 SORTED BY 100’s digit: 000, 001, 008, 027, 064, 125, 216, 343, 512, 729 Figure 7.22 Radix sort trace

Chapter 7 Sorting /* * Radix sort an array of Strings * Assume all are all ASCII * Assume all have same length */ public static void radixSortA( String [ ] arr, int stringLen ) { final int BUCKETS = 256; ArrayList<String> [ ] buckets = new ArrayList<>[ BUCKETS ]; for( int i = 0; i < BUCKETS; i++ ) buckets[ i ] = new ArrayList<>( ); for( int pos = stringLen - 1; pos >= 0; pos-- ) { for( String s : arr ) buckets[ s.charAt( pos ) ].add( s ); int idx = 0; for( ArrayList<String> thisBucket : buckets ) { for( String s : thisBucket ) arr[ idx++ ] = s; thisBucket.clear( ); } } } Figure 7.23 Simple implementation of radix sort for strings, using an ArrayList of buckets time. The most straightforward way of doing this is shown in Figure 7.23. In our code, we assume that all characters are ASCII, residing in the ﬁrst 256 positions of the Unicode character set. In each pass, we add an item to the appropriate bucket, and then after all the buckets are populated, we step through the buckets dumping everything back to the array. Notice that when a bucket is populated and emptied in the next pass, the order from the current pass is preserved. Counting radix sort is an alternative implementation of radix sort that avoids using ArrayLists. Instead, we maintain a count of how many items would go in each bucket; this information can go into an array count, so that count[k] is the number of items that are in bucket k. Then we can use another array offset, so that offset[k] represents the number of items whose value is strictly smaller than k. Then when we see a value k for the ﬁrst time in the ﬁnal scan, offset[k] tells us a valid array spot where it can be written to (but we have to use a temporary array for the write), and after that is done, offset[k]

7.11 Linear-Time Sorts: Bucket Sort and Radix Sort /* * Counting radix sort an array of Strings * Assume all are all ASCII * Assume all have same length */ public static void countingRadixSort( String [ ] arr, int stringLen ) { final int BUCKETS = 256; int N = arr.length; String [ ] buffer = new String [ N ]; String [ ] in = arr; String [ ] out = buffer; for( int pos = stringLen - 1; pos >= 0; pos-- ) { int [ ] count = new int [ BUCKETS + 1 ]; for( int i = 0; i < N; i++ ) count[ in[ i ].charAt( pos ) + 1 ]++; for( int b = 1; b <= BUCKETS; b++ ) count[ b ] += count[ b - 1 ]; for( int i = 0; i < N; i++ ) out[ count[ in[ i ].charAt( pos ) ]++ ] = in[ i ]; // swap in and out roles String [ ] tmp = in; in = out; out = tmp; } // if odd number of passes, in is buffer, out is arr; so copy back if( stringLen % 2 == 1 ) for( int i = 0; i < arr.length; i++ ) out[ i ] = in[ i ]; } Figure 7.24 Counting radix sort for ﬁxed-length strings

Chapter 7 Sorting /* * Radix sort an array of Strings * Assume all are all ASCII * Assume all have length bounded by maxLen */ public static void radixSort( String [ ] arr, int maxLen ) { final int BUCKETS = 256; ArrayList<String> [ ] wordsByLength = new ArrayList<>[ maxLen + 1 ]; ArrayList<String> [ ] buckets = new ArrayList<>[ BUCKETS ]; for( int i = 0; i < wordsByLength.length; i++ ) wordsByLength[ i ] = new ArrayList<>( ); for( int i = 0; i < BUCKETS; i++ ) buckets[ i ] = new ArrayList<>( ); for( String s : arr ) wordsByLength[ s.length( ) ].add( s ); int idx = 0; for( ArrayList<String> wordList : wordsByLength ) for( String s : wordList ) arr[ idx++ ] = s; int startingIndex = arr.length; for( int pos = maxLen - 1; pos >= 0; pos-- ) { startingIndex -= wordsByLength[ pos + 1 ].size( ); for( int i = startingIndex; i < arr.length; i++ ) buckets[ arr[ i ].charAt( pos ) ].add( arr[ i ] ); idx = startingIndex; for( ArrayList<String> thisBucket : buckets ) { for( String s : thisBucket ) arr[ idx++ ] = s; thisBucket.clear( ); } } } Figure 7.25 Radix sort for variable length strings

7.12 External Sorting can be incremented. Counting radix sort thus avoids the need to keep lists. As a further optimization, we can avoid using offset, by reusing the count array. The modiﬁcation is that we initially have count[k+1] represent the number of items that are in bucket k. Then after that information is computed, we can scan the count array from the smallest to largest index, and increment count[k] by count[k-1]. It is easy to verify that after this scan, the count array stores exactly the same information that would have been stored in offset. Figure 7.24 shows an implementation of counting radix sort. Lines 18 to 27 implement the logic above, assuming that the items are stored in array in, and the result of a single pass is placed in array out. Initially, in represents arr and out represents the temporary array, buffer. After each pass, we switch the roles of in and out. If there are an even number of passes, then at the end, out is referencing arr, so the sort is complete. Otherwise, we have to copy from the buffer back into arr. Generally, counting radix sort is prefereable to using ArrayLists, but it can suffer from poor locality (out is ﬁlled in non-sequentially) and thus surprisingly, it is not always faster than using an array of ArrayLists. We can extend either version of radix sort to work with variable-length strings. The basic algorithm is to ﬁrst sort the strings by their length. Instead of looking at all the strings, we can then look only at strings that we know are long enough. Since the string lengths are small numbers, the initial sort by length can be done by—bucket sort! Figure 7.25 shows this implementation of radix sort, with ArrayLists. Here, the words are grouped into buckets by length at lines 19–20, and then placed back into the array at lines 22–25. Lines 32–33 look at only those strings that have a character at position pos, by making use of the variable startingIndex, which is maintained at lines 27 and 30. Except for that, lines 27–43 in Figure 7.25 are the same as lines 14–27 in Figure 7.23. The running time of this version of radix sort is linear in the total number of characters in all the strings (each character appears exactly once at line 33, and the statement at line 39 executes precisiely as many times as the line 33). Radix sort for strings will perform especially well when the characters in the string are drawn from a reasonably small alphabet, and when the strings either are relatively short or are very similar. Because the O(N log N) comparison-based sorting algorithms will generally look only at a small number of characters in each string comparison, once the average string length starts getting large, radix sort’s advantage is minimized or evaporates completely. 7.12 External Sorting So far, all the algorithms we have examined require that the input ﬁt into main memory. There are, however, applications where the input is much too large to ﬁt into memory. This section will discuss external sorting algorithms, which are designed to handle very large inputs.

Chapter 7 Sorting 7.12.1 Why We Need New Algorithms Most of the internal sorting algorithms take advantage of the fact that memory is directly addressable. Shellsort compares elements a[i] and a[i-hk] in one time unit. Heapsort compares elements a[i] and a[i * 2+1] in one time unit. Quicksort, with median-of-three partitioning, requires comparing a[left], a[center], and a[right] in a constant number of time units. If the input is on a tape, then all these operations lose their efﬁciency, since elements on a tape can only be accessed sequentially. Even if the data is on a disk, there is still a practical loss of efﬁciency because of the delay required to spin the disk and move the disk head. To see how slow external accesses really are, create a random ﬁle that is large, but not too big to ﬁt in main memory. Read the ﬁle in and sort it using an efﬁcient algorithm. The time it takes to read the input is certain to be signiﬁcant compared to the time to sort the input, even though sorting is an O(N log N) operation and reading the input is only O(N). 7.12.2 Model for External Sorting The wide variety of mass storage devices makes external sorting much more device dependent than internal sorting. The algorithms that we will consider work on tapes, which are probably the most restrictive storage medium. Since access to an element on tape is done by winding the tape to the correct location, tapes can be efﬁciently accessed only in sequential order (in either direction). We will assume that we have at least three tape drives to perform the sorting. We need two drives to do an efﬁcient sort; the third drive simpliﬁes matters. If only one tape drive is present, then we are in trouble: Any algorithm will require  (N2) tape accesses. 7.12.3 The Simple Algorithm The basic external sorting algorithm uses the merging algorithm from mergesort. Suppose we have four tapes, Ta1, Ta2, Tb1, Tb2, which are two input and two output tapes. Depending on the point in the algorithm, the a and b tapes are either input tapes or output tapes. Suppose the data are initially on Ta1. Suppose further that the internal memory can hold (and sort) M records at a time. A natural ﬁrst step is to read M records at a time from the input tape, sort the records internally, and then write the sorted records alternately to Tb1 and Tb2. We will call each set of sorted records a run. When this is done, we rewind all the tapes. Suppose we have the same input as our example for Shellsort. Ta1 Ta2 Tb1 Tb2 If M = 3, then after the runs are constructed, the tapes will contain the data indicated in the following ﬁgure.

7.12 External Sorting Ta1 Ta2 Tb1 Tb2 Now Tb1 and Tb2 contain a group of runs. We take the ﬁrst run from each tape and merge them, writing the result, which is a run twice as long, onto Ta1. Recall that merging two sorted lists is simple; we need almost no memory, since the merge is performed as Tb1 and Tb2 advance. Then we take the next run from each tape, merge these, and write the result to Ta2. We continue this process, alternating between Ta1 and Ta2, until either Tb1 or Tb2 is empty. At this point either both are empty or there is one run left. In the latter case, we copy this run to the appropriate tape. We rewind all four tapes and repeat the same steps, this time using the a tapes as input and the b tapes as output. This will give runs of 4M. We continue the process until we get one run of length N. This algorithm will require ⌈log(N/M)⌉passes, plus the initial run-constructing pass. For instance, if we have 10 million records of 128 bytes each, and four megabytes of internal memory, then the ﬁrst pass will create 320 runs. We would then need nine more passes to complete the sort. Our example requires ⌈log 13/3⌉= 3 more passes, which are shown in the following ﬁgures. Ta1 Ta2 Tb1 Tb2 Ta1 Ta2 Tb1 Tb2 Ta1 Ta2 Tb1 Tb2 7.12.4 Multiway Merge If we have extra tapes, then we can expect to reduce the number of passes required to sort our input. We do this by extending the basic (two-way) merge to a k-way merge. Merging two runs is done by winding each input tape to the beginning of each run. Then the smaller element is found, placed on an output tape, and the appropriate input

Chapter 7 Sorting tape is advanced. If there are k input tapes, this strategy works the same way, the only difference being that it is slightly more complicated to ﬁnd the smallest of the k elements. We can ﬁnd the smallest of these elements by using a priority queue. To obtain the next element to write on the output tape, we perform a deleteMin operation. The appropriate input tape is advanced, and if the run on the input tape is not yet completed, we insert the new element into the priority queue. Using the same example as before, we distribute the input onto the three tapes. Ta1 Ta2 Ta3 Tb1 Tb2 Tb3 We then need two more passes of three-way merging to complete the sort. Ta1 Ta2 Ta3 Tb1 Tb2 Tb3 Ta1 Ta2 Ta3 Tb1 Tb2 Tb3 After the initial run construction phase, the number of passes required using k-way merging is ⌈logk (N/M)⌉, because the runs get k times as large in each pass. For the example above, the formula is veriﬁed, since ⌈log3(13/3)⌉= 2. If we have 10 tapes, then k = 5, and our large example from the previous section would require ⌈log5 320⌉= 4 passes. 7.12.5 Polyphase Merge The k-way merging strategy developed in the last section requires the use of 2k tapes. This could be prohibitive for some applications. It is possible to get by with only k + 1 tapes. As an example, we will show how to perform two-way merging using only three tapes.

7.12 External Sorting Suppose we have three tapes, T1, T2, and T3, and an input ﬁle on T1 that will produce 34 runs. One option is to put 17 runs on each of T2 and T3. We could then merge this result onto T1, obtaining one tape with 17 runs. The problem is that since all the runs are on one tape, we must now put some of these runs on T2 to perform another merge. The logical way to do this is to copy the ﬁrst eight runs from T1 onto T2 and then perform the merge. This has the effect of adding an extra half pass for every pass we do. An alternative method is to split the original 34 runs unevenly. Suppose we put 21 runs on T2 and 13 runs on T3. We would then merge 13 runs onto T1 before T3 was empty. At this point, we could rewind T1 and T3 and merge T1, with 13 runs, and T2, which has 8 runs, onto T3. We could then merge 8 runs until T2 was empty, which would leave 5 runs left on T1 and 8 runs on T3. We could then merge T1 and T3, and so on. The following table shows the number of runs on each tape after each pass. Run After After After After After After After Const. T3 + T2 T1 + T2 T1 + T3 T2 + T3 T1 + T2 T1 + T3 T2 + T3 T1 T2 T3 The original distribution of runs makes a great deal of difference. For instance, if 22 runs are placed on T2, with 12 on T3, then after the ﬁrst merge, we obtain 12 runs on T3 and 10 runs on T2. After another merge, there are 10 runs on T1 and 2 runs on T3. At this point the going gets slow, because we can only merge two sets of runs before T3 is exhausted. Then T1 has 8 runs and T2 has 2 runs. Again, we can only merge two sets of runs, obtaining T1 with 6 runs and T3 with 2 runs. After three more passes, T2 has two runs and the other tapes are empty. We must copy one run to another tape, and then we can ﬁnish the merge. It turns out that the ﬁrst distribution we gave is optimal. If the number of runs is a Fibonacci number FN, then the best way to distribute them is to split them into two Fibonacci numbers FN−1 and FN−2. Otherwise, it is necessary to pad the tape with dummy runs in order to get the number of runs up to a Fibonacci number. We leave the details of how to place the initial set of runs on the tapes as an exercise. We can extend this to a k-way merge, in which case we need kth order Fibonacci numbers for the distribution, where the kth order Fibonacci number is deﬁned as F(k)(N) = F(k)(N −1) + F(k)(N −2) + · · · + F(k)(N −k), with the appropriate initial conditions F(k)(N) = 0, 0 ≤N ≤k −2, F(k)(k −1) = 1. 7.12.6 Replacement Selection The last item we will consider is construction of the runs. The strategy we have used so far is the simplest possible: We read as many records as possible and sort them, writing the result to some tape. This seems like the best approach possible, until one realizes that as soon as the ﬁrst record is written to an output tape, the memory it used becomes available

Chapter 7 Sorting for another record. If the next record on the input tape is larger than the record we have just output, then it can be included in the run. Using this observation, we can give an algorithm for producing runs. This technique is commonly referred to as replacement selection. Initially, M records are read into memory and placed in a priority queue. We perform a deleteMin, writing the smallest (valued) record to the output tape. We read the next record from the input tape. If it is larger than the record we have just written, we can add it to the priority queue. Otherwise, it cannot go into the current run. Since the priority queue is smaller by one element, we can store this new element in the dead space of the priority queue until the run is completed and use the element for the next run. Storing an element in the dead space is similar to what is done in heapsort. We continue doing this until the size of the priority queue is zero, at which point the run is over. We start a new run by building a new priority queue, using all the elements in the dead space. Figure 7.26 shows the run construction for the small example we have been using, with M = 3. Dead elements are indicated by an asterisk. In this example, replacement selection produces only three runs, compared with the ﬁve runs obtained by sorting. Because of this, a three-way merge ﬁnishes in one pass instead of two. If the input is randomly distributed, replacement selection can be shown to produce runs of average length 2M. For our large example, we would expect 160 runs instead of 320 runs, so a ﬁve-way merge would require four passes. In this case, we have not saved a pass, although we might if we get lucky and have 125 runs or less. Since external sorts take so long, every pass saved can make a signiﬁcant difference in the running time. 3 Elements in Heap Array Output Next Element Read h[1] h[2] h[3] Run 1 12* 12* 35* 35* 12* 17* 17* 35* 12* End of Run Rebuild Heap Run 2 15* 15* End of Tape 15* 15* End of Run Rebuild Heap Run 3 Figure 7.26 Example of run construction

As we have seen, it is possible for replacement selection to do no better than the standard algorithm. However, the input is frequently sorted or nearly sorted to start with, in which case replacement selection produces only a few very long runs. This kind of input is common for external sorts and makes replacement selection extremely valuable.

C H A P T E R 8 The Disjoint Set Class In this chapter, we describe an efﬁcient data structure to solve the equivalence problem. The data structure is simple to implement. Each routine requires only a few lines of code, and a simple array can be used. The implementation is also extremely fast, requiring constant average time per operation. This data structure is also very interesting from a theoretical point of view, because its analysis is extremely difﬁcult; the functional form of the worst case is unlike any we have yet seen. For the disjoint set data structure, we will r Show how it can be implemented with minimal coding effort. r Greatly increase its speed, using just two simple observations. r Analyze the running time of a fast implementation. r See a simple application. 8.1 Equivalence Relations A relation R is deﬁned on a set S if for every pair of elements (a, b), a, b ∈S, a R b is either true or false. If a R b is true, then we say that a is related to b. An equivalence relation is a relation R that satisﬁes three properties: 1. (Reﬂexive) a R a, for all a ∈S. 2. (Symmetric) a R b if and only if b R a. 3. (Transitive) a R b and b R c implies that a R c. We will consider several examples. The ≤relationship is not an equivalence relationship. Although it is reﬂexive, since a ≤a, and transitive, since a ≤b and b ≤c implies a ≤c, it is not symmetric, since a ≤b does not imply b ≤a. Electrical connectivity, where all connections are by metal wires, is an equivalence relation. The relation is clearly reﬂexive, as any component is connected to itself. If a is electrically connected to b, then b must be electrically connected to a, so the relation is symmetric. Finally, if a is connected to b and b is connected to c, then a is connected to c. Thus electrical connectivity is an equivalence relation. Two cities are related if they are in the same country. It is easily veriﬁed that this is an equivalence relation. Suppose town a is related to b if it is possible to travel from a to b by taking roads. This relation is an equivalence relation if all the roads are two-way.

Chapter 8 The Disjoint Set Class 8.2 The Dynamic Equivalence Problem Given an equivalence relation ∼, the natural problem is to decide, for any a and b, if a ∼b. If the relation is stored as a two-dimensional array of Boolean variables, then, of course, this can be done in constant time. The problem is that the relation is usually not explicitly, but rather implicitly, deﬁned. As an example, suppose the equivalence relation is deﬁned over the ﬁve-element set {a1, a2, a3, a4, a5}. Then there are 25 pairs of elements, each of which is either related or not. However, the information a1 ∼a2, a3 ∼a4, a5 ∼a1, a4 ∼a2 implies that all pairs are related. We would like to be able to infer this quickly. The equivalence class of an element a ∈S is the subset of S that contains all the elements that are related to a. Notice that the equivalence classes form a partition of S: Every member of S appears in exactly one equivalence class. To decide if a ∼b, we need only to check whether a and b are in the same equivalence class. This provides our strategy to solve the equivalence problem. The input is initially a collection of N sets, each with one element. This initial representation is that all relations (except reﬂexive relations) are false. Each set has a different element, so that Si ∩Sj = Ø; this makes the sets disjoint. There are two permissible operations. The ﬁrst is find, which returns the name of the set (that is, the equivalence class) containing a given element. The second operation adds relations. If we want to add the relation a ∼b, then we ﬁrst see if a and b are already related. This is done by performing finds on both a and b and checking whether they are in the same equivalence class. If they are not, then we apply union. This operation merges the two equivalence classes containing a and b into a new equivalence class. From a set point of view, the result of ∪is to create a new set Sk = Si ∪Sj, destroying the originals and preserving the disjointness of all the sets. The algorithm to do this is frequently known as the disjoint set union/ﬁnd algorithm for this reason. This algorithm is dynamic because, during the course of the algorithm, the sets can change via the union operation. The algorithm must also operate online: When a find is performed, it must give an answer before continuing. Another possibility would be an off-line algorithm. Such an algorithm would be allowed to see the entire sequence of unions and finds. The answer it provides for each find must still be consistent with all the unions that were performed up until the find, but the algorithm can give all its answers after it has seen all the questions. The difference is similar to taking a written exam (which is generally off-line—you only have to give the answers before time expires), and an oral exam (which is online, because you must answer the current question before proceeding to the next question). Notice that we do not perform any operations comparing the relative values of elements but merely require knowledge of their location. For this reason, we can assume that all the elements have been numbered sequentially from 0 to N −1 and that the numbering can be determined easily by some hashing scheme. Thus, initially we have Si = {i} for i = 0 through N −1.1 1 This reﬂects the fact that array indices start at 0.

8.3 Basic Data Structure Our second observation is that the name of the set returned by find is actually fairly arbitrary. All that really matters is that find(a)==find(b) is true if and only if a and b are in the same set. These operations are important in many graph theory problems and also in compilers that process equivalence (or type) declarations. We will see an application later. There are two strategies to solve this problem. One ensures that the find instruction can be executed in constant worst-case time, and the other ensures that the union instruction can be executed in constant worst-case time. It has been shown that both cannot be done simultaneously in constant worst-case time. We will now brieﬂy discuss the ﬁrst approach. For the find operation to be fast, we could maintain, in an array, the name of the equivalence class for each element. Then find is just a simple O(1) lookup. Suppose we want to perform union(a,b). Suppose that a is in equivalence class i and b is in equivalence class j. Then we scan down the array, changing all i’s to j. Unfortunately, this scan takes (N). Thus, a sequence of N −1 unions (the maximum, since then everything is in one set) would take (N2) time. If there are (N2) find operations, this performance is ﬁne, since the total running time would then amount to O(1) for each union or find operation over the course of the algorithm. If there are fewer finds, this bound is not acceptable. One idea is to keep all the elements that are in the same equivalence class in a linked list. This saves time when updating, because we do not have to search through the entire array. This by itself does not reduce the asymptotic running time, because it is still possible to perform (N2) equivalence class updates over the course of the algorithm. If we also keep track of the size of each equivalence class, and when performing unions we change the name of the smaller equivalence class to the larger, then the total time spent for N −1 merges is O(N log N). The reason for this is that each element can have its equivalence class changed at most log N times, since every time its class is changed, its new equivalence class is at least twice as large as its old. Using this strategy, any sequence of M finds and up to N −1 unions takes at most O(M + N log N) time. In the remainder of this chapter, we will examine a solution to the union/ﬁnd problem that makes unions easy but finds hard. Even so, the running time for any sequence of at most M finds and up to N −1 unions will be only a little more than O(M + N). 8.3 Basic Data Structure Recall that the problem does not require that a find operation return any speciﬁc name, just that finds on two elements return the same answer if and only if they are in the same set. One idea might be to use a tree to represent each set, since each element in a tree has the same root. Thus, the root can be used to name the set. We will represent each set by a tree. (Recall that a collection of trees is known as a forest.) Initially, each set contains one element. The trees we will use are not necessarily binary trees, but their representation is easy, because the only information we will need is a parent link. The name of a set is given by the node at the root. Since only the name of the parent is required, we can assume that this tree is stored implicitly in an array: Each entry s[i] in the array represents the parent

Chapter 8 The Disjoint Set Class Figure 8.1 Eight elements, initially in different sets Figure 8.2 After union(4,5) Figure 8.3 After union(6,7) of element i. If i is a root, then s[i] = −1. In the forest in Figure 8.1, s[i] = −1 for 0 ≤i < 8. As with binary heaps, we will draw the trees explicitly, with the understanding that an array is being used. Figure 8.1 shows the explicit representation. We will draw the root’s parent link vertically for convenience. To perform a union of two sets, we merge the two trees by making the parent link of one tree’s root link to the root node of the other tree. It should be clear that this operation takes constant time. Figures 8.2, 8.3, and 8.4 represent the forest after each of union(4,5), union(6,7), union(4,6), where we have adopted the convention that the new root after the union(x,y) is x. The implicit representation of the last forest is shown in Figure 8.5. A find(x) on element x is performed by returning the root of the tree containing x. The time to perform this operation is proportional to the depth of the node representing x, assuming, of course, that we can ﬁnd the node representing x in constant time. Using the strategy above, it is possible to create a tree of depth N −1, so the worst-case running time of a find is (N). Typically, the running time is computed for a sequence of M intermixed instructions. In this case, M consecutive operations could take (MN) time in the worst case.

8.3 Basic Data Structure Figure 8.4 After union(4,6) –1 –1 –1 –1 –1 Figure 8.5 Implicit representation of previous tree public class DisjSets { public DisjSets( int numElements ) { /* Figure 8.7 */ } public void union( int root1, int root2 ) { /* Figures 8.8 and 8.14 */ } public int find( int x ) { /* Figures 8.9 and 8.16 */ } private int [ ] s; } Figure 8.6 Disjoint set class skeleton The code in Figures 8.6 through 8.9 represents an implementation of the basic algorithm, assuming that error checks have already been performed. In our routine, unions are performed on the roots of the trees. Sometimes the operation is performed by passing any two elements, and having the union perform two finds to determine the roots. The average-case analysis is quite hard to do. The least of the problems is that the answer depends on how to deﬁne average (with respect to the union operation). For instance, in the forest in Figure 8.4, we could say that since there are ﬁve trees, there are 5 · 4 = 20 equally likely results of the next union (as any two different trees can be unioned). Of course, the implication of this model is that there is only a 2 5 chance that the next union will involve the large tree. Another model might say that all unions between any two

Chapter 8 The Disjoint Set Class /** * Construct the disjoint sets object. * @param numElements the initial number of disjoint sets. */ public DisjSets( int numElements ) { s = new int [ numElements ]; for( int i = 0; i < s.length; i++ ) s[ i ] = -1; } Figure 8.7 Disjoint set initialization routine /** * Union two disjoint sets. * For simplicity, we assume root1 and root2 are distinct * and represent set names. * @param root1 the root of set 1. * @param root2 the root of set 2. */ public void union( int root1, int root2 ) { s[ root2 ] = root1; } Figure 8.8 union (not the best way) /** * Perform a find. * Error checks omitted again for simplicity. * @param x the element being searched for. * @return the set containing x. */ public int find( int x ) { if( s[ x ] < 0 ) return x; else return find( s[ x ] ); } Figure 8.9 A simple disjoint set find algorithm

8.4 Smart Union Algorithms elements in different trees are equally likely, so a larger tree is more likely to be involved in the next union than a smaller tree. In the example above, there is an 8 11 chance that the large tree is involved in the next union, since (ignoring symmetries) there are 6 ways in which to merge two elements in {0, 1, 2, 3}, and 16 ways to merge an element in {4, 5, 6, 7} with an element in {0, 1, 2, 3}. There are still more models and no general agreement on which is the best. The average running time depends on the model; (M), (M log N), and (MN) bounds have actually been shown for three different models, although the latter bound is thought to be more realistic. Quadratic running time for a sequence of operations is generally unacceptable. Fortunately, there are several ways of easily ensuring that this running time does not occur. 8.4 Smart Union Algorithms The unions above were performed rather arbitrarily, by making the second tree a subtree of the ﬁrst. A simple improvement is always to make the smaller tree a subtree of the larger, breaking ties by any method; we call this approach union-by-size. The three unions in the preceding example were all ties, and so we can consider that they were performed by size. If the next operation were union(3,4), then the forest in Figure 8.10 would form. Had the size heuristic not been used, a deeper tree would have been formed (Figure 8.11). We can prove that if unions are done by size, the depth of any node is never more than log N. To see this, note that a node is initially at depth 0. When its depth increases as a result of a union, it is placed in a tree that is at least twice as large as before. Thus, its depth can be increased at most log N times. (We used this argument in the quick-ﬁnd algorithm at the end of Section 8.2.) This implies that the running time for a find operation is O(log N), and a sequence of M operations takes O(M log N). The tree in Figure 8.12 shows the worst tree possible after 16 unions and is obtained if all unions are between equal-sized trees (the worst-case trees are binomial trees, discussed in Chapter 6). To implement this strategy, we need to keep track of the size of each tree. Since we are really just using an array, we can have the array entry of each root contain the negative of Figure 8.10 Result of union-by-size

Chapter 8 The Disjoint Set Class Figure 8.11 Result of an arbitrary union Figure 8.12 Worst-case tree for N = 16 the size of its tree. Thus, initially the array representation of the tree is all −1’s. When a union is performed, check the sizes; the new size is the sum of the old. Thus, union-by-size is not at all difﬁcult to implement and requires no extra space. It is also fast, on average. For virtually all reasonable models, it has been shown that a sequence of M operations requires O(M) average time if union-by-size is used. This is because when random unions are performed, generally very small (usually one-element) sets are merged with large sets throughout the algorithm. An alternative implementation, which also guarantees that all the trees will have depth at most O(log N), is union-by-height. We keep track of the height, instead of the size, of each tree and perform unions by making the shallow tree a subtree of the deeper tree. This is an easy algorithm, since the height of a tree increases only when two equally deep trees are joined (and then the height goes up by one). Thus, union-by-height is a trivial modiﬁcation of union-by-size. Since heights of zero would not be negative, we actually store the negative of height, minus an additional 1. Initially, all entries are −1. Figure 8.13 show a forest and its implicit representation for both union-by-size and union-by-height. The code in Figure 8.14 implements union-by-height.

8.4 Smart Union Algorithms –1 –1 –1 –3 –5 –1 –1 –1 - Figure 8.13 Forest with implicit representation for union-by-size and union-by-height /** * Union two disjoint sets using the height heuristic. * For simplicity, we assume root1 and root2 are distinct * and represent set names. * @param root1 the root of set 1. * @param root2 the root of set 2. */ public void union( int root1, int root2 ) { if( s[ root2 ] < s[ root1 ] ) // root2 is deeper s[ root1 ] = root2; // Make root2 new root else { if( s[ root1 ] == s[ root2 ] ) s[ root1 ]--; // Update height if same s[ root2 ] = root1; // Make root1 new root } } Figure 8.14 Code for union-by-height (rank)

Chapter 8 The Disjoint Set Class 8.5 Path Compression The union/ﬁnd algorithm, as described so far, is quite acceptable for most cases. It is very simple and linear on average for a sequence of M instructions (under all models). However, the worst case of O(M log N) can occur fairly easily and naturally. For instance, if we put all the sets on a queue and repeatedly dequeue the ﬁrst two sets and enqueue the union, the worst case occurs. If there are many more finds than unions, this running time is worse than that of the quick-ﬁnd algorithm. Moreover, it should be clear that there are probably no more improvements possible for the union algorithm. This is based on the observation that any method to perform the unions will yield the same worst-case trees, since it must break ties arbitrarily. Therefore, the only way to speed the algorithm up, without reworking the data structure entirely, is to do something clever on the find operation. The clever operation is known as path compression. Path compression is performed during a find operation and is independent of the strategy used to perform unions. Suppose the operation is find(x). Then the effect of path compression is that every node on the path from x to the root has its parent changed to the root. Figure 8.15 shows the effect of path compression after find(14) on the generic worst tree of Figure 8.12. The effect of path compression is that with an extra two link changes, nodes 12 and 13 are now one position closer to the root and nodes 14 and 15 are now two positions closer. Thus, the fast future accesses on these nodes will pay (we hope) for the extra work to do the path compression. As the code in Figure 8.16 shows, path compression is a trivial change to the basic find algorithm. The only change to the find routine is that s[x] is made equal to the value returned by find; thus after the root of the set is found recursively, x’s parent link references it. This occurs recursively to every node on the path to the root, so this implements path compression. When unions are done arbitrarily, path compression is a good idea, because there is an abundance of deep nodes and these are brought near the root by path compression. It has been proven that when path compression is done in this case, a sequence of M Figure 8.15 An example of path compression

8.6 Worst Case for Union-by-Rank and Path Compression /** * Perform a find with path compression. * Error checks omitted again for simplicity. * @param x the element being searched for. * @return the set containing x. */ public int find( int x ) { if( s[ x ] < 0 ) return x; else return s[ x ] = find( s[ x ] ); } Figure 8.16 Code for the disjoint set find with path compression operations requires at most O(M log N) time. It is still an open problem to determine what the average-case behavior is in this situation. Path compression is perfectly compatible with union-by-size, and thus both routines can be implemented at the same time. Since doing union-by-size by itself is expected to execute a sequence of M operations in linear time, it is not clear that the extra pass involved in path compression is worthwhile on average. Indeed, this problem is still open. However, as we shall see later, the combination of path compression and a smart union rule guarantees a very efﬁcient algorithm in all cases. Path compression is not entirely compatible with union-by-height, because path compression can change the heights of the trees. It is not at all clear how to recompute them efﬁciently. The answer is do not!! Then the heights stored for each tree become estimated heights (sometimes known as ranks), but it turns out that union-by-rank (which is what this has now become) is just as efﬁcient in theory as union-by-size. Furthermore, heights are updated less often than sizes. As with union-by-size, it is not clear whether path compression is worthwhile on average. What we will show in the next section is that with either union heuristic, path compression signiﬁcantly reduces the worst-case running time. 8.6 Worst Case for Union-by-Rank and Path Compression When both heuristics are used, the algorithm is almost linear in the worst case. Speciﬁcally, the time required in the worst case is (Mα(M, N)) (provided M ≥N), where α(M, N) is an incredibly slowly growing function that for all intents and purposes is at most 5 for any problem instance. However, α(M, N) is not a constant, so the running time is not linear. In the remainder of this section, we ﬁrst look at some very slow-growing functions, and then in Sections 8.6.2 to 8.6.4, we establish a bound on the worst-case for a sequence of at

Chapter 8 The Disjoint Set Class most N −1 unions, and M ﬁnd operations in an N-element universe in which union is by rank and ﬁnds use path compression. The same bound holds if union-by-rank is replaced with union-by-size. 8.6.1 Slowly Growing Functions Consider the recurrence: T(N) =  N ≤1 T(⌊f(N)⌋) + 1 N > 1 (8.1) In this equation, T(N) represents the number of times, starting at N, that we must iteratively apply f(N) until we reach 1 (or less). We assume that f(N) is a nicely deﬁned function that reduces N. Call the solution to the equation f∗(N). We have already encountered this recurrence when analyzing binary search. There, f(N) = N/2; each step halves N. We know that this can happen at most log N times until N reaches 1; hence we have f∗(N) = log N (we ignore low-order terms, etc.). Observe that in this case, f∗(N) is much less than f(N). Figure 8.17 shows the solution for T(N), for various f(N). In our case, we are most interested in f(N) = log N. The solution T(N) = log∗N is known as the iterated logarithm. The iterated logarithm, which represents the number of times the logarithm needs to be iteratively applied until we reach one, is a very slowly growing function. Observe that log∗2 = 1, log∗4 = 2, log∗16 = 3, log∗65536 = 4, and log∗265536 = 5. But keep in mind that 265536 is a 20,000-digit number. So while log∗N is a growing function, for all intents and purposes, it is at most 5. But we can still produce even more slowly growing functions. For instance, if f(N) = log∗N, then T(N) = log∗∗N. In fact, we can add stars at will to produce functions that grow slower and slower. f(N) f∗(N) N−1 N−1 N−2 N/2 N−c N/c N/2 log N N/c logc N √ N log log N log N log∗N log∗N log∗∗N log∗∗N log∗∗∗N Figure 8.17 Different values of the iterated function

8.6 Worst Case for Union-by-Rank and Path Compression 8.6.2 An Analysis By Recursive Decomposition We now establish a tight bound on the running time of a sequence of M = (N) union/ﬁnd operations. The unions and finds may occur in any order, but unions are done by rank and finds are done with path compression. We begin by establishing two lemmas concerning the properties of the ranks. Figure 8.18 gives a visual picture of both lemmas. Lemma 8.1. When executing a sequence of union instructions, a node of rank r > 0 must have at least one child of rank 0, 1, . . . , r −1. Proof. By induction. The basis r = 1 is clearly true. When a node grows from rank r −1 to rank r, it obtains a child of rank r −1. By the inductive hypothesis, it already has children of ranks 0, 1, . . . , r −2, thus establishing the lemma. The next lemma seems somewhat obvious but is used implicitly in the analysis. Lemma 8.2. At any point in the union/ﬁnd algorithm, the ranks of the nodes on a path from the leaf to a root increase monotonically. Proof. The lemma is obvious if there is no path compression. If, after path compression, some node v is a descendant of w, then clearly v must have been a descendant of w when only unions were considered. Hence the rank of v is less than the rank of w. Suppose we have two algorithms A and B. Algorithm A works and computes all the answers correctly, but algorithm B does not compute correctly, or even produce useful answers. Suppose, however, that every step in algorithm A can be mapped to an equivalent step in algorithm B. Then it is easy to see that the running time for algorithm B describes the running time for algorithm A, exactly. Figure 8.18 A large disjoint set tree (numbers below nodes are ranks)

Chapter 8 The Disjoint Set Class We can use this idea to analyze the running time of the disjoint sets data structure. We will describe an algorithm B whose running time is exactly the same as the disjoint sets structure, and then algorithm C, whose running time is exactly the same as algorithm B. Thus any bound for algorithm C will be a bound for the disjoint sets data structure. Partial Path Compression Algorithm A is our standard sequence of union-by-rank and ﬁnd with path compression operations. We design an algorithm B that will perform the exact same sequence of path compression operations as algorithm A. In algorithm B, we perform all the unions prior to any ﬁnd. Then each ﬁnd operation in algorithm A is replaced by a partial ﬁnd operation in algorithm B. A partial ﬁnd operation speciﬁes the search item and the node up to which the path compression is performed. The node that will be used is the node that would have been the root at the time the matching ﬁnd was performed in algorithm A. Figure 8.19 shows that algorithm A and algorithm B will get equivalent trees (forests) at the end, and it is easy to see that the exact same amount of parent changes are performed by algorithm A’s ﬁnds, compared to algorithm B’s partial ﬁnds. But algorithm B should be simpler to analyze, since we have removed the mixing of unions and ﬁnds from the equation. The basic quantity to analyze is the number of parent changes that can occur in any sequence of partial ﬁnds, since all but the top two nodes in any ﬁnd with path compression will obtain new parents. A Recursive Decomposition What we would like to do next is to divide each tree into two halves: a top half and a bottom half. We would then like to ensure that the number of partial ﬁnd operations in the top half plus the number of partial ﬁnd operations in the bottom half is exactly the same as the total number of partial ﬁnd operations. We would then like to write a formula for the total path compression cost in the tree in terms of the path compression cost in the top half plus the path compression cost in the bottom half. Without specifying how we g g c h c c e c g e g c a e g c b d a f h e b d a f g e f h e b d a d a f h b f h b d f h b Find (c) ⇒ Union (b, f) ⇒ Union (b, f) ⇒ Partial find (c, b) ⇒ d a Figure 8.19 Sequences of union and ﬁnd operations replaced with equivalent cost of union and partial ﬁnd operations

8.6 Worst Case for Union-by-Rank and Path Compression TOP BOTTOM x y Figure 8.20 Recursive decomposition, Case 1: Partial ﬁnd is entirely in bottom TOP BOTTOM x y Figure 8.21 Recursive decomposition, Case 2: Partial ﬁnd is entirely in top decide which nodes are in the top half, and which nodes are in the bottom half, we can look at Figures 8.20, 8.21, and 8.22, to see how most of what we want to do can work immediately. In Figure 8.20, the partial ﬁnd resides entirely in the bottom half. Thus one partial ﬁnd in the bottom half corresponds to one original partial ﬁnd, and the charges can be recursively assigned to the bottom half. In Figure 8.21, the partial ﬁnd resides entirely in the top half. Thus one partial ﬁnd in the top half corresponds to one original partial ﬁnd, and the charges can be recursively assigned to the top half. However, we run into lots of trouble when we reach Figure 8.22. Here x is in the bottom half, and y is in the top half. The path compression would require that all nodes from x to y’s child acquire y as its parent. For nodes in the top half, that is no problem, but for nodes in the bottom half this is a deal breaker: Any recursive charges to the bottom

Chapter 8 The Disjoint Set Class TOP BOTTOM y x Figure 8.22 Recursive decomposition, Case 3: Partial ﬁnd goes from bottom to top TOP BOTTOM x z y Figure 8.23 Recursive decomposition, Case 3: Path compression can be performed on the top nodes, but the bottom nodes must get new parents; the parents cannot be top parents, and they cannot be other bottom nodes have to keep everything in the bottom. So as Figure 8.23 shows, we can perform the path compression on the top, but while some nodes in the bottom will need new parents, it is not clear what to do, because the new parents for those bottom nodes cannot be top nodes, and the new parents cannot be other bottom nodes. The only option is to make a loop where these nodes’ parents are themselves and make sure these parent changes are correctly charged in our accounting. Although this is a new algorithm because it can no longer be used to generate an identical tree, we don’t need identical trees; we only need to be sure that each original partial ﬁnd can be mapped into a new partial ﬁnd operation, and that the charges are identical. Figure 8.24 shows what the new tree will look like, and so the big remaining issue is the accounting.

8.6 Worst Case for Union-by-Rank and Path Compression TOP BOTTOM x w z y Figure 8.24 Recursive decomposition, Case 3: The bottom node new parents are the nodes themselves Looking at Figure 8.24, we see that the path compression charges from x to y can be split into three parts. First there is the path compression from z (the ﬁrst top node on the upward path) to y. Clearly those charges are already accounted for recursively. Then there is the charge from the topmost-bottom node w to z. But that is only one unit, and there can be at most one of those per partial ﬁnd operation. In fact, we can do a little better: There can be at most one of those per partial ﬁnd operation on the top half. But how do we account for the parent changes on the path from x to w? One idea would be to argue that those changes would be exactly the same cost as if there were a partial ﬁnd from x to w. But there is a big problem with that argument: It converts an original partial ﬁnd into a partial ﬁnd on the top plus a partial ﬁnd on the bottom, which means the number of operations, M, would no longer be the same. Fortunately, there is a simpler argument: Since each node on the bottom can have its parent set to itself only once, the number of charges are limited by the number of nodes on the bottom, whose parents are also in the bottom (i.e. w is excluded). There is one important detail that we must verify. Can we get in trouble on a subsequent partial ﬁnd given that our reformulation detaches the nodes between x and w from the path to y? The answer is no. In the original partial ﬁnd, suppose any of the nodes between x and w are involved in a subsequent original partial ﬁnd. In that case, it will be with one of y’s ancestors, and when that happens, any of those nodes will be the topmost “bottom node” in our reformulation. Thus on the subsequent partial ﬁnd, the original partial ﬁnd’s parent change will have a corresponding one unit charge in our reformulation. We can now proceed with the analysis. Let M be the total number of original partial ﬁnd operations. Let Mt be the total number of partial ﬁnd operations performed exclusively on the top half, and let Mb be the total number of partial ﬁnd operations performed exclusively on the bottom half. Let N be the total number of nodes. Let Nt be the total number of tophalf nodes, and let Nb be the total number of bottom half nodes, and let Nnrb be the total number of non-root bottom nodes (i.e the number of bottom nodes whose parents are also bottom nodes prior to any partial ﬁnds).

Chapter 8 The Disjoint Set Class Lemma 8.3. M = Mt + Mb. Proof. In cases 1 and 3, each original partial ﬁnd operation is replaced by a partial ﬁnd on the top half, and in case 2, it is replaced by a partial ﬁnd on the bottom half. Thus each partial ﬁnd is replaced by exactly one partial ﬁnd operation on one of the halves. Our basic idea is that we are going to partition the nodes so that all nodes with rank s or lower are in the bottom, and the remaining nodes are in the top. The choice of s will be made later in the proof. The next lemma shows that we can provide a recursive formula for the number of parent changes, by splitting the charges into the top and bottom groups. One of the key ideas is that a recursive formula is written not only in terms of M and N, which would be obvious, but also in terms of the maximum rank in the group. Lemma 8.4. Let C(M, N, r) be the number of parent changes for a sequence of M ﬁnds with path compression on N items, whose maximum rank is r. Suppose we partition so that all nodes with rank at s or lower are in the bottom, and the remaining nodes are in the top. Assuming appropriate initial conditions, C(M, N, r) < C(Mt, Nt, r) + C(Mb, Nb, s) + Mt + Nnrb . Proof. The path compression that is performed in each of the three cases is covered by C(Mt, Nt, r) + C(Mb, Nb, s). Node w in case 3 is accounted for by Mt. Finally, all the other bottom nodes on the path are non-root nodes that can have their parent set to themselves at most once in the entire sequence of compressions. They are accounted for by Nnrb. If union-by-rank is used, then by Lemma 8.1, every top node has children of ranks 0, 1, . . ., s prior to the commencement of the partial ﬁnd operations. Each of those children are deﬁnitely root nodes in the bottom (their parent is a top node). So for each top node, s + 2 nodes (the s + 1 children plus the top node itself) are deﬁnitely not included in Nnrb. Thus, we can refomulate Lemma 8.4 as follows: Lemma 8.5. Let C(M, N, r) be the number of parent changes for a sequence of M ﬁnds with path compression on N items, whose maximum rank is r. Suppose we partition so that all nodes with rank at s or lower are in the bottom, and the remaining nodes are in the top. Assuming appropriate initial conditions, C(M, N, r) < C(Mt, Nt, r) + C(Mb, Nb, s) + Mt + N −(s + 2)Nt . Proof. Substitute Nnrb < N −(s + 2)Nt into Lemma 8.4.

8.6 Worst Case for Union-by-Rank and Path Compression If we look at Lemma 8.5, we see that C(M, N, r) is recursively deﬁned in terms of two smaller instances. Our basic goal at this point is to remove one of these instances, by providing a bound for it. What we would like to do is to remove C(Mt, Nt, r). Why? Because, if we do so, what is left is C(Mb, Nb, s). In that case, we have a recursive formula in which r is reduced to s. If s is small enough, we can make use of a variation of Equation 8.1, namely that the solution to T(N) =  N ≤1 T(⌊f(N)⌋) + M N > 1 (8.2) is O(M f∗(N)). So, let’s start with a simple bound for C(M, N, r): Theorem 8.1. C(M, N, r) < M + N log r. Proof. We start with Lemmas 8.5: C(M, N, r) < C(Mt, Nt, r) + C(Mb, Nb, s) + Mt + N −(s + 2)Nt (8.3) Observe that in the top half, there are only nodes of rank s+1, s+2, . . . , r, and thus no node can have its parent change more than (r−s−2) times. This yields a trivial bound of Nt(r−s−2) for C(Mt, Nt, r). Thus, C(M, N, r) < Nt(r −s −2) + C(Mb, Nb, s) + Mt + N −(s + 2)Nt (8.4) Combining terms, C(M, N, r) < Nt(r −2s −4) + C(Mb, Nb, s) + Mt + N (8.5) Select s = ⌊r/2⌋. Then r −2s −4 < 0, so C(M, N, r) < C(Mb, Nb, ⌊r/2⌋) + Mt + N (8.6) Equivalently, since according to Lemma 8.3, M = Mb+Mt (the proof falls apart without this), C(M, N, r) −M < C(Mb, Nb, ⌊r/2⌋) −Mb + N (8.7) Let D(M, N, r) = C(M, N, r) −M; then D(M, N, r) < D(Mb, Nb, ⌊r/2⌋) + N (8.8) which implies D(M, N, r) < N log r. This yields C(M, N, r) < M + N log r. Theorem 8.2. Any sequence of N −1 unions and M ﬁnds with path compression makes at most M + N log log N parent changes during the ﬁnds. Proof. The bound is immediate from Theorem 8.1 since r ≤log N.

Chapter 8 The Disjoint Set Class 8.6.3 An O( M log * N ) Bound The bound in Theorem 8.2 is pretty good, but with a little work, we can do even better. Recall, that a central idea of the recursive decomposition is choosing s to be as small as possible. But to do this, the other terms must also be small, and as s gets smaller, we would expect C(Mt, Nt, r) to get larger. But the bound for C(Mt, Nt, r) used a primitive estimate, and Theorem 8.1 itself can now be used to give a better estimate for this term. Since the C(Mt, Nt, r) estimate will now be lower, we will be able to use a lower s. Theorem 8.3. C(M, N, r) < 2M + N log∗r. Proof. From Lemma 8.5 we have, C(M, N, r) < C(Mt, Nt, r) + C(Mb, Nb, s) + Mt + N −(s + 2)Nt (8.9) and by Theorem 8.1, C(Mt, Nt, r) < Mt + Nt log r. Thus, C(M, N, r) < Mt + Nt log r + C(Mb, Nb, s) + Mt + N −(s + 2)Nt (8.10) Rearranging and combining terms yields C(M, N, r) < C(Mb, Nb, s) + 2Mt + N −(s −log r + 2)Nt (8.11) So choose s = ⌊log r⌋. Clearly, this choice implies that (s −log r + 2) > 0, and thus we obtain C(M, N, r) < C(Mb, Nb, ⌊log r⌋) + 2Mt + N (8.12) Rearranging as in Theorem 8.1, we obtain, C(M, N, r) −2M < C(Mb, Nb, ⌊log r⌋) −2Mb + N (8.13) This time, let D(M, N, r) = C(M, N, r) −2M; then D(M, N, r) < D(Mb, Nb, ⌊log r⌋) + N (8.14) which implies D(M, N, r) < N log∗r. This yields C(M, N, r) < 2M + N log∗r. 8.6.4 An O( M α(M, N) ) Bound Not surprisingly, we can now use Theorem 8.3 to improve Theorem 8.3: Theorem 8.4. C(M, N, r) < 3M + N log∗∗r. Proof. Following the steps in the proof of Theorem 8.3, we have

8.6 Worst Case for Union-by-Rank and Path Compression C(M, N, r) < C(Mt, Nt, r) + C(Mb, Nb, s) + Mt + N −(s + 2)Nt (8.15) and by Theorem 8.3, C(Mt, Nt, r) < 2Mt + Nt log∗r. Thus, C(M, N, r) < 2Mt + Nt log∗r + C(Mb, Nb, s) + Mt + N −(s + 2)Nt (8.16) Rearranging and combining terms yields C(M, N, r) < C(Mb, Nb, s) + 3Mt + N −(s −log∗r + 2)Nt (8.17) So choose s = log∗r to obtain C(M, N, r) < C(Mb, Nb, log∗r) + 3Mt + N (8.18) Rearranging as in Theorems 8.1 and 8.3, we obtain, C(M, N, r) −3M < C(Mb, Nb, log∗r) −3Mb + N (8.19) This time, let D(M, N, r) = C(M, N, r) −3M; then D(M, N, r) < D(Mb, Nb, log∗r) + N (8.20) which implies D(M, N, r) < N log∗∗r. This yields C(M, N, r) < 3M + N log∗∗r. Needless to say, we could continue this ad-inﬁnitim. Thus with a bit of math, we get a progression of bounds: C(M, N, r) < 2M + N log∗r C(M, N, r) < 3M + N log∗∗r C(M, N, r) < 4M + N log∗∗∗r C(M, N, r) < 5M + N log∗∗∗∗r C(M, N, r) < 6M + N log∗∗∗∗∗r Each of these bounds would seem to be better than the previous since, after all, the more ∗s the slower log∗∗...∗∗r grows. However, this ignores the fact that while log∗∗∗∗∗r is smaller than log∗∗∗∗r, the 6M term is NOT smaller than the 5M term. Thus what we would like to do is to optimize the number of ∗s that are used. Deﬁne α(M, N) to represent the optimal number of ∗s that will be used. Speciﬁcally, α(M, N) = min ⎧ ⎪⎨ ⎪⎩ i ≥1  log i times  !" ∗∗∗∗(log N) ≤(M/N) ⎫ ⎪⎬ ⎪⎭ Then, the running time of the union/ﬁnd algorithm can be bounded by O(Mα(M, N)). Theorem 8.5. Any sequence of N −1 unions and M ﬁnds with path compression makes at most (i + 1)M + N log i times  !" ∗∗∗∗(log N) parent changes during the ﬁnds.

Chapter 8 The Disjoint Set Class Proof. This follows from the above discussion, and the fact that r ≤log N. Theorem 8.6. Any sequence of N −1 unions and M ﬁnds with path compression makes at most Mα(M, N) + 2M parent changes during the ﬁnds. Proof. In Theorem 8.5, choose i to be α(M, N); thus we obtain a bound of (i+1)M+N(M/N), or Mα(M, N) + 2M. 8.7 An Application An example of the use of the union/ﬁnd data structure is the generation of mazes, such as the one shown in Figure 8.25. In Figure 8.25, the starting point is the top-left corner, and the ending point is the bottom-right corner. We can view the maze as a 50-by-88 rectangle of cells in which the top-left cell is connected to the bottom-right cell, and cells are separated from their neighboring cells via walls. A simple algorithm to generate the maze is to start with walls everywhere (except for the entrance and exit). We then continually choose a wall randomly, and knock it down if the cells that the wall separates are not already connected to each other. If we repeat this process until the starting and ending cells are connected, then we have a maze. It is actually Figure 8.25 A 50-by-88 maze

8.7 An Application {0} {1} {2} {3} {4} {5} {6} {7} {8} {9} {10} {11} {12} {13} {14} {15} {16} {17} {18} {19} {20} {21} {22} {23} {24} Figure 8.26 Initial state: all walls up, all cells in their own set better to continue knocking down walls until every cell is reachable from every other cell (this generates more false leads in the maze). We illustrate the algorithm with a 5-by-5 maze. Figure 8.26 shows the initial conﬁguration. We use the union/ﬁnd data structure to represent sets of cells that are connected to each other. Initially, walls are everywhere, and each cell is in its own equivalence class. Figure 8.27 shows a later stage of the algorithm, after a few walls have been knocked down. Suppose, at this stage, the wall that connects cells 8 and 13 is randomly targeted. Because 8 and 13 are already connected (they are in the same set), we would not remove the wall, as it would simply trivialize the maze. Suppose that cells 18 and 13 are randomly targeted next. By performing two find operations, we see that these are in different sets; {0, 1} {2} {3} {4, 6, 7, 8, 9, 13, 14} {5} {10, 11, 15} {12} {16, 17, 18, 22} {19} {20} {21} {23} {24} Figure 8.27 At some point in the algorithm: Several walls down, sets have merged; if at this point the wall between 8 and 13 is randomly selected, this wall is not knocked down, because 8 and 13 are already connected

Chapter 8 The Disjoint Set Class {0, 1} {2} {3} {4, 6, 7, 8, 9, 13, 14, 16, 17, 18, 22} {5} {10, 11, 15} {12} {19} {20} {21} {23} {24} Figure 8.28 Wall between squares 18 and 13 is randomly selected in Figure 8.22; this wall is knocked down, because 18 and 13 are not already connected; their sets are merged {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24} Figure 8.29 Eventually, 24 walls are knocked down; all elements are in the same set thus 18 and 13 are not already connected. Therefore, we knock down the wall that separates them, as shown in Figure 8.28. Notice that as a result of this operation, the sets containing 18 and 13 are combined via a union operation. This is because everything that was connected to 18 is now connected to everything that was connected to 13. At the end of the algorithm, depicted in Figure 8.29, everything is connected, and we are done. The running time of the algorithm is dominated by the union/ﬁnd costs. The size of the union/ﬁnd universe is equal to the number of cells. The number of find operations is proportional to the number of cells, since the number of removed walls is one less than the number of cells, while with care, we see that there are only about twice the number of walls as cells in the ﬁrst place. Thus, if N is the number of cells, since there are two ﬁnds per randomly targeted wall, this gives an estimate of between (roughly) 2N and 4N find operations throughout the algorithm. Therefore, the algorithm’s running time can be taken as O(N log∗N), and this algorithm quickly generates a maze.

C H A P T E R 9 Graph Algorithms In this chapter we discuss several common problems in graph theory. Not only are these algorithms useful in practice, they are also interesting because in many real-life applications they are too slow unless careful attention is paid to the choice of data structures. We will r Show several real-life problems, which can be converted to problems on graphs. r Give algorithms to solve several common graph problems. r Show how the proper choice of data structures can drastically reduce the running time of these algorithms. r See an important technique, known as depth-ﬁrst search, and show how it can be used to solve several seemingly nontrivial problems in linear time. 9.1 Deﬁnitions A graph G = (V, E) consists of a set of vertices, V, and a set of edges, E. Each edge is a pair (v, w), where v, w ∈V. Edges are sometimes referred to as arcs. If the pair is ordered, then the graph is directed. Directed graphs are sometimes referred to as digraphs. Vertex w is adjacent to v if and only if (v, w) ∈E. In an undirected graph with edge (v, w), and hence (w, v), w is adjacent to v and v is adjacent to w. Sometimes an edge has a third component, known as either a weight or a cost. A path in a graph is a sequence of vertices w1, w2, w3, . . . , wN such that (wi, wi+1) ∈E for 1 ≤i < N. The length of such a path is the number of edges on the path, which is equal to N −1. We allow a path from a vertex to itself; if this path contains no edges, then the path length is 0. This is a convenient way to deﬁne an otherwise special case. If the graph contains an edge (v, v) from a vertex to itself, then the path v, v is sometimes referred to as a loop. The graphs we will consider will generally be loopless. A simple path is a path such that all vertices are distinct, except that the ﬁrst and last could be the same. A cycle in a directed graph is a path of length at least 1 such that w1 = wN; this cycle is simple if the path is simple. For undirected graphs, we require that the edges be distinct. The logic of these requirements is that the path u, v, u in an undirected graph should not be considered a cycle, because (u, v) and (v, u) are the same edge. In a directed graph, these are different edges, so it makes sense to call this a cycle. A directed graph is acyclic if it has no cycles. A directed acyclic graph is sometimes referred to by its abbreviation, DAG.

Chapter 9 Graph Algorithms An undirected graph is connected if there is a path from every vertex to every other vertex. A directed graph with this property is called strongly connected. If a directed graph is not strongly connected, but the underlying graph (without direction to the arcs) is connected, then the graph is said to be weakly connected. A complete graph is a graph in which there is an edge between every pair of vertices. An example of a real-life situation that can be modeled by a graph is the airport system. Each airport is a vertex, and two vertices are connected by an edge if there is a nonstop ﬂight from the airports that are represented by the vertices. The edge could have a weight, representing the time, distance, or cost of the ﬂight. It is reasonable to assume that such a graph is directed, since it might take longer or cost more (depending on local taxes, for example) to ﬂy in different directions. We would probably like to make sure that the airport system is strongly connected, so that it is always possible to ﬂy from any airport to any other airport. We might also like to quickly determine the best ﬂight between any two airports. “Best” could mean the path with the fewest number of edges or could be taken with respect to one, or all, of the weight measures. Trafﬁc ﬂow can be modeled by a graph. Each street intersection represents a vertex, and each street is an edge. The edge costs could represent, among other things, a speed limit or a capacity (number of lanes). We could then ask for the shortest route or use this information to ﬁnd the most likely location for bottlenecks. In the remainder of this chapter, we will see several more applications of graphs. Many of these graphs can be quite large, so it is important that the algorithms we use be efﬁcient. 9.1.1 Representation of Graphs We will consider directed graphs (undirected graphs are similarly represented). Suppose, for now, that we can number the vertices, starting at 1. The graph shown in Figure 9.1 represents 7 vertices and 12 edges. Figure 9.1 A directed graph

9.1 Deﬁnitions One simple way to represent a graph is to use a two-dimensional array. This is known as an adjacency matrix representation. For each edge (u, v), we set A[u][v] to true; otherwise the entry in the array is false. If the edge has a weight associated with it, then we can set A[u][v] equal to the weight and use either a very large or a very small weight as a sentinel to indicate nonexistent edges. For instance, if we were looking for the cheapest airplane route, we could represent nonexistent ﬂights with a cost of ∞. If we were looking, for some strange reason, for the most expensive airplane route, we could use −∞(or perhaps 0) to represent nonexistent edges. Although this has the merit of extreme simplicity, the space requirement is (|V|2), which can be prohibitive if the graph does not have very many edges. An adjacency matrix is an appropriate representation if the graph is dense: |E| = (|V|2). In most of the applications that we shall see, this is not true. For instance, suppose the graph represents a street map. Assume a Manhattan-like orientation, where almost all the streets run either north–south or east–west. Therefore, any intersection is attached to roughly four streets, so if the graph is directed and all streets are two-way, then |E| ≈4|V|. If there are 3,000 intersections, then we have a 3,000-vertex graph with 12,000 edge entries, which would require an array of size 9,000,000. Most of these entries would contain zero. This is intuitively bad, because we want our data structures to represent the data that are actually there and not the data that are not present. If the graph is not dense, in other words, if the graph is sparse, a better solution is an adjacency list representation. For each vertex, we keep a list of all adjacent vertices. The space requirement is then O(|E| + |V|), which is linear in the size of the graph.1 The abstract representation should be clear from Figure 9.2. If the edges have weights, then this additional information is also stored in the adjacency lists. Adjacency lists are the standard way to represent graphs. Undirected graphs can be similarly represented; each edge (u, v) appears in two lists, so the space usage essentially doubles. A common requirement in graph algorithms is to ﬁnd all vertices adjacent to some given vertex v, and this can be done, in time proportional to the number of such vertices found, by a simple scan down the appropriate adjacency list. There are several alternatives for maintaining the adjacency lists. First, observe that the lists themselves can be maintained in any kind of List, namely ArrayLists or LinkedLists. However, for very sparse graphs, when using ArrayLists, the programmer may need to start the ArrayLists with a smaller capacity than the default; otherwise there could be signiﬁcant wasted space. Because it is important to be able to quickly obtain the list of adjacent vertices for any vertex, the two basic options are to use a map in which the keys are vertices and the values are adjacency lists, or to maintain each adjacency list as a data member of a Vertex class. The ﬁrst option is arguably simpler, but the second option can be faster, because it avoids repeated lookups in the map. In the second scenario, if the vertex is a String (for instance, an airport name, or the name of a street intersection), then a map can be used in which the key is the vertex name and the value is a Vertex and each Vertex object keeps a list of adjacent vertices, and perhaps also the original String name. 1 When we speak of linear-time graph algorithms, O(|E| + |V|) is the running time we require.

Chapter 9 Graph Algorithms 2, 4, 3 4, 5 6, 7, 3 4, 7 (empty) Figure 9.2 An adjacency list representation of a graph In most of the chapter, we present the graph algorithms using pseudocode. We will do this to save space and, of course, to make the presentation of the algorithms much clearer. At the end of Section 9.3, we provide a working Java implementation of a routine that makes underlying use of a shortest-path algorithm to obtain its answers. 9.2 Topological Sort A topological sort is an ordering of vertices in a directed acyclic graph, such that if there is a path from vi to vj, then vj appears after vi in the ordering. The graph in Figure 9.3 represents the course prerequisite structure at a state university in Miami. A directed edge (v, w) indicates that course v must be completed before course w may be attempted. A topological ordering of these courses is any course sequence that does not violate the prerequisite requirement. It is clear that a topological ordering is not possible if the graph has a cycle, since for two vertices v and w on the cycle, v precedes w and w precedes v. Furthermore, the ordering is not necessarily unique; any legal ordering will do. In the graph in Figure 9.4, v1, v2, v5, v4, v3, v7, v6 and v1, v2, v5, v4, v7, v3, v6 are both topological orderings. A simple algorithm to ﬁnd a topological ordering is ﬁrst to ﬁnd any vertex with no incoming edges. We can then print this vertex, and remove it, along with its edges, from the graph. Then we apply this same strategy to the rest of the graph. To formalize this, we deﬁne the indegree of a vertex v as the number of edges (u, v). We compute the indegrees of all vertices in the graph. Assuming that the indegree for each

9.2 Topological Sort MAC3311 COP3210 CAP3700 COP3337 COP3400 MAD2104 COP4555 CDA4101 COP3530 MAD3512 CDA4400 MAD3305 COP4225 COP4610 CIS4610 COP4540 COP5621 Figure 9.3 An acyclic graph representing course prerequisite structure v 1 v 2 v 3 v 4 v 5 v 6 v 7 Figure 9.4 An acyclic graph vertex is stored and that the graph is read into an adjacency list, we can then apply the algorithm in Figure 9.5 to generate a topological ordering. The method findNewVertexOfIndegreeZero scans the array looking for a vertex with indegree 0 that has not already been assigned a topological number. It returns null if no such vertex exists; this indicates that the graph has a cycle. Because findNewVertexOfIndegreeZero is a simple sequential scan of the array of vertices, each call to it takes O(|V|) time. Since there are |V| such calls, the running time of the algorithm is O(|V|2). By paying more careful attention to the data structures, it is possible to do better. The cause of the poor running time is the sequential scan through the array of vertices. If the

Chapter 9 Graph Algorithms void topsort( ) throws CycleFoundException { for( int counter = 0; counter < NUM_VERTICES; counter++ ) { Vertex v = findNewVertexOfIndegreeZero( ); if( v == null ) throw new CycleFoundException( ); v.topNum = counter; for each Vertex w adjacent to v w.indegree--; } } Figure 9.5 Simple topological sort pseudocode Indegree Before Dequeue # Vertex v1 v2 v3 v4 v5 v6 v7 Enqueue v1 v2 v5 v4 v3, v7 v6 Dequeue v1 v2 v5 v4 v3 v7 v6 Figure 9.6 Result of applying topological sort to the graph in Figure 9.4 graph is sparse, we would expect that only a few vertices have their indegrees updated during each iteration. However, in the search for a vertex of indegree 0, we look at (potentially) all the vertices, even though only a few have changed. We can remove this inefﬁciency by keeping all the (unassigned) vertices of indegree 0 in a special box. The findNewVertexOfIndegreeZero method then returns (and removes) any vertex in the box. When we decrement the indegrees of the adjacent vertices, we check each vertex and place it in the box if its indegree falls to 0. To implement the box, we can use either a stack or a queue; we will use a queue. First, the indegree is computed for every vertex. Then all vertices of indegree 0 are placed on an initially empty queue. While the queue is not empty, a vertex v is removed, and all vertices adjacent to v have their indegrees decremented. A vertex is put on the queue as soon as its indegree falls to 0. The topological ordering then is the order in which the vertices dequeue. Figure 9.6 shows the status after each phase.

9.2 Topological Sort void topsort( ) throws CycleFoundException { Queue<Vertex> q = new Queue<Vertex>( ); int counter = 0; for each Vertex v if( v.indegree == 0 ) q.enqueue( v ); while( !q.isEmpty( ) ) { Vertex v = q.dequeue( ); v.topNum = ++counter; // Assign next number for each Vertex w adjacent to v if( --w.indegree == 0 ) q.enqueue( w ); } if( counter != NUM_VERTICES ) throw new CycleFoundException( ); } Figure 9.7 Pseudocode to perform topological sort A pseudocode implementation of this algorithm is given in Figure 9.7. As before, we will assume that the graph is already read into an adjacency list and that the indegrees are computed and stored with the vertices. We also assume each vertex has a ﬁeld named topNum, in which to place its topological numbering. The time to perform this algorithm is O(|E| + |V|) if adjacency lists are used. This is apparent when one realizes that the body of the for loop is executed at most once per edge. Computing the indegrees can be done with the following code; this same logic shows that the cost of this computation is O(|E| + |V|), even though there are nested loops. for each Vertex v v.indegree = 0; for each Vertex v for each Vertex w adjacent to v w.indegree++; The queue operations are done at most once per vertex, and the other initialization steps, including the computation of indegrees, also take time proportional to the size of the graph.

Chapter 9 Graph Algorithms 9.3 Shortest-Path Algorithms In this section we examine various shortest-path problems. The input is a weighted graph: associated with each edge (vi, vj) is a cost ci,j to traverse the edge. The cost of a path v1v2 . . . vN is N−1 i=1 ci,i+1. This is referred to as the weighted path length. The unweighted path length is merely the number of edges on the path, namely, N −1. Single-Source Shortest-Path Problem. Given as input a weighted graph, G = (V, E), and a distinguished vertex, s, ﬁnd the shortest weighted path from s to every other vertex in G. For example, in the graph in Figure 9.8, the shortest weighted path from v1 to v6 has a cost of 6 and goes from v1 to v4 to v7 to v6. The shortest unweighted path between these vertices is 2. Generally, when it is not speciﬁed whether we are referring to a weighted or an unweighted path, the path is weighted if the graph is. Notice also that in this graph there is no path from v6 to v1. The graph in the preceding example has no edges of negative cost. The graph in Figure 9.9 shows the problems that negative edges can cause. The path from v5 to v4 has cost 1, but a shorter path exists by following the loop v5, v4, v2, v5, v4, which has cost −5. This path is still not the shortest, because we could stay in the loop arbitrarily long. Thus, the shortest path between these two points is undeﬁned. Similarly, the shortest path from v1 to v6 is undeﬁned, because we can get into the same loop. This loop is known as a negative-cost cycle; when one is present in the graph, the shortest paths are not deﬁned. Negative-cost edges are not necessarily bad, as the cycles are, but their presence seems to make the problem harder. For convenience, in the absence of a negative-cost cycle, the shortest path from s to s is zero. There are many examples where we might want to solve the shortest-path problem. If the vertices represent computers; the edges represent a link between computers; and the costs represent communication costs (phone bill per megabyte of data), delay costs (number of seconds required to transmit a megabyte), or a combination of these and other v 1 v 2 v 3 v 4 v 5 v 6 v 7 Figure 9.8 A directed graph G

9.3 Shortest-Path Algorithms –10 v 1 v 2 v 3 v 4 v 5 v 6 v 7 Figure 9.9 A graph with a negative-cost cycle factors, then we can use the shortest-path algorithm to ﬁnd the cheapest way to send electronic news from one computer to a set of other computers. We can model airplane or other mass transit routes by graphs and use a shortestpath algorithm to compute the best route between two points. In this and many practical applications, we might want to ﬁnd the shortest path from one vertex, s, to only one other vertex, t. Currently there are no algorithms in which ﬁnding the path from s to one vertex is any faster (by more than a constant factor) than ﬁnding the path from s to all vertices. We will examine algorithms to solve four versions of this problem. First, we will consider the unweighted shortest-path problem and show how to solve it in O(|E|+|V|). Next, we will show how to solve the weighted shortest-path problem if we assume that there are no negative edges. The running time for this algorithm is O(|E| log |V|) when implemented with reasonable data structures. If the graph has negative edges, we will provide a simple solution, which unfortunately has a poor time bound of O(|E| · |V|). Finally, we will solve the weighted problem for the special case of acyclic graphs in linear time. 9.3.1 Unweighted Shortest Paths Figure 9.10 shows an unweighted graph, G. Using some vertex, s, which is an input parameter, we would like to ﬁnd the shortest path from s to all other vertices. We are only interested in the number of edges contained on the path, so there are no weights on the edges. This is clearly a special case of the weighted shortest-path problem, since we could assign all edges a weight of 1. For now, suppose we are interested only in the length of the shortest paths, not in the actual paths themselves. Keeping track of the actual paths will turn out to be a matter of simple bookkeeping. Suppose we choose s to be v3. Immediately, we can tell that the shortest path from s to v3 is then a path of length 0. We can mark this information, obtaining the graph in Figure 9.11.

Chapter 9 Graph Algorithms v 1 v 2 v 3 v 4 v 5 v 6 v 7 Figure 9.10 An unweighted directed graph G v 1 v 2 v 3 v 4 v 5 v 6 v 7 Figure 9.11 Graph after marking the start node as reachable in zero edges Now we can start looking for all vertices that are a distance 1 away from s. These can be found by looking at the vertices that are adjacent to s. If we do this, we see that v1 and v6 are one edge from s. This is shown in Figure 9.12. We can now ﬁnd vertices whose shortest path from s is exactly 2, by ﬁnding all the vertices adjacent to v1 and v6 (the vertices at distance 1), whose shortest paths are not already known. This search tells us that the shortest path to v2 and v4 is 2. Figure 9.13 shows the progress that has been made so far. Finally we can ﬁnd, by examining vertices adjacent to the recently evaluated v2 and v4, that v5 and v7 have a shortest path of three edges. All vertices have now been calculated, and so Figure 9.14 shows the ﬁnal result of the algorithm. This strategy for searching a graph is known as breadth-ﬁrst search. It operates by processing vertices in layers: The vertices closest to the start are evaluated ﬁrst, and the most distant vertices are evaluated last. This is much the same as a level-order traversal for trees. Given this strategy, we must translate it into code. Figure 9.15 shows the initial conﬁguration of the table that our algorithm will use to keep track of its progress.

9.3 Shortest-Path Algorithms v 1 v 2 v 3 v 4 v 5 v 6 v 7 Figure 9.12 Graph after ﬁnding all vertices whose path length from s is 1 v 1 v 2 v 3 v 4 v 5 v 6 v 7 Figure 9.13 Graph after ﬁnding all vertices whose shortest path is 2 v 1 v 2 v 3 v 4 v 5 v 6 v 7 Figure 9.14 Final shortest paths

Chapter 9 Graph Algorithms v known dv pv v1 F ∞ v2 F ∞ v3 F v4 F ∞ v5 F ∞ v6 F ∞ v7 F ∞ Figure 9.15 Initial conﬁguration of table used in unweighted shortest-path computation For each vertex, we will keep track of three pieces of information. First, we will keep its distance from s in the entry dv. Initially all vertices are unreachable except for s, whose path length is 0. The entry in pv is the bookkeeping variable, which will allow us to print the actual paths. The entry known is set to true after a vertex is processed. Initially, all entries are not known, including the start vertex. When a vertex is marked known, we have a guarantee that no cheaper path will ever be found, and so processing for that vertex is essentially complete. The basic algorithm can be described in Figure 9.16. The algorithm in Figure 9.16 mimics the diagrams by declaring as known the vertices at distance d = 0, then d = 1, then d = 2, and so on, and setting all the adjacent vertices w that still have dw = ∞to a distance dw = d + 1. By tracing back through the pv variable, the actual path can be printed. We will see how when we discuss the weighted case. The running time of the algorithm is O(|V|2), because of the doubly nested for loops. An obvious inefﬁciency is that the outside loop continues until NUM_VERTICES −1, even if all the vertices become known much earlier. Although an extra test could be made to avoid this, it does not affect the worst-case running time, as can be seen by generalizing what happens when the input is the graph in Figure 9.17 with start vertex v9. We can remove the inefﬁciency in much the same way as was done for topological sort. At any point in time, there are only two types of unknown vertices that have dv ̸= ∞. Some have dv = currDist and the rest have dv = currDist + 1. Because of this extra structure, it is very wasteful to search through the entire table to ﬁnd a proper vertex. A very simple but abstract solution is to keep two boxes. Box #1 will have the unknown vertices with dv = currDist, and box #2 will have dv = currDist + 1. The test to ﬁnd an appropriate vertex v can be replaced by ﬁnding any vertex in box #1. After updating w (inside the innermost if block), we can add w to box #2. After the outermost for loop terminates, box #1 is empty, and box #2 can be transferred to box #1 for the next pass of the for loop. We can reﬁne this idea even further by using just one queue. At the start of the pass, the queue contains only vertices of distance currDist. When we add adjacent vertices of distance currDist + 1, since they enqueue at the rear, we are guaranteed that they will not be processed until after all the vertices of distance currDist have been processed. After the

9.3 Shortest-Path Algorithms void unweighted( Vertex s ) { for each Vertex v { v.dist = INFINITY; v.known = false; } s.dist = 0; for( int currDist = 0; currDist < NUM_VERTICES; currDist++ ) for each Vertex v if( !v.known && v.dist == currDist ) { v.known = true; for each Vertex w adjacent to v if( w.dist == INFINITY ) { w.dist = currDist + 1; w.path = v; } } } Figure 9.16 Pseudocode for unweighted shortest-path algorithm v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 8 v 9 Figure 9.17 A bad case for unweighted shortest-path algorithm using Figure 9.16 last vertex at distance currDist dequeues and is processed, the queue only contains vertices of distance currDist + 1, so this process perpetuates. We merely need to begin the process by placing the start node on the queue by itself. The reﬁned algorithm is shown in Figure 9.18. In the pseudocode, we have assumed that the start vertex, s, is passed as a parameter. Also, it is possible that the queue might empty prematurely, if some vertices are unreachable from the start node. In this case, a distance of INFINITY will be reported for these nodes, which is perfectly reasonable. Finally, the known ﬁeld is not used; once a vertex is processed it can never enter the queue again, so the fact that it need not be reprocessed is implicitly marked. Thus, the known ﬁeld can be discarded. Figure 9.19 shows how the values on the graph we have been using are changed during the algorithm. (it includes changes that would occur to known if we had kept it). Using the same analysis as was performed for topological sort, we see that the running time is O(|E| + |V|), as long as adjacency lists are used.

Chapter 9 Graph Algorithms void unweighted( Vertex s ) { Queue<Vertex> q = new Queue<Vertex>( ); for each Vertex v v.dist = INFINITY; s.dist = 0; q.enqueue( s ); while( !q.isEmpty( ) ) { Vertex v = q.dequeue( ); for each Vertex w adjacent to v if( w.dist == INFINITY ) { w.dist = v.dist + 1; w.path = v; q.enqueue( w ); } } } Figure 9.18 Pseudocode for unweighted shortest-path algorithm 9.3.2 Dijkstra’s Algorithm If the graph is weighted, the problem (apparently) becomes harder, but we can still use the ideas from the unweighted case. We keep all of the same information as before. Thus, each vertex is marked as either known or unknown. A tentative distance dv is kept for each vertex, as before. This distance turns out to be the shortest path length from s to v using only known vertices as intermediates. As before, we record pv, which is the last vertex to cause a change to dv. The general method to solve the single-source shortest-path problem is known as Dijkstra’s algorithm. This thirty-year-old solution is a prime example of a greedy algorithm. Greedy algorithms generally solve a problem in stages by doing what appears to be the best thing at each stage. For example, to make change in U.S. currency, most people count out the quarters ﬁrst, then the dimes, nickels, and pennies. This greedy algorithm gives change using the minimum number of coins. The main problem with greedy algorithms is that they do not always work. The addition of a 12-cent piece breaks the coin-changing algorithm for returning 15 cents, because the answer it gives (one 12-cent piece and three pennies) is not optimal (one dime and one nickel). Dijkstra’s algorithm proceeds in stages, just like the unweighted shortest-path algorithm. At each stage, Dijkstra’s algorithm selects a vertex v, which has the smallest dv

9.3 Shortest-Path Algorithms Initial State v3 Dequeued v1 Dequeued v6 Dequeued v known dv pv known dv pv known dv pv known dv pv v1 F ∞ F v3 T v3 T v3 v2 F ∞ F ∞ F v1 F v1 v3 F T T T v4 F ∞ F ∞ F v1 F v1 v5 F ∞ F ∞ F ∞ F ∞ v6 F ∞ F v3 F v3 T v3 v7 F ∞ F ∞ F ∞ F ∞ Q: v3 v1, v6 v6, v2, v4 v2, v4 v2 Dequeued v4 Dequeued v5 Dequeued v7 Dequeued v known dv pv known dv pv known dv pv known dv pv v1 T v3 T v3 T v3 T v3 v2 T v1 T v1 T v1 T v1 v3 T T T T v4 F v1 T v1 T v1 T v1 v5 F v2 F v2 T v2 T v2 v6 T v3 T v3 T v3 T v3 v7 F ∞ F v4 F v4 T v4 Q: v4, v5 v5, v7 v7 empty Figure 9.19 How the data change during the unweighted shortest-path algorithm among all the unknown vertices, and declares that the shortest path from s to v is known. The remainder of a stage consists of updating the values of dw. In the unweighted case, we set dw = dv + 1 if dw = ∞. Thus, we essentially lowered the value of dw if vertex v offered a shorter path. If we apply the same logic to the weighted case, then we should set dw = dv + cv,w if this new value for dw would be an improvement. Put simply, the algorithm decides whether or not it is a good idea to use v on the path to w. The original cost, dw, is the cost without using v; the cost calculated above is the cheapest path using v (and only known vertices). The graph in Figure 9.20 is our example. Figure 9.21 represents the initial conﬁguration, assuming that the start node, s, is v1. The ﬁrst vertex selected is v1, with path length 0. This vertex is marked known. Now that v1 is known, some entries need to be adjusted. The vertices adjacent to v1 are v2 and v4. Both these vertices get their entries adjusted, as indicated in Figure 9.22. Next, v4 is selected and marked known. Vertices v3, v5, v6, and v7 are adjacent, and it turns out that all require adjusting, as shown in Figure 9.23.

Chapter 9 Graph Algorithms v 1 v 2 v 3 v 4 v 5 v 6 v 7 Figure 9.20 The directed graph G (again) v known dv pv v1 F v2 F ∞ v3 F ∞ v4 F ∞ v5 F ∞ v6 F ∞ v7 F ∞ Figure 9.21 Initial conﬁguration of table used in Dijkstra’s algorithm v known dv pv v1 T v2 F v1 v3 F ∞ v4 F v1 v5 F ∞ v6 F ∞ v7 F ∞ Figure 9.22 After v1 is declared known Next, v2 is selected. v4 is adjacent but already known, so no work is performed on it. v5 is adjacent but not adjusted, because the cost of going through v2 is 2 + 10 = 12 and a path of length 3 is already known. Figure 9.24 shows the table after these vertices are selected.

9.3 Shortest-Path Algorithms v known dv pv v1 T v2 F v1 v3 F v4 v4 T v1 v5 F v4 v6 F v4 v7 F v4 Figure 9.23 After v4 is declared known v known dv pv v1 T v2 T v1 v3 F v4 v4 T v1 v5 F v4 v6 F v4 v7 F v4 Figure 9.24 After v2 is declared known v known dv pv v1 T v2 T v1 v3 T v4 v4 T v1 v5 T v4 v6 F v3 v7 F v4 Figure 9.25 After v5 and then v3 are declared known The next vertex selected is v5 at cost 3. v7 is the only adjacent vertex, but it is not adjusted, because 3 + 6 > 5. Then v3 is selected, and the distance for v6 is adjusted down to 3 + 5 = 8. The resulting table is depicted in Figure 9.25. Next v7 is selected; v6 gets updated down to 5 + 1 = 6. The resulting table is Figure 9.26. Finally, v6 is selected. The ﬁnal table is shown in Figure 9.27. Figure 9.28 graphically shows how edges are marked known and vertices updated during Dijkstra’s algorithm.

Chapter 9 Graph Algorithms v known dv pv v1 T v2 T v1 v3 T v4 v4 T v1 v5 T v4 v6 F v7 v7 T v4 Figure 9.26 After v7 is declared known v known dv pv v1 T v2 T v1 v3 T v4 v4 T v1 v5 T v4 v6 T v7 v7 T v4 Figure 9.27 After v6 is declared known and algorithm terminates To print out the actual path from a start vertex to some vertex v, we can write a recursive routine to follow the trail left in the p variables. We now give pseudocode to implement Dijkstra’s algorithm. Each Vertex stores various data ﬁelds that are used in the algorithm. This is shown in Figure 9.29. The path can be printed out using the recursive routine in Figure 9.30. The routine recursively prints the path all the way up to the vertex before v on the path and then just prints v. This works because the path is simple. Figure 9.31 shows the main algorithm, which is just a for loop to ﬁll up the table using the greedy selection rule. A proof by contradiction will show that this algorithm always works as long as no edge has a negative cost. If any edge has negative cost, the algorithm could produce the wrong answer (see Exercise 9.7(a)). The running time depends on how the vertices are manipulated, which we have yet to consider. If we use the obvious algorithm of sequentially scanning the vertices to ﬁnd the minimum dv, each phase will take O(|V|) time to ﬁnd the minimum, and thus O(|V|2) time will be spent ﬁnding the minimum over the course of the algorithm. The time for updating dw is constant per update, and there is at most one update per edge for a total of O(|E|). Thus, the total running time is O(|E|+|V|2) = O(|V|2). If the graph is dense, with |E| = (|V|2), this algorithm is not only simple but also essentially optimal, since it runs in time linear in the number of edges.

9.3 Shortest-Path Algorithms v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 1* v 2 v 3 v 4 v 5 v 6 v 7 v 1* v 2 v 3 v 4* v 5 v 6 v 7 v 1* v 2* v 3 v 4* v 5 v 6 v 7 v 1* v 2* v 3 v 4* v 5* v 6 v 7 v 1* v 2* v 3* v 4* v 5* v 6 v 7 v 1* v 2* v 3* v 4* v 5* v 6 v 7* v 1* v 2* v 3* v 4* v 5* v 6* v 7* ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ ∞ Figure 9.28 Stages of Dijkstra’s algorithm If the graph is sparse, with |E| = (|V|), this algorithm is too slow. In this case, the distances would need to be kept in a priority queue. There are actually two ways to do this; both are similar. Selection of the vertex v is a deleteMin operation, since once the unknown minimum vertex is found, it is no longer unknown and must be removed from future consideration. The update of w’s distance can be implemented two ways.

Chapter 9 Graph Algorithms class Vertex { public List adj; // Adjacency list public boolean known; public DistType dist; // DistType is probably int public Vertex path; // Other fields and methods as needed } Figure 9.29 Vertex class for Dijkstra’s algorithm /* * Print shortest path to v after dijkstra has run. * Assume that the path exists. */ void printPath( Vertex v ) { if( v.path != null ) { printPath( v.path ); System.out.print( " to " ); } System.out.print( v ); } Figure 9.30 Routine to print the actual shortest path One way treats the update as a decreaseKey operation. The time to ﬁnd the minimum is then O(log |V|), as is the time to perform updates, which amount to decreaseKey operations. This gives a running time of O(|E| log |V| + |V| log |V|) = O(|E| log |V|), an improvement over the previous bound for sparse graphs. Since priority queues do not efﬁciently support the find operation, the location in the priority queue of each value of di will need to be maintained and updated whenever di changes in the priority queue. If the priority queue is implemented by a binary heap, this will be messy. If a pairing heap (Chapter 12) is used, the code is not too bad. An alternate method is to insert w and the new value dw into the priority queue every time w’s distance changes. Thus, there may be more than one representative for each vertex in the priority queue. When the deleteMin operation removes the smallest vertex from the priority queue, it must be checked to make sure that it is not already known and, if it is, it is simply ignored and another deleteMin is performed. Although this method is superior from a software point of view, and is certainly much easier to code, the size of the priority queue could get to be as large as |E|. This does not affect the asymptotic time bounds, since |E| ≤|V|2 implies that log |E| ≤2 log |V|. Thus, we still get an O(|E| log |V|) algorithm. However, the space requirement does increase, and this could be important in

9.3 Shortest-Path Algorithms void dijkstra( Vertex s ) { for each Vertex v { v.dist = INFINITY; v.known = false; } s.dist = 0; while( there is an unknown distance vertex ) { Vertex v = smallest unknown distance vertex; v.known = true; for each Vertex w adjacent to v if( !w.known ) { DistType cvw = cost of edge from v to w; if( v.dist + cvw < w.dist ) { // Update w decrease( w.dist to v.dist + cvw ); w.path = v; } } } } Figure 9.31 Pseudocode for Dijkstra’s algorithm some applications. Moreover, because this method requires |E| deleteMins instead of only |V|, it is likely to be slower in practice. Notice that for the typical problems, such as computer mail and mass transit commutes, the graphs are typically very sparse because most vertices have only a couple of edges, so it is important in many applications to use a priority queue to solve this problem. There are better time bounds possible using Dijkstra’s algorithm if different data structures are used. In Chapter 11, we will see another priority queue data structure called the Fibonacci heap. When this is used, the running time is O(|E|+|V| log |V|). Fibonacci heaps have good theoretical time bounds but a fair amount of overhead, so it is not clear whether using Fibonacci heaps is actually better in practice than Dijkstra’s algorithm with binary heaps. To date, there are no meaningful average-case results for this problem.

Chapter 9 Graph Algorithms 9.3.3 Graphs with Negative Edge Costs If the graph has negative edge costs, then Dijkstra’s algorithm does not work. The problem is that once a vertex u is declared known, it is possible that from some other, unknown vertex v there is a path back to u that is very negative. In such a case, taking a path from s to v back to u is better than going from s to u without using v. Exercise 9.7(a) asks you to construct an explicit example. A tempting solution is to add a constant   to each edge cost, thus removing negative edges, calculate a shortest path on the new graph, and then use that result on the original. The naive implementation of this strategy does not work because paths with many edges become more weighty than paths with few edges. A combination of the weighted and unweighted algorithms will solve the problem, but at the cost of a drastic increase in running time. We forget about the concept of known vertices, since our algorithm needs to be able to change its mind. We begin by placing s on a queue. Then, at each stage, we dequeue a vertex v. We ﬁnd all vertices w adjacent to v such that dw > dv + cv,w. We update dw and pw, and place w on a queue if it is not already there. A bit can be set for each vertex to indicate presence in the queue. We repeat the process until the queue is empty. Figure 9.32 (almost) implements this algorithm. Although the algorithm works if there are no negative-cost cycles, it is no longer true that the code in the inner for loop is executed once per edge. Each vertex can dequeue at most |V| times, so the running time is O(|E| · |V|) if adjacency lists are used (Exercise 9.7(b)). This is quite an increase from Dijkstra’s algorithm, so it is fortunate that, in practice, edge costs are nonnegative. If negative-cost cycles are present, then the algorithm as written will loop indeﬁnitely. By stopping the algorithm after any vertex has dequeued |V| + 1 times, we can guarantee termination. 9.3.4 Acyclic Graphs If the graph is known to be acyclic, we can improve Dijkstra’s algorithm by changing the order in which vertices are declared known, otherwise known as the vertex selection rule. The new rule is to select vertices in topological order. The algorithm can be done in one pass, since the selections and updates can take place as the topological sort is being performed. This selection rule works because when a vertex v is selected, its distance, dv, can no longer be lowered, since by the topological ordering rule it has no incoming edges emanating from unknown nodes. There is no need for a priority queue with this selection rule; the running time is O(|E| + |V|), since the selection takes constant time. An acyclic graph could model some downhill skiing problem—we want to get from point a to b but can only go downhill, so clearly there are no cycles. Another possible application might be the modeling of (nonreversible) chemical reactions. We could have each vertex represent a particular state of an experiment. Edges would represent a transition from one state to another, and the edge weights might represent the energy released. If only transitions from a higher energy state to a lower are allowed, the graph is acyclic. A more important use of acyclic graphs is critical path analysis. The graph in Figure 9.33 will serve as our example. Each node represents an activity that must be

9.3 Shortest-Path Algorithms void weightedNegative( Vertex s ) { Queue<Vertex> q = new Queue<Vertex>( ); for each Vertex v v.dist = INFINITY; s.dist = 0; q.enqueue( s ); while( !q.isEmpty( ) ) { Vertex v = q.dequeue( ); for each Vertex w adjacent to v if( v.dist + cvw < w.dist ) { // Update w w.dist = v.dist + cvw; w.path = v; if( w is not already in q ) q.enqueue( w ); } } } Figure 9.32 Pseudocode for weighted shortest-path algorithm with negative edge costs performed, along with the time it takes to complete the activity. This graph is thus known as an activity-node graph. The edges represent precedence relationships: An edge (v, w) means that activity v must be completed before activity w may begin. Of course, this implies that the graph must be acyclic. We assume that any activities that do not depend (either directly or indirectly) on each other can be performed in parallel by different servers. This type of a graph could be (and frequently is) used to model construction projects. In this case, there are several important questions which would be of interest to answer. First, what is the earliest completion time for the project? We can see from the graph that 10 time units are required along the path A, C, F, H. Another important question is to determine which activities can be delayed, and by how long, without affecting the minimum completion time. For instance, delaying any of A, C, F, or H would push the completion time past 10 units. On the other hand, activity B is less critical and can be delayed up to two time units without affecting the ﬁnal completion time. To perform these calculations, we convert the activity-node graph to an event-node graph. Each event corresponds to the completion of an activity and all its dependent activities. Events reachable from a node v in the event-node graph may not commence until after the event v is completed. This graph can be constructed automatically or by hand. Dummy edges and nodes may need to be inserted in the case where an activity depends on

Chapter 9 Graph Algorithms start A (3) B (2) C (3) D (2) E (1) F (3) G (2) H (1) K (4) finish Figure 9.33 Activity-node graph 6' 7' 8' 10' A/3 B/2 C/3 D/2 E/1 F/3 G/2 K/4 H/1 Figure 9.34 Event-node graph several others. This is necessary in order to avoid introducing false dependencies (or false lack of dependencies). The event-node graph corresponding to the graph in Figure 9.33 is shown in Figure 9.34. To ﬁnd the earliest completion time of the project, we merely need to ﬁnd the length of the longest path from the ﬁrst event to the last event. For general graphs, the longest-path problem generally does not make sense, because of the possibility of positive-cost cycles. These are the equivalent of negative-cost cycles in shortest-path problems. If positive-cost cycles are present, we could ask for the longest simple path, but no satisfactory solution is known for this problem. Since the event-node graph is acyclic, we need not worry about cycles. In this case, it is easy to adapt the shortest-path algorithm to compute the earliest completion time for all nodes in the graph. If ECi is the earliest completion time for node i, then the applicable rules are EC1 = 0 ECw = max (v,w)∈E(ECv + cv,w)

9.3 Shortest-Path Algorithms 6' 7' 8' 10' A/3 B/2 C/3 D/2 E/1 F/3 G/2 K/4 H/1 Figure 9.35 Earliest completion times 6' 7' 8' 10' A/3 B/2 C/3 D/2 E/1 F/3 G/2 K/4 H/1 Figure 9.36 Latest completion times Figure 9.35 shows the earliest completion time for each event in our example event-node graph. We can also compute the latest time, LCi, that each event can ﬁnish without affecting the ﬁnal completion time. The formulas to do this are LCn = ECn LCv = min (v,w)∈E(LCw −cv,w) These values can be computed in linear time by maintaining, for each vertex, a list of all adjacent and preceding vertices. The earliest completion times are computed for vertices by their topological order, and the latest completion times are computed by reverse topological order. The latest completion times are shown in Figure 9.36. The slack time for each edge in the event-node graph represents the amount of time that the completion of the corresponding activity can be delayed without delaying the overall completion. It is easy to see that Slack(v,w) = LCw −ECv −cv,w Figure 9.37 shows the slack (as the third entry) for each activity in the event-node graph. For each node, the top number is the earliest completion time and the bottom entry is the latest completion time.

Chapter 9 Graph Algorithms 6' 7' 8' 10' A/3/0 B/2/2 C/3/0 D/2/1 E/1/2 F/3/0 G/2/2 K/4/2 H/1/0 Figure 9.37 Earliest completion time, latest completion time, and slack Some activities have zero slack. These are critical activities, which must ﬁnish on schedule. There is at least one path consisting entirely of zero-slack edges; such a path is a critical path. 9.3.5 All-Pairs Shortest Path Sometimes it is important to ﬁnd the shortest paths between all pairs of vertices in the graph. Although we could just run the appropriate single-source algorithm |V| times, we might expect a somewhat faster solution, especially on a dense graph, if we compute all the information at once. In Chapter 10, we will see an O(|V|3) algorithm to solve this problem for weighted graphs. Although, for dense graphs, this is the same bound as running a simple (non– priority queue) Dijkstra’s algorithm |V| times, the loops are so tight that the specialized all-pairs algorithm is likely to be faster in practice. On sparse graphs, of course, it is faster to run |V| Dijkstra’s algorithms coded with priority queues. 9.3.6 Shortest-Path Example In this section we write some Java routines to compute word ladders. In a word ladder each word is formed by changing one character in the ladder’s previous word. For instance, we can convert zero to five by a sequence of one-character substitutions as follows: zero hero here hire fire five. This is an unweighted shortest problem in which each word is a vertex, and two vertices have edges (in both directions) between them if they can be converted to each other with a one-character substitution. In Section 4.8, we described and wrote a Java routine that would create a Map in which the keys are words, and the values are Lists containing the words that can result from a one-character transformation. As such, this Map represents the graph, in adjacency list format, and we only need to write one routine to run the single-source unweighted shortest-path algorithm and a second routine to output the sequence of words, after the single-source shortest-path algorithm has completed. These two routines are both shown in Figure 9.38.

// Runs the shortest path calculation from the adjacency map, returns a List // that contains the sequence of word changes to get from first to second. // Returns null if no sequence can be found for any reason. public static List<String> findChain( Map<String,List<String>> adjacentWords, String first, String second ) { Map<String,String> previousWord = new HashMap<String,String>( ); LinkedList<String> q = new LinkedList<String>( ); q.addLast( first ); while( !q.isEmpty( ) ) { String current = q.removeFirst( ); List<String> adj = adjacentWords.get( current ); if( adj != null ) for( String adjWord : adj ) if( previousWord.get( adjWord ) == null ) { previousWord.put( adjWord, current ); q.addLast( adjWord ); } } previousWord.put( first, null ); return getChainFromPreviousMap( previousWord, first, second ); } // After the shortest path calculation has run, computes the List that // contains the sequence of word changes to get from first to second. // Returns null if there is no path. public static List<String> getChainFromPreviousMap( Map<String,String> prev, String first, String second ) { LinkedList<String> result = null; if( prev.get( second ) != null ) { result = new LinkedList<String>( ); for( String str = second; str != null; str = prev.get( str ) ) result.addFirst( str ); } return result; } Figure 9.38 Java code to ﬁnd word ladders

Chapter 9 Graph Algorithms The ﬁrst routine is findChain, which takes the Map representing the adjacency lists and the two words to be connected and returns a Map in which the keys are words, and the corresponding value is the word prior to the key on the shortest ladder starting at first. In other words, in the example above, if the starting word is zero, the value for key five is fire, the value for key fire is hire, the value for key hire is here, and so on. Clearly this provides enough information for the second routine, getChainFromPreviousMap, which can work its way backward. findChain is a direct implementation of the pseudocode in Figure 9.18. It assumes that first is a valid word, which is an easily testable condition prior to the call. The basic loop incorrectly assigns a previous entry for first (when the initial word adjacent to first is processed), so at line 25 that entry is repaired. getChainFromPrevMap uses the prev Map and second, which presumably is a key in the Map and returns the words used to form the word ladder, by working its way backward through prev. By using a LinkedList and inserting at the front, we obtain the word ladder in the correct order. It is possible to generalize this problem to allow single-character substitutions that include the deletion of a character or the addition of a character. To compute the adjacency list requires only a little more effort: in the last algorithm in Section 4.8, every time a representative for word w in group g is computed, we check if the representative is a word in group g −1. If it is, then the representative is adjacent to w (it is a single-character deletion), and w is adjacent to the representative (it is a single-character addition). It is also possible to assign a cost to a character deletion or insertion (that is higher than a simple substitution), and this yields a weighted shortest-path problem that can be solved with Dijkstra’s algorithm. 9.4 Network Flow Problems Suppose we are given a directed graph G = (V, E) with edge capacities cv,w. These capacities could represent the amount of water that could ﬂow through a pipe or the amount of trafﬁc that could ﬂow on a street between two intersections. We have two vertices: s, which we call the source, and t, which is the sink. Through any edge, (v, w), at most cv,w units of “ﬂow” may pass. At any vertex, v, that is not either s or t, the total ﬂow coming in must equal the total ﬂow going out. The maximum ﬂow problem is to determine the maximum amount of ﬂow that can pass from s to t. As an example, for the graph in Figure 9.39 on the left the maximum ﬂow is 5, as indicated by the graph on the right. Although this example graph is acyclic, this is not a requirement; our (eventual) algorithm will work even if the graph has a cycle. As required by the problem statement, no edge carries more ﬂow than its capacity. Vertex a has three units of ﬂow coming in, which it distributes to c and d. Vertex d takes three units of ﬂow from a and b and combines this, sending the result to t. A vertex can combine and distribute ﬂow in any manner that it likes, as long as edge capacities are not violated and as long as ﬂow conservation is maintained (what goes in must come out).

9.4 Network Flow Problems s a b c d t s a b c d t Figure 9.39 A graph (left) and its maximum ﬂow s a b c d t Figure 9.40 A cut in graph G partitions the vertices with s and t in different groups. The total edge cost across the cut is 5, proving that a ﬂow of 5 is maximum Looking at the graph, we see that s has edges of capacities 4 and 2 leaving it, and t has edges of capacities 3 and 3 entering it. So perhaps the maximum ﬂow could be 6 instead of 5. However, Figure 9.40 shows how we can prove that the maximum ﬂow is 5. We cut the graph into two parts; one part contains s and some other vertices; the other part contains t. Since ﬂow must cross through the cut, the total capacity of all edges (u, v) where u is in s’s partition and v is in t’s partition is a bound on the maximum ﬂow. These edges are (a, c) and (d, t), with total capacity 5, so the maximum ﬂow cannot exceed 5. Any graph has a large number of cuts; the cut with minimum total capacity provides a bound on the maximum ﬂow, and as it turns out (but it is not immediately obvious), the minimum cut capacity is exactly equal to the maximum ﬂow.

Chapter 9 Graph Algorithms 9.4.1 A Simple Maximum-Flow Algorithm A ﬁrst attempt to solve the problem proceeds in stages. We start with our graph, G, and construct a ﬂow graph Gf. Gf tells the ﬂow that has been attained at any stage in the algorithm. Initially all edges in Gf have no ﬂow, and we hope that when the algorithm terminates, Gf contains a maximum ﬂow. We also construct a graph, Gr, called the residual graph. Gr tells, for each edge, how much more ﬂow can be added. We can calculate this by subtracting the current ﬂow from the capacity for each edge. An edge in Gr is known as a residual edge. At each stage, we ﬁnd a path in Gr from s to t. This path is known as an augmenting path. The minimum edge on this path is the amount of ﬂow that can be added to every edge on the path. We do this by adjusting Gf and recomputing Gr. When we ﬁnd no path from s to t in Gr, we terminate. This algorithm is nondeterministic, in that we are free to choose any path from s to t; obviously some choices are better than others, and we will address this issue later. We will run this algorithm on our example. The graphs below are G, Gf, Gr, respectively. Keep in mind that there is a slight ﬂaw in this algorithm. The initial conﬁguration is in Figure 9.41. There are many paths from s to t in the residual graph. Suppose we select s, b, d, t. Then we can send two units of ﬂow through every edge on this path. We will adopt the convention that once we have ﬁlled (saturated) an edge, it is removed from the residual graph. We then obtain Figure 9.42. Next, we might select the path s, a, c, t, which also allows two units of ﬂow. Making the required adjustments gives the graphs in Figure 9.43. The only path left to select is s, a, d, t, which allows one unit of ﬂow. The resulting graphs are shown in Figure 9.44. The algorithm terminates at this point, because t is unreachable from s. The resulting ﬂow of 5 happens to be the maximum. To see what the problem is, suppose that with our initial graph, we chose the path s, a, d, t. This path allows three units of ﬂow and thus seems to be a good choice. The result of this choice, however, leaves only one path from s to t in the residual graph; it allows one more unit of ﬂow, and thus, our algorithm has s a b c d t s a b c d t s a b c d t Figure 9.41 Initial stages of the graph, ﬂow graph, and residual graph

9.4 Network Flow Problems s a b c d t s a b c d t s a b c d t Figure 9.42 G, Gf, Gr after two units of ﬂow added along s, b, d, t s a b c d t s a b c d t s a b c d t Figure 9.43 G, Gf, Gr after two units of ﬂow added along s, a, c, t s a b c d t s a b c d t s a b c d t Figure 9.44 G, Gf, Gr after one unit of ﬂow added along s, a, d, t—algorithm terminates

Chapter 9 Graph Algorithms s a b c d t s a b c d t s a b c d t Figure 9.45 G, Gf, Gr if initial action is to add three units of ﬂow along s, a, d, t—algorithm terminates after one more step with suboptimal solution s a b c d t s a b c d t s a b c d t Figure 9.46 Graphs after three units of ﬂow added along s, a, d, t using correct algorithm failed to ﬁnd an optimal solution. This is an example of a greedy algorithm that does not work. Figure 9.45 shows why the algorithm fails. In order to make this algorithm work, we need to allow the algorithm to change its mind. To do this, for every edge (v, w) with ﬂow fv,w in the ﬂow graph, we will add an edge in the residual graph (w, v) of capacity fv,w. In effect, we are allowing the algorithm to undo its decisions by sending ﬂow back in the opposite direction. This is best seen by example. Starting from our original graph and selecting the augmenting path s, a, d, t, we obtain the graphs in Figure 9.46. Notice that in the residual graph, there are edges in both directions between a and d. Either one more unit of ﬂow can be pushed from a to d, or up to three units can be pushed back—we can undo ﬂow. Now the algorithm ﬁnds the augmenting path s, b, d, a, c, t, of ﬂow 2. By pushing two units of ﬂow from d to a, the algorithm takes two units of ﬂow away from the edge (a, d) and is essentially changing its mind. Figure 9.47 shows the new graphs.

9.4 Network Flow Problems s a b c d t s a b c d t s a b c d t Figure 9.47 Graphs after two units of ﬂow added along s, b, d, a, c, t using correct algorithm s a b c d t s a b c d t Figure 9.48 The vertices reachable from s in the residual graph form one side of a cut; the unreachables form the other side of the cut. There is no augmenting path in this graph, so the algorithm terminates. Note that the same result would occur if at Figure 9.46, the augmenting path s, a, c, t was chosen which allows one unit of ﬂow, because then a subsequent augmenting path could be found. It is easy to see that if the algorithm terminates, then it must terminate with a maximum ﬂow. Termination implies that there is no path from s to t in the residual graph. So cut the residual graph, putting the vertices reachable from s on one side, and the unreachables (which include t) on the other side. Figure 9.48 shows the cut. Clearly any edges in the original graph G that cross the cut must be saturated; otherwise, there would be residual ﬂow remaining on one of the edges, which would then imply an edge that crosses the cut (in the wrong disallowed direction) in Gr. But that means that the ﬂow in G is exactly equal to the capacity of a cut in G; hence we have a maximum ﬂow. If the edge costs in the graph are integers, then the algorithm must terminate; each augmentation adds a unit of ﬂow, so we eventually reach the maximum ﬂow, though there

Chapter 9 Graph Algorithms s a b t Figure 9.49 The classic bad case for augmenting is no guarantee that this will be efﬁcient. In particular, if the capacities are all integers and the maximum ﬂow is f, then, since each augmenting path increases the ﬂow value by at least 1, f stages sufﬁce, and the total running time is O(f ·|E|), since an augmenting path can be found in O(|E|) time by an unweighted shortest-path algorithm. The classic example of why this is a bad running time is shown by the graph in Figure 9.49. The maximum ﬂow is seen by inspection to be 2,000,000 by sending 1,000,000 down each side. Random augmentations could continually augment along a path that includes the edge connected by a and b. If this were to occur repeatedly, 2,000,000 augmentations would be required, when we could get by with only 2. A simple method to get around this problem is always to choose the augmenting path that allows the largest increase in ﬂow. Finding such a path is similar to solving a weighted shortest-path problem and a single-line modiﬁcation to Dijkstra’s algorithm will do the trick. If capmax is the maximum edge capacity, then one can show that O(|E| log capmax) augmentations will sufﬁce to ﬁnd the maximum ﬂow. In this case, since O(|E| log |V|) time is used for each calculation of an augmenting path, a total bound of O(|E|2 log |V| log capmax) is obtained. If the capacities are all small integers, this reduces to O(|E|2 log |V|). Another way to choose augmenting paths is always to take the path with the least number of edges, with the plausible expectation that by choosing a path in this manner, it is less likely that a small, ﬂow-restricting edge will turn up on the path. With this rule, each augmenting step computes the shortest unweighted path from s to t in the residual graph, so assume that each vertex in the graph maintains dv, representing the shortest-path distance from s to v in the residual graph. Each augmenting step can add new edges into the residual graph, but it is clear that no dv can decrease, because an edge is added in the opposite direction of an existing shortest path. Each augmenting step saturates at least one edge. Suppose edge (u, v) is saturated; at that point, u had distance du and v had distance dv = du + 1; then (u, v) was removed from

9.5 Minimum Spanning Tree the residual graph, and edge (v, u) was added. (u, v) cannot reappear in the residual graph again, unless and until (v, u) appears in a future augmenting path. But if it does, then the distance to u at that point must be dv + 1, which would be two higher than at the time (u, v) was previously removed. This means that each time (u, v) reappears, u’s distance goes up by 2. This means that any edge can reappear at most |V|/2 times. Each augmentation causes some edge to reappear so the number of augmentations is O(|E||V|). Each step takes O(|E|), due to the unweighted shortest-path calculation, yielding an O(|E|2|V|) bound on the running time. Further data structure improvements are possible to this algorithm, and there are several, more complicated, algorithms. A long history of improved bounds has lowered the current best-known bound for this problem. Although no O(|E||V|) algorithm has been reported yet, algorithms with O(|E||V| log(|V|2/|E|)) and O(|E||V| + |V|2+ε) bounds have been discovered (see the references). There are also a host of very good bounds for special cases. For instance, O(|E||V|1/2) time ﬁnds a maximum ﬂow in a graph, having the property that all vertices except the source and sink have either a single incoming edge of capacity 1 or a single outgoing edge of capacity 1. These graphs occur in many applications. The analyses required to produce these bounds are rather intricate, and it is not clear how the worst-case results relate to the running times encountered in practice. A related, even more difﬁcult problem is the min-cost ﬂow problem. Each edge has not only a capacity, but also a cost per unit of ﬂow. The problem is to ﬁnd, among all maximum ﬂows, the one ﬂow of minimum cost. Both of these problems are being actively researched. 9.5 Minimum Spanning Tree The next problem we will consider is that of ﬁnding a minimum spanning tree in an undirected graph. The problem makes sense for directed graphs but appears to be more difﬁcult. Informally, a minimum spanning tree of an undirected graph G is a tree formed from graph edges that connects all the vertices of G at lowest total cost. A minimum spanning tree exists if and only if G is connected. Although a robust algorithm should report the case that G is unconnected, we will assume that G is connected and leave the issue of robustness as an exercise to the reader. In Figure 9.50 the second graph is a minimum spanning tree of the ﬁrst (it happens to be unique, but this is unusual). Notice that the number of edges in the minimum spanning tree is |V| −1. The minimum spanning tree is a tree because it is acyclic, it is spanning because it covers every vertex, and it is minimum for the obvious reason. If we need to wire a house with a minimum of cable (assuming no other electrical constraints), then a minimum spanning tree problem needs to be solved. For any spanning tree T, if an edge e that is not in T is added, a cycle is created. The removal of any edge on the cycle reinstates the spanning tree property. The cost of the spanning tree is lowered if e has lower cost than the edge that was removed. If, as a spanning tree is created, the edge that is added is the one of minimum cost that avoids creation of a cycle, then the cost of the resulting spanning tree cannot be improved, because any replacement edge would have cost at least as much as an edge already in the spanning tree.

Chapter 9 Graph Algorithms v 1 v 2 v 3 v 4 v 5 v 6 v 7 v 1 v 2 v 3 v 4 v 5 v 6 v 7 Figure 9.50 A graph G and its minimum spanning tree This shows that greed works for the minimum spanning tree problem. The two algorithms we present differ in how a minimum edge is selected. 9.5.1 Prim’s Algorithm One way to compute a minimum spanning tree is to grow the tree in successive stages. In each stage, one node is picked as the root, and we add an edge, and thus an associated vertex, to the tree. At any point in the algorithm, we can see that we have a set of vertices that have already been included in the tree; the rest of the vertices have not. The algorithm then ﬁnds, at each stage, a new vertex to add to the tree by choosing the edge (u, v) such that the cost of (u, v) is the smallest among all edges where u is in the tree and v is not. Figure 9.51 shows how this algorithm would build the minimum spanning tree, starting from v1. Initially, v1 is in the tree as a root with no edges. Each step adds one edge and one vertex to the tree. We can see that Prim’s algorithm is essentially identical to Dijkstra’s algorithm for shortest paths. As before, for each vertex we keep values dv and pv and an indication of whether it is known or unknown. dv is the weight of the shortest edge connecting v to a known vertex, and pv, as before, is the last vertex to cause a change in dv. The rest of the algorithm is exactly the same, with the exception that since the deﬁnition of dv is different, so is the

9.5 Minimum Spanning Tree v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 Figure 9.51 Prim’s algorithm after each stage v known dv pv v1 F v2 F ∞ v3 F ∞ v4 F ∞ v5 F ∞ v6 F ∞ v7 F ∞ Figure 9.52 Initial conﬁguration of table used in Prim’s algorithm update rule. For this problem, the update rule is even simpler than before: After a vertex v is selected, for each unknown w adjacent to v, dw = min(dw, cw,v). The initial conﬁguration of the table is shown in Figure 9.52. v1 is selected, and v2, v3, and v4 are updated. The table resulting from this is shown in Figure 9.53. The next vertex selected is v4. Every vertex is adjacent to v4. v1 is not examined, because it is known. v2 is unchanged, because it has dv = 2 and the edge cost from v4 to v2 is 3; all the rest are updated. Figure 9.54 shows the resulting table. The next vertex chosen is v2 (arbitrarily breaking a tie). This does not affect any distances. Then v3 is chosen, which affects the distance in v6, producing Figure 9.55. Figure 9.56 results from the selection of v7, which forces v6 and v5 to be adjusted. v6 and then v5 are selected, completing the algorithm.

Chapter 9 Graph Algorithms v known dv pv v1 T v2 F v1 v3 F v1 v4 F v1 v5 F ∞ v6 F ∞ v7 F ∞ Figure 9.53 The table after v1 is declared known v known dv pv v1 T v2 F v1 v3 F v4 v4 T v1 v5 F v4 v6 F v4 v7 F v4 Figure 9.54 The table after v4 is declared known v known dv pv v1 T v2 T v1 v3 T v4 v4 T v1 v5 F v4 v6 F v3 v7 F v4 Figure 9.55 The table after v2 and then v3 are declared known The ﬁnal table is shown in Figure 9.57. The edges in the spanning tree can be read from the table: (v2, v1), (v3, v4), (v4, v1), (v5, v7), (v6, v7), (v7, v4). The total cost is 16. The entire implementation of this algorithm is virtually identical to that of Dijkstra’s algorithm, and everything that was said about the analysis of Dijkstra’s algorithm applies here. Be aware that Prim’s algorithm runs on undirected graphs, so when coding it,

9.5 Minimum Spanning Tree v known dv pv v1 T v2 T v1 v3 T v4 v4 T v1 v5 F v7 v6 F v7 v7 T v4 Figure 9.56 The table after v7 is declared known v known dv pv v1 T v2 T v1 v3 T v4 v4 T v1 v5 T v7 v6 T v7 v7 T v4 Figure 9.57 The table after v6 and v5 are selected (Prim’s algorithm terminates) remember to put every edge in two adjacency lists. The running time is O(|V|2) without heaps, which is optimal for dense graphs, and O(|E| log |V|) using binary heaps, which is good for sparse graphs. 9.5.2 Kruskal’s Algorithm A second greedy strategy is continually to select the edges in order of smallest weight and accept an edge if it does not cause a cycle. The action of the algorithm on the graph in the preceding example is shown in Figure 9.58. Formally, Kruskal’s algorithm maintains a forest—a collection of trees. Initially, there are |V| single-node trees. Adding an edge merges two trees into one. When the algorithm terminates, there is only one tree, and this is the minimum spanning tree. Figure 9.59 shows the order in which edges are added to the forest. The algorithm terminates when enough edges are accepted. It turns out to be simple to decide whether edge (u, v) should be accepted or rejected. The appropriate data structure is the union/ﬁnd algorithm from Chapter 8. The invariant we will use is that at any point in the process, two vertices belong to the same set if and only if they are connected in the current spanning forest. Thus, each

Chapter 9 Graph Algorithms Edge Weight Action (v1, v4) Accepted (v6, v7) Accepted (v1, v2) Accepted (v3, v4) Accepted (v2, v4) Rejected (v1, v3) Rejected (v4, v7) Accepted (v3, v6) Rejected (v5, v7) Accepted Figure 9.58 Action of Kruskal’s algorithm on G v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 v1 v2 v3 v4 v5 v6 v7 Figure 9.59 Kruskal’s algorithm after each stage vertex is initially in its own set. If u and v are in the same set, the edge is rejected, because since they are already connected, adding (u, v) would form a cycle. Otherwise, the edge is accepted, and a union is performed on the two sets containing u and v. It is easy to see that this maintains the set invariant, because once the edge (u, v) is added to the spanning forest, if w was connected to u and x was connected to v, then x and w must now be connected, and thus belong in the same set. The edges could be sorted to facilitate the selection, but building a heap in linear time is a much better idea. Then deleteMins give the edges to be tested in order. Typically, only a small fraction of the edges need to be tested before the algorithm can terminate, although

9.6 Applications of Depth-First Search ArrayList<Edge> kruskal( List<Edge> edges, int numVertices ) { DisjSets ds = new DisjSets( numVertices ); PriorityQueue<Edge> pq = new PriorityQueue<>( edges ); List<Edge> mst = new ArrayList<>( ); while( mst.size( ) != numVertices - 1 ) { Edge e = pq.deleteMin( ); // Edge e = (u, v) SetType uset = ds.find( e.getu( ) ); SetType vset = ds.find( e.getv( ) ); if( uset != vset ) { // Accept the edge mst.add( e ); ds.union( uset, vset ); } } return mst; } Figure 9.60 Pseudocode for Kruskal’s algorithm it is always possible that all the edges must be tried. For instance, if there was an extra vertex v8 and edge (v5, v8) of cost 100, all the edges would have to be examined. Method kruskal in Figure 9.60 ﬁnds a minimum spanning tree. The worst-case running time of this algorithm is O(|E| log |E|), which is dominated by the heap operations. Notice that since |E| = O(|V|2), this running time is actually O(|E| log |V|). In practice, the algorithm is much faster than this time bound would indicate. 9.6 Applications of Depth-First Search Depth-ﬁrst search is a generalization of preorder traversal. Starting at some vertex, v, we process v and then recursively traverse all vertices adjacent to v. If this process is performed on a tree, then all tree vertices are systematically visited in a total of O(|E|) time, since |E| = (|V|). If we perform this process on an arbitrary graph, we need to be careful to avoid cycles. To do this, when we visit a vertex v, we mark it visited, since now we have been there, and recursively call depth-ﬁrst search on all adjacent vertices that are not already marked. We implicitly assume that for undirected graphs every edge (v, w) appears twice in the adjacency lists: once as (v, w) and once as (w, v). The procedure in Figure 9.61 performs a depth-ﬁrst search (and does absolutely nothing else) and is a template for the general style.

Chapter 9 Graph Algorithms void dfs( Vertex v ) { v.visited = true; for each Vertex w adjacent to v if( !w.visited ) dfs( w ); } Figure 9.61 Template for depth-ﬁrst search (pseudocode) For each vertex, the ﬁeld visited is initialized to false. By recursively calling the procedures only on nodes that have not been visited, we guarantee that we do not loop indeﬁnitely. If the graph is undirected and not connected, or directed and not strongly connected, this strategy might fail to visit some nodes. We then search for an unmarked node, apply a depth-ﬁrst traversal there, and continue this process until there are no unmarked nodes.2 Because this strategy guarantees that each edge is encountered only once, the total time to perform the traversal is O(|E| + |V|), as long as adjacency lists are used. 9.6.1 Undirected Graphs An undirected graph is connected if and only if a depth-ﬁrst search starting from any node visits every node. Because this test is so easy to apply, we will assume that the graphs we deal with are connected. If they are not, then we can ﬁnd all the connected components and apply our algorithm on each of these in turn. As an example of depth-ﬁrst search, suppose in the graph of Figure 9.62 we start at vertex A. Then we mark A as visited and call dfs(B) recursively. dfs(B) marks B as visited and calls dfs(C) recursively. dfs(C) marks C as visited and calls dfs(D) recursively. dfs(D) sees both A and B, but both of these are marked, so no recursive calls are made. dfs(D) also sees that C is adjacent but marked, so no recursive call is made there, and dfs(D) returns back to dfs(C). dfs(C) sees B adjacent, ignores it, ﬁnds a previously unseen vertex E adjacent, and thus calls dfs(E). dfs(E) marks E, ignores A and C, and returns to dfs(C). dfs(C) returns to dfs(B). dfs(B) ignores both A and D and returns. dfs(A) ignores both D and E and returns. (We have actually touched every edge twice, once as (v, w) and again as (w, v), but this is really once per adjacency list entry.) We graphically illustrate these steps with a depth-ﬁrst spanning tree. The root of the tree is A, the ﬁrst vertex visited. Each edge (v, w) in the graph is present in the tree. If, when we process (v, w), we ﬁnd that w is unmarked, or if, when we process (w, v), we ﬁnd that v is unmarked, we indicate this with a tree edge. If, when we process (v, w), we ﬁnd that w is already marked, and when processing (w, v), we ﬁnd that v is already marked, we draw 2 An efﬁcient way of implementing this is to begin the depth-ﬁrst search at v1. If we need to restart the depth-ﬁrst search, we examine the sequence vk, vk+1, . . . for an unmarked vertex, where vk−1 is the vertex where the last depth-ﬁrst search was started. This guarantees that throughout the algorithm, only O(|V|) is spent looking for vertices where new depth-ﬁrst search trees can be started.

9.6 Applications of Depth-First Search A B C D E Figure 9.62 An undirected graph A B C D E Figure 9.63 Depth-ﬁrst search of previous graph a dashed line, which we will call a back edge, to indicate that this “edge” is not really part of the tree. The depth-ﬁrst search of the graph in Figure 9.62 is shown in Figure 9.63. The tree will simulate the traversal we performed. A preorder numbering of the tree, using only tree edges, tells us the order in which the vertices were marked. If the graph is not connected, then processing all nodes (and edges) requires several calls to dfs, and each generates a tree. This entire collection is a depth-ﬁrst spanning forest.

Chapter 9 Graph Algorithms 9.6.2 Biconnectivity A connected undirected graph is biconnected if there are no vertices whose removal disconnects the rest of the graph. The graph in the example above is biconnected. If the nodes are computers and the edges are links, then if any computer goes down, network mail is unaffected, except, of course, at the down computer. Similarly, if a mass transit system is biconnected, users always have an alternate route should some terminal be disrupted. If a graph is not biconnected, the vertices whose removal would disconnect the graph are known as articulation points. These nodes are critical in many applications. The graph in Figure 9.64 is not biconnected: C and D are articulation points. The removal of C would disconnect G, and the removal of D would disconnect E and F, from the rest of the graph. Depth-ﬁrst search provides a linear-time algorithm to ﬁnd all articulation points in a connected graph. First, starting at any vertex, we perform a depth-ﬁrst search and number the nodes as they are visited. For each vertex v, we call this preorder number Num(v). Then, for every vertex v in the depth-ﬁrst search spanning tree, we compute the lowest-numbered vertex, which we call Low(v), that is reachable from v by taking zero or more tree edges and then possibly one back edge (in that order). The depth-ﬁrst search tree in Figure 9.65 shows the preorder number ﬁrst, and then the lowest-numbered vertex reachable under the rule described above. The lowest-numbered vertex reachable by A, B, and C is vertex 1 (A), because they can all take tree edges to D and then one back edge back to A. We can efﬁciently compute Low B A C D G E F Figure 9.64 A graph with articulation points C and D

9.6 Applications of Depth-First Search F, 6/4 E, 5/4 D, 4/1 C, 3/1 B, 2/1 A, 1/1 G, 7/7 Figure 9.65 Depth-ﬁrst tree for previous graph, with Num and Low by performing a postorder traversal of the depth-ﬁrst spanning tree. By the deﬁnition of Low, Low(v) is the minimum of 1. Num(v) 2. the lowest Num(w) among all back edges (v, w) 3. the lowest Low(w) among all tree edges (v, w) The ﬁrst condition is the option of taking no edges, the second way is to choose no tree edges and a back edge, and the third way is to choose some tree edges and possibly a back edge. This third method is succinctly described with a recursive call. Since we need to evaluate Low for all the children of v before we can evaluate Low(v), this is a postorder traversal. For any edge (v, w), we can tell whether it is a tree edge or a back edge merely by checking Num(v) and Num(w). Thus, it is easy to compute Low(v): We merely scan down v’s adjacency list, apply the proper rule, and keep track of the minimum. Doing all the computation takes O(|E| + |V|) time. All that is left to do is to use this information to ﬁnd articulation points. The root is an articulation point if and only if it has more than one child, because if it has two children, removing the root disconnects nodes in different subtrees, and if it has only one child,

Chapter 9 Graph Algorithms F, 4/2 E, 3/2 D, 2/1 C, 1/1 B, 6/1 A, 5/1 G, 7/7 Figure 9.66 Depth-ﬁrst tree that results if depth-ﬁrst search starts at C removing the root merely disconnects the root. Any other vertex v is an articulation point if and only if v has some child w such that Low(w) ≥Num(v). Notice that this condition is always satisﬁed at the root; hence the need for a special test. The if part of the proof is clear when we examine the articulation points that the algorithm determines, namely, C and D. D has a child E, and Low(E) ≥Num(D), since both are 4. Thus, there is only one way for E to get to any node above D, and that is by going through D. Similarly, C is an articulation point, because Low(G) ≥Num(C). To prove that this algorithm is correct, one must show that the only if part of the assertion is true (that is, this ﬁnds all articulation points). We leave this as an exercise. As a second example, we show (Figure 9.66) the result of applying this algorithm on the same graph, starting the depth-ﬁrst search at C. We close by giving pseudocode to implement this algorithm. We will assume that Vertex contains the data ﬁelds visited (initialized to false), num, low, and parent. We will also keep a (Graph) class variable called counter, which is initialized to 1, to assign the preorder traversal numbers, num. We also leave out the easily implemented test for the root. As we have already stated, this algorithm can be implemented by performing a preorder traversal to compute Num and then a postorder traversal to compute Low. A third traversal can be used to check which vertices satisfy the articulation point criteria. Performing three traversals, however, would be a waste. The ﬁrst pass is shown in Figure 9.67. The second and third passes, which are postorder traversals, can be implemented by the code in Figure 9.68. The last if statement handles a special case. If w is adjacent to v, then the recursive call to w will ﬁnd v adjacent to w. This is not a back edge, only an edge that has already been considered and needs to be ignored. Otherwise, the procedure computes the minimum of the various low and num entries, as speciﬁed by the algorithm. There is no rule that a traversal must be either preorder or postorder. It is possible to do processing both before and after the recursive calls. The procedure in Figure 9.69 combines the two routines assignNum and assignLow in a straightforward manner to produce the procedure findArt.

9.6 Applications of Depth-First Search // Assign Num and compute parents void assignNum( Vertex v ) { v.num = counter++; v.visited = true; for each Vertex w adjacent to v if( !w.visited ) { w.parent = v; assignNum( w ); } } Figure 9.67 Routine to assign Num to vertices (pseudocode) // Assign low; also check for articulation points. void assignLow( Vertex v ) { v.low = v.num; // Rule 1 for each Vertex w adjacent to v { if( w.num > v.num ) // Forward edge { assignLow( w ); if( w.low >= v.num ) System.out.println( v + " is an articulation point" ); v.low = min( v.low, w.low ); // Rule 3 } else if( v.parent != w ) // Back edge v.low = min( v.low, w.num ); // Rule 2 } } Figure 9.68 Pseudocode to compute Low and to test for articulation points (test for the root is omitted) 9.6.3 Euler Circuits Consider the three ﬁgures in Figure 9.70. A popular puzzle is to reconstruct these ﬁgures using a pen, drawing each line exactly once. The pen may not be lifted from the paper while the drawing is being performed. As an extra challenge, make the pen ﬁnish at the same point at which it started. This puzzle has a surprisingly simple solution. Stop reading if you would like to try to solve it.

Chapter 9 Graph Algorithms void findArt( Vertex v ) { v.visited = true; v.low = v.num = counter++; // Rule 1 for each Vertex w adjacent to v { if( !w.visited ) // Forward edge { w.parent = v; findArt( w ); if( w.low >= v.num ) System.out.println( v + " is an articulation point" ); v.low = min( v.low, w.low ); // Rule 3 } else if( v.parent != w ) // Back edge v.low = min( v.low, w.num ); // Rule 2 } } Figure 9.69 Testing for articulation points in one depth-ﬁrst search (test for the root is omitted) (pseudocode) Figure 9.70 Three drawings The ﬁrst ﬁgure can be drawn only if the starting point is the lower left- or right-hand corner, and it is not possible to ﬁnish at the starting point. The second ﬁgure is easily drawn with the ﬁnishing point the same as the starting point, but the third ﬁgure cannot be drawn at all within the parameters of the puzzle. We can convert this problem to a graph theory problem by assigning a vertex to each intersection. Then the edges can be assigned in the natural manner, as in Figure 9.71. After this conversion is performed, we must ﬁnd a path in the graph that visits every edge exactly once. If we are to solve the “extra challenge,” then we must ﬁnd a cycle that visits every edge exactly once. This graph problem was solved in 1736 by Euler and marked the beginning of graph theory. The problem is thus commonly referred to as an Euler path (sometimes Euler tour) or Euler circuit problem, depending on the speciﬁc problem

9.6 Applications of Depth-First Search Figure 9.71 Conversion of puzzle to graph statement. The Euler tour and Euler circuit problems, though slightly different, have the same basic solution. Thus, we will consider the Euler circuit problem in this section. The ﬁrst observation that can be made is that an Euler circuit, which must end on its starting vertex, is possible only if the graph is connected and each vertex has an even degree (number of edges). This is because, on the Euler circuit, a vertex is entered and then left. If any vertex v has odd degree, then eventually we will reach the point where only one edge into v is unvisited, and taking it will strand us at v. If exactly two vertices have odd degree, an Euler tour, which must visit every edge but need not return to its starting vertex, is still possible if we start at one of the odd-degree vertices and ﬁnish at the other. If more than two vertices have odd degree, then an Euler tour is not possible. The observations of the preceding paragraph provide us with a necessary condition for the existence of an Euler circuit. It does not, however, tell us that all connected graphs that satisfy this property must have an Euler circuit, nor does it give us guidance on how to ﬁnd one. It turns out that the necessary condition is also sufﬁcient. That is, any connected graph, all of whose vertices have even degree, must have an Euler circuit. Furthermore, a circuit can be found in linear time. We can assume that we know that an Euler circuit exists, since we can test the necessary and sufﬁcient condition in linear time. Then the basic algorithm is to perform a depth-ﬁrst search. There are a surprisingly large number of “obvious” solutions that do not work. Some of these are presented in the exercises. The main problem is that we might visit a portion of the graph and return to the starting point prematurely. If all the edges coming out of the start vertex have been used up, then part of the graph is untraversed. The easiest way to ﬁx this is to ﬁnd the ﬁrst vertex on this path that has an untraversed edge, and perform another depth-ﬁrst search. This will give another circuit, which can be spliced into the original. This is continued until all edges have been traversed. As an example, consider the graph in Figure 9.72. It is easily seen that this graph has an Euler circuit. Suppose we start at vertex 5, and traverse the circuit 5, 4, 10, 5. Then we are stuck, and most of the graph is still untraversed. The situation is shown in Figure 9.73. We then continue from vertex 4, which still has unexplored edges. A depth-ﬁrst search might come up with the path 4, 1, 3, 7, 4, 11, 10, 7, 9, 3, 4. If we splice this path into the previous path of 5, 4, 10, 5, then we get a new path of 5, 4, 1, 3, 7, 4, 11, 10, 7, 9, 3, 4, 10, 5.

Chapter 9 Graph Algorithms Figure 9.72 Graph for Euler circuit problem Figure 9.73 Graph remaining after 5, 4, 10, 5 Figure 9.74 Graph after the path 5, 4, 1, 3, 7, 4, 11, 10, 7, 9, 3, 4, 10, 5 The graph that remains after this is shown in Figure 9.74. Notice that in this graph all the vertices must have even degree, so we are guaranteed to ﬁnd a cycle to add. The remaining graph might not be connected, but this is not important. The next vertex on the path that has untraversed edges is vertex 3. A possible circuit would then be 3, 2, 8, 9, 6, 3. When spliced in, this gives the path 5, 4, 1, 3, 2, 8, 9, 6, 3, 7, 4, 11, 10, 7, 9, 3, 4, 10, 5. The graph that remains is in Figure 9.75. On this path, the next vertex with an untraversed edge is 9, and the algorithm ﬁnds the circuit 9, 12, 10, 9. When this is added to the current path, a circuit of 5, 4, 1, 3, 2, 8, 9, 12, 10, 9, 6, 3, 7, 4, 11, 10, 7, 9, 3, 4, 10, 5 is obtained. As all the edges are traversed, the algorithm terminates with an Euler circuit.

9.6 Applications of Depth-First Search Figure 9.75 Graph remaining after the path 5, 4, 1, 3, 2, 8, 9, 6, 3, 7, 4, 11, 10, 7, 9, 3, 4, 10, 5 To make this algorithm efﬁcient, we must use appropriate data structures. We will sketch some of the ideas, leaving the implementation as an exercise. To make splicing simple, the path should be maintained as a linked list. To avoid repetitious scanning of adjacency lists, we must maintain, for each adjacency list, the last edge scanned. When a path is spliced in, the search for a new vertex from which to perform the next depthﬁrst search must begin at the start of the splice point. This guarantees that the total work performed on the vertex search phase is O(|E|) during the entire life of the algorithm. With the appropriate data structures, the running time of the algorithm is O(|E| + |V|). A very similar problem is to ﬁnd a simple cycle, in an undirected graph, that visits every vertex. This is known as the Hamiltonian cycle problem. Although it seems almost identical to the Euler circuit problem, no efﬁcient algorithm for it is known. We shall see this problem again in Section 9.7. 9.6.4 Directed Graphs Using the same strategy as with undirected graphs, directed graphs can be traversed in linear time, using depth-ﬁrst search. If the graph is not strongly connected, a depth-ﬁrst search starting at some node might not visit all nodes. In this case we repeatedly perform depth-ﬁrst searches, starting at some unmarked node, until all vertices have been visited. As an example, consider the directed graph in Figure 9.76. We arbitrarily start the depth-ﬁrst search at vertex B. This visits vertices B, C, A, D, E, and F. We then restart at some unvisited vertex. Arbitrarily, we start at H, which visits J and I. Finally, we start at G, which is the last vertex that needs to be visited. The corresponding depth-ﬁrst search tree is shown in Figure 9.77. The dashed arrows in the depth-ﬁrst spanning forest are edges (v, w) for which w was already marked at the time of consideration. In undirected graphs, these are always back edges, but, as we can see, there are three types of edges that do not lead to new vertices. First, there are back edges, such as (A, B) and (I, H). There are also forward edges, such as (C, D) and (C, E), that lead from a tree node to a descendant. Finally, there are cross edges, such as (F, C) and (G, F), which connect two tree nodes that are not directly related. Depthﬁrst search forests are generally drawn with children and new trees added to the forest from

Chapter 9 Graph Algorithms A B D E C F G H I J Figure 9.76 A directed graph E D A C B F I J H G Figure 9.77 Depth-ﬁrst search of previous graph left to right. In a depth-ﬁrst search of a directed graph drawn in this manner, cross edges always go from right to left. Some algorithms that use depth-ﬁrst search need to distinguish between the three types of nontree edges. This is easy to check as the depth-ﬁrst search is being performed, and it is left as an exercise. One use of depth-ﬁrst search is to test whether or not a directed graph is acyclic. The rule is that a directed graph is acyclic if and only if it has no back edges. (The graph above has back edges, and thus is not acyclic.) The reader may remember that a topological sort

9.6 Applications of Depth-First Search can also be used to determine whether a graph is acyclic. Another way to perform topological sorting is to assign the vertices topological numbers N, N −1, . . . , 1 by postorder traversal of the depth-ﬁrst spanning forest. As long as the graph is acyclic, this ordering will be consistent. 9.6.5 Finding Strong Components By performing two depth-ﬁrst searches, we can test whether a directed graph is strongly connected, and if it is not, we can actually produce the subsets of vertices that are strongly connected to themselves. This can also be done in only one depth-ﬁrst search, but the method used here is much simpler to understand. First, a depth-ﬁrst search is performed on the input graph G. The vertices of G are numbered by a postorder traversal of the depth-ﬁrst spanning forest, and then all edges in G are reversed, forming Gr. The graph in Figure 9.78 represents Gr for the graph G shown in Figure 9.76; the vertices are shown with their numbers. The algorithm is completed by performing a depth-ﬁrst search on Gr, always starting a new depth-ﬁrst search at the highest-numbered vertex. Thus, we begin the depth-ﬁrst search of Gr at vertex G, which is numbered 10. This leads nowhere, so the next search is started at H. This call visits I and J. The next call starts at B and visits A, C, and F. The next calls after this are dfs(D) and ﬁnally dfs(E). The resulting depth-ﬁrst spanning forest is shown in Figure 9.79. A,3 B,6 D,2 E,1 C,4 F,5 G,10 H,9 I,7 J,8 Figure 9.78 Gr numbered by postorder traversal of G (from Figure 9.76)

Chapter 9 Graph Algorithms G J I H F C A B D E Figure 9.79 Depth-ﬁrst search of Gr—strong components are {G}, {H, I, J}, {B, A, C, F}, {D}, {E} Each of the trees (this is easier to see if you completely ignore all nontree edges) in this depth-ﬁrst spanning forest forms a strongly connected component. Thus, for our example, the strongly connected components are {G}, {H, I, J}, {B, A, C, F}, {D}, and {E}. To see why this algorithm works, ﬁrst note that if two vertices v and w are in the same strongly connected component, then there are paths from v to w and from w to v in the original graph G, and hence also in Gr. Now, if two vertices v and w are not in the same depth-ﬁrst spanning tree of Gr, clearly they cannot be in the same strongly connected component. To prove that this algorithm works, we must show that if two vertices v and w are in the same depth-ﬁrst spanning tree of Gr, there must be paths from v to w and from w to v. Equivalently, we can show that if x is the root of the depth-ﬁrst spanning tree of Gr containing v, then there is a path from x to v and from v to x. Applying the same logic to w would then give a path from x to w and from w to x. These paths would imply paths from v to w and w to v (going through x). Since v is a descendant of x in Gr’s depth-ﬁrst spanning tree, there is a path from x to v in Gr and thus a path from v to x in G. Furthermore, since x is the root, x has the higher postorder number from the ﬁrst depth-ﬁrst search. Therefore, during the ﬁrst depth-ﬁrst search, all the work processing v was completed before the work at x was completed. Since there is a path from v to x, it follows that v must be a descendant of x in the spanning tree for G—otherwise v would ﬁnish after x. This implies a path from x to v in G and completes the proof. 9.7 Introduction to NP-Completeness In this chapter, we have seen solutions to a wide variety of graph theory problems. All these problems have polynomial running times, and with the exception of the network ﬂow problem, the running time is either linear or only slightly more than linear (O(|E| log |E|)). We have also mentioned, in passing, that for some problems certain variations seem harder than the original.

9.7 Introduction to NP-Completeness Recall that the Euler circuit problem, which ﬁnds a path that touches every edge exactly once, is solvable in linear time. The Hamiltonian cycle problem asks for a simple cycle that contains every vertex. No linear algorithm is known for this problem. The single-source unweighted shortest-path problem for directed graphs is also solvable in linear time. No linear-time algorithm is known for the corresponding longestsimple-path problem. The situation for these problem variations is actually much worse than we have described. Not only are no linear algorithms known for these variations, but there are no known algorithms that are guaranteed to run in polynomial time. The best known algorithms for these problems could take exponential time on some inputs. In this section we will take a brief look at this problem. This topic is rather complex, so we will only take a quick and informal look at it. Because of this, the discussion may be (necessarily) somewhat imprecise in places. We will see that there are a host of important problems that are roughly equivalent in complexity. These problems form a class called the NP-complete problems. The exact complexity of these NP-complete problems has yet to be determined and remains the foremost open problem in theoretical computer science. Either all these problems have polynomial-time solutions or none of them do. 9.7.1 Easy vs. Hard When classifying problems, the ﬁrst step is to examine the boundaries. We have already seen that many problems can be solved in linear time. We have also seen some O(log N) running times, but these either assume some preprocessing (such as input already being read or a data structure already being built) or occur on arithmetic examples. For instance, the gcd algorithm, when applied on two numbers M and N, takes O(log N) time. Since the numbers consist of log M and log N bits respectively, the gcd algorithm is really taking time that is linear in the amount or size of input. Thus, when we measure running time, we will be concerned with the running time as a function of the amount of input. Generally, we cannot expect better than linear running time. At the other end of the spectrum lie some truly hard problems. These problems are so hard that they are impossible. This does not mean the typical exasperated moan, which means that it would take a genius to solve the problem. Just as real numbers are not sufﬁcient to express a solution to x2 < 0, one can prove that computers cannot solve every problem that happens to come along. These “impossible” problems are called undecidable problems. One particular undecidable problem is the halting problem. Is it possible to have your Java compiler have an extra feature that not only detects syntax errors, but also all inﬁnite loops? This seems like a hard problem, but one might expect that if some very clever programmers spent enough time on it, they could produce this enhancement. The intuitive reason that this problem is undecidable is that such a program might have a hard time checking itself. For this reason, these problems are sometimes called recursively undecidable. If an inﬁnite loop–checking program could be written, surely it could be used to check itself. We could then produce a program called LOOP. LOOP takes as input a program

Chapter 9 Graph Algorithms P and runs P on itself. It prints out the phrase YES if P loops when run on itself. If P terminates when run on itself, a natural thing to do would be to print out NO. Instead of doing that, we will have LOOP go into an inﬁnite loop. What happens when LOOP is given itself as input? Either LOOP halts, or it does not halt. The problem is that both these possibilities lead to contradictions, in much the same way as does the phrase “This sentence is a lie.” By our deﬁnition, LOOP(P) goes into an inﬁnite loop if P(P) terminates. Suppose that when P = LOOP, P(P) terminates. Then, according to the LOOP program, LOOP(P) is obligated to go into an inﬁnite loop. Thus, we must have LOOP(LOOP) terminating and entering an inﬁnite loop, which is clearly not possible. On the other hand, suppose that when P = LOOP, P(P) enters an inﬁnite loop. Then LOOP(P) must terminate, and we arrive at the same set of contradictions. Thus, we see that the program LOOP cannot possibly exist. 9.7.2 The Class NP A few steps down from the horrors of undecidable problems lies the class NP. NP stands for nondeterministic polynomial-time. A deterministic machine, at each point in time, is executing an instruction. Depending on the instruction, it then goes to some next instruction, which is unique. A nondeterministic machine has a choice of next steps. It is free to choose any that it wishes, and if one of these steps leads to a solution, it will always choose the correct one. A nondeterministic machine thus has the power of extremely good (optimal) guessing. This probably seems like a ridiculous model, since nobody could possibly build a nondeterministic computer, and because it would seem to be an incredible upgrade to your standard computer (every problem might now seem trivial). We will see that nondeterminism is a very useful theoretical construct. Furthermore, nondeterminism is not as powerful as one might think. For instance, undecidable problems are still undecidable, even if nondeterminism is allowed. A simple way to check if a problem is in NP is to phrase the problem as a yes/no question. The problem is in NP if, in polynomial time, we can prove that any “yes” instance is correct. We do not have to worry about “no” instances, since the program always makes the right choice. Thus, for the Hamiltonian cycle problem, a “yes” instance would be any simple circuit in the graph that includes all the vertices. This is in NP, since, given the path, it is a simple matter to check that it is really a Hamiltonian cycle. Appropriately phrased questions, such as “Is there a simple path of length > K?” can also easily be checked and are in NP. Any path that satisﬁes this property can be checked trivially. The class NP includes all problems that have polynomial-time solutions, since obviously the solution provides a check. One would expect that since it is so much easier to check an answer than to come up with one from scratch, there would be problems in NP that do not have polynomial-time solutions. To date no such problem has been found, so it is entirely possible, though not considered likely by experts, that nondeterminism is not such an important improvement. The problem is that proving exponential lower bounds is an extremely difﬁcult task. The information theory bound technique, which we used to show that sorting requires (N log N) comparisons, does not seem to be adequate for the task, because the decision trees are not nearly large enough.

9.7 Introduction to NP-Completeness Notice also that not all decidable problems are in NP. Consider the problem of determining whether a graph does not have a Hamiltonian cycle. To prove that a graph has a Hamiltonian cycle is a relatively simple matter—we just need to exhibit one. Nobody knows how to show, in polynomial time, that a graph does not have a Hamiltonian cycle. It seems that one must enumerate all the cycles and check them one by one. Thus the Non–Hamiltonian cycle problem is not known to be in NP. 9.7.3 NP-Complete Problems Among all the problems known to be in NP, there is a subset, known as the NP-complete problems, which contains the hardest. An NP-complete problem has the property that any problem in NP can be polynomially reduced to it. A problem P1 can be reduced to P2 as follows: Provide a mapping so that any instance of P1 can be transformed to an instance of P2. Solve P2, and then map the answer back to the original. As an example, numbers are entered into a pocket calculator in decimal. The decimal numbers are converted to binary, and all calculations are performed in binary. Then the ﬁnal answer is converted back to decimal for display. For P1 to be polynomially reducible to P2, all the work associated with the transformations must be performed in polynomial time. The reason that NP-complete problems are the hardest NP problems is that a problem that is NP-complete can essentially be used as a subroutine for any problem in NP, with only a polynomial amount of overhead. Thus, if any NP-complete problem has a polynomial-time solution, then every problem in NP must have a polynomial-time solution. This makes the NP-complete problems the hardest of all NP problems. Suppose we have an NP-complete problem P1. Suppose P2 is known to be in NP. Suppose further that P1 polynomially reduces to P2, so that we can solve P1 by using P2 with only a polynomial time penalty. Since P1 is NP-complete, every problem in NP polynomially reduces to P1. By applying the closure property of polynomials, we see that every problem in NP is polynomially reducible to P2: We reduce the problem to P1 and then reduce P1 to P2. Thus, P2 is NP-complete. As an example, suppose that we already know that the Hamiltonian cycle problem is NP-complete. The traveling salesman problem is as follows. Traveling Salesman Problem. Given a complete graph G = (V, E), with edge costs, and an integer K, is there a simple cycle that visits all vertices and has total cost ≤K? The problem is different from the Hamiltonian cycle problem, because all |V|(|V|−1)/2 edges are present and the graph is weighted. This problem has many important applications. For instance, printed circuit boards need to have holes punched so that chips, resistors, and other electronic components can be placed. This is done mechanically. Punching the hole is a quick operation; the time-consuming step is positioning the hole puncher. The time required for positioning depends on the distance traveled from hole to hole. Since we would like to punch every hole (and then return to the start for the next

Chapter 9 Graph Algorithms V1 V2 V3 V4 V5 V1 V2 V3 V4 V5 Figure 9.80 Hamiltonian cycle problem transformed to traveling salesman problem board), and minimize the total amount of time spent traveling, what we have is a traveling salesman problem. The traveling salesman problem is NP-complete. It is easy to see that a solution can be checked in polynomial time, so it is certainly in NP. To show that it is NP-complete, we polynomially reduce the Hamiltonian cycle problem to it. To do this we construct a new graph G′. G′ has the same vertices as G. For G′, each edge (v, w) has a weight of 1 if (v, w) ∈G, and 2 otherwise. We choose K = |V|. See Figure 9.80. It is easy to verify that G has a Hamiltonian cycle if and only if G′ has a traveling salesman tour of total weight |V|. There is now a long list of problems known to be NP-complete. To prove that some new problem is NP-complete, it must be shown to be in NP, and then an appropriate NP-complete problem must be transformed into it. Although the transformation to a traveling salesman problem was rather straightforward, most transformations are actually quite involved and require some tricky constructions. Generally, several different NP-complete problems are considered before the problem that actually provides the reduction. As we are only interested in the general ideas, we will not show any more transformations; the interested reader can consult the references. The alert reader may be wondering how the ﬁrst NP-complete problem was actually proven to be NP-complete. Since proving that a problem is NP-complete requires transforming it from another NP-complete problem, there must be some NP-complete problem for which this strategy will not work. The ﬁrst problem that was proven to be NP-complete was the satisﬁability problem. The satisﬁability problem takes as input a Boolean expression and asks whether the expression has an assignment to the variables that gives a value of true. Satisﬁability is certainly in NP, since it is easy to evaluate a Boolean expression and check whether the result is true. In 1971, Cook showed that satisﬁability was NP-complete by directly proving that all problems that are in NP could be transformed to satisﬁability. To do this, he used the one known fact about every problem in NP: Every problem in NP

can be solved in polynomial time by a nondeterministic computer. The formal model for a computer is known as a Turing machine. Cook showed how the actions of this machine could be simulated by an extremely complicated and long, but still polynomial, Boolean formula. This Boolean formula would be true if and only if the program which was being run by the Turing machine produced a “yes” answer for its input. Once satisﬁability was shown to be NP-complete, a host of new NP-complete problems, including some of the most classic problems, were also shown to be NP-complete. In addition to the satisﬁability, Hamiltonian circuit, traveling salesman, and longestpath problems, which we have already examined, some of the more well-known NPcomplete problems which we have not discussed are bin packing, knapsack, graph coloring, and clique. The list is quite extensive and includes problems from operating systems (scheduling and security), database systems, operations research, logic, and especially graph theory.

C H A P T E R 10 Algorithm Design Techniques So far, we have been concerned with the efﬁcient implementation of algorithms. We have seen that when an algorithm is given, the actual data structures need not be speciﬁed. It is up to the programmer to choose the appropriate data structure in order to make the running time as small as possible. In this chapter, we switch our attention from the implementation of algorithms to the design of algorithms. Most of the algorithms that we have seen so far are straightforward and simple. Chapter 9 contains some algorithms that are much more subtle, and some require an argument (in some cases lengthy) to show that they are indeed correct. In this chapter, we will focus on ﬁve of the common types of algorithms used to solve problems. For many problems, it is quite likely that at least one of these methods will work. Speciﬁcally, for each type of algorithm we will r See the general approach. r Look at several examples (the exercises at the end of the chapter provide many more examples). r Discuss, in general terms, the time and space complexity, where appropriate. 10.1 Greedy Algorithms The ﬁrst type of algorithm we will examine is the greedy algorithm. We have already seen three greedy algorithms in Chapter 9: Dijkstra’s, Prim’s, and Kruskal’s algorithms. Greedy algorithms work in phases. In each phase, a decision is made that appears to be good, without regard for future consequences. Generally, this means that some local optimum is chosen. This “take what you can get now” strategy is the source of the name for this class of algorithms. When the algorithm terminates, we hope that the local optimum is equal to the global optimum. If this is the case, then the algorithm is correct; otherwise, the algorithm has produced a suboptimal solution. If the absolute best answer is not required, then simple greedy algorithms are sometimes used to generate approximate answers, rather than using the more complicated algorithms generally required to generate an exact answer. There are several real-life examples of greedy algorithms. The most obvious is the coin-changing problem. To make change in U.S. currency, we repeatedly dispense the

Chapter 10 Algorithm Design Techniques largest denomination. Thus, to give out seventeen dollars and sixty-one cents in change, we give out a ten-dollar bill, a ﬁve-dollar bill, two one-dollar bills, two quarters, one dime, and one penny. By doing this, we are guaranteed to minimize the number of bills and coins. This algorithm does not work in all monetary systems, but fortunately, we can prove that it does work in the American monetary system. Indeed, it works even if two-dollar bills and ﬁfty-cent pieces are allowed. Trafﬁc problems provide an example where making locally optimal choices does not always work. For example, during certain rush hour times in Miami, it is best to stay off the prime streets even if they look empty, because trafﬁc will come to a standstill a mile down the road, and you will be stuck. Even more shocking, it is better in some cases to make a temporary detour in the direction opposite your destination in order to avoid all trafﬁc bottlenecks. In the remainder of this section, we will look at several applications that use greedy algorithms. The ﬁrst application is a simple scheduling problem. Virtually all scheduling problems are either NP-complete (or of similar difﬁcult complexity) or are solvable by a greedy algorithm. The second application deals with ﬁle compression and is one of the earliest results in computer science. Finally, we will look at an example of a greedy approximation algorithm. 10.1.1 A Simple Scheduling Problem We are given jobs j1, j2, . . . , jN, all with known running times t1, t2, . . . , tN, respectively. We have a single processor. What is the best way to schedule these jobs in order to minimize the average completion time? In this entire section, we will assume nonpreemptive scheduling: Once a job is started, it must run to completion. As an example, suppose we have the four jobs and associated running times shown in Figure 10.1. One possible schedule is shown in Figure 10.2. Because j1 ﬁnishes in 15 (time units), j2 in 23, j3 in 26, and j4 in 36, the average completion time is 25. A better schedule, which yields a mean completion time of 17.75, is shown in Figure 10.3. The schedule given in Figure 10.3 is arranged by shortest job ﬁrst. We can show that this will always yield an optimal schedule. Let the jobs in the schedule be ji1, ji2, . . . , jiN. The ﬁrst job ﬁnishes in time ti1. The second job ﬁnishes after ti1 + ti2, and the third job ﬁnishes after ti1 + ti2 + ti3. From this, we see that the total cost, C, of the schedule is C = N  k=1 (N −k + 1)tik (10.1) C = (N + 1) N  k=1 tik − N  k=1 k · tik (10.2) Notice that in Equation (10.2), the ﬁrst sum is independent of the job ordering, so only the second sum affects the total cost. Suppose that in an ordering there exists some x > y such that tix < tiy. Then a calculation shows that by swapping jix and jiy, the second sum increases, decreasing the total cost. Thus, any schedule of jobs in which the times are

10.1 Greedy Algorithms Job Time j1 j2 j3 j4 Figure 10.1 Jobs and times j 1 j 2 j 3 j 4 Figure 10.2 Schedule #1 j 3 j 2 j 4 j 1 Figure 10.3 Schedule #2 (optimal) not monotonically nondecreasing must be suboptimal. The only schedules left are those in which the jobs are arranged by smallest running time ﬁrst, breaking ties arbitrarily. This result indicates the reason the operating system scheduler generally gives precedence to shorter jobs. The Multiprocessor Case We can extend this problem to the case of several processors. Again we have jobs j1, j2, . . . , jN, with associated running times t1, t2, . . . , tN, and a number P of processors. We will assume without loss of generality that the jobs are ordered, shortest running time ﬁrst. As an example, suppose P = 3, and the jobs are as shown in Figure 10.4. Figure 10.5 shows an optimal arrangement to minimize mean completion time. Jobs j1, j4, and j7 are run on Processor 1. Processor 2 handles j2, j5, and j8, and Processor 3 runs the remaining jobs. The total time to completion is 165, for an average of 165 9 = 18.33. The algorithm to solve the multiprocessor case is to start jobs in order, cycling through processors. It is not hard to show that no other ordering can do better, although if the number of processors P evenly divides the number of jobs N, there are many optimal orderings. This is obtained by, for each 0 ≤i < N/P, placing each of the jobs jiP+1 through j(i+1)P on a different processor. In our case, Figure 10.6 shows a second optimal solution.

Chapter 10 Algorithm Design Techniques Job Time j1 j2 j3 j4 j5 j6 j7 j8 j9 Figure 10.4 Jobs and times j 1 j 4 j 7 j 2 j 5 j 8 j 3 j 6 j 9 5 6 Figure 10.5 An optimal solution for the multiprocessor case Even if P does not divide N exactly, there can still be many optimal solutions, even if all the job times are distinct. We leave further investigation of this as an exercise. Minimizing the Final Completion Time We close this section by considering a very similar problem. Suppose we are only concerned with when the last job ﬁnishes. In our two examples above, these completion times are 40 and 38, respectively. Figure 10.7 shows that the minimum ﬁnal completion time is 34, and this clearly cannot be improved, because every processor is always busy. Although this schedule does not have minimum mean completion time, it has merit in that the completion time of the entire sequence is earlier. If the same user owns all these jobs, then this is the preferable method of scheduling. Although these problems are very similar, this new problem turns out to be NP-complete; it is just another way of phrasing the knapsack or bin-packing problems, which we will encounter later in this section. Thus,

10.1 Greedy Algorithms j 1 j 5 j 9 j 2 j 4 j 7 j 3 j 6 j 8 5 6 14 15 Figure 10.6 A second optimal solution for the multiprocessor case j 2 j 5 j 8 j 6 j 9 j 1 j 3 j 4 j 7 14 16 Figure 10.7 Minimizing the ﬁnal completion time minimizing the ﬁnal completion time is apparently much harder than minimizing the mean completion time. 10.1.2 Huffman Codes In this section, we consider a second application of greedy algorithms, known as ﬁle compression. The normal ASCII character set consists of roughly 100 “printable” characters. In order to distinguish these characters, ⌈log 100⌉= 7 bits are required. Seven bits allow the representation of 128 characters, so the ASCII character set adds some other “nonprintable” characters. An eighth bit is added as a parity check. The important point, however, is that if the size of the character set is C, then ⌈log C⌉bits are needed in a standard encoding. Suppose we have a ﬁle that contains only the characters a, e, i, s, t, plus blank spaces and newlines. Suppose further, that the ﬁle has ten a’s, ﬁfteen e’s, twelve i’s, three s’s, four t’s,

Chapter 10 Algorithm Design Techniques Character Code Frequency Total Bits a e i s t space newline Total Figure 10.8 Using a standard coding scheme a e i s t sp nl Figure 10.9 Representation of the original code in a tree thirteen blanks, and one newline. As the table in Figure 10.8 shows, this ﬁle requires 174 bits to represent, since there are 58 characters and each character requires three bits. In real life, ﬁles can be quite large. Many of the very large ﬁles are output of some program and there is usually a big disparity between the most frequent and least frequent characters. For instance, many large data ﬁles have an inordinately large amount of digits, blanks, and newlines, but few q’s and x’s. We might be interested in reducing the ﬁle size in the case where we are transmitting it over a slow phone line. Also, since on virtually every machine, disk space is precious, one might wonder if it would be possible to provide a better code and reduce the total number of bits required. The answer is that this is possible, and a simple strategy achieves 25 percent savings on typical large ﬁles and as much as 50 to 60 percent savings on many large data ﬁles. The general strategy is to allow the code length to vary from character to character and to ensure that the frequently occurring characters have short codes. Notice that if all the characters occur with the same frequency, then there are not likely to be any savings. The binary code that represents the alphabet can be represented by the binary tree shown in Figure 10.9. The tree in Figure 10.9 has data only at the leaves. The representation of each character can be found by starting at the root and recording the path, using a 0 to indicate the left branch and a 1 to indicate the right branch. For instance, s is reached by going left, then

10.1 Greedy Algorithms a e i s t sp nl Figure 10.10 A slightly better tree right, and ﬁnally right. This is encoded as 011. This data structure is sometimes referred to as a trie. If character ci is at depth di and occurs fi times, then the cost of the code is equal to  difi. A better code than the one given in Figure 10.9 can be obtained by noticing that the newline is an only child. By placing the newline symbol one level higher at its parent, we obtain the new tree in Figure 10.10. This new tree has cost of 173, but is still far from optimal. Notice that the tree in Figure 10.10 is a full tree: All nodes either are leaves or have two children. An optimal code will always have this property, since otherwise, as we have already seen, nodes with only one child could move up a level. If the characters are placed only at the leaves, any sequence of bits can always be decoded unambiguously. For instance, suppose 0100111100010110001000111 is the encoded string. 0 is not a character code, 01 is not a character code, but 010 represents i, so the ﬁrst character is i. Then 011 follows, giving an s. Then 11 follows, which is a newline. The remainder of the code is a, space, t, i, e, and newline. Thus, it does not matter if the character codes are different lengths, as long as no character code is a preﬁx of another character code. Such an encoding is known as a preﬁx code. Conversely, if a character is contained in a nonleaf node, it is no longer possible to guarantee that the decoding will be unambiguous. Putting these facts together, we see that our basic problem is to ﬁnd the full binary tree of minimum total cost (as deﬁned above), where all characters are contained in the leaves. The tree in Figure 10.11 shows the optimal tree for our sample alphabet. As can be seen in Figure 10.12, this code uses only 146 bits. Notice that there are many optimal codes. These can be obtained by swapping children in the encoding tree. The main unresolved question, then, is how the coding tree is constructed. The algorithm to do this was given by Huffman in 1952. Thus, this coding system is commonly referred to as a Huffman code. Huffman’s Algorithm Throughout this section we will assume that the number of characters is C. Huffman’s algorithm can be described as follows: We maintain a forest of trees. The weight of a tree is equal to the sum of the frequencies of its leaves. C−1 times, select the two trees, T1 and T2, of smallest weight, breaking ties arbitrarily, and form a new tree with subtrees T1 and T2.

Chapter 10 Algorithm Design Techniques s nl t a e i sp Figure 10.11 Optimal preﬁx code Character Code Frequency Total Bits a e i s t space newline Total Figure 10.12 Optimal preﬁx code a e i s t sp nl Figure 10.13 Initial stage of Huffman’s algorithm At the beginning of the algorithm, there are C single-node trees—one for each character. At the end of the algorithm there is one tree, and this is the optimal Huffman coding tree. A worked example will make the operation of the algorithm clear. Figure 10.13 shows the initial forest; the weight of each tree is shown in small type at the root. The two trees of lowest weight are merged together, creating the forest shown in Figure 10.14. We will name the new root T1, so that future merges can be stated unambiguously. We have made s the left child arbitrarily; any tiebreaking procedure can be used. The total weight of the new tree is just the sum of the weights of the old trees, and can thus be easily computed.

10.1 Greedy Algorithms a e i t sp s T1 nl Figure 10.14 Huffman’s algorithm after the ﬁrst merge a e i sp s T1 nl T2 t Figure 10.15 Huffman’s algorithm after the second merge e i sp s T1 nl T2 t T3 a Figure 10.16 Huffman’s algorithm after the third merge It is also a simple matter to create the new tree, since we merely need to get a new node, set the left and right links, and record the weight. Now there are six trees, and we again select the two trees of smallest weight. These happen to be T1 and t, which are then merged into a new tree with root T2 and weight 8. This is shown in Figure 10.15. The third step merges T2 and a, creating T3, with weight 10 + 8 = 18. Figure 10.16 shows the result of this operation. After the third merge is completed, the two trees of lowest weight are the single-node trees representing i and the blank space. Figure 10.17 shows how these trees are merged into the new tree with root T4. The ﬁfth step is to merge the trees with roots e and T3, since these trees have the two smallest weights. The result of this step is shown in Figure 10.18. Finally, the optimal tree, which was shown in Figure 10.11, is obtained by merging the two remaining trees. Figure 10.19 shows this optimal tree, with root T6. We will sketch the ideas involved in proving that Huffman’s algorithm yields an optimal code; we will leave the details as an exercise. First, it is not hard to show by contradiction that the tree must be full, since we have already seen how a tree that is not full is improved. Next, we must show that the two least frequent characters α and β must be the two deepest nodes (although other nodes may be as deep). Again, this is easy to show by

Chapter 10 Algorithm Design Techniques e i T4 sp s T1 nl T2 t T3 a Figure 10.17 Huffman’s algorithm after the fourth merge i T4 sp s T1 nl T2 t T3 a T5 e Figure 10.18 Huffman’s algorithm after the ﬁfth merge contradiction, since if either α or β is not a deepest node, then there must be some γ that is (recall that the tree is full). If α is less frequent than γ , then we can improve the cost by swapping them in the tree. We can then argue that the characters in any two nodes at the same depth can be swapped without affecting optimality. This shows that an optimal tree can always be found that contains the two least frequent symbols as siblings; thus the ﬁrst step is not a mistake. The proof can be completed by using an induction argument. As trees are merged, we consider the new character set to be the characters in the roots. Thus, in our example, after four merges, we can view the character set as consisting of e and the metacharacters T3 and T4. This is probably the trickiest part of the proof; you are urged to ﬁll in all of the details. The reason that this is a greedy algorithm is that at each stage we perform a merge without regard to global considerations. We merely select the two smallest trees. If we maintain the trees in a priority queue, ordered by weight, then the running time is O(C log C), since there will be one buildHeap, 2C −2 deleteMins, and C −2 inserts, on a priority queue that never has more than C elements. A simple implementation of the priority queue, using a list, would give an O(C2) algorithm. The choice of priority queue implementation depends on how large C is. In the typical case of an ASCII character set, C is small enough that the quadratic running time is acceptable. In such an application, virtually all the running time will be spent on the disk I/O required to read the input ﬁle and write out the compressed version.

10.1 Greedy Algorithms s T1 nl T2 t T3 a T5 e T6 i T4 sp Figure 10.19 Huffman’s algorithm after the ﬁnal merge There are two details that must be considered. First, the encoding information must be transmitted at the start of the compressed ﬁle, since otherwise it will be impossible to decode. There are several ways of doing this; see Exercise 10.4. For small ﬁles, the cost of transmitting this table will override any possible savings in compression, and the result will probably be ﬁle expansion. Of course, this can be detected and the original left intact. For large ﬁles, the size of the table is not signiﬁcant. The second problem is that as described, this is a two-pass algorithm. The ﬁrst pass collects the frequency data and the second pass does the encoding. This is obviously not a desirable property for a program dealing with large ﬁles. Some alternatives are described in the references. 10.1.3 Approximate Bin Packing In this section, we will consider some algorithms to solve the bin-packing problem. These algorithms will run quickly but will not necessarily produce optimal solutions. We will prove, however, that the solutions that are produced are not too far from optimal. We are given N items of sizes s1, s2, . . . , sN. All sizes satisfy 0 < si ≤1. The problem is to pack these items in the fewest number of bins, given that each bin has unit capacity. As an example, Figure 10.20 shows an optimal packing for an item list with sizes 0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8. There are two versions of the bin-packing problem. The ﬁrst version is online bin packing. In this version, each item must be placed in a bin before the next item can be processed. The second version is the off-line bin-packing problem. In an off-line algorithm, we do not need to do anything until all the input has been read. The distinction between online and off-line algorithms was discussed in Section 8.2. Online Algorithms The ﬁrst issue to consider is whether or not an online algorithm can actually always give an optimal answer, even if it is allowed unlimited computation. Remember that even

Chapter 10 Algorithm Design Techniques B 1 0.2 0.8 B 2 0.7 0.3 B 3 0.4 0.1 0.5 Figure 10.20 Optimal packing for 0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8 though unlimited computation is allowed, an online algorithm must place an item before processing the next item and cannot change its decision. To show that an online algorithm cannot always give an optimal solution, we will give it particularly difﬁcult data to work on. Consider an input sequence I1 of M small items of weight 1 2 −ϵ followed by M large items of weight 1 2 + ϵ, 0 < ϵ < 0.01. It is clear that these items can be packed in M bins if we place one small item and one large item in each bin. Suppose there were an optimal online algorithm A that could perform this packing. Consider the operation of algorithm A on the sequence I2, consisting of only M small items of weight 1 2 −ϵ. I2 can be packed in ⌈M/2⌉bins. However, A will place each item in a separate bin, since A must yield the same results on I2 as it does for the ﬁrst half of I1, and the ﬁrst half of I1 is exactly the same input as I2. This means that A will use twice as many bins as is optimal for I2. What we have proven is that there is no optimal algorithm for online bin packing. What the argument above shows is that an online algorithm never knows when the input might end, so any performance guarantees it provides must hold at every instant throughout the algorithm. If we follow the foregoing strategy, we can prove the following. Theorem 10.1. There are inputs that force any online bin-packing algorithm to use at least 4 3 the optimal number of bins. Proof. Suppose otherwise, and suppose for simplicity that M is even. Consider any online algorithm A running on the input sequence I1, above. Recall that this sequence consists of M small items followed by M large items. Let us consider what the algorithm A has done after processing the Mth item. Suppose A has already used b bins. At this point in the algorithm, the optimal number of bins is M/2, because we can place two elements in each bin. Thus we know that 2b/M < 4 3, by our assumption of a better-than- 4 performance guarantee. Now consider the performance of algorithm A after all items have been packed. All bins created after the bth bin must contain exactly one item, since all small items are placed in the ﬁrst b bins, and two large items will not ﬁt in a bin. Since the

10.1 Greedy Algorithms B 1 0.2 0.5 empty B 2 0.4 empty B 3 0.7 0.1 empty B 4 0.3 empty B 5 0.8 empty Figure 10.21 Next ﬁt for 0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8 ﬁrst b bins can have at most two items each, and the remaining bins have one item each, we see that packing 2M items will require at least 2M −b bins. Since the 2M items can be optimally packed using M bins, our performance guarantee assures us that (2M −b)/M < 4 3. The ﬁrst inequality implies that b/M < 2 3, and the second inequality implies that b/M > 2 3, which is a contradiction. Thus, no online algorithm can guarantee that it will produce a packing with less than 4 3 the optimal number of bins. There are three simple algorithms that guarantee that the number of bins used is no more than twice optimal. There are also quite a few more complicated algorithms with better guarantees. Next Fit Probably the simplest algorithm is next ﬁt. When processing any item, we check to see whether it ﬁts in the same bin as the last item. If it does, it is placed there; otherwise, a new bin is created. This algorithm is incredibly simple to implement and runs in linear time. Figure 10.21 shows the packing produced for the same input as Figure 10.20. Not only is next ﬁt simple to program, its worst-case behavior is also easy to analyze. Theorem 10.2. Let M be the optimal number of bins required to pack a list I of items. Then next ﬁt never uses more than 2M bins. There exist sequences such that next ﬁt uses 2M −2 bins. Proof. Consider any adjacent bins Bj and Bj+1. The sum of the sizes of all items in Bj and Bj+1 must be larger than 1, since otherwise all of these items would have been placed in Bj. If we apply this result to all pairs of adjacent bins, we see that at most half of the space is wasted. Thus next ﬁt uses at most twice the optimal number of bins. To see that this ratio, 2, is tight, suppose that the N items have size si = 0.5 if i is odd and si = 2/N if i is even. Assume N is divisible by 4. The optimal packing, shown in Figure 10.22, consists of N/4 bins, each containing 2 elements of size 0.5, and one

Chapter 10 Algorithm Design Techniques B 1 0.5 0.5 B 2 0.5 0.5 . . . BN/4 0.5 0.5 BN/4+1 2/N 2/N 2/N . . . 2/N 2/N 2/N  Figure 10.22 Optimal packing for 0.5, 2/N, 0.5, 2/N, 0.5, 2/N, . . . B 1 0.5 2/N empty B 2 0.5 2/N empty . . . BN/2 0.5 2/N empty Figure 10.23 Next ﬁt packing for 0.5, 2/N, 0.5, 2/N, 0.5, 2/N, . . . bin containing the N/2 elements of size 2/N, for a total of (N/4) + 1. Figure 10.23 shows that next ﬁt uses N/2 bins. Thus, next ﬁt can be forced to use almost twice as many bins as optimal. First Fit Although next ﬁt has a reasonable performance guarantee, it performs poorly in practice, because it creates new bins when it does not need to. In the sample run, it could have placed the item of size 0.3 in either B1 or B2, rather than create a new bin. The ﬁrst ﬁt strategy is to scan the bins in order and place the new item in the ﬁrst bin that is large enough to hold it. Thus, a new bin is created only when the results of previous placements have left no other alternative. Figure 10.24 shows the packing that results from ﬁrst ﬁt on our standard input. A simple method of implementing ﬁrst ﬁt would process each item by scanning down the list of bins sequentially. This would take O(N2). It is possible to implement ﬁrst ﬁt to run in O(N log N); we leave this as an exercise.

10.1 Greedy Algorithms B 1 0.2 0.5 0.1 empty B 2 0.4 0.3 empty B 3 0.7 empty B 4 0.8 empty Figure 10.24 First ﬁt for 0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8 B 1→BM 1/ 7 + ε 1/ 7 + ε 1/ 7 + ε 1/ 7 + ε 1/ 7 + ε 1/ 7 + ε empty ... BM+1→B 4M 1/ 3 + ε 1/ 3 + ε empty ... B 4M+1→B 10M 1/ 2 + ε empty Figure 10.25 A case where ﬁrst ﬁt uses 10M bins instead of 6M A moment’s thought will convince you that at any point, at most one bin can be more than half empty, since if a second bin were also half empty, its contents would ﬁt into the ﬁrst bin. Thus, we can immediately conclude that ﬁrst ﬁt guarantees a solution with at most twice the optimal number of bins. On the other hand, the bad case that we used in the proof of next ﬁt’s performance bound does not apply for ﬁrst ﬁt. Thus, one might wonder if a better bound can be proven. The answer is yes, but the proof is complicated. Theorem 10.3. Let M be the optimal number of bins required to pack a list I of items. Then ﬁrst ﬁt never uses more than 17 10M + 10 bins. There exist sequences such that ﬁrst ﬁt uses 10(M −1) bins. Proof. See the references at the end of the chapter. An example where ﬁrst ﬁt does almost as poorly as the previous theorem would indicate is shown in Figure 10.25. The input consists of 6M items of size 1 7 + ϵ, followed by

Chapter 10 Algorithm Design Techniques B 1 0.2 0.5 0.1 empty B 2 0.4 empty B 3 0.7 0.3 B 4 0.8 empty Figure 10.26 Best ﬁt for 0.2, 0.5, 0.4, 0.7, 0.1, 0.3, 0.8 6M items of size 1 3 +ϵ, followed by 6M items of size 1 2 +ϵ. One simple packing places one item of each size in a bin and requires 6M bins. First ﬁt requires 10M bins. When ﬁrst ﬁt is run on a large number of items with sizes uniformly distributed between 0 and 1, empirical results show that ﬁrst ﬁt uses roughly 2 percent more bins than optimal. In many cases, this is quite acceptable. Best Fit The third online strategy we will examine is best ﬁt. Instead of placing a new item in the ﬁrst spot that is found, it is placed in the tightest spot among all bins. A typical packing is shown in Figure 10.26. Notice that the item of size 0.3 is placed in B3, where it ﬁts perfectly, instead of B2. One might expect that since we are now making a more educated choice of bins, the performance guarantee would improve. This is not the case, because the generic bad cases are the same. Best ﬁt is never more than roughly 1.7 times as bad as optimal, and there are inputs for which it (nearly) achieves this bound. Nevertheless, best ﬁt is also simple to code, especially if an O(N log N) algorithm is required, and it does perform better for random inputs. Off-line Algorithms If we are allowed to view the entire item list before producing an answer, then we should expect to do better. Indeed, since we can eventually ﬁnd the optimal packing by exhaustive search, we already have a theoretical improvement over the online case. The major problem with all the online algorithms is that it is hard to pack the large items, especially when they occur late in the input. The natural way around this is to sort the items, placing the largest items ﬁrst. We can then apply ﬁrst ﬁt or best ﬁt, yielding the algorithms ﬁrst ﬁt decreasing and best ﬁt decreasing, respectively. Figure 10.27 shows that in our case this yields an optimal solution (although, of course, this is not true in general). In this section, we will deal with ﬁrst ﬁt decreasing. The results for best ﬁt decreasing are almost identical. Since it is possible that the item sizes are not distinct, some authors

10.1 Greedy Algorithms B 1 0.8 0.2 B 2 0.7 0.3 B 3 0.5 0.4 0.1 Figure 10.27 First ﬁt for 0.8, 0.7, 0.5, 0.4, 0.3, 0.2, 0.1 prefer to call the algorithm ﬁrst ﬁt nonincreasing. We will stay with the original name. We will also assume, without loss of generality, that input sizes are already sorted. The ﬁrst remark we can make is that the bad case, which showed ﬁrst ﬁt using 10M bins instead of 6M bins, does not apply when the items are sorted. We will show that if an optimal packing uses M bins, then ﬁrst ﬁt decreasing never uses more than (4M + 1)/3 bins. The result depends on two observations. First, all the items with weight larger than 3 will be placed in the ﬁrst M bins. This implies that all the items in the extra bins have weight at most 1 3. The second observation is that the number of items in the extra bins can be at most M −1. Combining these two results, we ﬁnd that at most ⌈(M −1)/3⌉extra bins can be required. We now prove these two observations. Lemma 10.1. Let the N items have (sorted in decreasing order) input sizes s1, s2, . . . , sN, respectively, and suppose that the optimal packing is M bins. Then all items that ﬁrst ﬁt decreasing places in extra bins have size at most 1 3. Proof. Suppose the ith item is the ﬁrst placed in bin M + 1. We need to show that si ≤1 3. We will prove this by contradiction. Assume si > 1 3. It follows that s1, s2, . . . , si−1 > 1 3, since the sizes are arranged in sorted order. From this it follows that all bins B1, B2, . . ., BM have at most two items each. Consider the state of the system after the (i −1)st item is placed in a bin, but before the ith item is placed. We now want to show that (under the assumption that si > 1 3) the ﬁrst M bins are arranged as follows: First there are some bins with exactly one element, and then the remaining bins have two elements. Suppose there were two bins Bx and By, such that 1 ≤x < y ≤M, Bx has two items, and By has one item. Let x1 and x2 be the two items in Bx, and let y1 be the item in By. x1 ≥y1, since x1 was placed in the earlier bin. x2 ≥si, by similar reasoning. Thus, x1 + x2 ≥y1 + si. This implies that si could be placed in By. By our assumption this is not possible. Thus, if si > 1 3, then, at the time that we try to process si, the ﬁrst

Chapter 10 Algorithm Design Techniques M bins are arranged such that the ﬁrst j have one element and the next M −j have two elements. To prove the lemma we will show that there is no way to place all the items in M bins, which contradicts the premise of the lemma. Clearly, no two items s1, s2, . . . , sj can be placed in one bin, by any algorithm, since if they could, ﬁrst ﬁt would have done so too. We also know that ﬁrst ﬁt has not placed any of the items of size sj+1, sj+2, . . . , si into the ﬁrst j bins, so none of them ﬁt. Thus, in any packing, speciﬁcally the optimal packing, there must be j bins that do not contain these items. It follows that the items of size sj+1, sj+2, . . . , si−1 must be contained in some set of M −j bins, and from previous considerations, the total number of such items is 2(M −j).1 The proof is completed by noting that if si > 1 3, there is no way for si to be placed in one of these M bins. Clearly, it cannot go in one of the j bins, since if it could, then ﬁrst ﬁt would have done so too. To place it in one of the remaining M −j bins requires distributing 2(M−j)+1 items into the M−j bins. Thus, some bin would have to have three items, each of which is larger than 1 3, a clear impossibility. This contradicts the fact that all the sizes can be placed in M bins, so the original assumption must be incorrect. Thus, si ≤1 3. Lemma 10.2. The number of objects placed in extra bins is at most M −1. Proof. Assume that there are at least M objects placed in extra bins. We know that N i=1 si ≤M, since all the objects ﬁt in M bins. Suppose that Bj is ﬁlled with Wj total weight for 1 ≤j ≤M. Suppose the ﬁrst M extra objects have sizes x1, x2, . . . , xM. Then, since the items in the ﬁrst M bins plus the ﬁrst M extra items are a subset of all the items, it follows that N  i=1 si ≥ M  j=1 Wj + M  j=1 xj ≥ M  j=1 (Wj + xj) Now Wj + xj > 1, since otherwise the item corresponding to xj would have been placed in Bj. Thus N  i=1 si > M  j=1 1 > M But this is impossible if the N items can be packed in M bins. Thus, there can be at most M −1 extra items. 1 Recall that ﬁrst ﬁt packed these elements into M −j bins and placed two items in each bin. Thus, there are 2(M −j) items.

10.1 Greedy Algorithms Theorem 10.4. Let M be the optimal number of bins required to pack a list I of items. Then ﬁrst ﬁt decreasing never uses more than (4M + 1)/3 bins. Proof. There are at most M −1 extra items, of size at most 1 3. Thus, there can be at most ⌈(M −1)/3⌉extra bins. The total number of bins used by ﬁrst ﬁt decreasing is thus at most ⌈(4M −1)/3⌉≤(4M + 1)/3. It is possible to prove a much tighter bound for both ﬁrst ﬁt decreasing and next ﬁt decreasing. Theorem 10.5. Let M be the optimal number of bins required to pack a list I of items. Then ﬁrst ﬁt decreasing never uses more than 11 9 M + 6 9 bins. There exist sequences such that ﬁrst ﬁt decreasing uses 11 9 M + 6 9 bins. Proof. The upper bound requires a very complicated analysis. The lower bound is exhibited by a sequence consisting of 6k + 4 elements of size 1 2 + ϵ, followed by 6k + 4 elements of size 1 4 +2ϵ, followed by 6k+4 elements of size 1 4 +ϵ, followed by 12k+8 elements of size 1 4 −2ϵ. Figure 10.28 shows that the optimal packing requires 9k + 6 bins, but ﬁrst ﬁt decreasing uses 11k + 8 bins. Set M = 9k + 6, and the result follows. In practice, ﬁrst ﬁt decreasing performs extremely well. If sizes are chosen uniformly over the unit interval, then the expected number of extra bins is ( √ M). Bin packing is a ﬁne example of how simple greedy heuristics can give good results. B 1→ → B6k + 4 B6k + 5 9k + 6 B → B6k + 5 8k + 5 B → B8k + 7 11k + 7 B 11k + 8 B 8k + 6 B Optimal First Fit Decreasing 1/4 − 2ε 1/4 − 2ε 1/4 − 2ε 1/4 − 2ε empty 1/4 − 2ε 1/4 + ε 1/4 − 2ε 1/4 − 2ε 1/4 − 2ε 1/2 + ε 1/4 + ε 1/4 − 2ε 1/4 + ε 1/4 + ε 1/4 + ε empty B1→B6k + 4 1/2 + ε 1/4 + 2ε empty 1/4 + 2ε 1/4 + 2ε 1/4 − 2ε 1/4 − 2ε Figure 10.28 Example where ﬁrst ﬁt decreasing uses 11k + 8 bins, but only 9k + 6 bins are required

Chapter 10 Algorithm Design Techniques 10.2 Divide and Conquer Another common technique used to design algorithms is divide and conquer. Divide-and conquer-algorithms consist of two parts: Divide: Smaller problems are solved recursively (except, of course, base cases). Conquer: The solution to the original problem is then formed from the solutions to the subproblems. Traditionally, routines in which the text contains at least two recursive calls are called divide-and-conquer algorithms, while routines whose text contains only one recursive call are not. We generally insist that the subproblems be disjoint (that is, essentially nonoverlapping). Let us review some of the recursive algorithms that have been covered in this text. We have already seen several divide-and-conquer algorithms. In Section 2.4.3, we saw an O(N log N) solution to the maximum subsequence sum problem. In Chapter 4, we saw linear-time tree traversal strategies. In Chapter 7, we saw the classic examples of divide and conquer, namely mergesort and quicksort, which have O(N log N) worst-case and averagecase bounds, respectively. We have also seen several examples of recursive algorithms that probably do not classify as divide and conquer, but merely reduce to a single simpler case. In Section 1.3, we saw a simple routine to print a number. In Chapter 2, we used recursion to perform efﬁcient exponentiation. In Chapter 4, we examined simple search routines for binary search trees. In Section 6.6, we saw simple recursion used to merge leftist heaps. In Section 7.7, an algorithm was given for selection that takes linear average time. The disjoint set find operation was written recursively in Chapter 8. Chapter 9 showed routines to recover the shortest path in Dijkstra’s algorithm and other procedures to perform depth-ﬁrst search in graphs. None of these algorithms are really divide-and-conquer algorithms, because only one recursive call is performed. We have also seen, in Section 2.4, a very bad recursive routine to compute the Fibonacci numbers. This could be called a divide-and-conquer algorithm, but it is terribly inefﬁcient, because the problem really is not divided at all. In this section, we will see more examples of the divide-and-conquer paradigm. Our ﬁrst application is a problem in computational geometry. Given N points in a plane, we will show that the closest pair of points can be found in O(N log N) time. The exercises describe some other problems in computational geometry which can be solved by divide and conquer. The remainder of the section shows some extremely interesting, but mostly theoretical, results. We provide an algorithm that solves the selection problem in O(N) worst-case time. We also show that 2 N-bit numbers can be multiplied in o(N2) operations and that two N × N matrices can be multiplied in o(N3) operations. Unfortunately, even though these algorithms have better worst-case bounds than the conventional algorithms, none are practical except for very large inputs.

10.2 Divide and Conquer 10.2.1 Running Time of Divide-and-Conquer Algorithms All the efﬁcient divide-and-conquer algorithms we will see divide the problems into subproblems, each of which is some fraction of the original problem, and then perform some additional work to compute the ﬁnal answer. As an example, we have seen that mergesort operates on two problems, each of which is half the size of the original, and then uses O(N) additional work. This yields the running time equation (with appropriate initial conditions) T(N) = 2T(N/2) + O(N) We saw in Chapter 7 that the solution to this equation is O(N log N). The following theorem can be used to determine the running time of most divide-and-conquer algorithms. Theorem 10.6. The solution to the equation T(N) = aT(N/b) + (Nk), where a ≥1 and b > 1, is T(N) = ⎧ ⎪⎨ ⎪⎩ O(Nlogb a) if a > bk O(Nk log N) if a = bk O(Nk) if a < bk Proof. Following the analysis of mergesort in Chapter 7, we will assume that N is a power of b; thus, let N = bm. Then N/b = bm−1 and Nk = (bm)k = bmk = bkm = (bk)m. Let us assume T(1) = 1, and ignore the constant factor in (Nk). Then we have T(bm) = aT(bm−1) + (bk)m If we divide through by am, we obtain the equation T(bm) am = T(bm−1) am−1 + & bk a 'm (10.3) We can apply this equation for other values of m, obtaining T(bm−1) am−1 = T(bm−2) am−2 + & bk a 'm−1 (10.4) T(bm−2) am−2 = T(bm−3) am−3 + & bk a 'm−2 (10.5) . . . T(b1) a1 = T(b0) a0 + & bk a '1 (10.6) We use our standard trick of adding up the telescoping Equations (10.3) through (10.6). Virtually all the terms on the left cancel the leading terms on the right, yielding

Chapter 10 Algorithm Design Techniques T(bm) am = 1 + m  i=1 & bk a 'i (10.7) = m  i=0 & bk a 'i (10.8) Thus T(N) = T(bm) = am m  i=0 & bk a 'i (10.9) If a > bk, then the sum is a geometric series with ratio smaller than 1. Since the sum of inﬁnite series would converge to a constant, this ﬁnite sum is also bounded by a constant, and thus Equation (10.10) applies: T(N) = O(am) = O(alogb N) = O(Nlogb a) (10.10) If a = bk, then each term in the sum is 1. Since the sum contains 1 + logb N terms and a = bk implies that logb a = k, T(N) = O(am logb N) = O(Nlogb a logb N) = O(Nk logb N) = O(Nk log N) (10.11) Finally, if a < bk, then the terms in the geometric series are larger than 1, and the second formula in Section 1.2.3 applies. We obtain T(N) = am (bk/a)m+1 −1 (bk/a) −1 = O(am(bk/a)m) = O((bk)m) = O(Nk) (10.12) proving the last case of the theorem. As an example, mergesort has a = b = 2 and k = 1. The second case applies, giving the answer O(N log N). If we solve three problems, each of which is half the original size, and combine the solutions with O(N) additional work, then a = 3, b = 2, and k = 1. Case 1 applies here, giving a bound of O(Nlog2 3) = O(N1.59). An algorithm that solved three half-sized problems, but required O(N2) work to merge the solution, would have an O(N2) running time, since the third case would apply. There are two important cases that are not covered by Theorem 10.6. We state two more theorems, leaving the proofs as exercises. Theorem 10.7 generalizes the previous theorem. Theorem 10.7. The solution to the equation T(N) = aT(N/b) + (Nk logp N), where a ≥1, b > 1, and p ≥0 is

10.2 Divide and Conquer T(N) = ⎧ ⎪⎨ ⎪⎩ O(Nlogb a) if a > bk O(Nk logp+1 N) if a = bk O(Nk logp N) if a < bk Theorem 10.8. If k i=1 αi < 1, then the solution to the equation T(N) = k i=1 T(αiN) + O(N) is T(N) = O(N). 10.2.2 Closest-Points Problem The input to our ﬁrst problem is a list P of points in a plane. If p1 = (x1, y1) and p2 = (x2, y2), then the Euclidean distance between p1 and p2 is [(x1 −x2)2 + (y1 −y2)2]1/2. We are required to ﬁnd the closest pair of points. It is possible that two points have the same position; in that case that pair is the closest, with distance zero. If there are N points, then there are N(N −1)/2 pairs of distances. We can check all of these, obtaining a very short program, but at the expense of an O(N2) algorithm. Since this approach is just an exhaustive search, we should expect to do better. Let us assume that the points have been sorted by x coordinate. At worst, this adds O(N log N) to the ﬁnal time bound. Since we will show an O(N log N) bound for the entire algorithm, this sort is essentially free, from a complexity standpoint. Figure 10.29 shows a small sample point set P. Since the points are sorted by x coordinate, we can draw an imaginary vertical line that partitions the point set into two halves, PL and PR. This is certainly simple to do. Now we have almost exactly the same situation as we saw in the maximum subsequence sum problem in Section 2.4.3. Either the closest points are both in PL, or they are both in PR, or one is in PL and the other is in PR. Let us call these distances dL, dR, and dC. Figure 10.30 shows the partition of the point set and these three distances. Figure 10.29 A small point set

Chapter 10 Algorithm Design Techniques dL dR dC Figure 10.30 P partitioned into PL and PR; shortest distances are shown We can compute dL and dR recursively. The problem, then, is to compute dC. Since we would like an O(N log N) solution, we must be able to compute dC with only O(N) additional work. We have already seen that if a procedure consists of two half-sized recursive calls and O(N) additional work, then the total time will be O(N log N). Let δ = min(dL, dR). The ﬁrst observation is that we only need to compute dC if dC improves on δ. If dC is such a distance, then the two points that deﬁne dC must be within δ of the dividing line; we will refer to this area as a strip. As shown in Figure 10.31, this observation limits the number of points that need to be considered (in our case, δ = dR). δ δ dL dR p 1 p 2 p 3 p 4 p 5 p 6 p 7 Figure 10.31 Two-lane strip, containing all points considered for dC strip

10.2 Divide and Conquer // Points are all in the strip for( i = 0; i < numPointsInStrip; i++ ) for( j = i + 1; j < numPointsInStrip; j++ ) if( dist(pi, pj) < δ ) δ = dist(pi, pj); Figure 10.32 Brute-force calculation of min(δ, dC) There are two strategies that can be tried to compute dC. For large point sets that are uniformly distributed, the number of points that are expected to be in the strip is very small. Indeed, it is easy to argue that only O( √ N) points are in the strip on average. Thus, we could perform a brute-force calculation on these points in O(N) time. The pseudocode in Figure 10.32 implements this strategy, assuming the Java convention that the points are indexed starting at 0. In the worst case, all the points could be in the strip, so this strategy does not always work in linear time. We can improve this algorithm with the following observation: The y coordinates of the two points that deﬁne dC can differ by at most δ. Otherwise, dC > δ. Suppose that the points in the strip are sorted by their y coordinates. Therefore, if pi and pj’s y coordinates differ by more than δ, then we can proceed to pi+1. This simple modiﬁcation is implemented in Figure 10.33. This extra test has a signiﬁcant effect on the running time, because for each pi only a few points pj are examined before pi’s and pj’s y coordinates differ by more than δ and force an exit from the inner for loop. Figure 10.34 shows, for instance, that for point p3, only the two points p4 and p5 lie in the strip within δ vertical distance. In the worst case, for any point pi, at most 7 points pj are considered. This is because these points must lie either in the δ by δ square in the left half of the strip or in the δ by δ square in the right half of the strip. On the other hand, all the points in each δ by δ square are separated by at least δ. In the worst case, each square contains four points, one at each corner. One of these points is pi, leaving at most seven points to be considered. This worst-case situation is shown in Figure 10.35. Notice that even though pL2 and pR1 have the same coordinates, they could be different points. For the actual analysis, it is only // Points are all in the strip and sorted by y-coordinate for( i = 0; i < numPointsInStrip; i++ ) for( j = i + 1; j < numPointsInStrip; j++ ) if( pi and pj’s y-coordinates differ by more than δ ) break; // Go to next pi. else if( dist(pi, pj) < δ ) δ = dist(pi, pj); Figure 10.33 Reﬁned calculation of min(δ, dC)

Chapter 10 Algorithm Design Techniques δ δ dL dR  p 1 p 2 p 3 p 4 p 5 p 6 p 7 δ Figure 10.34 Only p4 and p5 are considered in the second for loop Left half ( λ x λ) Right half ( λ x λ) pL1 pL2 pL3 pL4 pR1 pR2 pR3 pR4 Figure 10.35 At most eight points ﬁt in the rectangle; there are two coordinates shared by two points each important that the number of points in the λ by 2λ rectangle be O(1), and this much is certainly clear. Because at most seven points are considered for each pi, the time to compute a dC that is better than δ is O(N). Thus, we appear to have an O(N log N) solution to the closestpoints problem, based on the two half-sized recursive calls plus the linear extra work to combine the two results. However, we do not quite have an O(N log N) solution yet. The problem is that we have assumed that a list of points sorted by y coordinate is available. If we perform this sort for each recursive call, then we have O(N log N) extra work:

10.2 Divide and Conquer This gives an O(N log2 N) algorithm. This is not all that bad, especially when compared to the brute force O(N2). However, it is not hard to reduce the work for each recursive call to O(N), thus ensuring an O(N log N) algorithm. We will maintain two lists. One is the point list sorted by x coordinate, and the other is the point list sorted by y coordinate. We will call these lists P and Q, respectively. These can be obtained by a preprocessing sorting step at cost O(N log N) and thus does not affect the time bound. PL and QL are the lists passed to the left-half recursive call, and PR and QR are the lists passed to the right-half recursive call. We have already seen that P is easily split in the middle. Once the dividing line is known, we step through Q sequentially, placing each element in QL or QR as appropriate. It is easy to see that QL and QR will be automatically sorted by y coordinate. When the recursive calls return, we scan through the Q list and discard all the points whose x coordinates are not within the strip. Then Q contains only points in the strip, and these points are guaranteed to be sorted by their y coordinates. This strategy ensures that the entire algorithm is O(N log N), because only O(N) extra work is performed. 10.2.3 The Selection Problem The selection problem requires us to ﬁnd the kth smallest element in a collection S of N elements. Of particular interest is the special case of ﬁnding the median. This occurs when k = ⌈N/2⌉. In Chapters 1, 6, and 7 we have seen several solutions to the selection problem. The solution in Chapter 7 uses a variation of quicksort and runs in O(N) average time. Indeed, it is described in Hoare’s original paper on quicksort. Although this algorithm runs in linear average time, it has a worst case of O(N2). Selection can easily be solved in O(N log N) worst-case time by sorting the elements, but for a long time it was unknown whether or not selection could be accomplished in O(N) worst-case time. The quickselect algorithm outlined in Section 7.7.6 is quite efﬁcient in practice, so this was mostly a question of theoretical interest. Recall that the basic algorithm is a simple recursive strategy. Assuming that N is larger than the cutoff point where elements are simply sorted, an element v, known as the pivot, is chosen. The remaining elements are placed into two sets, S1 and S2. S1 contains elements that are guaranteed to be no larger than v, and S2 contains elements that are no smaller than v. Finally, if k ≤|S1|, then the kth smallest element in S can be found by recursively computing the kth smallest element in S1. If k = |S1| + 1, then the pivot is the kth smallest element. Otherwise, the kth smallest element in S is the (k −|S1| −1)st smallest element in S2. The main difference between this algorithm and quicksort is that there is only one subproblem to solve instead of two. In order to obtain a linear algorithm, we must ensure that the subproblem is only a fraction of the original and not merely only a few elements smaller than the original. Of course, we can always ﬁnd such an element if we are willing to spend some time to do so. The difﬁcult problem is that we cannot spend too much time ﬁnding the pivot. For quicksort, we saw that a good choice for pivot was to pick three elements and use their median. This gives some expectation that the pivot is not too bad but does not

Chapter 10 Algorithm Design Techniques provide a guarantee. We could choose 21 elements at random, sort them in constant time, use the 11th largest as pivot, and get a pivot that is even more likely to be good. However, if these 21 elements were the 21 largest, then the pivot would still be poor. Extending this, we could use up to O(N/log N) elements, sort them using heapsort in O(N) total time, and be almost certain, from a statistical point of view, of obtaining a good pivot. In the worst case, however, this does not work because we might select the O(N/log N) largest elements, and then the pivot would be the [N −O(N/log N)]th largest element, which is not a constant fraction of N. The basic idea is still useful. Indeed, we will see that we can use it to improve the expected number of comparisons that quickselect makes. To get a good worst case, however, the key idea is to use one more level of indirection. Instead of ﬁnding the median from a sample of random elements, we will ﬁnd the median from a sample of medians. The basic pivot selection algorithm is as follows: 1. Arrange the N elements into ⌊N/5⌋groups of ﬁve elements, ignoring the (at most four) extra elements. 2. Find the median of each group. This gives a list M of ⌊N/5⌋medians. 3. Find the median of M. Return this as the pivot, v. We will use the term median-of-median-of-ﬁve partitioning to describe the quickselect algorithm that uses the pivot selection rule given above. We will now show that median-of-median-of-ﬁve partitioning guarantees that each recursive subproblem is at most roughly 70 percent as large as the original. We will also show that the pivot can be computed quickly enough to guarantee an O(N) running time for the entire selection algorithm. Let us assume for the moment that N is divisible by 5, so there are no extra elements. Suppose also that N/5 is odd, so that the set M contains an odd number of elements. This provides some symmetry, as we shall see. We are thus assuming, for convenience, that N is of the form 10k + 5. We will also assume that all the elements are distinct. The actual algorithm must make sure to handle the case where this is not true. Figure 10.36 shows how the pivot might be chosen when N = 45. In Figure 10.36, v represents the element which is selected by the algorithm as pivot. Since v is the median of nine elements, and we are assuming that all elements are distinct, there must be four medians that are larger than v and four that are smaller. We denote these by L and S, respectively. Consider a group of ﬁve elements with a large median (type L). The median of the group is smaller than two elements in the group and larger than two elements in the group. We will let H represent the huge elements. These are elements that are known to be larger than a large median. Similarly, T represents the tiny elements, which are smaller than a small median. There are 10 elements of type H: Two are in each of the groups with an L type median, and two elements are in the same group as v. Similarly, there are 10 elements of type T. Elements of type L or H are guaranteed to be larger than v, and elements of type S or T are guaranteed to be smaller than v. There are thus guaranteed to be 14 large and 14 small elements in our problem. Therefore, a recursive call could be on at most 45−14−1 = 30 elements.

10.2 Divide and Conquer H H L H H L H H v T T S T T H H L S T T H H L S T T S T T Sorted groups of five elements Medians Figure 10.36 How the pivot is chosen Let us extend this analysis to general N of the form 10k + 5. In this case, there are k elements of type L and k elements of type S. There are 2k + 2 elements of type H, and also 2k + 2 elements of type T. Thus, there are 3k + 2 elements that are guaranteed to be larger than v and 3k + 2 elements that are guaranteed to be smaller. Thus, in this case, the recursive call can contain at most 7k + 2 < 0.7N elements. If N is not of the form 10k + 5, similar arguments can be made without affecting the basic result. It remains to bound the running time to obtain the pivot element. There are two basic steps. We can ﬁnd the median of ﬁve elements in constant time. For instance, it is not hard to sort ﬁve elements in eight comparisons. We must do this ⌊N/5⌋times, so this step takes O(N) time. We must then compute the median of a group of ⌊N/5⌋elements. The obvious way to do this is to sort the group and return the element in the middle. But this takes O(⌊N/5⌋log⌊N/5⌋) = O(N log N) time, so this does not work. The solution is to call the selection algorithm recursively on the ⌊N/5⌋elements. This completes the description of the basic algorithm. There are still some details that need to be ﬁlled in if an actual implementation is desired. For instance, duplicates must be handled correctly, and the algorithm needs a cutoff large enough to ensure that the recursive calls make progress. There is quite a large amount of overhead involved, and this algorithm is not practical at all, so we will not describe any more of the details that need to be considered. Even so, from a theoretical standpoint, the algorithm is a major breakthrough, because, as the following theorem shows, the running time is linear in the worst case. Theorem 10.9. The running time of quickselect using median-of-median-of-ﬁve partitioning is O(N).

Chapter 10 Algorithm Design Techniques Proof. The algorithm consists of two recursive calls of size 0.7N and 0.2N, plus linear extra work. By Theorem 10.8, the running time is linear. Reducing the Average Number of Comparisons Divide and conquer can also be used to reduce the expected number of comparisons required by the selection algorithm. Let us look at a concrete example. Suppose we have a group S of 1,000 numbers and are looking for the 100th smallest number, which we will call X. We choose a subset S′ of S consisting of 100 numbers. We would expect that the value of X is similar in size to the 10th smallest number in S′. More speciﬁcally, the ﬁfth smallest number in S′ is almost certainly less than X, and the 15th smallest number in S′ is almost certainly greater than X. More generally, a sample S′ of s elements is chosen from the N elements. Let δ be some number, which we will choose later so as to minimize the average number of comparisons used by the procedure. We ﬁnd the (v1 = ks/N −δ)th and (v2 = ks/N + δ)th smallest elements in S′. Almost certainly, the kth smallest element in S will fall between v1 and v2, so we are left with a selection problem on 2δ elements. With low probability, the kth smallest element does not fall in this range, and we have considerable work to do. However, with a good choice of s and δ, we can ensure, by the laws of probability, that the second case does not adversely affect the total work. If an analysis is performed, we ﬁnd that if s = N2/3 log1/3 N and δ = N1/3 log2/3 N, then the expected number of comparisons is N + k + O(N2/3 log1/3 N), which is optimal except for the low-order term. (If k > N/2, we can consider the symmetric problem of ﬁnding the (N −k)th largest element.) Most of the analysis is easy to do. The last term represents the cost of performing the two selections to determine v1 and v2. The average cost of the partitioning, assuming a reasonably clever strategy, is equal to N plus the expected rank of v2 in S, which is N + k + O(Nδ/s). If the kth element winds up in S′, the cost of ﬁnishing the algorithm is equal to the cost of selection on S′, namely, O(s). If the kth smallest element doesn’t wind up in S′, the cost is O(N). However, s and δ have been chosen to guarantee that this happens with very low probability o(1/N), so the expected cost of this possibility is o(1), which is a term that goes to zero as N gets large. An exact calculation is left as Exercise 10.22. This analysis shows that ﬁnding the median requires about 1.5N comparisons on average. Of course, this algorithm requires some ﬂoating-point arithmetic to compute s, which can slow down the algorithm on some machines. Even so, experiments have shown that if correctly implemented, this algorithm compares favorably with the quickselect implementation in Chapter 7. 10.2.4 Theoretical Improvements for Arithmetic Problems In this section we describe a divide-and-conquer algorithm that multiplies two N-digit numbers. Our previous model of computation assumed that multiplication was done in constant time, because the numbers were small. For large numbers, this assumption is no

10.2 Divide and Conquer longer valid. If we measure multiplication in terms of the size of numbers being multiplied, then the natural multiplication algorithm takes quadratic time. The divide-and-conquer algorithm runs in subquadratic time. We also present the classic divide-and-conquer algorithm that multiplies two N by N matrices in subcubic time. Multiplying Integers Suppose we want to multiply two N-digit numbers X and Y. If exactly one of X and Y is negative, then the answer is negative; otherwise it is positive. Thus, we can perform this check and then assume that X, Y ≥0. The algorithm that almost everyone uses when multiplying by hand requires (N2) operations, because each digit in X is multiplied by each digit in Y. If X = 61,438,521 and Y = 94,736,407, XY = 5,820,464,730,934,047. Let us break X and Y into two halves, consisting of the most signiﬁcant and least signiﬁcant digits, respectively. Then XL = 6,143, XR = 8,521, YL = 9,473, and YR = 6,407. We also have X = XL104 + XR and Y = YL104 + YR. It follows that XY = XLYL108 + (XLYR + XRYL)104 + XRYR Notice that this equation consists of four multiplications, XLYL, XLYR, XRYL, and XRYR, which are each half the size of the original problem (N/2 digits). The multiplications by 108 and 104 amount to the placing of zeros. This and the subsequent additions add only O(N) additional work. If we perform these four multiplications recursively using this algorithm, stopping at an appropriate base case, then we obtain the recurrence T(N) = 4T(N/2) + O(N) From Theorem 10.6, we see that T(N) = O(N2), so, unfortunately, we have not improved the algorithm. To achieve a subquadratic algorithm, we must use less than four recursive calls. The key observation is that XLYR + XRYL = (XL −XR)(YR −YL) + XLYL + XRYR Thus, instead of using two multiplications to compute the coefﬁcient of 104, we can use one multiplication, plus the result of two multiplications that have already been performed. Figure 10.37 shows how only three recursive subproblems need to be solved. It is easy to see that now the recurrence equation satisﬁes T(N) = 3T(N/2) + O(N) and so we obtain T(N) = O(Nlog2 3) = O(N1.59). To complete the algorithm, we must have a base case, which can be solved without recursion. When both numbers are one-digit, we can do the multiplication by table lookup. If one number has zero digits, then we return zero. In practice, if we were to use this algorithm, we would choose the base case to be that which is most convenient for the machine. Although this algorithm has better asymptotic performance than the standard quadratic algorithm, it is rarely used, because for small N the overhead is signiﬁcant, and for larger N there are even better algorithms. These algorithms also make extensive use of divide and conquer.

Chapter 10 Algorithm Design Techniques Function Value Computational Complexity XL 6,143 Given XR 8,521 Given YL 9,473 Given YR 6,407 Given D1 = XL −XR −2,378 O(N) D2 = YR −YL −3,066 O(N) XLYL 58,192,639 T(N/2) XRYR 54,594,047 T(N/2) D1D2 7,290,948 T(N/2) D3 = D1D2 + XLYL + XRYR 120,077,634 O(N) XRYR 54,594,047 Computed above D3104 1,200,776,340,000 O(N) XLYL108 5,819,263,900,000,000 O(N) XLYL108 + D3104 + XRYR 5,820,464,730,934,047 O(N) Figure 10.37 The divide-and-conquer algorithm in action Matrix Multiplication A fundamental numerical problem is the multiplication of two matrices. Figure 10.38 gives a simple O(N3) algorithm to compute C = AB, where A, B, and C are N×N matrices. The algorithm follows directly from the deﬁnition of matrix multiplication. To compute Ci,j, we compute the dot product of the ith row in A with the jth column in B. As usual, arrays begin at index 0. For a long time it was assumed that (N3) was required for matrix multiplication. However, in the late sixties Strassen showed how to break the (N3) barrier. The basic idea of Strassen’s algorithm is to divide each matrix into four quadrants, as shown in Figure 10.39. Then it is easy to show that C1,1 = A1,1B1,1 + A1,2B2,1 C1,2 = A1,1B1,2 + A1,2B2,2 C2,1 = A2,1B1,1 + A2,2B2,1 C2,2 = A2,1B1,2 + A2,2B2,2 As an example, to perform the multiplication AB AB = ⎡ ⎢⎢⎣ ⎤ ⎥⎥⎦ ⎡ ⎢⎢⎣ ⎤ ⎥⎥⎦

10.2 Divide and Conquer /** * Standard matrix multiplication. * Arrays start at 0. * Assumes a and b are square. */ public static int [ ][ ] multiply( int [ ][ ] a, int [ ][ ] b ) { int n = a.length; int [ ][ ] c = new int[ n ][ n ]; for( int i = 0; i < n; i++ ) // Initialization for( int j = 0; j < n; j++ ) c[ i ][ j ] = 0; for( int i = 0; i < n; i++ ) for( int j = 0; j < n; j++ ) for( int k = 0; k < n; k++ ) c[ i ][ j ] += a[ i ][ k ] * b[ k ][ j ]; return c; } Figure 10.38 Simple O(N3) matrix multiplication  A1,1 A1,2 A2,1 A2,2   B1,1 B1,2 B2,1 B2,2  =  C1,1 C1,2 C2,1 C2,2  Figure 10.39 Decomposing AB = C into four quadrants we deﬁne the following eight N/2 by N/2 matrices: A1,1 = 3  A1,2 = 1  B1,1 = 5  B1,2 = 9  A2,1 = 5  A2,2 = 2  B2,1 = 1  B2,2 = 8  We could then perform eight N/2 by N/2 matrix multiplications and four N/2 by N/2 matrix additions. The matrix additions take O(N2) time. If the matrix multiplications are done recursively, then the running time satisﬁes T(N) = 8T(N/2) + O(N2) From Theorem 10.6, we see that T(N) = O(N3), so we do not have an improvement. As we saw with integer multiplication, we must reduce the number of subproblems below 8.

Chapter 10 Algorithm Design Techniques Strassen used a strategy similar to the integer multiplication divide-and-conquer algorithm and showed how to use only seven recursive calls by carefully arranging the computations. The seven multiplications are M1 = (A1,2 −A2,2)(B2,1 + B2,2) M2 = (A1,1 + A2,2)(B1,1 + B2,2) M3 = (A1,1 −A2,1)(B1,1 + B1,2) M4 = (A1,1 + A1,2)B2,2 M5 = A1,1(B1,2 −B2,2) M6 = A2,2(B2,1 −B1,1) M7 = (A2,1 + A2,2)B1,1 Once the multiplications are performed, the ﬁnal answer can be obtained with eight more additions. C1,1 = M1 + M2 −M4 + M6 C1,2 = M4 + M5 C2,1 = M6 + M7 C2,2 = M2 −M3 + M5 −M7 It is straightforward to verify that this tricky ordering produces the desired values. The running time now satisﬁes the recurrence T(N) = 7T(N/2) + O(N2) The solution of this recurrence is T(N) = O(Nlog2 7) = O(N2.81). As usual, there are details to consider, such as the case when N is not a power of 2, but these are basically minor nuisances. Strassen’s algorithm is worse than the straightforward algorithm until N is fairly large. It does not generalize for the case where the matrices are sparse (contain many zero entries), and it does not easily parallelize. When run with ﬂoating-point entries, it is less stable numerically than the classic algorithm. Thus, it has only limited applicability. Nevertheless, it represents an important theoretical milestone and certainly shows that in computer science, as in many other ﬁelds, even though a problem seems to have an intrinsic complexity, nothing is certain until proven. 10.3 Dynamic Programming In the previous section, we saw that a problem that can be mathematically expressed recursively can also be expressed as a recursive algorithm, in many cases yielding a signiﬁcant performance improvement over a more naïve exhaustive search.

10.3 Dynamic Programming Any recursive mathematical formula could be directly translated to a recursive algorithm, but the underlying reality is that often the compiler will not do justice to the recursive algorithm, and an inefﬁcient program results. When we suspect that this is likely to be the case, we must provide a little more help to the compiler, by rewriting the recursive algorithm as a nonrecursive algorithm that systematically records the answers to the subproblems in a table. One technique that makes use of this approach is known as dynamic programming. 10.3.1 Using a Table Instead of Recursion In Chapter 2, we saw that the natural recursive program to compute the Fibonacci numbers is very inefﬁcient. Recall that the program shown in Figure 10.40 has a running time T(N) that satisﬁes T(N) ≥T(N −1)+T(N −2). Since T(N) satisﬁes the same recurrence relation as the Fibonacci numbers and has the same initial conditions, T(N) in fact grows at the same rate as the Fibonacci numbers and is thus exponential. On the other hand, since to compute FN, all that is needed is FN−1 and FN−2, we only need to record the two most recently computed Fibonacci numbers. This yields the O(N) algorithm in Figure 10.41. The reason that the recursive algorithm is so slow is because of the algorithm used to simulate recursion. To compute FN, there is one call to FN−1 and FN−2. However, since FN−1 recursively makes a call to FN−2 and FN−3, there are actually two separate calls to compute FN−2. If one traces out the entire algorithm, then we can see that FN−3 is computed three times, FN−4 is computed ﬁve times, FN−5 is computed eight times, and so on. As Figure 10.42 shows, the growth of redundant calculations is explosive. If the compiler’s recursion simulation algorithm were able to keep a list of all precomputed values and not make a recursive call for an already solved subproblem, then this exponential explosion would be avoided. This is why the program in Figure 10.41 is so much more efﬁcient. As a second example, we saw in Chapter 7 how to solve the recurrence C(N) = (2/N)N−1 i=0 C(i) + N, with C(0) = 1. Suppose that we want to check, numerically, /** * Compute Fibonacci numbers as described in Chapter 1. */ public static int fib( int n ) { if( n <= 1 ) return 1; else return fib( n - 1 ) + fib( n - 2 ); } Figure 10.40 Inefﬁcient algorithm to compute Fibonacci numbers

Chapter 10 Algorithm Design Techniques /** * Compute Fibonacci numbers as described in Chapter 1. */ public static int fibonacci( int n ) { if( n <= 1 ) return 1; int last = 1; int nextToLast = 1; int answer = 1; for( int i = 2; i <= n; i++ ) { answer = last + nextToLast; nextToLast = last; last = answer; } return answer; } Figure 10.41 Linear algorithm to compute Fibonacci numbers F1 F0 F2 F1 F3 F1 F0 F2 F4 F1 F0 F2 F1 F3 F5 F1 F0 F2 F1 F3 F1 F0 F2 F4 F6 Figure 10.42 Trace of the recursive calculation of Fibonacci numbers whether the solution we obtained is correct. We could then write the simple program in Figure 10.43 to evaluate the recursion. Once again, the recursive calls duplicate work. In this case, the running time T(N) satisﬁes T(N) = N−1 i=0 T(i) + N, because, as shown in Figure 10.44, there is one (direct) recursive call of each size from 0 to N −1, plus O(N) additional work (where else have we seen the tree shown in Figure 10.44?). Solving for T(N), we ﬁnd that it grows exponentially. By using a table, we obtain the program in Figure 10.45. This program avoids the redundant recursive calls and runs in O(N2). It is not a perfect program; as an exercise, you should make the simple change that reduces its running time to O(N).

10.3 Dynamic Programming public static double eval( int n ) { if( n == 0 ) return 1.0; else { double sum = 0.0; for( int i = 0; i < n; i++ ) sum += eval( i ); return 2.0 * sum / n + n; } } Figure 10.43 Recursive method to evaluate C(N) = 2/N N−1 i=0 C(i) + N C1 C0 C2 C0 C3 C1 C0 C0 C4 C1 C0 C2 C0 C1 C0 C0 C5 C1 C0 C2 C0 C3 C1 C0 C0 C1 C0 C2 C0 C1 C0 C0 Figure 10.44 Trace of the recursive calculation in eval public static double eval( int n ) { double [ ] c = new double [ n + 1 ]; c[ 0 ] = 1.0; for( int i = 1; i <= n; i++ ) { double sum = 0.0; for( int j = 0; j < i; j++ ) sum += c[ j ]; c[ i ] = 2.0 * sum / i + i; } return c[ n ]; } Figure 10.45 Evaluating C(N) = 2/N N−1 i=0 C(i) + N with a table

Chapter 10 Algorithm Design Techniques 10.3.2 Ordering Matrix Multiplications Suppose we are given four matrices, A, B, C, and D, of dimensions A = 50 × 10, B = 10×40, C = 40×30, and D = 30×5. Although matrix multiplication is not commutative, it is associative, which means that the matrix product ABCD can be parenthesized, and thus evaluated, in any order. The obvious way to multiply two matrices of dimensions p × q and q × r, respectively, uses pqr scalar multiplications. (Using a theoretically superior algorithm such as Strassen’s algorithm does not signiﬁcantly alter the problem we will consider, so we will assume this performance bound.) What is the best way to perform the three matrix multiplications required to compute ABCD? In the case of four matrices, it is simple to solve the problem by exhaustive search, since there are only ﬁve ways to order the multiplications. We evaluate each case below: r (A((BC)D)): Evaluating BC requires 10 × 40 × 30 = 12,000 multiplications. Evaluating (BC)D requires the 12,000 multiplications to compute BC, plus an additional 10 × 30 × 5 = 1,500 multiplications, for a total of 13,500. Evaluating (A((BC)D)) requires 13,500 multiplications for (BC)D, plus an additional 50 × 10 × 5 = 2,500 multiplications, for a grand total of 16,000 multiplications. r (A(B(CD))): Evaluating CD requires 40 × 30 × 5 = 6,000 multiplications. Evaluating B(CD) requires the 6,000 multiplications to compute CD, plus an additional 10 × 40 × 5 = 2,000 multiplications, for a total of 8,000. Evaluating (A(B(CD))) requires 8,000 multiplications for B(CD), plus an additional 50 × 10 × 5 = 2,500 multiplications, for a grand total of 10,500 multiplications. r ((AB)(CD)): Evaluating CD requires 40 × 30 × 5 = 6,000 multiplications. Evaluating AB requires 50 × 10 × 40 = 20,000 multiplications. Evaluating ((AB)(CD)) requires 6,000 multiplications for CD, 20,000 multiplications for AB, plus an additional 50 × 40 × 5 = 10,000 multiplications for a grand total of 36,000 multiplications. r (((AB)C)D): Evaluating AB requires 50 × 10 × 40 = 20,000 multiplications. Evaluating (AB)C requires the 20,000 multiplications to compute AB, plus an additional 50 × 40 × 30 = 60,000 multiplications, for a total of 80,000. Evaluating (((AB)C)D) requires 80,000 multiplications for (AB)C, plus an additional 50 × 30 × 5 = 7,500 multiplications, for a grand total of 87,500 multiplications. r ((A(BC))D): Evaluating BC requires 10 × 40 × 30 = 12,000 multiplications. Evaluating A(BC) requires the 12,000 multiplications to compute BC, plus an additional 50 × 10 × 30 = 15,000 multiplications, for a total of 27,000. Evaluating ((A(BC))D) requires 27,000 multiplications for A(BC), plus an additional 50 × 30 × 5 = 7,500 multiplications, for a grand total of 34,500 multiplications. The calculations show that the best ordering uses roughly one-ninth the number of multiplications as the worst ordering. Thus, it might be worthwhile to perform a few calculations to determine the optimal ordering. Unfortunately, none of the obvious greedy strategies seems to work. Moreover, the number of possible orderings grows quickly. Suppose we deﬁne T(N) to be this number. Then T(1) = T(2) = 1, T(3) = 2, and T(4) = 5, as we have seen. In general,

10.3 Dynamic Programming T(N) = N−1  i=1 T(i)T(N −i) To see this, suppose that the matrices are A1, A2, . . . , AN, and the last multiplication performed is (A1A2 · · · Ai)(Ai+1Ai+2 · · · AN). Then there are T(i) ways to compute (A1A2 · · · Ai) and T(N−i) ways to compute (Ai+1Ai+2 · · · AN). Thus, there are T(i)T(N−i) ways to compute (A1A2 · · · Ai)(Ai+1Ai+2 · · · AN) for each possible i. The solution of this recurrence is the well-known Catalan numbers, which grow exponentially. Thus, for large N, an exhaustive search through all possible orderings is useless. Nevertheless, this counting argument provides a basis for a solution that is substantially better than exponential. Let ci be the number of columns in matrix Ai for 1 ≤i ≤N. Then Ai has ci−1 rows, since otherwise the multiplications are not valid. We will deﬁne c0 to be the number of rows in the ﬁrst matrix, A1. Suppose mLeft,Right is the number of multiplications required to multiply ALeftALeft+1 · · · ARight−1 ARight. For consistency, mLeft,Left = 0. Suppose the last multiplication is (ALeft · · · Ai) (Ai+1 · · · ARight), where Left ≤i < Right. Then the number of multiplications used is mLeft,i + mi+1,Right + cLeft−1cicRight. These three terms represent the multiplications required to compute (ALeft · · · Ai), (Ai+1 · · · ARight), and their product, respectively. If we deﬁne MLeft,Right to be the number of multiplications required in an optimal ordering, then, if Left < Right, MLeft,Right = min Left≤i<Right{MLeft,i + Mi+1,Right + cLeft−1cicRight} This equation implies that if we have an optimal multiplication arrangement of ALeft · · · ARight, the subproblems ALeft · · · Ai and Ai+1 · · · ARight cannot be performed suboptimally. This should be clear, since otherwise we could improve the entire result by replacing the suboptimal computation by an optimal computation. The formula translates directly to a recursive program, but, as we have seen in the last section, such a program would be blatantly inefﬁcient. However, since there are only approximately N2/2 values of MLeft,Right that ever need to be computed, it is clear that a table can be used to store these values. Further examination shows that if Right −Left = k, then the only values Mx,y that are needed in the computation of MLeft,Right satisfy y−x < k. This tells us the order in which we need to compute the table. If we want to print out the actual ordering of the multiplications in addition to the ﬁnal answer M1,N, then we can use the ideas from the shortest-path algorithms in Chapter 9. Whenever we make a change to MLeft,Right, we record the value of i that is responsible. This gives the simple program shown in Figure 10.46. Although the emphasis of this chapter is not coding, it is worth noting that many programmers tend to shorten variable names to a single letter. c, i, and k are used as single-letter variables because this agrees with the names we have used in the description of the algorithm, which is very mathematical. However, it is generally best to avoid l as a variable name, because l looks too much like 1 and can make for very difﬁcult debugging if you make a transcription error.

Chapter 10 Algorithm Design Techniques /** * Compute optimal ordering of matrix multiplication. * c contains the number of columns for each of the n matrices. * c[ 0 ] is the number of rows in matrix 1. * The minimum number of multiplications is left in m[ 1 ][ n ]. * Actual ordering is computed via another procedure using lastChange. * m and lastChange are indexed starting at 1, instead of 0. * Note: Entries below main diagonals of m and lastChange * are meaningless and uninitialized. */ public static void optMatrix( int [ ] c, long [ ][ ] m, int [ ][ ] lastChange ) { int n = c.length - 1; for( int left = 1; left <= n; left++ ) m[ left ][ left ] = 0; for( int k = 1; k < n; k++ ) // k is right - left for( int left = 1; left <= n - k; left++ ) { // For each position int right = left + k; m[ left ][ right ] = INFINITY; for( int i = left; i < right; i++ ) { long thisCost = m[ left ][ i ] + m[ i + 1 ][ right ] + c[ left - 1 ] * c[ i ] * c[ right ]; if( thisCost < m[ left ][ right ] ) // Update min { m[ left ][ right ] = thisCost; lastChange[ left ][ right ] = i; } } } } Figure 10.46 Program to ﬁnd optimal ordering of matrix multiplications Returning to the algorithmic issues, this program contains a triply nested loop and is easily seen to run in O(N3) time. The references describe a faster algorithm, but since the time to perform the actual matrix multiplication is still likely to be much larger than the time to compute the optimal ordering, this algorithm is still quite practical.

10.3 Dynamic Programming 10.3.3 Optimal Binary Search Tree Our second dynamic programming example considers the following input: We are given a list of words, w1, w2, . . . , wN, and ﬁxed probabilities p1, p2, . . . , pN of their occurrence. The problem is to arrange these words in a binary search tree in a way that minimizes the expected total access time. In a binary search tree, the number of comparisons needed to access an element at depth d is d + 1, so if wi is placed at depth di, then we want to minimize N i=1 pi(1 + di). As an example, Figure 10.47 shows seven words along with their probability of occurrence in some context. Figure 10.48 shows three possible binary search trees. Their searching costs are shown in Figure 10.49. The ﬁrst tree was formed using a greedy strategy. The word with the highest probability of being accessed was placed at the root. The left and right subtrees were then formed recursively. The second tree is the perfectly balanced search tree. Neither of these trees is optimal, as demonstrated by the existence of the third tree. From this we can see that neither of the obvious solutions works. This is initially surprising, since the problem appears to be very similar to the construction of a Huffman encoding tree, which, as we have already seen, can be solved by a greedy algorithm. Construction of an optimal binary search tree is harder, because the data are not constrained to appear only at the leaves, and also because the tree must satisfy the binary search tree property. Word Probability a 0.22 am 0.18 and 0.20 egg 0.05 if 0.25 the 0.02 two 0.08 Figure 10.47 Sample input for optimal binary search tree problem if a and egg two the am if a and egg two the am if a and egg two the am Figure 10.48 Three possible binary search trees for data in previous table

Chapter 10 Algorithm Design Techniques Input Tree #1 Tree #2 Tree #3 Word Probability Access Cost Access Cost Access Cost wi pi Once Sequence Once Sequence Once Sequence a 0.22 0.44 0.66 0.44 am 0.18 0.72 0.36 0.54 and 0.20 0.60 0.60 0.20 egg 0.05 0.20 0.05 0.15 if 0.25 0.25 0.75 0.50 the 0.02 0.06 0.04 0.08 two 0.08 0.16 0.24 0.24 Totals 1.00 2.43 2.70 2.15 Figure 10.49 Comparison of the three binary search trees A dynamic programming solution follows from two observations. Once again, suppose we are trying to place the (sorted) words wLeft, wLeft+1, . . . , wRight−1, wRight into a binary search tree. Suppose the optimal binary search tree has wi as the root, where Left ≤i ≤ Right. Then the left subtree must contain wLeft, . . . , wi−1, and the right subtree must contain wi+1, . . . , wRight (by the binary search tree property). Further, both of these subtrees must also be optimal, since otherwise they could be replaced by optimal subtrees, which would give a better solution for wLeft, . . . , wRight. Thus, we can write a formula for the cost CLeft,Right of an optimal binary search tree. Figure 10.50 may be helpful. If Left > Right, then the cost of the tree is 0; this is the null case, which we always have for binary search trees. Otherwise, the root costs pi. The left subtree has a cost of CLeft,i−1, relative to its root, and the right subtree has a cost of Ci+1,Right relative to its root. As Figure 10.50 shows, each node in these subtrees is one level deeper from wi than from their respective roots, so we must add i−1 j=Left pj and Right j=i+1 pj. This gives the formula wi wLeft→wi −1 wi +1→wRight Figure 10.50 Structure of an optimal binary search tree

10.3 Dynamic Programming CLeft,Right = min Left≤i≤Right ⎧ ⎨ ⎩pi + CLeft,i−1 + Ci+1,Right + i−1  j=Left pj + Right  j=i+1 pj ⎫ ⎬ ⎭ = min Left≤i≤Right ⎧ ⎨ ⎩CLeft,i−1 + Ci+1,Right + Right  j=Left pj ⎫ ⎬ ⎭ From this equation, it is straightforward to write a program to compute the cost of the optimal binary search tree. As usual, the actual search tree can be maintained by saving the value of i that minimizes CLeft,Right. The standard recursive routine can be used to print the actual tree. Figure 10.51 shows the table that will be produced by the algorithm. For each subrange of words, the cost and root of the optimal binary search tree are maintained. The bottommost entry computes the optimal binary search tree for the entire set of words in the input. The optimal tree is the third tree shown in Figure 10.48. The precise computation for the optimal binary search tree for a particular subrange, namely, am..if, is shown in Figure 10.52. It is obtained by computing the minimum-cost tree obtained by placing am, and, egg, and if at the root. For instance, when and is placed at the root, the left subtree contains am..am (of cost 0.18, via previous calculation), the right subtree contains egg..if (of cost 0.35), and pam + pand + pegg + pif = 0.68, for a total cost of 1.21. Iteration=1 a..a am..am and..and egg..egg if..if the..the two..two Left=1 Left=2 Left=3 Left=4 Left=5 Left=6 Left=7 .22 a .18 am .20 and .05 egg .25 if .02 the .08 two Iteration=2 a..am am..and and..egg egg..if if..the the..two .58 a .56 and .30 and .35 if .29 if .12 two Iteration=3 a..and am..egg and..if egg..the if..two 1.02 am .66 and .80 if .39 if .47 if Iteration=4 a..egg am..if and..the egg..two 1.17 am 1.21 and .84 if .57 if Iteration=5 a..if am..the and..two 1.83 and 1.27 and 1.02 if Iteration=6 a..the am..two 1.89 and 1.53 and Iteration=7 a..two 2.15 and Figure 10.51 Computation of the optimal binary search tree for sample input

Chapter 10 Algorithm Design Techniques am (null) and..if and am..am egg..if egg am..and if..if if am..egg (null) 0 + 0.80 + 0.68 = 1.48 0.18 + 0.35 + 0.68 = 1.21 0.56 + 0.25 + 0.68 = 1.49 0.66 + 0 + 0.68 = 1.34 Figure 10.52 Computation of table entry (1.21, and) for am..if The running time of this algorithm is O(N3), because when it is implemented, we obtain a triple loop. An O(N2) algorithm for the problem is sketched in the exercises. 10.3.4 All-Pairs Shortest Path Our third and ﬁnal dynamic programming application is an algorithm to compute shortest weighted paths between every pair of points in a directed graph G = (V, E). In Chapter 9, we saw an algorithm for the single-source shortest-path problem, which ﬁnds the shortest path from some arbitrary vertex s to all others. That algorithm (Dijkstra’s) runs in O(|V|2) time on dense graphs, but substantially faster on sparse graphs. We will give a short algorithm to solve the all-pairs problem for dense graphs. The running time of the algorithm is O(|V|3), which is not an asymptotic improvement over |V| iterations of Dijkstra’s algorithm but could be faster on a very dense graph, because its loops are tighter. The algorithm also performs correctly if there are negative edge costs, but no negative-cost cycles; Dijkstra’s algorithm fails in this case. Let us recall the important details of Dijkstra’s algorithm (the reader may wish to review Section 9.3). Dijkstra’s algorithm starts at a vertex s and works in stages. Each vertex in the graph is eventually selected as an intermediate vertex. If the current selected vertex is v, then for each w ∈V, we set dw = min(dw, dv + cv,w). This formula says that the best distance to w (from s) is either the previously known distance to w from s, or the result of going from s to v (optimally) and then directly from v to w. Dijkstra’s algorithm provides the idea for the dynamic programming algorithm: we select the vertices in sequential order. We will deﬁne Dk,i,j to be the weight of the shortest

10.3 Dynamic Programming path from vi to vj that uses only v1, v2, . . . , vk as intermediates. By this deﬁnition, D0,i,j = ci,j, where ci,j is ∞if (vi, vj) is not an edge in the graph. Also, by deﬁnition, D|V|,i,j is the shortest path from vi to vj in the graph. As Figure 10.53 shows, when k > 0 we can write a simple formula for Dk,i,j. The shortest path from vi to vj that uses only v1, v2, . . . , vk as intermediates is the shortest path that either does not use vk as an intermediate at all, or consists of the merging of the two paths vi →vk and vk →vj, each of which uses only the ﬁrst k−1 vertices as intermediates. This leads to the formula /** * Compute all-shortest paths. * a[ ][ ] contains the adjacency matrix with * a[ i ][ i ] presumed to be zero. * d[ ] contains the values of the shortest path. * Vertices are numbered starting at 0; all arrays * have equal dimension. A negative cycle exists if * d[ i ][ i ] is set to a negative value. * Actual path can be computed using path[ ][ ]. * NOT_A_VERTEX is -1 */ public static void allPairs( int [ ][ ] a, int [ ][ ] d, int [ ][ ] path ) { int n = a.length; // Initialize d and path for( int i = 0; i < n; i++ ) for( int j = 0; j < n; j++ ) { d[ i ][ j ] = a[ i ][ j ]; path[ i ][ j ] = NOT_A_VERTEX; } for( int k = 0; k < n; k++ ) // Consider each vertex as an intermediate for( int i = 0; i < n; i++ ) for( int j = 0; j < n; j++ ) if( d[ i ][ k ] + d[ k ][ j ] < d[ i ][ j ] ) { // Update shortest path d[ i ][ j ] = d[ i ][ k ] + d[ k ][ j ]; path[ i ][ j ] = k; } } Figure 10.53 All-pairs shortest path

Chapter 10 Algorithm Design Techniques Dk,i,j = min * Dk−1,i,j, Dk−1,i,k + Dk−1,k,j + The time requirement is once again O(|V|3). Unlike the two previous dynamic programming examples, this time bound has not been substantially lowered by another approach. Because the kth stage depends only on the (k −1)th stage, it appears that only two |V| × |V| matrices need to be maintained. However, using k as an intermediate vertex on a path that starts or ﬁnishes with k does not improve the result unless there is a negative cycle. Thus, only one matrix is necessary, because Dk−1,i,k = Dk,i,k and Dk−1,k,j = Dk,k,j, which implies that none of the terms on the right change values and need to be saved. This observation leads to the simple program in Figure 10.53, which numbers vertices starting at zero to conform with Java’s conventions. On a complete graph, where every pair of vertices is connected (in both directions), this algorithm is almost certain to be faster than |V| iterations of Dijkstra’s algorithm, because the loops are so tight. Lines 17 through 22 can be executed in parallel, as can lines 26 through 33. Thus, this algorithm seems to be well suited for parallel computation. Dynamic programming is a powerful algorithm design technique, which provides a starting point for a solution. It is essentially the divide-and-conquer paradigm of solving simpler problems ﬁrst, with the important difference being that the simpler problems are not a clear division of the original. Because subproblems are repeatedly solved, it is important to record their solutions in a table rather than recompute them. In some cases, the solution can be improved (although it is certainly not always obvious and frequently difﬁcult), and in other cases, the dynamic programming technique is the best approach known. In some sense, if you have seen one dynamic programming problem, you have seen them all. More examples of dynamic programming can be found in the exercises and references. 10.4 Randomized Algorithms Suppose you are a professor who is giving weekly programming assignments. You want to make sure that the students are doing their own programs or, at the very least, understand the code they are submitting. One solution is to give a quiz on the day that each program is due. On the other hand, these quizzes take time out of class, so it might only be practical to do this for roughly half of the programs. Your problem is to decide when to give the quizzes. Of course, if the quizzes are announced in advance, that could be interpreted as an implicit license to cheat for the 50 percent of the programs that will not get a quiz. One could adopt the unannounced strategy of giving quizzes on alternate programs, but students would ﬁgure out the strategy before too long. Another possibility is to give quizzes on what seem like the important programs, but this would likely lead to similar quiz patterns from semester to semester. Student grapevines being what they are, this strategy would probably be worthless after a semester.

10.4 Randomized Algorithms One method that seems to eliminate these problems is to use a coin. A quiz is made for every program (making quizzes is not nearly as time-consuming as grading them), and at the start of class, the professor will ﬂip a coin to decide whether the quiz is to be given. This way, it is impossible to know before class whether or not the quiz will occur, and these patterns do not repeat from semester to semester. Thus, the students will have to expect that a quiz will occur with 50 percent probability, regardless of previous quiz patterns. The disadvantage is that it is possible that there is no quiz for an entire semester. This is not a likely occurrence, unless the coin is suspect. Each semester, the expected number of quizzes is half the number of programs, and with high probability, the number of quizzes will not deviate much from this. This example illustrates what we call randomized algorithms. At least once during the algorithm, a random number is used to make a decision. The running time of the algorithm depends not only on the particular input, but also on the random numbers that occur. The worst-case running time of a randomized algorithm is often the same as the worstcase running time of the nonrandomized algorithm. The important difference is that a good randomized algorithm has no bad inputs, but only bad random numbers (relative to the particular input). This may seem like only a philosophical difference, but actually it is quite important, as the following example shows. Consider two variants of quicksort. Variant A uses the ﬁrst element as pivot, while variant B uses a randomly chosen element as pivot. In both cases, the worst-case running time is (N2), because it is possible at each step that the largest element is chosen as pivot. The difference between these worst cases is that there is a particular input that can always be presented to variant A to cause the bad running time. Variant A will run in (N2) time every single time it is given an already sorted list. If variant B is presented with the same input twice, it will have two different running times, depending on what random numbers occur. Throughout the text, in our calculations of running times, we have assumed that all inputs are equally likely. This is not true, because nearly sorted input, for instance, occurs much more often than is statistically expected, and this causes problems, particularly for quicksort and binary search trees. By using a randomized algorithm, the particular input is no longer important. The random numbers are important, and we can get an expected running time, where we now average over all possible random numbers instead of over all possible inputs. Using quicksort with a random pivot gives an O(N log N)-expectedtime algorithm. This means that for any input, including already-sorted input, the running time is expected to be O(N log N), based on the statistics of random numbers. An expected running-time bound is somewhat stronger than an average-case bound but, of course, is weaker than the corresponding worst-case bound. On the other hand, as we saw in the selection problem, solutions that obtain the worst-case bound are frequently not as practical as their average-case counterparts. Randomized algorithms usually are. Randomized algorithms were used implicitly in perfect and universal hashing (Sections 5.7 and 5.8). In this section we will examine two additional uses of randomization. First, we will see a novel scheme for supporting the binary search tree operations in O(log N) expected time. Once again, this means that there are no bad inputs, just bad random numbers. From a theoretical point of view, this is not terribly exciting, since balanced

Chapter 10 Algorithm Design Techniques search trees achieve this bound in the worst case. Nevertheless, the use of randomization leads to relatively simple algorithms for searching, inserting, and especially deleting. Our second application is a randomized algorithm to test the primality of large numbers. The algorithm we present runs quickly but occasionally makes an error. The probability of error can, however, be made negligibly small. 10.4.1 Random Number Generators Since our algorithms require random numbers, we must have a method to generate them. Actually, true randomness is virtually impossible to do on a computer, since these numbers will depend on the algorithm and thus cannot possibly be random. Generally, it sufﬁces to produce pseudorandom numbers, which are numbers that appear to be random. Random numbers have many known statistical properties; pseudorandom numbers satisfy most of these properties. Surprisingly, this too is much easier said than done. Suppose we only need to ﬂip a coin; thus, we must generate a 0 (for heads) or 1 (for tails) randomly. One way to do this is to examine the system clock. The clock might record time as an integer that counts the number of seconds since some starting time. We could then use the lowest bit. The problem is that this does not work well if a sequence of random numbers is needed. One second is a long time, and the clock might not change at all while the program is running. Even if the time was recorded in units of microseconds, if the program was running by itself the sequence of numbers that would be generated would be far from random, since the time between calls to the generator would be essentially identical on every program invocation. We see, then, that what is really needed is a sequence of random numbers.2 These numbers should appear independent. If a coin is ﬂipped and heads appears, the next coin ﬂip should still be equally likely to come up heads or tails. The simplest method to generate random numbers is the linear congruential generator, which was ﬁrst described by Lehmer in 1951. Numbers x1, x2, . . . are generated satisfying xi+1 = A xi mod M To start the sequence, some value of x0 must be given. This value is known as the seed. If x0 = 0, then the sequence is far from random, but if A and M are correctly chosen, then any other 1 ≤x0 < M is equally valid. If M is prime, then xi is never 0. As an example, if M = 11, A = 7, and x0 = 1, then the numbers generated are 7, 5, 2, 3, 10, 4, 6, 9, 8, 1, 7, 5, 2, . . . Notice that after M −1 = 10 numbers, the sequence repeats. Thus, this sequence has a period of M −1, which is as large as possible (by the pigeonhole principle). If M is prime, there are always choices of A that give a full period of M −1. Some choices of A do not; if A = 5 and x0 = 1, the sequence has a short period of 5. 2 We will use random in place of pseudorandom in the rest of this section.

10.4 Randomized Algorithms 5, 3, 4, 9, 1, 5, 3, 4, . . . If M is chosen to be a large, 31-bit prime, the period should be signiﬁcantly large for most applications. Lehmer suggested the use of the 31-bit prime M = 231−1 = 2,147,483,647. For this prime, A = 48,271 is one of the many values that gives a full-period generator. Its use has been well studied and is recommended by experts in the ﬁeld. We will see later that with random number generators, tinkering usually means breaking, so one is well advised to stick with this formula until told otherwise.3 This seems like a simple routine to implement. Generally, a class variable is used to hold the current value in the sequence of x’s. When debugging a program that uses random numbers, it is probably best to set x0 = 1, so that the same random sequence occurs all the time. When the program seems to work, either the system clock can be used or the user can be asked to input a value for the seed. It is also common to return a random real number in the open interval (0, 1) (0 and 1 are not possible values); this can be done by dividing by M. From this, a random number in any closed interval [α, β] can be computed by normalizing. This yields the “obvious” class in Figure 10.54 which, unfortunately, is erroneous. The problem with this class is that the multiplication could overﬂow; although this is not an error, it affects the result and thus the pseudorandomness. Even though we could use 64-bit longs, this would slow down the computation. Schrage gave a procedure in which all the calculations can be done on a 32-bit machine without overﬂow. We compute the quotient and remainder of M/A and deﬁne these as Q and R, respectively. In our case, Q = 44,488, R = 3,399, and R < Q. We have xi+1 = A xi mod M = A xi −M ,A xi M - = A xi −M ,xi Q - + M ,xi Q - −M ,A xi M - = A xi −M ,xi Q - + M ,xi Q - − ,A xi M -	 Since xi = Q⌊xi Q⌋+ ximod Q, we can replace the leading A xi and obtain xi+1 = A  Q ,xi Q - + xi mod Q −M ,xi Q - + M ,xi Q - − ,A xi M -	 = (AQ −M) ,xi Q - + A(xi mod Q) + M ,xi Q - − ,A xi M -	 3 For instance, it seems that xi+1 = (48,271xi + 1) mod(231 −1) would somehow be even more random. This illustrates how fragile these generators are. [48,271(179,424,105) + 1] mod(231 −1) = 179,424,105 so if the seed is 179,424,105, the generator gets stuck in a cycle of period 1.

Chapter 10 Algorithm Design Techniques public class Random { private static final int A = 48271; private static final int M = 2147483647; public Random( ) { state = System.currentTimeMillis( ) % Integer.MAX_VALUE ; } /** * Return a pseudorandom int, and change the * internal state. DOES NOT WORK. * @return the pseudorandom int. */ public int randomIntWRONG( ) { return state = ( A * state ) % M; } /** * Return a pseudorandom double in the open range 0..1 * and change the internal state. * @return the pseudorandom double. */ public double random0_1( ) { return (double) randomInt( ) / M; } private int state; } Figure 10.54 Random number generator that does not work Since M = AQ + R, it follows that AQ −M = −R. Thus, we obtain xi+1 = A(xi mod Q) −R ,xi Q - + M ,xi Q - − ,A xi M -	 The term δ(xi) = ⌊xi Q⌋−⌊A xi M ⌋is either 0 or 1, because both terms are integers and their difference lies between 0 and 1. Thus, we have xi+1 = A(xi mod Q) −R ,xi Q - + Mδ(xi)

10.4 Randomized Algorithms public class Random { private static final int A = 48271; private static final int M = 2147483647; private static final int Q = M / A; private static final int R = M % A; /** * Return a pseudorandom int, and change the internal state. * @return the pseudorandom int. */ public int randomInt( ) { int tmpState = A * ( state % Q ) - R * ( state / Q ); if( tmpState >= 0 ) state = tmpState; else state = tmpState + M; return state; } // Remainder of this class is the same as Figure 10.54 Figure 10.55 Random number generator that does not overﬂow A quick check shows that because R < Q, all the remaining terms can be calculated without overﬂow (this is one of the reasons for choosing A = 48,271). Furthermore, δ(xi) = 1 only if the remaining terms evaluate to less than zero. Thus δ(xi) does not need to be explicitly computed but can be determined by a simple test. This leads to the revisions in Figure 10.55. One might be tempted to assume that all machines have a random number generator at least as good as the one in Figure 10.55 in their standard library. Sadly, this is not true. Many libraries have generators based on the function xi+1 = (A xi + C) mod 2B where B is chosen to match the number of bits in the machine’s integer, and C is odd. Unfortunately, these generators always produce values of xi that alternate between even and odd—hardly a desirable property. Indeed, the lower k bits cycle with period 2k (at best). Many other random number generators have much smaller cycles than the one provided in Figure 10.55. These are not suitable for the case where long sequences of random numbers are needed. The Java library and the UNIX drand48 function use a generator of this form. However, they use a 48-bit linear congruential generator and return only the

Chapter 10 Algorithm Design Techniques high 32 bits, thus avoiding the cycling problem in the low-order bits. The constants are A = 25,214,903,917, B = 48, and C = 13. Because Java provides 64-bit longs, implementing a basic 48-bit random number generator in standard Java can be illustrated in only a page of code. It is somewhat slower than the 31-bit random number generator, but not much so, and yields a signiﬁcantly longer period. Figure 10.56 shows a respectable implementation of this random number generator. Lines 7–10 show the basic constants of the random number generator. Because M is a power of 2, we can use bitwise operators. M = 2B can be computed by a bit shift, and instead of using the modulus operator %, we can use a bitwise and operator. This is because MASK=M-1 consists of the low 48 bits all set to 1, and a bitwise and operator with MASK thus has the effect of yielding a 48-bit result. The next routine returns a speciﬁed number (at most 32) of random bits from the computed state, using the high-order bits which are more random than the lower bits. Line 34 is a direct application of the previously stated linear congruential formula, and line 36 is a bitwise shift (zero-ﬁlled in the high bits to avoid negative numbers). randomInt obtains 32 bits, while random0_1 obtains 53 bits (representing the mantissa; the other 11 bits of a double represent the exponent) in two separate calls. The 48-bit random number generator (and even the 31-bit generator) is quite adequate for many applications, simple to implement in 64-bit arithmetic, and uses little space. However, linear congruential generators are unsuitable for some applications, such as cryptography or in simulations that require large numbers of highly independent and uncorrelated random numbers. In those cases, the Java class java.security.SecureRandom should be used. 10.4.2 Skip Lists Our ﬁrst use of randomization is a data structure that supports both searching and insertion in O(log N) expected time. As mentioned in the introduction to this section, this means that the running time for each operation on any input sequence has expected value O(log N), where the expectation is based on the random number generator. It is possible to add deletion and all the operations that involve ordering and obtain expected time bounds that match the average time bounds of binary search trees. The simplest possible data structure to support searching is the linked list. Figure 10.57 shows a simple linked list. The time to perform a search is proportional to the number of nodes that have to be examined, which is at most N. Figure 10.58 shows a linked list in which every other node has an additional link to the node two ahead of it in the list. Because of this, at most ⌈N/2⌉+ 1 nodes are examined in the worst case. We can extend this idea and obtain Figure 10.59. Here, every fourth node has a link to the node four ahead. Only ⌈N/4⌉+ 2 nodes are examined. The limiting case of this argument is shown in Figure 10.60. Every 2ith node has a link to the node 2i ahead of it. The total number of links has only doubled, but now at most ⌈log N⌉nodes are examined during a search. It is not hard to see that the total time spent for a search is O(log N), because the search consists of either advancing to a new node or

10.4 Randomized Algorithms /** * Random number class, using a 48-bit * linear congruential generator. */ public class Random48 { private static final long A = 25_214_903_917L; private static final long B = 48; private static final long C = 11; private static final long M = (1L<<B); private static final long MASK = M-1; public Random48( ) { state = System.nanoTime( ) & MASK; } public int randomInt( ) { return next( 32 ); } public double random0_1( ) { return ( ( (long) ( next( 26 ) ) << 27 ) + next( 27 ) / (double) ( 1L << 53 ); } /** * Return specified number of random bits * @param bits number of bits to return * @return specified random bits * @throws IllegalArgumentException if bits is more than 32 */ private int next( int bits ) { if( bits <= 0 || bits > 32 ) throw new IllegalArgumentException( ); state = ( A * state + C ) & MASK; return (int) ( state >>> ( B - bits ) ); } private long state; } Figure 10.56 48-bit random number generator Figure 10.57 Simple linked list

Chapter 10 Algorithm Design Techniques Figure 10.58 Linked list with links to two cells ahead Figure 10.59 Linked list with links to four cells ahead dropping to a lower link in the same node. Each of these steps consumes at most O(log N) total time during a search. Notice that the search in this data structure is essentially a binary search. The problem with this data structure is that it is much too rigid to allow efﬁcient insertion. The key to making this data structure usable is to relax the structure conditions slightly. We deﬁne a level k node to be a node that has k links. As Figure 10.60 shows, the ith link in any level k node (k ≥i) links to the next node with at least i levels. This is an easy property to maintain; however, Figure 10.60 shows a more restrictive property than this. We thus drop the restriction that the ith link links to the node 2i ahead, and we replace it with the less restrictive condition above. When it comes time to insert a new element, we allocate a new node for it. We must at this point decide what level the node should be. Examining Figure 10.60, we ﬁnd that roughly half the nodes are level 1 nodes, roughly a quarter are level 2, and, in general, approximately 1/2i nodes are level i. We choose the level of the node randomly, in accordance with this probability distribution. The easiest way to do this is to ﬂip a coin until a head occurs and use the total number of ﬂips as the node level. Figure 10.61 shows a typical skip list. Figure 10.60 Linked list with links to 2i cells ahead Figure 10.61 A skip list

10.4 Randomized Algorithms 20 * * * Figure 10.62 Before and after an insertion Given this, the skip list algorithms are simple to describe. To perform a search, we start at the highest link at the header. We traverse along this level until we ﬁnd that the next node is larger than the one we are looking for (or null). When this occurs, we go to the next lower level and continue the strategy. When progress is stopped at level 1, either we are in front of the node we are looking for, or it is not in the list. To perform an insert, we proceed as in a search, and keep track of each point where we switch to a lower level. The new node, whose level is determined randomly, is then spliced into the list. This operation is shown in Figure 10.62. A cursory analysis shows that since the expected number of nodes at each level is unchanged from the original (nonrandomized) algorithm, the total amount of work that is expected to be performed traversing to nodes on the same level is unchanged. This tells us that these operations have O(log N) expected costs. Of course, a more formal proof is required, but it is not much different from this. Skip lists are similar to hash tables, in that they require an estimate of the number of elements that will be in the list (so that the number of levels can be determined). If an estimate is not available, we can assume a large number or use a technique similar to rehashing. Experiments have shown that skip lists are as efﬁcient as many balanced search tree implementations and are certainly much simpler to implement in many languages. Skip lists also have efﬁcient concurrent implementations, unlike balanced binary search trees. Hence they are provided in the Java library class java.util.concurrent.ConcurrentSkipList. 10.4.3 Primality Testing In this section we examine the problem of determining whether or not a large number is prime. As was mentioned at the end of Chapter 2, some cryptography schemes depend on the difﬁculty of factoring a large, 400-digit number into two 200-digit primes. In order to implement this scheme, we need a method of generating these two primes. If d is the number of digits in N, the obvious method of testing for the divisibility by odd numbers from 3 to √ N requires roughly 1 √ N divisions, which is about 10d/2 and is completely impractical for 200-digit numbers.

Chapter 10 Algorithm Design Techniques In this section, we will give a polynomial-time algorithm that can test for primality. If the algorithm declares that the number is not prime, we can be certain that the number is not prime. If the algorithm declares that the number is prime, then, with high probability but not 100 percent certainty, the number is prime. The error probability does not depend on the particular number that is being tested but instead depends on random choices made by the algorithm. Thus, this algorithm occasionally makes a mistake, but we will see that the error ratio can be made arbitrarily negligible. The key to the algorithm is a well-known theorem due to Fermat. Theorem 10.10. (Fermat’s Lesser Theorem) If P is prime, and 0 < A < P, then AP−1 ≡1 (mod P). Proof. A proof of this theorem can be found in any textbook on number theory. For instance, since 67 is prime, 266 ≡1 (mod 67). This suggests an algorithm to test whether a number N is prime. Merely check whether 2N−1 ≡1 (mod N). If 2N−1 ̸≡ 1 (mod N), then we can be certain that N is not prime. On the other hand, if the equality holds, then N is probably prime. For instance, the smallest N that satisﬁes 2N−1 ≡1 (mod N) but is not prime is N = 341. This algorithm will occasionally make errors, but the problem is that it will always make the same errors. Put another way, there is a ﬁxed set of N for which it does not work. We can attempt to randomize the algorithm as follows: Pick 1 < A < N −1 at random. If AN−1 ≡1 (mod N), declare that N is probably prime, otherwise declare that N is deﬁnitely not prime. If N = 341, and A = 3, we ﬁnd that 3340 ≡56 (mod 341). Thus, if the algorithm happens to choose A = 3, it will get the correct answer for N = 341. Although this seems to work, there are numbers that fool even this algorithm for most choices of A. One such set of numbers is known as the Carmichael numbers. These are not prime but satisfy AN−1 ≡1 (mod N) for all 0 < A < N that are relatively prime to N. The smallest such number is 561. Thus, we need an additional test to improve the chances of not making an error. In Chapter 7, we proved a theorem related to quadratic probing. A special case of this theorem is the following: Theorem 10.11. If P is prime and 0 < X < P, the only solutions to X2 ≡1 (mod P) are X = 1, P −1. Proof. X2 ≡1 (mod P) implies that X2 −1 ≡0 (mod P). This implies (X −1)(X + 1) ≡0 (mod P). Since P is prime, 0 < X < P, and P must divide either (X −1) or (X + 1), the theorem follows. Therefore, if at any point in the computation of AN−1 (mod N) we discover a violation of this theorem, we can conclude that N is deﬁnitely not prime. If we use pow, from Section 2.4.4, we see that there will be several opportunities to apply this test. We modify this routine to perform operations mod N, and apply the test of Theorem 10.11. This strategy is implemented in the pseudocode shown in Figure 10.63.

/** * Method that implements the basic primality test. If witness does not return 1, * n is definitely composite. Do this by computing aˆi (mod n) and looking for * nontrivial square roots of 1 along the way. */ private static long witness( long a, long i, long n ) { if( i == 0 ) return 1; long x = witness( a, i / 2, n ); if( x == 0 ) // If n is recursively composite, stop return 0; // n is not prime if we find a nontrivial square root of 1 long y = ( x * x ) % n; if( y == 1 && x != 1 && x != n - 1 ) return 0; if( i % 2 != 0 ) y = ( a * y ) % n; return y; } /** * The number of witnesses queried in randomized primality test. */ public static final int TRIALS = 5; /** * Randomized primality test. * Adjust TRIALS to increase confidence level. * @param n the number to test. * @return if false, n is definitely not prime. * If true, n is probably prime. */ public static boolean isPrime( long n ) { Random r = new Random( ); for( int counter = 0; counter < TRIALS; counter++ ) if( witness( r.randomLong( 2, n - 2 ), n - 1, n ) != 1 ) return false; return true; } Figure 10.63 A probabilistic primality testing algorithm

Chapter 10 Algorithm Design Techniques Recall that if witness returns anything but 1, it has proven that N cannot be prime. The proof is nonconstructive, because it gives no method of actually ﬁnding the factors. It has been shown that for any (sufﬁciently large) N, at most (N −9)/4 values of A fool this algorithm. Thus, if A is chosen at random, and the algorithm answers that N is (probably) prime, then the algorithm is correct at least 75 percent of the time. Suppose witness is run 50 times. The probability that the algorithm is fooled once is at most 1 4. Thus, the probability that 50 independent random trials fool the algorithm is never more than 1/450 = 2−100. This is actually a very conservative estimate, which holds for only a few choices of N. Even so, one is more likely to see a hardware error than an incorrect claim of primality. Randomized algorithms for primality testing are important because they have long been signiﬁcantly faster than the best nonrandomized algorithms, and although the randomized algorithm can occasionally produce a false positive, the chances of this happening can be made small enough to be negligible. For many years, it was suspected that it was possible to test deﬁnitively the primality of a d-digit number in time polynomial in d, but no such algorithm was known. Recently, however, deterministic polynomial time algorithms for primality testing have been discovered. While these algorithms are tremendously exciting theoretical results, they are not yet competitive with the randomized algorithms. The end of chapter references provide more information. 10.5 Backtracking Algorithms The last algorithm design technique we will examine is backtracking. In many cases, a backtracking algorithm amounts to a clever implementation of exhaustive search, with generally unfavorable performance. This is not always the case, however, and even so, in some cases, the savings over a brute-force exhaustive search can be signiﬁcant. Performance is, of course, relative: an O(N2) algorithm for sorting is pretty bad, but an O(N5) algorithm for the traveling salesman (or any NP-complete) problem would be a landmark result. A practical example of a backtracking algorithm is the problem of arranging furniture in a new house. There are many possibilities to try, but typically only a few are actually considered. Starting with no arrangement, each piece of furniture is placed in some part of the room. If all the furniture is placed and the owner is happy, then the algorithm terminates. If we reach a point where all subsequent placement of furniture is undesirable, we have to undo the last step and try an alternative. Of course, this might force another undo, and so forth. If we ﬁnd that we undo all possible ﬁrst steps, then there is no placement of furniture that is satisfactory. Otherwise, we eventually terminate with a satisfactory arrangement. Notice that although this algorithm is essentially brute force, it does not try all possibilities directly. For instance, arrangements that consider placing the sofa in the kitchen are never tried. Many other bad arrangements are discarded early, because an undesirable subset of the arrangement is detected. The elimination of a large group of possibilities in one step is known as pruning.

10.5 Backtracking Algorithms We will see two examples of backtracking algorithms. The ﬁrst is a problem in computational geometry. Our second example shows how computers select moves in games, such as chess and checkers. 10.5.1 The Turnpike Reconstruction Problem Suppose we are given N points, p1, p2, . . . , pN, located on the x-axis. xi is the x coordinate of pi. Let us further assume that x1 = 0 and the points are given from left to right. These N points determine N(N −1)/2 (not necessarily unique) distances d1, d2, . . . , dN between every pair of points of the form |xi −xj| (i ̸= j). It is clear that if we are given the set of points, it is easy to construct the set of distances in O(N2) time. This set will not be sorted, but if we are willing to settle for an O(N2 log N) time bound, the distances can be sorted, too. The turnpike reconstruction problem is to reconstruct a point set from the distances. This ﬁnds applications in physics and molecular biology (see the references for pointers to more speciﬁc information). The name derives from the analogy of points to turnpike exits on East Coast highways. Just as factoring seems harder than multiplication, the reconstruction problem seems harder than the construction problem. Nobody has been able to give an algorithm that is guaranteed to work in polynomial time. The algorithm that we will present generally runs in O(N2 log N) but can take exponential time in the worst case. Of course, given one solution to the problem, an inﬁnite number of others can be constructed by adding an offset to all the points. This is why we insist that the ﬁrst point is anchored at 0 and that the point set that constitutes a solution is output in nondecreasing order. Let D be the set of distances, and assume that |D| = M = N(N −1)/2. As an example, suppose that D = {1, 2, 2, 2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 8, 10} Since |D| = 15, we know that N = 6. We start the algorithm by setting x1 = 0. Clearly, x6 = 10, since 10 is the largest element in D. We remove 10 from D. The points that we have placed and the remaining distances are as shown in the following ﬁgure. x1 = 0 x6 = 10 D = {1, 2, 2, 2, 3, 3, 3, 4, 5, 5, 5, 6, 7, 8} The largest remaining distance is 8, which means that either x2 = 2 or x5 = 8. By symmetry, we can conclude that the choice is unimportant, since either both choices lead to solutions (which are mirror images of each other), or neither do, so we can set x5 = 8 without affecting the solution. We then remove the distances x6 −x5 = 2 and x5 −x1 = 8 from D, obtaining x1 = 0 x6 = 10 D = {1, 2, 2, 3, 3, 3, 4, 5, 5, 5, 6, 7} x5 = 8

Chapter 10 Algorithm Design Techniques The next step is not obvious. Since 7 is the largest value in D, either x4 = 7 or x2 = 3. If x4 = 7, then the distances x6 −7 = 3 and x5 −7 = 1 must also be present in D. A quick check shows that indeed they are. On the other hand, if we set x2 = 3, then 3−x1 = 3 and x5 −3 = 5 must be present in D. These distances are also in D, so we have no guidance on which choice to make. Thus, we try one and see if it leads to a solution. If it turns out that it does not, we can come back and try the other. Trying the ﬁrst choice, we set x4 = 7, which leaves x1 = 0 x6 = 10 D = {2, 2, 3, 3, 4, 5, 5, 5, 6} x5 = 8 x4 = 7 At this point, we have x1 = 0, x4 = 7, x5 = 8, and x6 = 10. Now the largest distance is 6, so either x3 = 6 or x2 = 4. But if x3 = 6, then x4 −x3 = 1, which is impossible, since 1 is no longer in D. On the other hand, if x2 = 4 then x2 −x0 = 4, and x5 −x2 = 4. This is also impossible, since 4 only appears once in D. Thus, this line of reasoning leaves no solution, so we backtrack. Since x4 = 7 failed to produce a solution, we try x2 = 3. If this also fails, we give up and report no solution. We now have x1 = 0 x6 = 10 D = {1, 2, 2, 3, 3, 4, 5, 5, 6} x2 = 3 x5 = 8 Once again, we have to choose between x4 = 6 and x3 = 4. x3 = 4 is impossible, because D only has one occurrence of 4, and two would be implied by this choice. x4 = 6 is possible, so we obtain x1 = 0 x6 = 10 D = {1, 2, 3, 5, 5} x2 = 3 x4 = 6 x5 = 8 The only remaining choice is to assign x3 = 5; this works because it leaves D empty, and so we have a solution. x1 = 0 x6 = 10 D = {} x2 = 3 x4 = 6 x3 = 5 x5 = 8 Figure 10.64 shows a decision tree representing the actions taken to arrive at the solution. Instead of labeling the branches, we have placed the labels in the branches’ destination nodes. A node with an asterisk indicates that the points chosen are inconsistent with the given distances; nodes with two asterisks have only impossible nodes as children, and thus represent an incorrect path. The pseudocode to implement this algorithm is mostly straightforward. The driving routine, turnpike, is shown in Figure 10.65. It receives the point array x (which need not

10.5 Backtracking Algorithms x 1=0, x6=10 x 5=8 x 4=7** x 2=3 x 3=6* x 2=4* x 3=4* x 4=6 x 3=5 Figure 10.64 Decision tree for the worked turnpike reconstruction example boolean turnpike( int [ ] x, DistSet d, int n ) { x[ 1 ] = 0; x[ n ] = d.deleteMax( ); x[ n - 1 ] = d.deleteMax( ); if( x[ n ] - x[ n - 1 ] ∈d ) { d.remove( x[ n ] - x[ n - 1 ] ); return place( x, d, n, 2, n - 2 ); } else return false; } Figure 10.65 Turnpike reconstruction algorithm: driver routine (pseudocode) be initialized) and the distance set D and N.4 If a solution is discovered, then true will be returned, the answer will be placed in x, and D will be empty. Otherwise, false will be returned, x will be undeﬁned, and the distance set D will be untouched. The routine sets x1, xN−1, and xN, as described above, alters D, and calls the backtracking algorithm place to place the other points. We presume that a check has already been made to ensure that |D| = N(N −1)/2. 4 We have used one-letter variable names, which is generally poor style, for consistency with the worked example. We also, for simplicity, do not give the type of variables. Finally, we index arrays starting at 1, instead of 0.

Chapter 10 Algorithm Design Techniques The more difﬁcult part is the backtracking algorithm, which is shown in Figure 10.66. Like most backtracking algorithms, the most convenient implementation is recursive. We pass the same arguments plus the boundaries Left and Right; xLeft, . . . , xRight are the x coordinates of points that we are trying to place. If D is empty (or Left > Right), then a solution has been found, and we can return. Otherwise, we ﬁrst try to place xRight = Dmax. If all the appropriate distances are present (in the correct quantity), then we tentatively place this point, remove these distances, and try to ﬁll from Left to Right −1. If the distances are not present, or the attempt to ﬁll Left to Right −1 fails, then we try setting xLeft = xN −dmax, using a similar strategy. If this does not work, then there is no solution; otherwise a solution has been found, and this information is eventually passed back to turnpike by the return statement and x array. The analysis of the algorithm involves two factors. Suppose lines 9 through 11 and 18 through 20 are never executed. We can maintain D as a balanced binary search (or splay) tree (this would require a code modiﬁcation, of course). If we never backtrack, there are at most O(N2) operations involving D, such as deletion and the contains implied at lines 4 and 12 to 13. This claim is obvious for deletions, since D has O(N2) elements and no element is ever reinserted. Each call to place uses at most 2N contains, and since place never backtracks in this analysis, there can be at most 2N2 contains operations. Thus, if there is no backtracking, the running time is O(N2 log N). Of course, backtracking happens, and if it happens repeatedly, then the performance of the algorithm is affected. This can be forced to happen by construction of a pathological case. Experiments have shown that if the points have integer coordinates distributed uniformly and randomly from [0, Dmax], where Dmax = (N2), then, almost certainly, at most one backtrack is performed during the entire algorithm. 10.5.2 Games As our last application, we will consider the strategy that a computer might use to play a strategic game, such as checkers or chess. We will use, as an example, the much simpler game of tic-tac-toe, because it makes the points easier to illustrate. Tic-tac-toe is a draw if both sides play optimally. By performing a careful case-by-case analysis, it is not a difﬁcult matter to construct an algorithm that never loses and always wins when presented the opportunity. This can be done, because certain positions are known traps and can be handled by a lookup table. Other strategies, such as taking the center square when it is available, make the analysis simpler. If this is done, then by using a table we can always choose a move based only on the current position. Of course, this strategy requires the programmer, and not the computer, to do most of the thinking. Minimax Strategy The more general strategy is to use an evaluation function to quantify the “goodness” of a position. A position that is a win for a computer might get the value of +1; a draw could get 0; and a position that the computer has lost would get a −1. A position for which this assignment can be determined by examining the board is known as a terminal position.

10.5 Backtracking Algorithms /** * Backtracking algorithm to place the points x[left] ... x[right]. * x[1]...x[left-1] and x[right+1]...x[n] already tentatively placed. * If place returns true, then x[left]...x[right] will have values. */ boolean place( int [ ] x, DistSet d, int n, int left, int right ) { int dmax; boolean found = false; if( d.isEmpty( ) ) return true; dmax = d.findMax( ); // Check if setting x[right] = dmax is feasible. if( | x[j] - dmax | ∈d for all 1≤j<left and right<j≤n ) { x[right] = dmax; // Try x[right]=dmax for( 1≤j<left, right<j≤n ) d.remove( | x[j] - dmax | ); found = place( x, d, n, left, right-1 ); if( !found ) // Backtrack for( 1≤j<left, right<j≤n ) // Undo the deletion d.insert( | x[j] - dmax | ); } // If first attempt failed, try to see if setting // x[left]=x[n]-dmax is feasible. if( !found && ( | x[n] - dmax - x[j] | ∈d for all 1≤j<left and right<j≤n ) ) { x[left] = x[n] - dmax; // Same logic as before for( 1≤j<left, right<j≤n ) d.remove( | x[n] - dmax - x[j] | ); found = place( x, d, n, left+1, right ); if( !found ) // Backtrack for( 1≤j<left, right<j≤n ) // Undo the deletion d.insert( | x[n] - dmax - x[j] | ); } return found; } Figure 10.66 Turnpike reconstruction algorithm: backtracking steps (pseudocode)

Chapter 10 Algorithm Design Techniques If a position is not terminal, the value of the position is determined by recursively assuming optimal play by both sides. This is known as a minimax strategy, because one player (the human) is trying to minimize the value of the position, while the other player (the computer) is trying to maximize it. A successor position of P is any position Ps that is reachable from P by playing one move. If the computer is to move when in some position P, it recursively evaluates the value of all the successor positions. The computer chooses the move with the largest value; this is the value of P. To evaluate any successor position Ps, all of Ps’s successors are recursively evaluated, and the smallest value is chosen. This smallest value represents the most favorable reply for the human player. The code in Figure 10.67 makes the computer’s strategy more clear. Lines 22 through 25 evaluate immediate wins or draws. If neither of these cases apply, then the position is nonterminal. Recalling that value should contain the maximum of all possible successor positions, line 28 initializes it to the smallest possible value, and the loop in lines 29 through 42 searches for improvements. Each successor position is recursively evaluated in turn by lines 32 through 34. This is recursive, because, as we will see, findHumanMove calls findCompMove. If the human’s response to a move leaves the computer with a more favorable position than that obtained with the previously best computer move, then the value and bestMove are updated. Figure 10.68 shows the method for the human’s move selection. The logic is virtually identical, except that the human player chooses the move that leads to the lowest-valued position. Indeed, it is not difﬁcult to combine these two procedures into one by passing an extra variable, which indicates whose turn it is to move. This does make the code somewhat less readable, so we have stayed with separate routines. Since these routines must pass back both the value of the position and the best move, we pass these two variables in a MoveInfo object. We leave supporting routines as an exercise. The most costly computation is the case where the computer is asked to pick the opening move. Since at this stage the game is a forced draw, the computer selects square 1.5 A total of 97,162 positions were examined, and the calculation took a few seconds. No attempt was made to optimize the code. When the computer moves second, the number of positions examined is 5,185 if the human selects the center square, 9,761 when a corner square is selected, and 13,233 when a noncorner edge square is selected. For more complex games, such as checkers and chess, it is obviously infeasible to search all the way to the terminal nodes.6 In this case, we have to stop the search after a certain depth of recursion is reached. The nodes where the recursion is stopped become terminal nodes. These terminal nodes are evaluated with a function that estimates the 5 We numbered the squares starting from the top left and moving right. However, this is only important for the supporting routines. 6 It is estimated that if this search were conducted for chess, at least 10100 positions would be examined for the ﬁrst move. Even if the improvements described later in this section were incorporated, this number could not be reduced to a practical level.

public class MoveInfo { public int move; public int value; public MoveInfo( int m, int v ) { move = m; value = v; } } /** * Recursive method to find best move for computer. * MoveInfo.move returns a number from 1-9 indicating square. * Possible evaluations satisfy COMP_LOSS < DRAW < COMP_WIN. * Complementary method findHumanMove is Figure 10.68. */ public MoveInfo findCompMove( ) { int i, responseValue; int value, bestMove = 1; MoveInfo quickWinInfo; if( fullBoard( ) ) value = DRAW; else if( ( quickWinInfo = immediateCompWin( ) ) != null ) return quickWinInfo; else { value = COMP_LOSS; for( i = 1; i <= 9; i++ ) // Try each square if( isEmpty( i ) ) { place( i, COMP ); responseValue = findHumanMove( ).value; unplace( i ); // Restore board if( responseValue > value ) { // Update best move value = responseValue; bestMove = i; } } } return new MoveInfo( bestMove, value ); } Figure 10.67 Minimax tic-tac-toe algorithm: computer selection

Chapter 10 Algorithm Design Techniques public MoveInfo findHumanMove( ) { int i, responseValue; int value, bestMove = 1; MoveInfo quickWinInfo; if( fullBoard( ) ) value = DRAW; else if( ( quickWinInfo = immediateHumanWin( ) ) != null ) return quickWinInfo; else { value = COMP_WIN; for( i = 1; i <= 9; i++ ) // Try each square { if( isEmpty( i ) ) { place( i, HUMAN ); responseValue = findCompMove( ).value; unplace( i ); // Restore board if( responseValue < value ) { // Update best move value = responseValue; bestMove = i; } } } } return new MoveInfo( bestMove, value ); } Figure 10.68 Minimax tic-tac-toe algorithm: human selection value of the position. For instance, in a chess program, the evaluation function measures such variables as the relative amount and strength of pieces and positional factors. The evaluation function is crucial for success, because the computer’s move selection is based on maximizing this function. The best computer chess programs have surprisingly sophisticated evaluation functions. Nevertheless, for computer chess, the single most important factor seems to be number of moves of look-ahead the program is capable of. This is sometimes known as ply; it is

10.5 Backtracking Algorithms X X O X O X ... X X O X O X ... Figure 10.69 Two searches that arrive at identical position equal to the depth of the recursion. To implement this, an extra parameter is given to the search routines. The basic method to increase the look-ahead factor in game programs is to come up with methods that evaluate fewer nodes without losing any information. One method which we have already seen is to use a table to keep track of all positions that have been evaluated. For instance, in the course of searching for the ﬁrst move, the program will examine the positions in Figure 10.69. If the values of the positions are saved, the second occurrence of a position need not be recomputed; it essentially becomes a terminal position. The data structure that records this is known as a transposition table; it is almost always implemented by hashing. In many cases, this can save considerable computation. For instance, in a chess endgame, where there are relatively few pieces, the time savings can allow a search to go several levels deeper. α–β Pruning Probably the most signiﬁcant improvement one can obtain in general is known as α–β pruning. Figure 10.70 shows the trace of the recursive calls used to evaluate some hypothetical position in a hypothetical game. This is commonly referred to as a game tree. (We have avoided the use of this term until now, because it is somewhat misleading: No tree is actually constructed by the algorithm. The game tree is just an abstract concept.) The value of the game tree is 44. Figure 10.71 shows the evaluation of the same game tree, with several (but not all possible) unevaluated nodes. Almost half of the terminal nodes have not been checked. We show that evaluating them would not change the value at the root. First, consider node D. Figure 10.72 shows the information that has been gathered when it is time to evaluate D. At this point, we are still in findHumanMove and are contemplating a call to findCompMove on D. However, we already know that findHumanMove will return at most 40, since it is a min node. On the other hand, its max node parent has already found a sequence that guarantees 44. Nothing that D does can possibly increase this value. Therefore, D does not need to be evaluated. This pruning

Chapter 10 Algorithm Design Techniques Max Min Max Min Max Figure 10.70 A hypothetical game tree A B C D Max Min Max Min Max Figure 10.71 A pruned game tree of the tree is known as α pruning. An identical situation occurs at node B. To implement α pruning, findCompMove passes its tentative maximum (α) to findHumanMove. If the tentative minimum of findHumanMove falls below this value, then findHumanMove returns immediately. A similar thing happens at nodes A and C. This time, we are in the middle of a findCompMove and are about to make a call to findHumanMove to evaluate C. Figure 10.73 shows the situation that is encountered at node C. However, the findHumanMove, at the min level, which has called findCompMove, has already determined that it can force a value of at most 44 (recall that low values are good for the human side). Since findCompMove has a tentative maximum of 68, nothing that C does will affect the result at the min level. Therefore, C should not be evaluated. This type of pruning is known as β pruning; it is the symmetric version of α pruning. When both techniques are combined, we have α–β pruning. Implementing α–β pruning requires surprisingly little code. Figure 10.74 shows half of the α–β pruning scheme (minus type declarations); you should have no trouble coding the other half.

10.5 Backtracking Algorithms ≥ ≤ D? Max Min Figure 10.72 The node marked ? is unimportant ≥ ≤ C? Min Max Figure 10.73 The node marked ? is unimportant To take full advantage of α–β pruning, game programs usually try to apply the evaluation function to nonterminal nodes in an attempt to place the best moves early in the search. The result is even more pruning than one would expect from a random ordering of the nodes. Other techniques, such as searching deeper in more active lines of play, are also employed. In practice, α–β pruning limits the searching to only O( √ N) nodes, where N is the size of the full game tree. This is a huge savings and means that searches using α–β pruning can go twice as deep as compared to an unpruned tree. Our tic-tac-toe example is not ideal, because there are so many identical values, but even so, the initial search of 97,162 nodes is reduced to 4,493 nodes. (These counts include nonterminal nodes.) In many games, computers are among the best players in the world. The techniques used are very interesting and can be applied to more serious problems. More details can be found in the references.

Chapter 10 Algorithm Design Techniques /** * Same as before, but perform alpha-beta pruning. * The main routine should make the call with * alpha = COMP_LOSS and beta = COMP_WIN. */ public MoveInfo findCompMove( int alpha, int beta ) { int i, responseValue; int value, bestMove = 1; MoveInfo quickWinInfo; if( fullBoard( ) ) value = DRAW; else if( ( quickWinInfo = immediateCompWin( ) ) != null ) return quickWinInfo; else { value = alpha; for( i = 1; i <= 9 && value < beta; i++ ) // Try each square { if( isEmpty( i ) ) { place( i, COMP ); responseValue = findHumanMove( value, beta ).value; unplace( i ); // Restore board if( responseValue > value ) { // Update best move value = responseValue; bestMove = i; } } } } return new MoveInfo( bestMove, value ); } Figure 10.74 Minimax tic-tac-toe algorithm with α–β pruning: computer selection

C H A P T E R 11 Amortized Analysis In this chapter, we will analyze the running times for several of the advanced data structures that have been presented in Chapters 4 and 6. In particular, we will consider the worst-case running time for any sequence of M operations. This contrasts with the more typical analysis, in which a worst-case bound is given for any single operation. As an example, we have seen that AVL trees support the standard tree operations in O(log N) worst-case time per operation. AVL trees are somewhat complicated to implement, not only because there are a host of cases, but also because height balance information must be maintained and updated correctly. The reason that AVL trees are used is that a sequence of (N) operations on an unbalanced search tree could require (N2) time, which would be expensive. For search trees, the O(N) worst-case running time of an operation is not the real problem. The major problem is that this could happen repeatedly. Splay trees offer a pleasant alternative. Although any operation can still require (N) time, this degenerate behavior cannot occur repeatedly, and we can prove that any sequence of M operations takes O(M log N) worst-case time (total). Thus, in the long run this data structure behaves as though each operation takes O(log N). We call this an amortized time bound. Amortized bounds are weaker than the corresponding worst-case bounds, because there is no guarantee for any single operation. Since this is generally not important, we are willing to sacriﬁce the bound on a single operation, if we can retain the same bound for the sequence of operations and at the same time simplify the data structure. Amortized bounds are stronger than the equivalent average-case bound. For instance, binary search trees have O(log N) average time per operation, but it is still possible for a sequence of M operations to take O(MN) time. Because deriving an amortized bound requires us to look at an entire sequence of operations instead of just one, we expect that the analysis will be more tricky. We will see that this expectation is generally realized. In this chapter we shall r Analyze the binomial queue operations. r Analyze skew heaps. r Introduce and analyze the Fibonacci heap. r Analyze splay trees.

Chapter 11 Amortized Analysis 11.1 An Unrelated Puzzle Consider the following puzzle: Two kittens are placed on opposite ends of a football ﬁeld, 100 yards apart. They walk toward each other at the speed of 10 yards per minute. At the same time, their mother is at one end of the ﬁeld. She can run at 100 yards per minute. The mother runs from one kitten to the other, making turns with no loss of speed, until the kittens (and thus the mother) meet at midﬁeld. How far does the mother run? It is not hard to solve this puzzle with a brute-force calculation. We leave the details to you, but one expects that this calculation will involve computing the sum of an inﬁnite geometric series. Although this straightforward calculation will lead to an answer, it turns out that a much simpler solution can be arrived at by introducing an extra variable, namely, time. Because the kittens are 100 yards apart and approach each other at a combined velocity of 20 yards per minute, it takes them ﬁve minutes to get to midﬁeld. Since the mother runs 100 yards per minute, her total is 500 yards. This puzzle illustrates the point that sometimes it is easier to solve a problem indirectly than directly. The amortized analyses that we will perform will use this idea. We will introduce an extra variable, known as the potential, to allow us to prove results that seem very difﬁcult to establish otherwise. 11.2 Binomial Queues The ﬁrst data structure we will look at is the binomial queue of Chapter 6, which we now review brieﬂy. Recall that a binomial tree B0 is a one-node tree, and for k > 0, the binomial tree Bk is built by melding two binomial trees Bk−1 together. Binomial trees B0 through B4 are shown in Figure 11.1. The rank of a node in a binomial tree is equal to the number of children; in particular, the rank of the root of Bk is k. A binomial queue is a collection of heap-ordered binomial trees, in which there can be at most one binomial tree Bk for any k. Two binomial queues, H1 and H2, are shown in Figure 11.2. The most important operation is merge. To merge two binomial queues, an operation similar to addition of binary integers is performed: At any stage we may have zero, one, two, or possibly three Bk trees, depending on whether or not the two priority queues contain a Bk tree and whether or not a Bk tree is carried over from the previous step. If there is zero or one Bk tree, it is placed as a tree in the resultant binomial queue. If there are two Bk trees, they are melded into a Bk+1 tree and carried over; if there are three Bk trees, one is placed as a tree in the binomial queue and the other two are melded and carried over. The result of merging H1 and H2 is shown in Figure 11.3. Insertion is performed by creating a one-node binomial queue and performing a merge. The time to do this is M + 1, where M represents the smallest type of binomial tree BM not present in the binomial queue. Thus, insertion into a binomial queue that has a B0 tree but no B1 tree requires two steps. Deletion of the minimum is accomplished by removing the

11.2 Binomial Queues B 3 B 2 B 1 B 0 B 4 Figure 11.1 Binomial trees B0, B1, B2, B3, and B4 H 2: H 1: Figure 11.2 Two binomial queues H1 and H2 H 3: 13 Figure 11.3 Binomial queue H3: the result of merging H1 and H2

Chapter 11 Amortized Analysis minimum and splitting the original binomial queue into two binomial queues, which are then merged. A less terse explanation of these operations is given in Chapter 6. We consider a very simple problem ﬁrst. Suppose we want to build a binomial queue of N elements. We know that building a binary heap of N elements can be done in O(N), so we expect a similar bound for binomial queues. Claim. A binomial queue of N elements can be built by N successive insertions in O(N) time. The claim, if true, would give an extremely simple algorithm. Since the worst-case time for each insertion is O(log N), it is not obvious that the claim is true. Recall that if this algorithm were applied to binary heaps, the running time would be O(N log N). To prove the claim, we could do a direct calculation. To measure the running time, we deﬁne the cost of each insertion to be one time unit plus an extra unit for each linking step. Summing this cost over all insertions gives the total running time. This total is N units plus the total number of linking steps. The 1st, 3rd, 5th, and all odd-numbered steps require no linking steps, since there is no B0 present at the time of insertion. Thus, half the insertions require no linking steps. A quarter of the insertions require only one linking step (2nd, 6th, 10th, and so on). An eighth requires two, and so on. We could add this all up and bound the number of linking steps by N, proving the claim. This brute-force calculation will not help when we try to analyze a sequence of operations that include more than just insertions, so we will use another approach to prove this result. Consider the result of an insertion. If there is no B0 tree present at the time of the insertion, then the insertion costs a total of one unit, using the same accounting as above. The result of the insertion is that there is now a B0 tree, and thus we have added one tree to the forest of binomial trees. If there is a B0 tree but no B1 tree, then the insertion costs two units. The new forest will have a B1 tree but will no longer have a B0 tree, so the number of trees in the forest is unchanged. An insertion that costs three units will create a B2 tree but destroy a B0 and B1 tree, yielding a net loss of one tree in the forest. In fact, it is easy to see that, in general, an insertion that costs c units results in a net increase of 2 −c trees in the forest, because a Bc−1 tree is created but all Bi trees 0 ≤i < c −1 are removed. Thus, expensive insertions remove trees, while cheap insertions create trees. Let Ci be the cost of the ith insertion. Let Ti be the number of trees after the ith insertion. T0 = 0 is the number of trees initially. Then we have the invariant Ci + (Ti −Ti−1) = 2 (11.1) We then have C1 + (T1 −T0) = 2 C2 + (T2 −T1) = 2 ... CN−1 + (TN−1 −TN−2) = 2 CN + (TN −TN−1) = 2

11.2 Binomial Queues If we add all these equations, most of the Ti terms cancel, leaving N  i=1 Ci + TN −T0 = 2N or equivalently, N  i=1 Ci = 2N −(TN −T0) Recall that T0 = 0 and TN, the number of trees after the N insertions, is certainly not negative, so (TN −T0) is not negative. Thus N  i=1 Ci ≤2N which proves the claim. During the buildBinomialQueue routine, each insertion had a worst-case time of O(log N), but since the entire routine used at most 2N units of time, the insertions behaved as though each used no more than two units each. This example illustrates the general technique we will use. The state of the data structure at any time is given by a function known as the potential. The potential function is not maintained by the program but rather is an accounting device that will help with the analysis. When operations take less time than we have allocated for them, the unused time is “saved” in the form of a higher potential. In our example, the potential of the data structure is simply the number of trees. In the analysis above, when we have insertions that use only one unit instead of the two units that are allocated, the extra unit is saved for later by an increase in potential. When operations occur that exceed the allotted time, then the excess time is accounted for by a decrease in potential. One may view the potential as representing a savings account. If an operation uses less than its allotted time, the difference is saved for use later on by more expensive operations. Figure 11.4 shows the cumulative running time used by buildBinomialQueue over a sequence of insertions. Observe that the running time never exceeds 2N and that the potential in the binomial queue after any insertion measures the amount of savings. Once a potential function is chosen, we write the main equation: Tactual +  Potential = Tamortized (11.2) Tactual, the actual time of an operation, represents the exact (observed) amount of time required to execute a particular operation. In a binary search tree, for example, the actual time to perform a contains(x) is 1 plus the depth of the node containing x. If we sum the basic equation over the entire sequence, and if the ﬁnal potential is at least as large as the initial potential, then the amortized time is an upper bound on the actual time used during the execution of the sequence. Notice that while Tactual varies from operation to operation, Tamortized is stable. Picking a potential function that proves a meaningful bound is a very tricky task; there is no one method that is used. Generally, many potential functions are tried before the one

Chapter 11 Amortized Analysis 2N Total Time Total Potential Figure 11.4 A sequence of N inserts that works is found. Nevertheless, the discussion above suggests a few rules, which tell us the properties that good potential functions have. The potential function should r Always assume its minimum at the start of the sequence. A popular method of choosing potential functions is to ensure that the potential function is initially 0, and always nonnegative. All the examples that we will encounter use this strategy. r Cancel a term in the actual time. In our case, if the actual cost was c, then the potential change was 2 −c. When these are added, an amortized cost of 2 is obtained. This is shown in Figure 11.5. We can now perform a complete analysis of binomial queue operations. Theorem 11.1. The amortized running times of insert, deleteMin, and merge are O(1), O(log N), and O(log N), respectively, for binomial queues. Proof. The potential function is the number of trees. The initial potential is 0, and the potential is always nonnegative, so the amortized time is an upper bound on the actual time. The analysis for insert follows from the argument above. For merge, assume the two queues have N1 and N2 nodes with T1 and T2 trees, respectively. Let N = N1+N2. The actual time to perform the merge is O(log(N1) + log(N2)) = O(log N). After the merge, there can be at most log N trees, so the potential can increase by at most O(log N). This gives an amortized bound of O(log N). The deleteMin bound follows in a similar manner.

11.3 Skew Heaps −10 −8 −6 −4 −2 insert cost Potential Change Figure 11.5 The insertion cost and potential change for each operation in a sequence 11.3 Skew Heaps The analysis of binomial queues is a fairly easy example of an amortized analysis. We now look at skew heaps. As is common with many of our examples, once the right potential function is found, the analysis is easy. The difﬁcult part is choosing a meaningful potential function. Recall that for skew heaps, the key operation is merging. To merge two skew heaps, we merge their right paths and make this the new left path. For each node on the new path, except the last, the old left subtree is attached as the right subtree. The last node on the new left path is known to not have a right subtree, so it is silly to give it one. The bound does not depend on this exception, and if the routine is coded recursively, this is what will happen naturally. Figure 11.6 shows the result of merging two skew heaps. Suppose we have two heaps, H1 and H2, and there are r1 and r2 nodes on their respective right paths. Then the actual time to perform the merge is proportional to r1 +r2, so we will drop the Big-Oh notation and charge one unit of time for each node on the paths. Since the heaps have no structure, it is possible that all the nodes in both heaps lie on the right path, and this would give a (N) worst-case bound to merge the heaps (Exercise 11.3 asks you to construct an example). We will show that the amortized time to merge two skew heaps is O(log N). What is needed is some sort of a potential function that captures the effect of skew heap operations. Recall that the effect of a merge is that every node on the right path is moved to the left path, and its old left child becomes the new right child. One idea might be to classify each node as a right node or left node, depending on whether or not it is a right child, and use the number of right nodes as a potential function. Although the potential is

Chapter 11 Amortized Analysis + Figure 11.6 Merging of two skew heaps initially 0 and always nonnegative, the problem is that the potential does not decrease after a merge and thus does not adequately reﬂect the savings in the data structure. The result is that this potential function cannot be used to prove the desired bound. A similar idea is to classify nodes as either heavy or light, depending on whether or not the right subtree of any node has more nodes than the left subtree. Deﬁnition 11.1. A node p is heavy if the number of descendants of p’s right subtree is at least half of the number of descendants of p, and light otherwise. Note that the number of descendants of a node includes the node itself. As an example, Figure 11.7 shows a skew heap. The nodes with values 15, 3, 6, 12, and 7 are heavy, and all other nodes are light. The potential function we will use is the number of heavy nodes in the (collection of) heaps. This seems like a good choice, because a long right path will contain an inordinate Figure 11.7 Skew heap—heavy nodes are 3, 6, 7, 12, and 15

11.3 Skew Heaps number of heavy nodes. Because nodes on this path have their children swapped, these nodes will be converted to light nodes as a result of the merge. Theorem 11.2. The amortized time to merge two skew heaps is O(log N). Proof. Let H1 and H2 be the two heaps, with N1 and N2 nodes respectively. Suppose the right path of H1 has l1 light nodes and h1 heavy nodes, for a total of l1 + h1. Likewise, H2 has l2 light and h2 heavy nodes on its right path, for a total of l2 + h2 nodes. If we adopt the convention that the cost of merging two skew heaps is the total number of nodes on their right paths, then the actual time to perform the merge is l1 + l2 + h1 + h2. Now the only nodes whose heavy/light status can change are nodes that are initially on the right path (and wind up on the left path), since no other nodes have their subtrees altered. This is shown by the example in Figure 11.8. If a heavy node is initially on the right path, then after the merge it must become a light node. The other nodes that were on the right path were light and may or may not become heavy, but since we are proving an upper bound, we will have to assume the worst, which is that they become heavy and increase the potential. Then the net change in the number of heavy nodes is at most l1 + l2 −h1 −h2. Adding the actual time and the potential change [Equation (11.2)] gives an amortized bound of 2(l1+l2). Now we must show that l1 + l2 = O(log N). Since l1 and l2 are the number of light nodes on the original right paths, and the right subtree of a light node is less than half the size of the tree rooted at the light node, it follows directly that the number of light nodes on the right path is at most log N1 + log N2, which is O(log N). The proof is completed by noting that the initial potential is 0 and that the potential is always nonnegative. It is important to verify this, since otherwise the amortized time does not bound the actual time and is meaningless. Since the insert and deleteMin operations are basically just merges, they also have O(log N) amortized bounds. + L L L H L L L L L L H L Figure 11.8 Change in heavy/light status after a merge

Chapter 11 Amortized Analysis 11.4 Fibonacci Heaps In Section 9.3.2, we showed how to use priority queues to improve on the naïve O(|V|2) running time of Dijkstra’s shortest-path algorithm. The important observation was that the running time was dominated by |E| decreaseKey operations and |V| insert and deleteMin operations. These operations take place on a set of size at most |V|. By using a binary heap, all these operations take O(log |V|) time, so the resulting bound for Dijkstra’s algorithm can be reduced to O(|E| log |V|). In order to lower this time bound, the time required to perform the decreaseKey operation must be improved. d-heaps, which were described in Section 6.5, give an O(logd |V|) time bound for the decreaseKey operation as well as for insert, but an O(d logd |V|) bound for deleteMin. By choosing d to balance the costs of |E| decreaseKey operations with |V| deleteMin operations, and remembering that d must always be at least 2, we see that a good choice for d is d = max(2, ⌊|E|/|V|⌋) This improves the time bound for Dijkstra’s algorithm to O(|E| log(2+⌊|E|/|V|⌋) |V|) The Fibonacci heap is a data structure that supports all the basic heap operations in O(1) amortized time, with the exception of deleteMin and delete, which take O(log N) amortized time. It immediately follows that the heap operations in Dijkstra’s algorithm will require a total of O(|E| + |V| log |V|) time. Fibonacci heaps1 generalize binomial queues by adding two new concepts: A different implementation of decreaseKey: The method we have seen before is to percolate the element up toward the root. It does not seem reasonable to expect an O(1) amortized bound for this strategy, so a new method is needed. Lazy merging: Two heaps are merged only when it is required to do so. This is similar to lazy deletion. For lazy merging, merges are cheap, but because lazy merging does not actually combine trees, the deleteMin operation could encounter lots of trees, making that operation expensive. Any one deleteMin could take linear time, but it is always possible to charge the time to previous merge operations. In particular, an expensive deleteMin must have been preceded by a large number of unduly cheap merges, which were able to store up extra potential. 11.4.1 Cutting Nodes in Leftist Heaps In binary heaps, the decreaseKey operation is implemented by lowering the value at a node and then percolating it up toward the root until heap order is established. In the worst 1 The name comes from a property of this data structure, which we will prove later in the section.

11.4 Fibonacci Heaps ... N−4 N−3 N−2 N−1 Figure 11.9 Decreasing N −1 to 0 via percolate up would take (N) time Figure 11.10 Sample leftist heap H case, this can take O(log N) time, which is the length of the longest path toward the root in a balanced tree. This strategy does not work if the tree that represents the priority queue does not have O(log N) depth. As an example, if this strategy is applied to leftist heaps, then the decreaseKey operation could take (N) time, as the example in Figure 11.9 shows. We see that for leftist heaps, another strategy is needed for the decreaseKey operation. Our example will be the leftist heap in Figure 11.10. Suppose we want to decrease the key with value 9 down to 0. If we make the change, we ﬁnd that we have created a violation of heap order, which is indicated by a dashed line in Figure 11.11. We do not want to percolate the 0 to the root, because, as we have seen, there are cases where this could be expensive. The solution is to cut the heap along the dashed line, thus creating two trees, and then merge the two trees back into one. Let X be the node to which the decreaseKey operation is being applied, and let P be its parent. After the cut, we have

Chapter 11 Amortized Analysis P X Figure 11.11 Decreasing 9 to 0 creates a heap order violation X P H 1 T2 Figure 11.12 The two trees after the cut two trees, namely, H1 with root X, and T2, which is the original tree with H1 removed. The situation is shown in Figure 11.12. If these two trees were both leftist heaps, then they could be merged in O(log N) time, and we would be done. It is easy to see that H1 is a leftist heap, since none of its nodes have had any changes in their descendants. Thus, since all of its nodes originally satisﬁed the leftist property, they still must. Nevertheless, it seems that this scheme will not work, because T2 is not necessarily leftist. However, it is easy to reinstate the leftist heap property by using two observations: r Only nodes on the path from P to the root of T2 can be in violation of the leftist heap property; these can be ﬁxed by swapping children. r Since the maximum right path length has at most ⌊log(N + 1)⌋nodes, we only need to check the ﬁrst ⌊log(N + 1)⌋nodes on the path from P to the root of T2. Figure 11.13 shows H1 and T2 after T2 is converted to a leftist heap.

11.4 Fibonacci Heaps H 1 H 2 Figure 11.13 T2 converted to the leftist heap H2 Figure 11.14 decreaseKey(X, 9) completed by merging H1 and H2 Because we can convert T2 to the leftist heap H2 in O(log N) steps, and then merge H1 and H2, we have an O(log N) algorithm for performing the decreaseKey operation in leftist heaps. The heap that results in our example is shown in Figure 11.14. 11.4.2 Lazy Merging for Binomial Queues The second idea that is used by Fibonacci heaps is lazy merging. We will apply this idea to binomial queues and show that the amortized time to perform a merge operation (as well as insertion, which is a special case) is O(1). The amortized time for deleteMin will still be O(log N). The idea is as follows: To merge two binomial queues, merely concatenate the two lists of binomial trees, creating a new binomial queue. This new queue may have several trees of the same size, so it violates the binomial queue property. We will call this a lazy binomial queue in order to maintain consistency. This is a fast operation that always takes constant (worst-case) time. As before, an insertion is done by creating a one-node binomial queue and merging. The difference is that the merge is lazy.

Chapter 11 Amortized Analysis Figure 11.15 Lazy binomial queue Figure 11.16 Lazy binomial queue after removing the smallest element (3) The deleteMin operation is much more painful, because it is where we ﬁnally convert the lazy binomial queue back into a standard binomial queue, but, as we will show, it is still O(log N) amortized time—but not O(log N) worst-case time, as before. To perform a deleteMin, we ﬁnd (and eventually return) the minimum element. As before, we delete it from the queue, making each of its children new trees. We then merge all the trees into a binomial queue by merging two equal-sized trees until it is no longer possible. As an example, Figure 11.15 shows a lazy binomial queue. In a lazy binomial queue, there can be more than one tree of the same size. To perform the deleteMin, we remove the smallest element, as before, and obtain the tree in Figure 11.16. We now have to merge all the trees and obtain a standard binomial queue. A standard binomial queue has at most one tree of each rank. In order to do this efﬁciently, we must be able to perform the merge in time proportional to the number of trees present (T) (or log N, whichever is larger). To do this, we form an array of lists, L0, L1, . . . , LRmax+1, where Rmax is the rank of the largest tree. Each list LR contains all of the trees of rank R. The procedure in Figure 11.17 is then applied. for( R = 0; R <= ⌊log N⌋; R++ ) while( |LR| >= 2 ) { Remove two trees from LR; Merge the two trees into a new tree; Add the new tree to LR+1; } Figure 11.17 Procedure to reinstate a binomial queue

11.4 Fibonacci Heaps Figure 11.18 Combining the binomial trees into a binomial queue Each time through the loop, at lines 4 through 6, the total number of trees is reduced by 1. This means that this part of the code, which takes constant time per execution, can only be performed T −1 times, where T is the number of trees. The for loop counters and tests at the end of the while loop take O(log N) time, so the running time is O(T + log N), as required. Figure 11.18 shows the execution of this algorithm on the previous collection of binomial trees. Amortized Analysis of Lazy Binomial Queues To carry out the amortized analysis of lazy binomial queues, we will use the same potential function that was used for standard binomial queues. Thus, the potential of a lazy binomial queue is the number of trees. Theorem 11.3. The amortized running times of merge and insert are both O(1) for lazy binomial queues. The amortized running time of deleteMin is O(log N).

Chapter 11 Amortized Analysis Proof. The potential function is the number of trees in the collection of binomial queues. The initial potential is 0, and the potential is always nonnegative. Thus, over a sequence of operations, the total amortized time is an upper bound on the total actual time. For the merge operation, the actual time is constant, and the number of trees in the collection of binomial queues is unchanged, so, by Equation (11.2), the amortized time is O(1). For the insert operation, the actual time is constant, and the number of trees can increase by at most 1, so the amortized time is O(1). The deleteMin operation is more complicated. Let R be the rank of the tree that contains the minimum element, and let T be the number of trees. Thus, the potential at the start of the deleteMin operation is T. To perform a deleteMin, the children of the smallest node are split off into separate trees. This creates T + R trees, which must be merged into a standard binomial queue. The actual time to perform this is T + R + log N, if we ignore the constant in the Big-Oh notation, by the argument above.2 Once this is done, there can be at most log N trees remaining, so the potential function can increase by at most (log N) −T. Adding the actual time and the change in potential gives an amortized bound of 2 log N + R. Since all the trees are binomial trees, we know that R ≤log N. Thus we arrive at an O(log N) amortized time bound for the deleteMin operation. 11.4.3 The Fibonacci Heap Operations As we mentioned before, the Fibonacci heap combines the leftist heap decreaseKey operation with the lazy binomial queue merge operation. Unfortunately, we cannot use both operations without a slight modiﬁcation. The problem is that if arbitrary cuts are made in the binomial trees, the resulting forest will no longer be a collection of binomial trees. Because of this, it will no longer be true that the rank of every tree is at most ⌊log N⌋. Since the amortized bound for deleteMin in lazy binomial queues was shown to be 2 log N + R, we need R = O(log N) for the deleteMin bound to hold. In order to ensure that R = O(log N), we apply the following rules to all nonroot nodes: r Mark a (nonroot) node the ﬁrst time that it loses a child (because of a cut). r If a marked node loses another child, then cut it from its parent. This node now becomes the root of a separate tree and is no longer marked. This is called a cascading cut, because several of these could occur in one decreaseKey operation. Figure 11.19 shows one tree in a Fibonacci heap prior to a decreaseKey operation. When the node with key 39 is changed to 12, the heap order is violated. Therefore, the node is cut from its parent, becoming the root of a new tree. Since the node containing 33 is marked, this is its second lost child, and thus it is cut from its parent (10). Now 10 has 2 We can do this because we can place the constant implied by the Big-Oh notation in the potential function and still get the cancellation of terms, which is needed in the proof.

11.4 Fibonacci Heaps 33* 10* 17* 8* 11* Figure 11.19 A tree in the Fibonacci heap prior to decreasing 39 to 12 5* 17* 8* 11* Figure 11.20 The resulting segment of the Fibonacci heap after the decreaseKey operation lost its second child, so it is cut from 5. The process stops here, since 5 was unmarked. The node 5 is now marked. The result is shown in Figure 11.20. Notice that 10 and 33, which used to be marked nodes, are no longer marked, because they are now root nodes. This will be a crucial observation in our proof of the time bound. 11.4.4 Proof of the Time Bound Recall that the reason for marking nodes is that we needed to bound the rank (number of children) R of any node. We will now show that any node with N descendants has rank O(log N). Lemma 11.1. Let X be any node in a Fibonacci heap. Let ci be the ith oldest child of X. Then the rank of ci is at least i −2. Proof. At the time when ci was linked to X, X already had (older) children c1, c2, . . . , ci−1. Thus, X had at least i −1 children when it linked to ci. Since nodes are linked only if they have the same rank, it follows that at the time that ci was linked to X, ci had

Chapter 11 Amortized Analysis at least i −1 children. Since that time, it could have lost at most one child, or else it would have been cut from X. Thus, ci has at least i −2 children. From Lemma 11.1, it is easy to show that any node of rank R must have a lot of descendants. Lemma 11.2. Let Fk be the Fibonacci numbers deﬁned (in Section 1.2) by F0 = 1, F1 = 1, and Fk = Fk−1 + Fk−2. Any node of rank R ≥1 has at least FR+1 descendants (including itself). Proof. Let SR be the smallest tree of rank R. Clearly, S0 = 1 and S1 = 2. By Lemma 11.1, a tree of rank R must have subtrees of rank at least R −2, R −3, . . . , 1, and 0, plus another subtree, which has at least one node. Along with the root of SR itself, this gives a minimum value for SR>1 of SR = 2 + R−2 i=0 Si. It is easy to show that SR = FR+1 (Exercise 1.11(a)). Because it is well known that the Fibonacci numbers grow exponentially, it immediately follows that any node with s descendants has rank at most O(log s). Thus, we have Lemma 11.3. The rank of any node in a Fibonacci heap is O(log N). Proof. Immediate from the discussion above. If all we were concerned about were the time bounds for the merge, insert, and deleteMin operations, then we could stop here and prove the desired amortized time bounds. Of course, the whole point of Fibonacci heaps is to obtain an O(1) time bound for decreaseKey as well. The actual time required for a decreaseKey operation is 1 plus the number of cascading cuts that are performed during the operation. Since the number of cascading cuts could be much more than O(1), we will need to pay for this with a loss in potential. If we look at Figure 11.20, we see that the number of trees actually increases with each cascading cut, so we will have to enhance the potential function to include something that decreases during cascading cuts. Notice that we cannot just throw out the number of trees from the potential function, since then we will not be able to prove the time bound for the merge operation. Looking at Figure 11.20 again, we see that a cascading cut causes a decrease in the number of marked nodes, because each node that is the victim of a cascading cut becomes an unmarked root. Since each cascading cut costs 1 unit of actual time and increases the tree potential by 1, we will count each marked node as two units of potential. This way, we have a chance of canceling out the number of cascading cuts. Theorem 11.4. The amortized time bounds for Fibonacci heaps are O(1) for insert, merge, and decreaseKey and O(log N) for deleteMin.

11.5 Splay Trees Proof. The potential is the number of trees in the collection of Fibonacci heaps plus twice the number of marked nodes. As usual, the initial potential is 0 and is always nonnegative. Thus, over a sequence of operations, the total amortized time is an upper bound on the total actual time. For the merge operation, the actual time is constant, and the number of trees and marked nodes is unchanged, so, by Equation (11.2), the amortized time is O(1). For the insert operation, the actual time is constant, the number of trees increases by 1, and the number of marked nodes is unchanged. Thus, the potential increases by at most 1, so the amortized time is O(1). For the deleteMin operation, let R be the rank of the tree that contains the minimum element, and let T be the number of trees before the operation. To perform a deleteMin, we once again split the children of a tree, creating an additional R new trees. Notice that, although this can remove marked nodes (by making them unmarked roots), this cannot create any additional marked nodes. These R new trees, along with the other T trees, must now be merged, at a cost of T+R+log N = T+O(log N), by Lemma 11.3. Since there can be at most O(log N) trees, and the number of marked nodes cannot increase, the potential change is at most O(log N) −T. Adding the actual time and potential change gives the O(log N) amortized bound for deleteMin. Finally, for the decreaseKey operation, let C be the number of cascading cuts. The actual cost of a decreaseKey is C + 1, which is the total number of cuts performed. The ﬁrst (noncascading) cut creates a new tree and thus increases the potential by 1. Each cascading cut creates a new tree but converts a marked node to an unmarked (root) node, for a net loss of one unit per cascading cut. The last cut also can convert an unmarked node (in Figure 11.20 it is node 5) into a marked node, thus increasing the potential by 2. The total change in potential is thus at most 3 −C. Adding the actual time and the potential change gives a total of 4, which is O(1). 11.5 Splay Trees As a ﬁnal example, we analyze the running time of splay trees. Recall, from Chapter 4, that after an access of some item X is performed, a splaying step moves X to the root by a series of three operations: zig, zig-zag, and zig-zig. These tree rotations are shown in Figure 11.21. We adopt the convention that if a tree rotation is being performed at node X, then prior to the rotation P is its parent and (if X is not a child of the root) G is its grandparent. Recall that the time required for any tree operation on node X is proportional to the number of nodes on the path from the root to X. If we count each zig operation as one rotation and each zig-zig or zig-zag as two rotations, then the cost of any access is equal to 1 plus the number of rotations. In order to show an O(log N) amortized bound for the splaying step, we need a potential function that can increase by at most O(log N) over the entire splaying step but that will also cancel out the number of rotations performed during the step. It is not at all easy to ﬁnd a potential function that satisﬁes these criteria. A simple ﬁrst guess at a potential

Chapter 11 Amortized Analysis P X C B A X P A C B G D P X A C B X P G A B C D X B A P C G D X A P B G C D Figure 11.21 zig, zig-zag, and zig-zig operations; each has a symmetric case (not shown) function might be the sum of the depths of all the nodes in the tree. This does not work, because the potential can increase by (N) during an access. A canonical example of this occurs when elements are inserted in sequential order. A potential function  that does work is deﬁned as (T) =  i∈T log S(i) where S(i) represents the number of descendants of i (including i itself). The potential function is the sum, over all nodes i in the tree T, of the logarithm of S(i). To simplify the notation, we will deﬁne R(i) = log S(i) This makes (T) =  i∈T R(i) R(i) represents the rank of node i. The terminology is similar to what we used in the analysis of the disjoint set algorithm, binomial queues, and Fibonacci heaps. In all these data structures, the meaning of rank is somewhat different, but the rank is generally meant to be on the order (magnitude) of the logarithm of the size of the tree. For a tree T with N

11.5 Splay Trees nodes, the rank of the root is simply R(T) = log N. Using the sum of ranks as a potential function is similar to using the sum of heights as a potential function. The important difference is that while a rotation can change the heights of many nodes in the tree, only X, P, and G can have their ranks changed. Before proving the main theorem, we need the following lemma. Lemma 11.4. If a + b ≤c, and a and b are both positive integers, then log a + log b ≤2 log c −2 Proof. By the arithmetic-geometric mean inequality, √ ab ≤(a + b)/2 Thus √ ab ≤c/2 Squaring both sides gives ab ≤c2/4 Taking logarithms of both sides proves the lemma. With the preliminaries taken care of, we are ready to prove the main theorem. Theorem 11.5. The amortized time to splay a tree with root T at node X is at most 3(R(T) −R(X)) + 1 = O(log N). Proof. The potential function is the sum of the ranks of the nodes in T. If X is the root of T, then there are no rotations, so there is no potential change. The actual time is 1 to access the node; thus, the amortized time is 1 and the theorem is true. Thus, we may assume that there is at least one rotation. For any splaying step, let Ri(X) and Si(X) be the rank and size of X before the step, and let Rf(X) and Sf(X) be the rank and size of X immediately after the splaying step. We will show that the amortized time required for a zig is at most 3(Rf(X) −Ri(X)) + 1 and that the amortized time for either a zig-zag or zig-zig is at most 3(Rf(X) −Ri(X)). We will show that when we add over all steps, the sum telescopes to the desired time bound. Zig step: For the zig step, the actual time is 1 (for the single rotation), and the potential change is Rf(X) + Rf(P) −Ri(X) −Ri(P). Notice that the potential change is easy to compute, because only X’s and P’s trees change size. Thus, using AT to represent amortized time, ATzig = 1 + Rf(X) + Rf(P) −Ri(X) −Ri(P)

Chapter 11 Amortized Analysis From Figure 11.21 we see that Si(P) ≥Sf(P); thus, it follows that Ri(P) ≥Rf(P). Thus, ATzig ≤1 + Rf(X) −Ri(X) Since Sf(X) ≥Si(X), it follows that Rf(X) −Ri(X) ≥0, so we may increase the right side, obtaining ATzig ≤1 + 3(Rf(X) −Ri(X)) Zig-zag step: For the zig-zag case, the actual cost is 2, and the potential change is Rf(X) + Rf(P) + Rf(G) −Ri(X) −Ri(P) −Ri(G). This gives an amortized time bound of ATzig-zag = 2 + Rf(X) + Rf(P) + Rf(G) −Ri(X) −Ri(P) −Ri(G) From Figure 11.21 we see that Sf(X) = Si(G), so their ranks must be equal. Thus, we obtain ATzig-zag = 2 + Rf(P) + Rf(G) −Ri(X) −Ri(P) We also see that Si(P) ≥Si(X). Consequently, Ri(X) ≤Ri(P). Making this substitution gives ATzig-zag ≤2 + Rf(P) + Rf(G) −2Ri(X) From Figure 11.21 we see that Sf(P) + Sf(G) ≤Sf(X). If we apply Lemma 11.4, we obtain log Sf(P) + log Sf(G) ≤2 log Sf(X) −2 By the deﬁnition of rank, this becomes Rf(P) + Rf(G) ≤2Rf(X) −2 Substituting this, we obtain ATzig-zag ≤2Rf(X) −2Ri(X) ≤2(Rf(X) −Ri(X)) Since Rf(X) ≥Ri(X), we obtain ATzig-zag ≤3(Rf(X) −Ri(X)) Zig-zig step: The third case is the zig-zig. The proof of this case is very similar to the zig-zag case. The important inequalities are Rf(X) = Ri(G), Rf(X) ≥Rf(P), Ri(X) ≤Ri(P), and Si(X) + Sf(G) ≤Sf(X). We leave the details as Exercise 11.8. The amortized cost of an entire splay is the sum of the amortized costs of each splay step. Figure 11.22 shows the steps that are performed in a splay at node 2. Let R1(2), R2(2), R3(2), and R4(2) be the rank of node 2 in each of the four trees. The cost of the ﬁrst step, which is a zig-zag, is at most 3(R2(2) −R1(2)). The cost of the second step, which is a zig-zig, is 3(R3(2) −R2(2)). The last step is a zig and has cost no larger than 3(R4(2) −R3(2)) + 1. The total cost thus telescopes to 3(R4(2) −R1(2)) + 1.

11.5 Splay Trees Figure 11.22 The splaying steps involved in splaying at node 2 In general, by adding up the amortized costs of all the rotations, of which at most one can be a zig, we see that the total amortized cost to splay at node X is at most 3(Rf(X) −Ri(X)) + 1, where Ri(X) is the rank of X before the ﬁrst splaying step and Rf(X) is the rank of X after the last splaying step. Since the last splaying step leaves X at the root, we obtain an amortized bound of 3(R(T) −Ri(X)) + 1, which is O(log N). Because every operation on a splay tree requires a splay, the amortized cost of any operation is within a constant factor of the amortized cost of a splay. Thus, all splay tree access operations take O(log N) amortized time. To show that insertions and deletions take O(log N), amortized time, potential changes that occur either prior to or after the splaying step should be accounted for. In the case of insertion, assume we are inserting into an N−1 node tree. Thus, after the insertion, we have an N-node tree, and the splaying bound applies. However, the insertion at the leaf node adds potential prior to the splay to each node on the path from the leaf node to the root. Let n1, n2, . . . , nk be the nodes on the path prior to the insertion of the leaf (nk is the root), and assume they have sizes s1, s2, . . . , sk. After the insertions, the sizes are s1 + 1, s2 + 1, . . . , sk + 1. (The leaf will contribute 0 to the potential so we can ignore it.) Note that (excluding the root node) sj + 1 ≤sj+1, so the new rank of nj is no more than the old rank of nj+1. Thus, the increase of ranks, which is the maximum increase in potential that results from adding a new leaf, is limited by the new rank of the root, which is O(log N). A deletion consists of a nonsplaying step that attaches one tree to another. This does increase the rank of one node, but that is limited by log N (and is compensated by the removal of a node, which at the time was a root). Thus the splaying costs accurately bound the cost of a deletion. By using a more general potential function, it is possible to show that splay trees have several remarkable properties. This is discussed in more detail in the exercises.

C H A P T E R 12 Advanced Data Structures and Implementation In this chapter, we discuss six data structures with an emphasis on practicality. We begin by examining alternatives to the AVL tree discussed in Chapter 4. These include an optimized version of the splay tree, the red-black tree, and the treap. We also examine the sufﬁx tree, which allows searching for a pattern in a large text. We then examine a data structure that can be used for multidimensional data. In this case, each item may have several keys. The k-d tree allows searching relative to any key. Finally, we examine the pairing heap, which seems to be the most practical alternative to the Fibonacci heap. Recurring themes include r Nonrecursive, top-down (instead of bottom-up) search tree implementations when appropriate. r Detailed, optimized implementations that make use of, among other things, sentinel nodes. 12.1 Top-Down Splay Trees In Chapter 4, we discussed the basic splay tree operation. When an item X is inserted as a leaf, a series of tree rotations, known as a splay, makes X the new root of the tree. A splay is also performed during searches, and if an item is not found, a splay is performed on the last node on the access path. In Chapter 11, we showed that the amortized cost of a splay tree operation is O(log N). A direct implementation of this strategy requires a traversal from the root down the tree, and then a bottom-up traversal to implement the splaying step. This can be done either by maintaining parent links, or by storing the access path on a stack. Unfortunately, both methods require a substantial amount of overhead, and both must handle many special cases. In this section, we show how to perform rotations on the initial access path. The result is a procedure that is faster in practice, uses only O(1) extra space, but retains the O(log N) amortized time bound. Figure 12.1 shows the rotations for the zig, zig-zig, and zig-zag cases. (As is customary, three symmetric rotations are omitted.) At any point in the access, we have a current node

Chapter 12 Advanced Data Structures and Implementation L R L R B B A A Y A Y X X R C A X L R L C B Y Z A Z X R B C Y X B L R L C A Y Z B Z X Y Figure 12.1 Top-down splay rotations: zig, zig-zig, and zig-zag X that is the root of its subtree; this is represented in our diagrams as the “middle” tree.1 Tree L stores nodes in the tree T that are less than X, but not in X’s subtree; similarly tree R stores nodes in the tree T that are larger than X, but not in X’s subtree. Initially, X is the root of T, and L and R are empty. If the rotation should be a zig, then the tree rooted at Y becomes the new root of the middle tree. X and subtree B are attached as a left child of the smallest item in R; X’s left child is logically made null.2 As a result, X is the new smallest item in R. Note carefully that Y does not have to be a leaf for the zig case to apply. If we are searching for an item that is smaller than Y, and Y has no left child (but does have a right child), then the zig case will apply. For the zig-zig case, we have a similar dissection. The crucial point is that a rotation between X and Y is performed. The zig-zag case brings the bottom node Z to the top in the middle tree and attaches subtrees X and Y to R and L, respectively. Note that Y is attached to, and then becomes, the largest item in L. The zig-zag step can be simpliﬁed somewhat because no rotations are performed. Instead of making Z the root of the middle tree, we make Y the root. This is shown in Figure 12.2. This simpliﬁes the coding because the action for the zig-zag case becomes 1 For simplicity we don’t distinguish between a “node” and the item in the node. 2 In the code, the smallest node in R does not have a null left link because there is no need for it. This means that printTree(r) will include some items that logically are not in R.

12.1 Top-Down Splay Trees L R C A B Y Z A B Y Z X L R C X Figure 12.2 Simpliﬁed top-down zig-zag L R B A X L R A B X Figure 12.3 Final arrangement for top-down splaying identical to the zig case. This would seem advantageous because testing for a host of cases is time-consuming. The disadvantage is that by descending only one level, we have more iterations in the splaying procedure. Once we have performed the ﬁnal splaying step, Figure 12.3 shows how L, R, and the middle tree are arranged to form a single tree. Note carefully that the result is different from bottom-up splaying. The crucial fact is that the O(log N) amortized bound is preserved (Exercise 12.1). An example of the top-down splaying algorithm is shown in Figure 12.4. We attempt to access 19 in the tree. The ﬁrst step is a zig-zag. In accordance with (a symmetric version of) Figure 12.2, we bring the subtree rooted at 25 to the root of the middle tree and attach 12 and its left subtree to L. Next we have a zig-zig: 15 is elevated to the root of the middle tree, and a rotation between 20 and 25 is performed, with the resulting subtree being attached to R. The search for 19 then results in a terminal zig. The middle tree’s new root is 18, and 15 and its left subtree are attached as a right child of L’s largest node. The reassembly, in accordance with Figure 12.3, terminates the splay step. We will use a header with left and right links to eventually reference the roots of the left and right trees. Since these trees are initially empty, a header is used to correspond to the min or max node of the right or left tree, respectively, in this initial state. This way the code can avoid checking for empty trees. The ﬁrst time the left tree becomes nonempty, the right link will get initialized and will not change in the future; thus it will contain the root of the left tree at the end of the top-down search. Similarly, the left link will eventually contain the root of the right tree. The SplayTree class, whose skeleton is shown in Figure 12.5, includes a constructor that is used to allocate the nullNode sentinel. We use the sentinel nullNode to represent logically a null reference. We will repeatedly use this technique to simplify the code (and consequently make the code somewhat faster). Figure 12.6 (shown on page 546) gives the

Chapter 12 Advanced Data Structures and Implementation Empty Empty Empty simplified zig-zag zig-zig zig reassemble  Figure 12.4 Steps in top-down splay (access 19 in top tree)

12.1 Top-Down Splay Trees public class SplayTree<AnyType extends Comparable<? super AnyType>> { public SplayTree( ) { nullNode = new BinaryNode<>( null ); nullNode.left = nullNode.right = nullNode; root = nullNode; } private BinaryNode<AnyType> splay( AnyType x, BinaryNode<AnyType> t ) { /* Figure 12.6 */ } public void insert( AnyType x ) { /* Figure 12.7 */ } public void remove( AnyType x ) { /* Figure 12.8 */ } public AnyType findMin( ) { /* See online code */ } public AnyType findMax( ) { /* See online code */ } public boolean contains( AnyType x ) { /* See online code */ } public void makeEmpty( ) { root = nullNode; } public boolean isEmpty( ) { return root == nullNode; } // Basic node stored in unbalanced binary search trees private static class BinaryNode<AnyType> { /* Same as in Figure 4.16 */ } private BinaryNode<AnyType> root; private BinaryNode<AnyType> nullNode; private BinaryNode<AnyType> header = new BinaryNode<>( null ); // For splay private BinaryNode<AnyType> newNode = null; // Used between different inserts private BinaryNode<AnyType> rotateWithLeftChild( BinaryNode<AnyType> k2 ) { /* See online code */ } private BinaryNode<AnyType> rotateWithRightChild( BinaryNode<AnyType> k1 ) { /* See online code */ } } Figure 12.5 Splay trees: class skeleton

/** * Internal method to perform a top-down splay. * The last accessed node becomes the new root. * @param x the target item to splay around. * @param t the root of the subtree to splay. * @return the subtree after the splay. */ private BinaryNode<AnyType> splay( AnyType x, BinaryNode<AnyType> t ) { BinaryNode<AnyType> leftTreeMax, rightTreeMin; header.left = header.right = nullNode; leftTreeMax = rightTreeMin = header; nullNode.element = x; // Guarantee a match for( ; ; ) if( x.compareTo( t.element ) < 0 ) { if( x.compareTo( t.left.element ) < 0 ) t = rotateWithLeftChild( t ); if( t.left == nullNode ) break; // Link Right rightTreeMin.left = t; rightTreeMin = t; t = t.left; } else if( x.compareTo( t.element ) > 0 ) { if( x.compareTo( t.right.element ) > 0 ) t = rotateWithRightChild( t ); if( t.right == nullNode ) break; // Link Left leftTreeMax.right = t; leftTreeMax = t; t = t.right; } else break; leftTreeMax.right = t.left; rightTreeMin.left = t.right; t.left = header.right; t.right = header.left; return t; } Figure 12.6 Top-down splaying method

12.1 Top-Down Splay Trees /** * Insert into the tree. * @param x the item to insert. */ public void insert( AnyType x ) { if( newNode == null ) newNode = new BinaryNode<>( null ); newNode.element = x; if( root == nullNode ) { newNode.left = newNode.right = nullNode; root = newNode; } else { root = splay( x, root ); if( x.compareTo( root.element ) < 0 ) { newNode.left = root.left; newNode.right = root; root.left = nullNode; root = newNode; } else if( x.compareTo( root.element ) > 0 ) { newNode.right = root.right; newNode.left = root; root.right = nullNode; root = newNode; } else return; // No duplicates } newNode = null; // So next insert will call new } Figure 12.7 Top-down splay tree insert code for the splaying procedure. The header node allows us to be certain that we can attach X to the largest node in R without having to worry that R might be empty (and similarly for the symmetric case dealing with L).

Chapter 12 Advanced Data Structures and Implementation As we mentioned above, before the reassembly at the end of the splay, header.left and header.right reference the roots of R and L, respectively (this is not a typo—follow the links). Except for this detail, the code is relatively straightforward. Figure 12.7 shows the method to insert an item into a tree. A new node is allocated (if necessary), and if the tree is empty, a one-node tree is created. Otherwise we splay root around the inserted value x. If the data in the new root is equal to x, we have a duplicate; instead of reinserting x, we preserve newNode for a future insertion and return immediately. If the new root contains a value larger than x, then the new root and its right subtree become a right subtree of newNode, and root’s left subtree becomes the left subtree of newNode. Similar logic applies if root’s new root contains a value smaller than x. In either case, newNode becomes the new root. In Chapter 4, we showed that deletion in splay trees is easy, because a splay will place the target of the deletion at the root. We close by showing the deletion routine in Figure 12.8. It is indeed rare that a deletion procedure is shorter than the corresponding insertion procedure. /** * Remove from the tree. * @param x the item to remove. */ public void remove( AnyType x ) { BinaryNode<AnyType> newTree; // If x is found, it will be at the root root = splay( x, root ); if( root.element.compareTo( x ) != 0 ) return; // Item not found; do nothing if( root.left == nullNode ) newTree = root.right; else { // Find the maximum in the left subtree // Splay it to the root; and then attach right child newTree = root.left; newTree = splay( x, newTree ); newTree.right = root.right; } root = newTree; } Figure 12.8 Top-down deletion procedure

12.2 Red-Black Trees 12.2 Red-Black Trees A historically popular alternative to the AVL tree is the red-black tree. Operations on redblack trees take O(log N) time in the worst case, and, as we will see, a careful nonrecursive implementation (for insertion) can be done relatively effortlessly (compared with AVL trees). A red-black tree is a binary search tree with the following coloring properties: 1. Every node is colored either red or black. 2. The root is black. 3. If a node is red, its children must be black. 4. Every path from a node to a null reference must contain the same number of black nodes. A consequence of the coloring rules is that the height of a red-black tree is at most 2 log(N + 1). Consequently, searching is guaranteed to be a logarithmic operation. Figure 12.9 shows a red-black tree. Red nodes are shown with double circles. The difﬁculty, as usual, is inserting a new item into the tree. The new item, as usual, is placed as a leaf in the tree. If we color this item black, then we are certain to violate condition 4, because we will create a longer path of black nodes. Thus the item must be colored red. If the parent is black, we are done. If the parent is already red, then we will violate condition 3 by having consecutive red nodes. In this case, we have to adjust the tree to ensure that condition 3 is enforced (without introducing a violation of condition 4). The basic operations that are used to do this are color changes and tree rotations. 12.2.1 Bottom-Up Insertion As we have already mentioned, if the parent of the newly inserted item is black, we are done. Thus insertion of 25 into the tree in Figure 12.9 is trivial. Figure 12.9 Example of a red-black tree (insertion sequence is: 10, 85, 15, 70, 20, 60, 30, 50, 65, 80, 90, 40, 5, 55)

Chapter 12 Advanced Data Structures and Implementation There are several cases (each with a mirror image symmetry) to consider if the parent is red. First, suppose that the sibling of the parent is black (we adopt the convention that null nodes are black). This would apply for an insertion of 3 or 8, but not for the insertion of 99. Let X be the newly added leaf, P be its parent, S be the sibling of the parent (if it exists), and G be the grandparent. Only X and P are red in this case; G is black, because otherwise there would be two consecutive red nodes prior to the insertion, in violation of red-black rules. Adopting the splay tree terminology, X, P, and G can form either a zig-zig chain or a zig-zag chain (in either of two directions). Figure 12.10 shows how we can rotate the tree for the case where P is a left child (note there is a symmetric case). Even though X is a leaf, we have drawn a more general case that allows X to be in the middle of the tree. We will use this more general rotation later. The ﬁrst case corresponds to a single rotation between P and G, and the second case corresponds to a double rotation, ﬁrst between X and P and then between X and G. When we write the code, we have to keep track of the parent, the grandparent, and, for reattachment purposes, the great-grandparent. In both cases, the subtree’s new root is colored black, and so even if the original great-grandparent was red, we removed the possibility of two consecutive red nodes. Equally important, the number of black nodes on the paths into A, B, and C has remained unchanged as a result of the rotations. So far so good. But what happens if S is red, as is the case when we attempt to insert 79 in the tree in Figure 12.9? In that case, initially there is one black node on the path from the subtree’s root to C. After the rotation, there must still be only one black node. But in both cases, there are three nodes (the new root, G, and S) on the path to C. Since only one may be black, and since we cannot have consecutive red nodes, it follows that we’d have to color both S and the subtree’s new root red, and G (and our fourth node) black. That’s great, but what happens if the great-grandparent is also red? In that case, we C A B B A A A B1 B2 C P S X G X X G P S C B1 B2 P S C S G X P G Figure 12.10 Zig rotation and zig-zag rotation work if S is black

12.2 Red-Black Trees could percolate this procedure up toward the root as is done for B-trees and binary heaps, until we no longer have two consecutive red nodes, or we reach the root (which will be recolored black). 12.2.2 Top-Down Red-Black Trees Implementing the percolation would require maintaining the path using a stack or parent links. We saw that splay trees are more efﬁcient if we use a top-down procedure, and it turns out that we can apply a top-down procedure to red-black trees that guarantees that S won’t be red. The procedure is conceptually easy. On the way down, when we see a node X that has two red children, we make X red and the two children black. (If X is the root, after the color ﬂip it will be red but can be made black immediately to restore property 2.) Figure 12.11 shows this color ﬂip. This will induce a red-black violation only if X’s parent P is also red. But in that case, we can apply the appropriate rotations in Figure 12.10. What if X’s parent’s sibling is red? This possibility has been removed by our actions on the way down, and so X’s parent’s sibling can’t be red! Speciﬁcally, if on the way down the tree we see a node Y that has two red children, we know that Y’s grandchildren must be black, and that since Y’s children are made black too, even after the rotation that may occur, we won’t see another red node for two levels. Thus when we see X, if X’s parent is red, it is not possible for X’s parent’s sibling to be red also. As an example, suppose we want to insert 45 into the tree in Figure 12.9. On the way down the tree, we see node 50, which has two red children. Thus, we perform a color ﬂip, making 50 red, and 40 and 55 black. Now 50 and 60 are both red. We perform the single rotation between 60 and 70, making 60 the black root of 30’s right subtree, and 70 and 50 both red. We then continue, performing an identical action if we see other nodes on the path that contain two red children. When we get to the leaf, we insert 45 as a red node, and since the parent is black, we are done. The resulting tree is shown in Figure 12.12. As Figure 12.12 shows, the red-black tree that results is frequently very well balanced. Experiments suggest that the average red-black tree is about as deep as an average AVL tree and that, consequently, the searching times are typically near optimal. The advantage of red-black trees is the relatively low overhead required to perform insertion, and the fact that in practice rotations occur relatively infrequently. An actual implementation is complicated not only by the host of possible rotations, but also by the possibility that some subtrees (such as 10’s right subtree) might be empty, and the special case of dealing with the root (which among other things, has no parent). Thus, we use two sentinel nodes: one for the root, and nullNode, which indicates a null reference, c1 c2 c1 c2 X X Figure 12.11 Color ﬂip: Only if X’s parent is red do we continue with a rotation

Chapter 12 Advanced Data Structures and Implementation Figure 12.12 Insertion of 45 into Figure 12.9 as it did for splay trees. The root sentinel will store the key −∞and a right link to the real root. Because of this, the searching and printing procedures need to be adjusted. The recursive routines are trickiest. Figure 12.13 shows how the inorder traversal is rewritten. Figure 12.14 shows the RedBlackTree skeleton (omitting the methods), along with the constructors. Next, Figure 12.15 shows the routine to perform a single rotation. Because /** * Print the tree contents in sorted order. */ public void printTree( ) { if( isEmpty( ) ) System.out.println( "Empty tree" ); else printTree( header.right ); } /** * Internal method to print a subtree in sorted order. * @param t the node that roots the subtree. */ private void printTree( RedBlackNode<AnyType> t ) { if( t != nullNode ) { printTree( t.left ); System.out.println( t.element ); printTree( t.right ); } } Figure 12.13 Inorder traversal for tree and two sentinels

12.2 Red-Black Trees public class RedBlackTree<AnyType extends Comparable<? super AnyType>> { /** * Construct the tree. */ public RedBlackTree( ) { nullNode = new RedBlackNode<>( null ); nullNode.left = nullNode.right = nullNode; header = new RedBlackNode<>( null ); header.left = header.right = nullNode; } private static class RedBlackNode<AnyType> { // Constructors RedBlackNode( AnyType theElement ) { this( theElement, null, null ); } RedBlackNode( AnyType theElement, RedBlackNode<AnyType> lt, RedBlackNode<AnyType> rt ) { element = theElement; left = lt; right = rt; color = RedBlackTree.BLACK; } AnyType element; // The data in the node RedBlackNode<AnyType> left; // Left child RedBlackNode<AnyType> right; // Right child int color; // Color } private RedBlackNode<AnyType> header; private RedBlackNode<AnyType> nullNode; private static final int BLACK = 1; // BLACK must be 1 private static final int RED = 0; } Figure 12.14 Class skeleton and initialization routines the resultant tree must be attached to a parent, rotate takes the parent node as a parameter. Rather than keeping track of the type of rotation as we descend the tree, we pass item as a parameter. Since we expect very few rotations during the insertion procedure, it turns out that it is not only simpler, but actually faster, to do it this way. rotate simply returns the result of performing an appropriate single rotation. Finally, we provide the insertion procedure in Figure 12.16. The routine handleReorient is called when we encounter a node with two red children, and also when we insert a leaf.

Chapter 12 Advanced Data Structures and Implementation /** * Internal routine that performs a single or double rotation. * Because the result is attached to the parent, there are four cases. * Called by handleReorient. * @param item the item in handleReorient. * @param parent the parent of the root of the rotated subtree. * @return the root of the rotated subtree. */ private RedBlackNode<AnyType> rotate( AnyType item, RedBlackNode<AnyType> parent ) { if( compare( item, parent ) < 0 ) return parent.left = compare( item, parent.left ) < 0 ? rotateWithLeftChild( parent.left ) : // LL rotateWithRightChild( parent.left ) ; // LR else return parent.right = compare( item, parent.right ) < 0 ? rotateWithLeftChild( parent.right ) : // RL rotateWithRightChild( parent.right ); // RR } /** * Compare item and t.element, using compareTo, with * caveat that if t is header, then item is always larger. * This routine is called if it is possible that t is header. * If it is not possible for t to be header, use compareTo directly. */ private final int compare( AnyType item, RedBlackNode<AnyType> t ) { if( t == header ) return 1; else return item.compareTo( t.element ); } Figure 12.15 rotate method The most tricky part is the observation that a double rotation is really two single rotations, and is done only when branching to X (represented in the insert method by current) takes opposite directions. As we mentioned in the earlier discussion, insert must keep track of the parent, grandparent, and great-grandparent as the tree is descended. Since these are shared with handleReorient, we make these class members. Note that after a rotation, the values stored in the grandparent and great-grandparent are no longer correct. However, we are assured that they will be restored by the time they are next needed.

12.2 Red-Black Trees // Used in insert routine and its helpers private RedBlackNode<AnyType> current; private RedBlackNode<AnyType> parent; private RedBlackNode<AnyType> grand; private RedBlackNode<AnyType> great; /** * Internal routine that is called during an insertion * if a node has two red children. Performs flip and rotations. * @param item the item being inserted. */ private void handleReorient( AnyType item ) { // Do the color flip current.color = RED; current.left.color = BLACK; current.right.color = BLACK; if( parent.color == RED ) // Have to rotate { grand.color = RED; if( ( compare( item, grand ) < 0 ) != ( compare( item, parent ) < 0 ) ) parent = rotate( item, grand ); // Start dbl rotate current = rotate( item, great ); current.color = BLACK; } header.right.color = BLACK; // Make root black } /** * Insert into the tree. * @param item the item to insert. */ public void insert( AnyType item ) { current = parent = grand = header; nullNode.element = item; while( compare( item, current ) != 0 ) { great = grand; grand = parent; parent = current; current = compare( item, current ) < 0 ? current.left : current.right; Figure 12.16 Insertion procedure

Chapter 12 Advanced Data Structures and Implementation // Check if two red children; fix if so if( current.left.color == RED && current.right.color == RED ) handleReorient( item ); } // Insertion fails if already present if( current != nullNode ) return; current = new RedBlackNode<>( item, nullNode, nullNode ); // Attach to parent if( compare( item, parent ) < 0 ) parent.left = current; else parent.right = current; handleReorient( item ); } Figure 12.16 (continued) 12.2.3 Top-Down Deletion Deletion in red-black trees can also be performed top-down. Everything boils down to being able to delete a leaf. This is because to delete a node that has two children, we replace it with the smallest node in the right subtree; that node, which must have at most one child, is then deleted. Nodes with only a right child can be deleted in the same manner, while nodes with only a left child can be deleted by replacement with the largest node in the left subtree, and subsequent deletion of that node. Note that for red-black trees, we don’t want to use the strategy of bypassing for the case of a node with one child because that may connect two red nodes in the middle of the tree, making enforcement of the red-black condition difﬁcult. Deletion of a red leaf is, of course, trivial. If a leaf is black, however, the deletion is more complicated because removal of a black node will violate condition 4. The solution is to ensure during the top-down pass that the leaf is red. Throughout this discussion, let X be the current node, T be its sibling, and P be their parent. We begin by coloring the root sentinel red. As we traverse down the tree, we attempt to ensure that X is red. When we arrive at a new node, we are certain that P is red (inductively, by the invariant we are trying to maintain), and that X and T are black (because we can’t have two consecutive red nodes). There are two main cases. First, suppose X has two black children. Then there are three subcases, which are shown in Figure 12.17. If T also has two black children, we can ﬂip the colors of X, T, and P to maintain the invariant. Otherwise, one of T’s children is red. Depending on which

12.2 Red-Black Trees X T     P X T P X T     P R P A A R1 R2 C2 T     R C2 R2 R1 X X T     P R P A A R1 R2 C1 R T R2 R1 C1 X Figure 12.17 Three cases when X is a left child and has two black children one it is,3 we can apply the rotation shown in the second and third cases of Figure 12.17. Note carefully that this case will apply for the leaf, because nullNode is considered to be black. Otherwise, one of X’s children is red. In this case, we fall through to the next level, obtaining new X, T, and P. If we’re lucky, X will land on the red child, and we can continue onward. If not, we know that T will be red, and X and P will be black. We can rotate T and P, making X’s new parent red; X and its grandparent will, of course, be black. At this point we can go back to the ﬁrst main case. 3 If both children are red, we can apply either rotation. As usual, there are symmetric rotations for the case when X is a right child that are not shown.

Chapter 12 Advanced Data Structures and Implementation 12.3 Treaps Our last type of binary search tree, known as the treap, is probably the simplest of all. Like the skip list, it uses random numbers and gives O(log N) expected time behavior for any input. Searching time is identical to an unbalanced binary search tree (and thus slower than balanced search trees), while insertion time is only slightly slower than a recursive unbalanced binary search tree implementation. Although deletion is much slower, it is still O(log N) expected time. The treap is so simple that we can describe it without a picture. Each node in the tree stores an item, a left and right link, and a priority that is randomly assigned when the node is created. A treap is a binary search tree with the property that the node priorities satisfy heap order: Any node’s priority must be at least as large as its parent’s. A collection of distinct items each of which has a distinct priority can only be represented by one treap. This is easily deduced by induction, since the node with the lowest priority must be the root. Consequently, the tree is formed on the basis of the N! possible arrangements of priority instead of the N! item orderings. The node declarations are straightforward, requiring only the addition of the priority ﬁeld, as shown in Figure 12.18. The sentinel nullNode will have priority of ∞. A single, shared, Random object generates random priorities. Insertion into the treap is simple: After an item is added as a leaf, we rotate it up the treap until its priority satisﬁes heap order. It can be shown that the expected number of rotations is less than 2. After the item to be deleted has been found, it can be deleted by increasing its priority to ∞and rotating it down through the path of lowpriority children. Once it is a leaf, it can be removed. The routines in Figure 12.19 and Figure 12.20 implement these strategies using recursion. A nonrecursive implementation is left for the reader (Exercise 12.11). For deletion, note that when the node is logically a private static class TreapNode<AnyType> { // Constructors TreapNode( AnyType theElement ) { this( theElement, null, null ); } TreapNode( AnyType theElement, TreapNode<AnyType> lt, TreapNode<AnyType> rt ) { element = theElement; left = lt; right = rt; priority = randomObj.randomInt( ); } AnyType element; // The data in the node TreapNode<AnyType> left; // Left child TreapNode<AnyType> right; // Right child int priority; // Priority private static Random randomObj = new Random( ); } Figure 12.18 Node declaration for treaps

12.3 Treaps /** * Internal method to insert into a subtree. * @param x the item to insert. * @param t the node that roots the subtree. * @return the new root of the subtree. */ private TreapNode<AnyType> insert( AnyType x, TreapNode<AnyType> t ) { if( t == nullNode ) return new TreapNode<>( x, nullNode, nullNode ); int compareResult = x.compareTo( t.element ); if( compareResult < 0 ) { t.left = insert( x, t.left ); if( t.left.priority < t.priority ) t = rotateWithLeftChild( t ); } else if( compareResult > 0 ) { t.right = insert( x, t.right ); if( t.right.priority < t.priority ) t = rotateWithRightChild( t ); } // Otherwise, it’s a duplicate; do nothing return t; } Figure 12.19 Treaps: insertion routine leaf, it still has nullNode as both its left and right children. Consequently, it is rotated with the right child. After the rotation, t is nullNode, and the left child stores the item that is to be deleted. Thus we change t.left to reference the nullNode sentinel. Note also that our implementation assumes that there are no duplicates; if this is not true, then the remove could fail (why?). The treap is particularly easy to implement because we never have to worry about adjusting the priority ﬁeld. One of the difﬁculties of the balanced tree approaches is that it is difﬁcult to track down errors that result from failing to update balance information in the course of an operation. In terms of total lines for a reasonable insertion and deletion package, the treap, especially a nonrecursive implementation, seems like the hands-down winner.

Chapter 12 Advanced Data Structures and Implementation /** * Internal method to remove from a subtree. * @param x the item to remove. * @param t the node that roots the subtree. * @return the new root of the subtree. */ private TreapNode<AnyType> remove( AnyType x, TreapNode<AnyType> t ) { if( t != nullNode ) { int compareResult = x.compareTo( t.element ); if( compareResult < 0 ) t.left = remove( x, t.left ); else if( compareResult > 0 ) t.right = remove( x, t.right ); else { // Match found if( t.left.priority < t.right.priority ) t = rotateWithLeftChild( t ); else t = rotateWithRightChild( t ); if( t != nullNode ) // Continue on down t = remove( x, t ); else t.left = nullNode; // At a leaf } } return t; } Figure 12.20 Treaps: deletion procedure 12.4 Sufﬁx Arrays and Sufﬁx Trees One of the most fundamental problems in data processing is to ﬁnd the location of a pattern P in a text T. For instance, we may be interested in answering questions such as r Is there a substring of T matching P? r How many times does P appear in T? r Where are all occurrences of P in T?

12.4 Sufﬁx Arrays and Sufﬁx Trees Assuming that the size of P is less than T (and usually it is signiﬁcantly less), then we would reasonably expect that the time to solve this problem for a given P and T would be at least linear in the length of T, and in fact there are several O( | T | ) algorithms. However, we are interested in a more common problem, in which T is ﬁxed, and queries with different P occur frequently. For instance, T could be a huge archive of email messages, and we are interested in repeatedly searching the email messages for different patterns. In this case, we are willing to preprocess T into a nice form that would make each individual search much more efﬁcient, taking time signiﬁcantly less than linear in the size of T—either logarithmic in the size of T, or even better, independent of T and dependent only on the length of P. One such data structure is the sufﬁx array and sufﬁx tree (that sounds like two data structures, but as we will see, they are basically equivalent, and trade time for space). 12.4.1 Sufﬁx Arrays A sufﬁx array for a text T is simply an array of all sufﬁxes of T arranged in sorted order. For instance, suppose our text string is banana. Then the sufﬁx array for banana is shown in Figure 12.21: A sufﬁx array that stores the sufﬁxes explicitly would seem to require quadratic space, since it stores one string of each length 1 to N (where N is the length of T). In Java, this is not exactly true, since Java strings are implemented by maintaining an array of characters and a starting and ending index. This means that when a String is created via a call to substring, the same array of characters is shared, and the additional memory requirement is only the starting and ending index for the new substring. Nonetheless, even this could be considered to be too much space: The sufﬁx array would be constructed for the text, not the pattern, and the text could be huge. Thus it is common for a practical implementation to store only the starting indices of the sufﬁxes in the sufﬁx array, instead of the entire substring. Figure 12.22 shows the indices that would be stored. The sufﬁx array by itself is extremely powerful. For instance, if a pattern P occurs in the text, then it must be a preﬁx of some sufﬁx. A binary search of the sufﬁx array would be enough to determine if the pattern P is in the text: The binary search either lands on P, or P would be between two values, one smaller than P and one larger than P. If P is a preﬁx of some substring, it is a preﬁx of the larger value found at the end of the binary search. a ana anana banana na nana Figure 12.21 Sufﬁxes for “banana”

Chapter 12 Advanced Data Structures and Implementation Index Substring Being Represented a ana anana banana na nana Figure 12.22 Sufﬁx array that stores only Indices (full substrings shown for reference) Immediately, this reduces the query time to O( | P | log | T | ), where the log | T | is the binary search, and the | P | is the cost of the comparison at each step. We can also use the sufﬁx array to ﬁnd the number of occurrences of P: They will be stored sequentially in the sufﬁx array, thus two binary searches sufﬁx to ﬁnd a range of sufﬁxes that will be guaranteed to begin with P. One way to speed this search is to compute the longest common preﬁx (LCP) for each consecutive pair of substrings; if this computation is done as the sufﬁx array is built, then each query to ﬁnd the number of occurrences of P can be sped up to O( | P | + log | T | ) although this is not obvious. Figure 12.23 shows the LCP computed for each substring, relative to the preceding substring. The longest common preﬁx also provides information about the longest pattern that occurs twice in the text: Look for the largest LCP value, and take that many characters of the corresponding substring. In Figure 12.23, this is 3, and the longest repeated pattern is ana. Figure 12.24 shows simple code to compute the sufﬁx array and longest common preﬁx information for any string. Lines 28–30 compute the sufﬁxes, and then the sufﬁxes are sorted at line 32. Lines 34–35 compute the sufﬁxes’ starting indices, and lines 37–39 compute the longest common preﬁxes for adjacent entries by calling the computeLCP routine written at lines 4–13. The running time of the sufﬁx array computation is dominated by the sorting step, which uses O( N log N ) comparisons. In many circumstances this can be reasonably acceptable performance. For instance, a sufﬁx array for a 3,000,000-character Index LCP Substring Being Represented - a ana anana banana na nana Figure 12.23 Sufﬁx array for “banana”; includes longest common preﬁx (LCP)

12.4 Sufﬁx Arrays and Sufﬁx Trees /* * Returns the LCP for any two strings */ public static int computeLCP( String s1, String s2 ) { int i = 0; while( i < s1.length( ) && i < s2.length( ) && s1.charAt( i ) == s2.charAt( i ) ) i++; return i; } /* * Fill in the suffix array and LCP information for String str * @param str the input String * @param SA existing array to place the suffix array * @param LCP existing array to place the LCP information */ public static void createSuffixArray( String str, int [ ] SA, int [ ] LCP ) { if( SA.length != str.length( ) || LCP.length != str.length( ) ) throw new IllegalArgumentException( ); int N = str.length( ); String [ ] suffixes = new String[ N ]; for( int i = 0; i < N; i++ ) suffixes[ i ] = str.substring( i ); Arrays.sort( suffixes ); for( int i = 0; i < N; i++ ) SA[ i ] = N - suffixes[ i ].length( ); LCP[ 0 ] = 0; for( int i = 1; i < N; i++ ) LCP[ i ] = computeLCP( suffixes[ i - 1 ], suffixes[ i ] ); } Figure 12.24 Simple algorithm to create sufﬁx array and LCP array

Chapter 12 Advanced Data Structures and Implementation English-language novel can be built in just a few seconds. However, the O( N log N ) cost, based on the number of comparisons, hides the fact that a String comparison between s1 and s2 takes time that depends on LCP(s1, s2), So while it is true that almost all these comparisons end quickly when run on the sufﬁxes found in natural language processing, the comparisons will be expensive in applications where there are many long common substrings. One such example occurs in pattern searching of DNA, whose alphabet consists of four characters (A, C, G, T) and whose strings can be huge. For instance, the DNA string for a human chromosome 22 has roughly 35 million characters, with a maximum LCP of approximately 200,000, and an average LCP of nearly 2,000. And even the HTML/Java distribution for JDK 1.3 (much smaller than the current distribution) is nearly 70 million characters, with a maximum LCP of roughly 37,000 and an average LCP of roughly 14,000. In the degenerate case of a String that contains only one character, repeated N times, it is easy to see that each comparison takes O(N) time, and the total cost is O( N2 log N ). In Section 12.4.3, we will show a linear-time algorithm to construct the sufﬁx array. 12.4.2 Sufﬁx Trees Sufﬁx arrays are easily searchable by binary search, but the binary search itself automatically implies log T cost. What we would like to do is ﬁnd a matching sufﬁx even more efﬁciently. One idea is to store the sufﬁxes in a trie. A binary trie was seen in our discussion of Huffman codes in Section 10.1.2. The basic idea of the trie is to store the sufﬁxes in a tree. At the root, instead of having two branches, we would have one branch for each possible ﬁrst character. Then at the next level, we would have one branch for the next character, and so on. At each level we are doing multiway branching, much like radix sort, and thus we can ﬁnd a match in time that would depend only on the length of the match. In Figure 12.25, we see on the left a basic trie to store the sufﬁxes of the string deed. These sufﬁxes are d, deed, ed, and eed. In this trie, internal branching nodes are drawn in circles, and the sufﬁxes that are reached are drawn in rectangles. Each branch is labeled with the character that is chosen, but the branch prior to a completed sufﬁx has no label. This representation could waste signiﬁcant space if there are many nodes that have only one child. Thus in Figure 12.25, we see an equivalent representation on the right, known as a compressed trie. Here, single-branch nodes are collapsed into a single node. Notice that although the branches now have multicharacter labels, all the labels for the branches of any given node must have unique ﬁrst characters. Thus it is still just as easy as before to choose which branch to take. Thus we can see that a search for a pattern P depends only on the length of the pattern P, as desired. (We assume that the letters of the alphabet are represented by numbers 1, 2, . . . . Then each node stores an array representing each possible branch and we can locate the appropriate branch in constant time. The empty edge label can be represented by 0.) If the original string has length N, the total number of branches is less than 2N. However, this by itself does not mean that the compressed trie uses linear space: The labels on the edges take up space. The total length of all the labels on the compressed trie in Figure 12.25 is exactly one less than the number of internal branching nodes in the original trie in Figure 12.25. And of course writing all the sufﬁxes in the leaves could

12.4 Sufﬁx Arrays and Sufﬁx Trees d d d d ed eed deed ed eed d d d ed eed deed d e e e e e → Figure 12.25 Left: trie representing the sufﬁxes for deed: {d, deed, ed, eed}; right: compressed trie that collapses single-node branches take quadratic space. So if the original used quadratic space, so does the compressed trie. Fortunately, we can get by with linear space as follows: 1. In the leaves, we use the index where the sufﬁx begins (as in the sufﬁx array). 2. In the internal nodes, we store the number of common characters matched from the root until the internal node; this number represents the letter depth. Figure 12.26 shows how the compressed trie is stored for the sufﬁxes of banana. The leaves are simply the indices of the starting points for each sufﬁx. The internal node with a letter depth of 1 is representing the common string “a” in all nodes that are below it. The internal node with a letter depth of 3 is representing the common string “ana” in all nodes that are below it. And the internal node with a letter depth of 2 is representing the common string “na” in all nodes that are below it. In fact, this analysis makes clear that a sufﬁx tree is equivalent to a sufﬁx array plus an LCP array. a a ana anana na na na na na nana banana banana → Figure 12.26 Compressed trie representing the sufﬁxes for banana: {a, ana, anana, banana, na, nana}. Left: the explicit representation; right: the implicit representation that stores only one integer (plus branches) per node

Chapter 12 Advanced Data Structures and Implementation If we have a sufﬁx tree, we can compute the sufﬁx array and the LCP array by performing an inorder traversal of the tree (compare Figure 12.23 with the sufﬁx tree in Figure 12.26). At that time we can compute the LCP as follows: If the sufﬁx node value PLUS the letter depth of the parent is equal to N, then use the letter depth of the grandparent as the LCP; otherwise use the parent’s letter depth as the LCP. In Figure 12.26, if we proceed inorder, we obtain for our sufﬁxes and LCP values Sufﬁx = 5, with LCP = 0 (the grandparent) because 5 + 1 equals 6 Sufﬁx = 3, with LCP = 1 (the grandparent) because 3 + 3 equals 6 Sufﬁx = 1, with LCP = 3 (the parent) because 1 + 3 does not equal 6 Sufﬁx = 0, with LCP = 0 (the parent) because 0 + 0 does not equal 6 Sufﬁx = 4, with LCP = 0 (the grandparent) because 4 + 2 equals 6 Sufﬁx = 2, with LCP = 2 (the parent) because 2 + 2 does not equal 6 This transformation can clearly be done in linear time. The sufﬁx array and LCP array also uniquely deﬁne the sufﬁx tree. First, create a root with letter depth 0. Then search the LCP array (ignoring position 0, for which LCP is not really deﬁned) for all occurrences of the minimum (which at this phase will be the zeros). Once these minimums are found, they will partition the array (view the LCP as residing between adjacent elements). For instance, in our example, there are two zeros in the LCP array, which partitions the sufﬁx array into three portions: one portion containing the sufﬁxes {5, 3, 1}, another portion containing the sufﬁx {0}, and the third portion containing the sufﬁxes {4, 2}. The internal nodes for these portions can be built recursively, and then the sufﬁx leaves can be attached with an inorder traversal. Although it is not obvious, with care the sufﬁx tree can be generated in linear time from the sufﬁx array and LCP array. The sufﬁx tree solves many problems efﬁciently, especially if we augment each internal node to also maintain the number of sufﬁxes stored below it. A small sampling of sufﬁx tree applications includes the following: 1. Find the longest repeated substring in T: Traverse the tree, ﬁnding the internal node with the largest number letter depth; this represents the maximum LCP. The running time is O( | T | ). This generalizes to the longest substring repeated at least k times. 2. Find the longest common substring in two strings T1 and T2: Form a string T1#T2 where # is a character that is not in either string. Then build a sufﬁx tree for the resulting string and ﬁnd the deepest internal node that has at least one sufﬁx that starts prior to the #, and one that starts after the #. This can be done in time proportional to the total size of the strings and generalizes to an O( k N ) algorithm for k strings of total length N. 3. Find the number of occurrences of the pattern P: Assuming that the sufﬁx tree is augmented so that each leaf keeps track of the number of sufﬁxes below it, simply follow the path down the internal node; the ﬁrst internal node that is a preﬁx of P provides the answer; if there is no such node, the answer is either zero or one and is found by checking the sufﬁx at which the search terminates. This takes time proportional to the length of the pattern P and is independent of the size of |T|. 4. Find the most common substring of a speciﬁed length L > 1: Return the internal node with largest size amongst those with letter depth at least L. This takes time O( | T | ).

12.4 Sufﬁx Arrays and Sufﬁx Trees 12.4.3 Linear-Time Construction of Sufﬁx Arrays and Sufﬁx Trees In Section 12.4.1 we showed the simplest algorithm to construct a sufﬁx array and an LCP array, but this algorithm has O( N2 log N ) worst-case running time for an N-character string and can occur if the string has sufﬁxes with long common preﬁxes. In this section we describe an O( N ) worst-case time algorithm to compute the sufﬁx array. This algorithm can also be enhanced to compute the LCP array in linear time, but there is also a very simple linear-time algorithm to compute the LCP array from the sufﬁx array (see Exercise 12.9 and complete code in Figure 12.50). Either way, we can thus also build a sufﬁx tree in linear time. This algorithm makes use of divide and conquer. The basic idea is as follows: 1. Choose a sample A of sufﬁxes. 2. Sort the sample A by recursion. 3. Sort the remaining sufﬁxes, B, by using the now-sorted sample of sufﬁxes A. 4. Merge A and B. To get an intuition of how step 3 might work, suppose the sample A of sufﬁxes are all sufﬁxes that start at an odd index. Then the remaining sufﬁxes B, are those sufﬁxes that start at an even index. So suppose we have computed the sorted set of sufﬁxes A. To compute the sorted set of sufﬁxes B, we would in effect need to sort all the sufﬁxes that start at even indices. But these sufﬁxes each consist of a single ﬁrst character in an even position, followed by a string that starts with the second character, which must be in an odd position. Thus the string that starts in the second character is exactly a string that is in A. So to sort all the sufﬁxes B, we can do something similar to a radix sort: First sort the strings in B starting from the second character. This should take linear time, since the sorted order of A is already known. Then stably sort on the ﬁrst character of the strings in B. Thus B could be sorted in linear time, after A is sorted recursively. If A and B could then be merged in linear time, we would have a linear-time algorithm. The algorithm we present uses a different sampling step, that admits a simple linear-time merging step. As we describe the algorithm, we will also show how it computes the sufﬁx array for the string ABRACADABRA. We adopt the following conventions: S[i] represents the ith character of string S S[i =>] represents the sufﬁx of S starting at index i <> represents an array Step 1: Sort the characters in the string, assigning them numbers sequentially starting at 1. Then use those numbers for the remainder of the algorithm. Note that the numbers that are assigned depend on the text. So, if the text contains DNA characters A, C, G, and T only, then there will be only four numbers. Then pad the array with three 0’s to avoid boundary cases. If we assume that the alphabet is a ﬁxed size, then the sort takes some constant amount of time.

Chapter 12 Advanced Data Structures and Implementation Input String, S A B R A C A D A B R A New Problem Index Figure 12.27 Mapping of character in string to an array of integers Example: In our example, the mapping is A = 1, B = 2, C = 3, D = 4, and R = 5; the transformation can be visualized in Figure 12.27. Step 2: Divide the text into three groups: S0 = < S[3i]S[3i + 1]S[3i + 2] for i = 0, 1, 2, . . . > S1 = < S[3i + 1]S[3i + 2]S[3i + 3] for i = 0, 1, 2, . . . > S2 = < S[3i + 2]S[3i + 3]S[3i + 4] for i = 0, 1, 2, . . . > The idea is that each of S0, S1, S2 consists of roughly N/3 symbols, but the symbols are no longer the original alphabet, but instead each new symbol is some group of three symbols from the original alphabet. We will call these tri-characters. Most importantly, the sufﬁxes of S0, S1, and S2 combine to form the sufﬁxes of S. Thus one idea would be to recursively compute the sufﬁxes of S0, S1, and S2 (which by deﬁnition implicitly represent sorted strings) and then merge the results in linear time. However, since this would be three recursive calls on problems 1/3 the original size, that would result in an O( N log N ) algorithm. So the idea is going to be to avoid one of the three recursive calls, by computing two of the sufﬁx groups recursively and using that information to compute the third sufﬁx group. Example: In our example, if we look at the original character set and use $ to represent the padded character, we get S0 = [ABR], [ACA], [DAB], [RA$] S1 = [BRA], [CAD], [ABR], [A$$] S2 = [RAC], [ADA], [BRA] We can see that in S0, S1, and S2, each tri-character is now a trio of characters from the original alphabet. Using that alphabet, S0 and S1 are arrays of length four and S2 is an array of length three. S0, S1, and S2 thus have four, four, and three sufﬁxes, respectively. S0’s sufﬁxes are [ABR][ACA][DAB][RA$], [ACA][DAB][RA$], [DAB][RA$], [RA$], which clearly correspond to the sufﬁxes ABRACADABRA, ACADABRA, DABRA, and RA in the original string S. In the original string S these sufﬁxes are located at indices 0, 3, 6, and 9, respectively, so looking at all three of S0, S1, and S2, we can see that each Si represents the sufﬁxes that are located at indices i mod 3 in S.

12.4 Sufﬁx Arrays and Sufﬁx Trees Step 3: Concatenate S1 and S2 and recursively compute the sufﬁx array. In order to compute this sufﬁx array, we will need to sort the new alphabet of tri-characters. This can be done in linear time by three passes of radix sort, since the old characters were already sorted in step 1. If in fact all the tri-characters in the new alphabet are unique, then we do not even need to bother with a recursive call. Making three passes of radix sort takes linear time. If T(N) is the running time of the sufﬁx array construction algorithm, then the recursive call takes T(2N/3) time. Example: In our example S1S2 = [BRA], [CAD], [ABR], [A$$], [RAC], [ADA], [BRA] The sorted sufﬁxes that will be computed recursively will represent tri-character strings as shown in Figure 12.28. Notice that these are not exactly the same as the corresponding sufﬁxes in S; however, if we strip out characters starting at the ﬁrst $, we do have a match of sufﬁxes. Also note that the indices returned by the recursive call do not correspond directly to the indices in S, though it is a simple matter to map them back. So to see how the algorithm actually forms the recursive call, observe that three passes of radix sort will assign the following alphabet: [A$$] = 1, [ABR] = 2, [ADA] = 3, [BRA] = 4, [CAD] = 5, [RAC] = 6. Figure 12.29 shows the mapping of tri-characters, the resulting array that is formed for S1, S2, and the resulting sufﬁx array that is computed recursively. Step 4: Compute the sufﬁx array for S0. This is easy to do because S0[i = >] = S[3i = >] = S[3i] S[3i + 1 = >] = S[3i]S1[i = >] = S0[i]S1[i = >] Since our recursive call has already sorted all S1[i = >], we can do step 4 with a simple two-pass radix sort: The ﬁrst pass is on S1[i = >], and the second pass is on S0[i]. Index Substring Being Represented [A$$] [RAC] [ADA] [BRA] [ABR] [A$$] [RAC] [ADA] [BRA] [ADA] [BRA] [BRA] [BRA] [CAD] [ABR] [A$$] [RAC] [ADA] [BRA] [CAD] [ABR] [A$$] [RAC] [ADA] [BRA] [RAC] [ADA] [BRA] Figure 12.28 Sufﬁx array for S1 S2 in tri-character set

Chapter 12 Advanced Data Structures and Implementation S1S2 [BRA] [CAD] [ABR] [A$$] [RAC] [ADA] [BRA] Integers SA[S1S2] Index Figure 12.29 Mapping of tri-characters, the resulting array that is formed for S1, S2, and the resulting sufﬁx array that is computed recursively Example: In our example S0 = [ABR], [ACA], [DAB], [RA$] From the recursive call in step 3, we can rank the sufﬁxes in S1 and S2. Figure 12.30, how the indices in the original string can be referenced from the recursively computed sufﬁx array and shows how the sufﬁx array from Figure 12.29 leads to a ranking of sufﬁxes among S1 + S2. Entries in the next to last row are easily obtained from the prior two rows. In the last row, the ith entry is given by the location of i in the row labelled SA[S1, S2]. The ranking established in S1 can be used directly for the ﬁrst radix sort pass on S0. Then we do a second pass on the single characters from S, using the prior radix sort to break ties. Notice that it is convenient if S1 has exactly as many elements as S0. Figure 12.31 shows how we can compute the sufﬁx array for S0. At this point, we now have the sufﬁx array for S0 and for the combined group S1 and S2. Since this is a two-pass radix sort, this step takes O( N ). Step 5: Merge the two sufﬁx arrays using the standard algorithm to merge two sorted lists. The only issue is that we must be able to compare each sufﬁx pair in constant time. There are two cases. Case 1: Comparing an S0 element with an S1 element: Compare the ﬁrst letter; if they do not match, we are done; otherwise, compare the remainder of S0 (which is an S1 sufﬁx) with the remainder of S1 (which is an S2 sufﬁx); those are already ordered, so we are done. Case 2: Comparing an S0 element with an S2 element: Compare at most the ﬁrst two letters; if we still have a match, then at that point compare the remainder of S0 (which S1 S2 [BRA] [CAD] [ABR] [A$$] [RAC] [ADA] [BRA] Index in S SA[S1S2] SA using S’s indices Rank in group Figure 12.30 Ranking of sufﬁxes based on sufﬁx array shown in Figure 12.29

12.4 Sufﬁx Arrays and Sufﬁx Trees S0 [ABR] [ACA] [DAB] [RA$] Index Index of second element Radix Pass 1 ordering Radix Pass 2 ordering Rank in group SA, using S’s indices add one to above last line of Figure 12.30 stably radix sort by ﬁrst char using results of previous line using results of previous line Figure 12.31 Computing sufﬁx array for S0 after skipping the two letters becomes an S2 sufﬁx) with the remainder of S2 (which after skipping two letters becomes an S1 sufﬁx); as in case 1, those sufﬁxes are already ordered by SA12 so we are done. Example: In our example, we have to merge A A D R SA for S0 ↑ with A A A B B C R SA for S1 and S2 ↑ The ﬁrst comparison is between index 0 (an A), which is an S0 element and index 10 (also an A) which is an S1 element. Since that is a tie, we now have to compare index 1 with index 11. Normally this would have already been computed, since index 1 is S1, while index 11 is in S2. However, this is special because index 11 is past the end of the string; consequently it always represents the earlier sufﬁx lexicographically, and the ﬁrst element in the ﬁnal sufﬁx array is 10. We advance in the second group and now we have. A A D R SA for S0 ↑ A A A B B C R SA for S1 and S2 ↑ Final SA Input S A B R A C A D A B R A Index

Chapter 12 Advanced Data Structures and Implementation Again the ﬁrst characters match, so we compare indices 1 and 8, and this is already computed, with index 8 having the smaller string. So that means that now 7 goes into the ﬁnal sufﬁx array, and we advance the second group, obtaining A A D R SA for S0 ↑ A A A B B C R SA for S1 and S2 ↑ Final SA Input S A B R A C A D A B R A Index Once again, the ﬁrst characters match, so now we have to compare indices 1 and 6. Since this is a comparison between an S1 element and an S0 element, we cannot look up the result. Thus we have to compare characters directly. Index 1 contains a B and index 6 contains a D, so index 1 wins. Thus 0 goes into the ﬁnal sufﬁx array and we advance the ﬁrst group. A A D R SA for S0 ↑ A A A B B C R SA for S1 and S2 ↑ Final SA Input S A B R A C A D A B R A Index The same situation occurs on the next comparison between a pair of A’s; the second comparison is between index 4 (a C) and index 6 (a D), so the element from the ﬁrst group advances.

12.4 Sufﬁx Arrays and Sufﬁx Trees A A D R SA for S0 ↑ A A A B B C R SA for S1 and S2 ↑ Final SA Input S A B R A C A D A B R A Index At this point, there are no ties for a while, so we quickly advance to the last characters of each group: A A D R SA for S0 ↑ A A A B B C R SA for S1 and S2 ↑ Final SA Input S A B R A C A D A B R A Index Finally, we get to the end. The comparison between two R’s requires that we compare the next characters, which are at indices 10 and 3. Since this comparison is between an S1 element and an S0 element, as we saw before, we cannot look up the result and must compare directly. But those are also the same, so now we have to compare indices 11 and 4, which is an automatic winner for index 11 (since it is past the end of the string). Thus the R in index 9 advances, and then we can ﬁnish the merge. Notice that had we not been at the end of the string, we could have used the fact that the comparison is between an S2 element and an S1 element, which means the ordering would have been obtainable from the sufﬁx array for S1 + S2.

Chapter 12 Advanced Data Structures and Implementation A A D R SA for S0 ↑ A A A B B C R SA for S1 and S2 ↑ Final SA Input S A B R A C A D A B R A Index Since this is a standard merge, with at most two comparisons per sufﬁx pair, this step takes linear time. The entire algorithm thus satisﬁes T(N) = T(2N/3) + O( N ) and takes linear time. Although we have only computed the sufﬁx array, the LCP information can also be /* * Fill in the suffix array and LCP information for String str * @param str the input String * @param sa existing array to place the suffix array * @param LCP existing array to place the LCP information */ public static void createSuffixArray( String str, int [ ] sa, int [ ] LCP ) { if( sa.length != str.length( ) || LCP.length != str.length( ) ) throw new IllegalArgumentException( ); int N = str.length( ); int [ ] s = new int[ N + 3 ]; int [ ] SA = new int[ N + 3 ]; for( int i = 0; i < N; i++ ) s[ i ] = str.charAt( i ); makeSuffixArray( s, SA, N, 256 ); for( int i = 0; i < N; i++ ) sa[ i ] = SA[ i ]; makeLCPArray( s, sa, LCP ); // Figure 12.50 and Exercise 12.9 } Figure 12.32 Code to set up the ﬁrst call to makeSuffixArray; create appropriate size arrays, and to keep things simple; just use the 256 ASCII character codes

12.4 Sufﬁx Arrays and Sufﬁx Trees computed as the algorithm runs, but there are some tricky details that are involved, and often the LCP information is computed by a separate linear-time algorithm. We close by providing a working implementation to compute sufﬁx arrays; rather than fully implementing step 1 to sort the original characters, we’ll assume only a small set of ASCII characters (residing in values 1–255) are present in the string. In Figure 12.32, we allocate the arrays that have three extra slots for padding and call makeSuffixArray, which is the basic linear-time algorithm. Figure 12.33 shows makeSuffixArray. At lines 12–16, it allocates all the needed arrays and makes sure that S0 and S1 have the same number of elements (lines 17–22); it then delegates work to assignNames, computeSl2, computeS0, and merge. // find the suffix array SA of s[ 0..n-1 ] in {1..K}ˆn // require s[ n ] = s[ n + 1 ] = s[ n + 2 ] = 0, n >= 2 public static void makeSuffixArray( int [ ] s, int [ ] SA, int n, int K ) { int n0 = ( n + 2 ) / 3; int n1 = ( n + 1 ) / 3; int n2 = n / 3; int t = n0 - n1; // 1 iff n%3 == 1 int n12 = n1 + n2 + t; int [ ] s12 = new int[ n12 + 3 ]; int [ ] SA12 = new int[ n12 + 3 ]; int [ ] s0 = new int[ n0 ]; int [ ] SA0 = new int[ n0 ]; // generate positions in s for items in s12 // the "+t" adds a dummy S1 suffix if n%3 == 1 // at that point, the size of s12 is n12 for( int i = 0, j = 0; i < n + t; i++ ) if( i % 3 != 0 ) s12[ j++ ] = i; int K12 = assignNames( s, s12, SA12, n0, n12, K ); computeS12( s12, SA12, n12, K12 ); computeS0( s, s0, SA0, SA12, n0, n12, K ); merge( s, s12, SA, SA0, SA12, n, n0, n12, t ); } Figure 12.33 The main routine for linear-time sufﬁx array construction

Chapter 12 Advanced Data Structures and Implementation // Assigns the new tri-character names. // At end of routine, SA will have indices into s, in sorted order // and s12 will have new character names // Returns the number of names assigned; note that if // this value is the same as n12, then SA is a suffix array for s12. private static int assignNames( int [ ] s, int [ ] s12, int [ ] SA12, int n0, int n12, int K ) { // radix sort the new character trios radixPass( s12 , SA12, s, 2, n12, K ); radixPass( SA12, s12 , s, 1, n12, K ); radixPass( s12 , SA12, s, 0, n12, K ); // find lexicographic names of triples int name = 0; int c0 = -1, c1 = -1, c2 = -1; for( int i = 0; i < n12; i++ ) { if( s[ SA12[ i ] ] != c0 || s[ SA12[ i ] + 1 ] != c1 || s[ SA12[ i ] + 2 ] != c2 ) { name++; c0 = s[ SA12[ i ] ]; c1 = s[ SA12[ i ] + 1 ]; c2 = s[ SA12[ i ] + 2 ]; } if( SA12[ i ] % 3 == 1 ) s12[ SA12[ i ] / 3 ] = name; // S1 else s12[ SA12[ i ] / 3 + n0 ] = name; // S2 } return name; } Figure 12.34 Routine to compute and assign the tri-character names assignNames, shown in Figure 12.34, begins by performing three passes of radix sort. Then, it assigns names (i.e., numbers), sequentially using the next available number if the current item has a different trio of characters than the prior item (recall that the tricharacters have already been sorted by the three passes of radix sort, and also recall that S0 and S1 have the same size, so at line 32, adding n0 adds the number of elements in S1). We can use the basic counting radix sort from Chapter 7 to obtain a linear-time sort. This

12.4 Sufﬁx Arrays and Sufﬁx Trees // stably sort in[0..n-1] with indices into s that has keys in 0..K // into out[0..n-1]; sort is relative to offset into s // uses counting radix sort private static void radixPass( int [ ] in, int [ ] out, int [ ] s, int offset, int n, int K ) { int [ ] count = new int[ K + 2 ]; for( int i = 0; i < n; i++ ) count[ s[ in[ i ] + offset ] + 1 ]++; for( int i = 1; i <= K + 1; i++ ) count[ i ] += count[ i - 1 ]; for( int i = 0; i < n; i++ ) out[ count[ s[ in[ i ] + offset ] ]++ ] = in[ i ]; } // stably sort in[0..n-1] with indices into s that has keys in 0..K // into out[0..n-1] // uses counting radix sort private static void radixPass( int [ ] in, int [ ] out, int [ ] s, int n, int K ) { radixPass( in, out, s, 0, n, K ); } Figure 12.35 A counting radix sort for the sufﬁx array code is shown in Figure 12.35. The array in represents the indexes into s; the result of the radix sort is that the indices are sorted so that the characters in s are sorted at those indices (where the indices are offset as speciﬁed). Figure 12.36 contains the routines to compute the sufﬁx arrays for s12, and then s0. Finally, the merge routine is shown in Figure 12.37, with some supporting routines in Figure 12.38. The merge routine has the same basic look and feel as the standard merging algorithm seen in Figure 7.10.

Chapter 12 Advanced Data Structures and Implementation // Compute the suffix array for s12, placing result into SA12 private static void computeS12( int [ ] s12, int [ ] SA12, int n12, int K12 ) { if( K12 == n12 ) // if unique names, don’t need recursion for( int i = 0; i < n12; i++ ) SA12[ s12[ i ] - 1 ] = i; else { makeSuffixArray( s12, SA12, n12, K12 ); // store unique names in s12 using the suffix array for( int i = 0; i < n12; i++ ) s12[ SA12[ i ] ] = i + 1; } } private static void computeS0( int [ ] s, int [ ] s0, int [ ] SA0, int [ ] SA12, int n0, int n12, int K ) { for( int i = 0, j = 0; i < n12; i++ ) if( SA12[ i ] < n0 ) s0[ j++ ] = 3 * SA12[ i ]; radixPass( s0, SA0, s, n0, K ); } Figure 12.36 Compute the sufﬁx array for s12 (possibly recursively) and the sufﬁx array for s0 12.5 k-d Trees Suppose that an advertising company maintains a database and needs to generate mailing labels for certain constituencies. A typical request might require sending out a mailing to people who are between the ages of 34 and 49 and whose annual income is between $100,000 and $150,000. This problem is known as a two-dimensional range query. In one dimension, the problem can be solved by a simple recursive algorithm in O(M + log N) average time, by traversing a preconstructed binary search tree. Here M is the number of matches reported by the query. We would like to obtain a similar bound for two or more dimensions. The two-dimensional search tree has the simple property that branching on odd levels is done with respect to the ﬁrst key, and branching on even levels is done with respect to the second key. The root is arbitrarily chosen to be an odd level. Figure 12.39 shows a 2-d tree. Insertion into a 2-d tree is a trivial extension of insertion into a binary search

12.5 k-d Trees // merge sorted SA0 suffixes and sorted SA12 suffixes private static void merge( int [ ] s, int [ ] s12, int [ ] SA, int [ ] SA0, int [ ] SA12, int n, int n0, int n12, int t ) { int p = 0, k = 0; while( t != n12 && p != n0 ) { int i = getIndexIntoS( SA12, t, n0 ); // S12 index in s int j = SA0[ p ]; // S0 index in s if( suffix12IsSmaller( s, s12, SA12, n0, i, j, t ) ) { SA[ k++ ] = i; t++; } else { SA[ k++ ] = j; p++; } } while( p < n0 ) SA[ k++ ] = SA0[ p++ ]; while( t < n12 ) SA[ k++ ] = getIndexIntoS( SA12, t++, n0 ); } Figure 12.37 Merge the sufﬁx arrays SA0 and SA12 tree: As we go down the tree, we need to maintain the current level. To keep our code simple, we assume that a basic item is an array of two elements. We then need to toggle the level between 0 and 1. Figure 12.40 shows the code to perform an insertion. We use recursion in this section; a nonrecursive implementation that would be used in practice is straightforward and left as Exercise 12.17. One difﬁculty is duplicates, particularly since several items can agree in one key. Our code allows duplicates and always places them in right branches; clearly this can be a problem if there are too many duplicates. A moment’s thought will convince you that a randomly constructed 2-d tree has the same structural properties as a random binary search tree: The height is O(log N) on average, but O(N) in the worst case. Unlike binary search trees, for which clever O(log N) worst-case variants exist, there are no schemes that are known to guarantee a balanced 2-d tree. The problem is that such a scheme would likely be based on tree rotations, and tree rotations don’t work in 2-d trees.

Chapter 12 Advanced Data Structures and Implementation private static int getIndexIntoS( int [ ] SA12, int t, int n0 ) { if( SA12[ t ] < n0 ) return SA12[ t ] * 3 + 1; else return ( SA12[ t ] - n0 ) * 3 + 2; } // True if [a1 a2] <= [b1 b2] private static boolean leq( int a1, int a2, int b1, int b2 ) { return a1 < b1 || a1 == b1 && a2 <= b2; } // True if [a1 a2 a3] <= [b1 b2 b3] private static boolean leq( int a1, int a2, int a3, int b1, int b2, int b3 ) { return a1 < b1 || a1 == b1 && leq( a2, a3, b2, b3 ); } private static boolean suffix12IsSmaller( int [ ] s, int [ ] s12, int [ ] SA12, int n0, int i, int j, int t ) { if( SA12[ t ] < n0 ) // s1 vs s0; can break tie after 1 char return leq( s[ i ], s12[ SA12[ t ] + n0 ], s[ j ], s12[ j / 3 ] ); else // s2 vs s0; can break tie after 2 chars return leq( s[ i ], s[ i + 1 ], s12[ SA12[ t ] - n0 + 1 ], s[ j ], s[ j + 1 ], s12[ j / 3 + n0 ] ); } Figure 12.38 Supporting routines for merging the sufﬁx arrays SA0 and SA12 53, 14 27, 28 67, 51 30, 11 29, 16 40, 26 7, 39 32, 29 82, 64 73, 75 15, 61 38, 23 31, 85 70, 3 99, 90 Figure 12.39 Sample 2-d tree

12.5 k-d Trees public void insert( AnyType [ ] x ) { root = insert( x, root, 0 ); } private KdNode<AnyType> insert( AnyType [ ] x, KdNode<AnyType> t, int level ) { if( t == null ) t = new KdNode<>( x ); else if( x[ level ].compareTo( t.data[ level ] ) < 0 ) t.left = insert( x, t.left, 1 - level ); else t.right = insert( x, t.right, 1 - level ); return t; } Figure 12.40 Insertion into 2-d trees The best one can do is to periodically rebalance the tree by reconstructing a subtree, as described in the exercises. Similarly, there are no deletion algorithms beyond the obvious lazy deletion strategy. If all the items arrive before we need to process queries, then we can construct a perfectly balanced 2-d tree in O(N log N) time; we leave this as Exercise 12.15c. Several kinds of queries are possible on a 2-d tree. We can ask for an exact match, or a match based on one of the two keys; the latter type of request is a partial match query. Both of these are special cases of an (orthogonal) range query. An orthogonal range query gives all items whose ﬁrst key is between a speciﬁed set of values and whose second key is between another speciﬁed set of values. This is exactly the problem that was described in the introduction to this section. A range query is easily solved by a recursive tree traversal, as shown in Figure 12.41. By testing before making a recursive call, we can avoid unnecessarily visiting all nodes. To ﬁnd a speciﬁc item, we can set low equal to high equal to the item we are searching for. To perform a partial match query, we set the range for the key not involved in the match to −∞to ∞. The other range is set with the low and high point equal to the value of the key involved in the match. An insertion or exact match search in a 2-d tree takes time that is proportional to the depth of the tree, namely, O(log N) on average and O(N) in the worst case. The running time of a range search depends on how balanced the tree is, whether or not a partial match is requested, and how many items are actually found. We mention three results that have been shown. For a perfectly balanced tree, a range query could take O(M + √ N) time in the worst case, to report M matches. At any node, we may have to visit two of the four grandchildren, leading to the equation T(N) = 2T(N/4) + O(1). In practice, however, these searches tend to be very efﬁcient, and even the worst case is not poor because for typical N, the difference between √ N and log N is compensated by the smaller constant that is hidden in the Big-Oh notation.

Chapter 12 Advanced Data Structures and Implementation /** * Print items satisfying * low[ 0 ] <= x[ 0 ] <= high[ 0 ] and * low[ 1 ] <= x[ 1 ] <= high[ 1 ]. */ public void printRange( AnyType [ ] low, AnyType [ ] high ) { printRange( low, high, root, 0 ); } private void printRange( AnyType [ ] low, AnyType [ ] high, KdNode<AnyType> t, int level ) { if( t != null ) { if( low[ 0 ].compareTo( t.data[ 0 ] ) <= 0 && low[ 1 ].compareTo( t.data[ 1 ] ) <= 0 && high[ 0 ].compareTo( t.data[ 0 ] ) >= 0 && high[ 1 ].compareTo( t.data[ 1 ] ) >= 0 ) System.out.println( "(" + t.data[ 0 ] + "," + t.data[ 1 ] + ")" ); if( low[ level ].compareTo( t.data[ level ] ) <= 0 ) printRange( low, high, t.left, 1 - level ); if( high[ level ].compareTo( t.data[ level ] ) >= 0 ) printRange( low, high, t.right, 1 - level ); } } Figure 12.41 2-d trees: range search For a randomly constructed tree, the average running time of a partial match query is O(M + Nα), where α = (−3 + √ 17)/2 (see below). A recent, and somewhat surprising, result is that this essentially describes the average running time of a range search of a random 2-d tree. For k dimensions, the same algorithm works; we just cycle through the keys at each level. However, in practice the balance starts getting worse because typically the effect of duplicates and nonrandom inputs becomes more pronounced. We leave the coding details as an exercise for the reader and mention the analytical results: For a perfectly balanced tree, the worst-case running time of a range query is O(M + kN1−1/k). In a randomly constructed k-d tree, a partial match query that involves p of the k keys takes O(M + Nα), where α is the (only) positive root of (2 + α)p(1 + α)k−p = 2k

12.6 Pairing Heaps Computation of α for various p and k is left as an exercise; the value for k = 2 and p = 1 is reﬂected in the result stated above for partial matching in random 2-d trees. Although there are several exotic structures that support range searching, the k-d tree is probably the simplest such structure that achieves respectable running times. 12.6 Pairing Heaps The last data structure we examine is the pairing heap. The analysis of the pairing heap is still open, but when decreaseKey operations are needed, it seems to outperform other heap structures. The most likely reason for its efﬁciency is its simplicity. The pairing heap is represented as a heap-ordered tree. Figure 12.42 shows a sample pairing heap. The actual pairing heap implementation uses a left child, right sibling representation as discussed in Chapter 4. The decreaseKey operation, as we will see, requires that each node contain an additional link. A node that is a leftmost child contains a link to its parent; otherwise the node is a right sibling and contains a link to its left sibling. We’ll refer to this ﬁeld as the prev ﬁeld. Figure 12.43 shows the actual representation of the pairing heap in Figure 12.42. Figure 12.42 Sample pairing heap: abstract representation Figure 12.43 Actual representation of previous pairing heap

Chapter 12 Advanced Data Structures and Implementation F S   A B C F   A B S   C + S   B A F   C F   S F   S Figure 12.44 compareAndLink merges two subheaps We begin by sketching the basic operations. To merge two pairing heaps, we make the heap with the larger root a left child of the heap with the smaller root. Insertion is, of course, a special case of merging. To perform a decreaseKey, we lower the value in the requested node. Because we are not maintaining parent links for all nodes, we don’t know if this violates the heap order. Thus we cut the adjusted node from its parent and complete the decreaseKey by merging the two heaps that result. To perform a deleteMin, we remove the root, creating a collection of heaps. If there are c children of the root, then c −1 calls to the merge procedure will reassemble the heap. The most important detail is the method used to perform the merge and how the c −1 merges are applied. Figure 12.44 shows how two subheaps are combined. The procedure is generalized to allow the second subheap to have siblings. As we mentioned earlier, the subheap with the larger root is made a leftmost child of the other subheap. The code is straightforward and shown in Figure 12.45. Notice that we have several instances in which a node reference is tested against null before assigning its prev ﬁeld; this suggests that perhaps it would be useful to have a nullNode sentinel, which was customary in this chapter’s search tree implementations. decreaseKey requires a position object, which is just an interface that PairNode implements. Figure 12.46 shows the PairNode class and Position interface, which are both nested in the PairingHeap class. The insert and decreaseKey operations are, then, simple implementations of the abstract description. Since the position of an item is determined (irrevocably) when an item is ﬁrst inserted, insert returns the PairNode it creates back to the caller. The code is shown in Figure 12.47. Our routine for decreaseKey throws an exception if the new value is not smaller than the old; otherwise, the resulting structure might not obey heap order. The basic deleteMin procedure follows directly from the abstract description and is shown in Figure 12.48. The element ﬁeld is set to null, so if the Position is used in a decreaseKey, it will be possible for decreaseKey to detect that the Position is no longer valid. The devil, of course, is in the details: How is combineSiblings implemented? Several variants have been proposed, but none has been shown to provide the same amortized

12.6 Pairing Heaps /** * Internal method that is the basic operation to maintain order. * Links first and second together to satisfy heap order. * @param first root of tree 1, which may not be null. * first.nextSibling MUST be null on entry. * @param second root of tree 2, which may be null. * @return result of the tree merge. */ private PairNode<AnyType> compareAndLink( PairNode<AnyType> first, PairNode<AnyType> second ) { if( second == null ) return first; if( second.element.compareTo( first.element ) < 0 ) { // Attach first as leftmost child of second second.prev = first.prev; first.prev = second; first.nextSibling = second.leftChild; if( first.nextSibling != null ) first.nextSibling.prev = first; second.leftChild = first; return second; } else { // Attach second as leftmost child of first second.prev = first; first.nextSibling = second.nextSibling; if( first.nextSibling != null ) first.nextSibling.prev = first; second.nextSibling = first.leftChild; if( second.nextSibling != null ) second.nextSibling.prev = second; first.leftChild = second; return first; } } Figure 12.45 Pairing heaps: routine to merge two subheaps

Chapter 12 Advanced Data Structures and Implementation public class PairingHeap<AnyType extends Comparable<? super AnyType>> { /** * The Position interface represents a type that can * be used for the decreaseKey operation. */ public interface Position<AnyType> { AnyType getValue( ); } private static class PairNode<AnyType> implements Position<AnyType> { public PairNode( AnyType theElement ) { element = theElement; leftChild = nextSibling = prev = null; } public AnyType getValue( ) { return element; } public AnyType element; public PairNode<AnyType> leftChild; public PairNode<AnyType> nextSibling; public PairNode<AnyType> prev; } private PairNode<AnyType> root; private int theSize; // Rest of class follows } Figure 12.46 PairNode class and Position interface in PairingHeap bounds as the Fibonacci heap. It has recently been shown that almost all the proposed methods are in fact theoretically less efﬁcient than the Fibonacci heap. Even so, the method coded in Figure 12.49 always seems to perform as well as or better than other heap structures, including the binary heap, for the typical graph theory uses that involve a host of decreaseKey operations. This method, known as two-pass merging, is the simplest and most practical of the many variants that have been suggested. We ﬁrst scan left to right, merging pairs of children.4 After the ﬁrst scan, we have half as many trees to merge. A second scan is then performed, right to left. At each step we merge the rightmost tree remaining from the 4 We must be careful if there is an odd number of children. When that happens, we merge the last child with the result of the rightmost merge to complete the ﬁrst scan.

/** * Insert into the priority queue, and return a Position * that can be used by decreaseKey. Duplicates are allowed. * @param x the item to insert. * @return the Position (PairNode) containing the newly inserted item. */ public Position<AnyType> insert( AnyType x ) { PairNode<AnyType> newNode = new PairNode<>( x ); if( root == null ) root = newNode; else root = compareAndLink( root, newNode ); theSize++; return newNode; } /** * Change the value of the item stored in the pairing heap. * @param pos any Position returned by insert. * @param newVal the new value, which must be smaller than the currently stored value. * @throws IllegalArgumentException if pos is null, deleteMin has * been performed on pos, or new value is larger than old. */ public void decreaseKey( Position<AnyType> pos, AnyType newVal ) { PairNode<AnyType> p = (PairNode<AnyType>) pos; if( p == null || p.element == null || p.element.compareTo( newVal ) < 0 ) throw new IllegalArgumentException( ); p.element = newVal; if( p != root ) { if( p.nextSibling != null ) p.nextSibling.prev = p.prev; if( p.prev.leftChild == p ) p.prev.leftChild = p.nextSibling; else p.prev.nextSibling = p.nextSibling; p.nextSibling = null; root = compareAndLink( root, p ); } } Figure 12.47 Pairing heaps: insert and decreaseKey

Chapter 12 Advanced Data Structures and Implementation /** * Remove the smallest item from the priority queue. * @return the smallest item. * @throws UnderflowException if pairing heap is empty. */ public AnyType deleteMin( ) { if( isEmpty( ) ) throw new UnderflowException( ); AnyType x = findMin( ); root.element = null; // null it out in case used in decreaseKey if( root.leftChild == null ) root = null; else root = combineSiblings( root.leftChild ); theSize--; return x; } Figure 12.48 Pairing heap deleteMin ﬁrst scan with the current merged result. As an example, if we have eight children, c1 through c8, the ﬁrst scan performs the merges c1 and c2, c3 and c4, c5 and c6, and c7 and c8. As a result we obtain d1, d2, d3, and d4. We perform the second pass by merging d3 and d4; d2 is then merged with that result, and then d1 is merged with the result of the previous merge. Our implementation requires an array to store the subtrees. In the worst case, N −1 items could be children of the root, but declaring an array of size N inside of combineSiblings would give an O(N) algorithm so we use an expanding array instead. Other merging strategies are discussed in the exercises. The only simple merging strategy that is easily seen to be poor is a left-to-right single-pass merge (Exercise 12.29). The pairing heap is a good example of “simple is better” and seems to be the method of choice for serious applications requiring the decreaseKey or merge operation.

chapt er primitive java The primary focus of this book is problem-solving techniques that allow the construction of sophisticated, time-efﬁcient programs. Nearly all of the material discussed is applicable in any programming language. Some would argue that a broad pseudocode description of these techniques could sufﬁce to demonstrate concepts. However, we believe that working with live code is vitally important. There is no shortage of programming languages available. This text uses Java, which is popular both academically and commercially. In the ﬁrst four chapters, we discuss the features of Java that are used throughout the book. Unused features and technicalities are not covered. Those looking for deeper Java information will ﬁnd it in the many Java books that are available. We begin by discussing the part of the language that mirrors a 1970s programming language such as Pascal or C. This includes primitive types, basic operations, conditional and looping constructs, and the Java equivalent of functions. In this chapter, we will see n Some of the basics of Java, including simple lexical elements n The Java primitive types, including some of the operations that  primitive-typed variables can perform

chapter 1 primitive java n How conditional statements and loop constructs are implemented in  Java n An introduction to the static method—the Java equivalent of the  function and procedure that is used in non-object-oriented languages 1.1 the general environment How are Java application programs entered, compiled, and run? The answer, of course, depends on the particular platform that hosts the Java compiler. javac compiles  .java ﬁles and generates .class ﬁles  containing bytecode. java invokes the Java  interpreter (which is  also known as the  Virtual Machine). Java source code resides in ﬁles whose names end with the .java sufﬁx. The local compiler, javac, compiles the program and generates .class ﬁles, which contain bytecode. Java bytecodes represent the portable intermediate language that is interpreted by running the Java interpreter, java. The interpreter is also known as the Virtual Machine. For Java programs, input can come from one of many places: n The terminal, whose input is denoted as standard input n Additional parameters in the invocation of the Virtual Machine— command-line arguments n A GUI component n A ﬁle Command-line arguments are particularly important for specifying program options. They are discussed in Section 2.4.5. Java provides mechanisms to read and write ﬁles. This is discussed brieﬂy in Section 2.6.3 and in more detail in Section 4.5.3 as an example of the decorator pattern. Many operating systems provide an alternative known as ﬁle redirection, in which the operating system arranges to take input from (or send output to) a ﬁle in a manner that is transparent to the running program. On Unix (and also from an MS/DOS window), for instance, the command java Program < inputfile > outputfile automatically arranges things so that any terminal reads are redirected to come from inputfile and terminal writes are redirected to go to outputfile.

1.2 the first program 1.2 the first program Let us begin by examining the simple Java program shown in Figure 1.1. This program prints a short phrase to the terminal. Note the line numbers shown on the left of the code are not part of the program. They are supplied for easy reference. Place the program in the source ﬁle FirstProgram.java and then compile and run it. Note that the name of the source ﬁle must match the name of the class (shown on line 4), including case conventions. If you are using the JDK, the commands are1 javac FirstProgram.java java FirstProgram 1.2.1   comments Java has three forms of comments. The ﬁrst form, which is inherited from C, begins with the token /* and ends with */. Here is an example: /* This is a     two-line comment */ Comments do not nest. Comments make  code easier for  humans to read.  Java has three  forms of comments. The second form, which is inherited from C++, begins with the token //. There is no ending token. Rather, the comment extends to the end of the line. This is shown on lines 1 and 2 in Figure 1.1. The third form begins with /** instead of /*. This form can be used to provide information to the javadoc utility, which will generate documentation from comments. This form is discussed in Section 3.3. 1. If you are using Sun’s JDK, javac and java are used directly. Otherwise, in a typical interactive development environment (IDE), such as Netbeans or Eclipse these commands are executed behind the scenes on your behalf. figure 1.1 A simple first program // First program // MW, 5/1/10 public class FirstProgram {     public static void main( String [ ] args )     {         System.out.println( "Is there anybody out there?" );     } }

chapter 1 primitive java Comments exist to make code easier for humans to read. These humans include other programmers who may have to modify or use your code, as well as yourself. A well-commented program is a sign of a good programmer. 1.2.2   main When the program  is run, the special  method main is  invoked. A Java program consists of a collection of interacting classes, which contain methods. The Java equivalent of the function or procedure is the static method, which is described in Section 1.6. When any program is run, the special static method main is invoked. Line 6 of Figure 1.1 shows that the static method main is invoked, possibly with command-line arguments. The parameter types of main and the void return type shown are required. 1.2.3   terminal output println is used to  perform output. The program in Figure 1.1 consists of a single statement, shown on line 8. println is the primary output mechanism in Java. Here, a constant string is placed on the standard output stream System.out by applying a println method. Input and output is discussed in more detail in Section 2.6. For now we mention only that the same syntax is used to perform output for any entity, whether that entity is an integer, ﬂoating point, string, or some other type. 1.3 primitive types Java deﬁnes eight primitive types. It also allows the programmer great ﬂexibility to deﬁne new types of objects, called classes. However, primitive types and user-deﬁned types have important differences in Java. In this section, we examine the primitive types and the basic operations that can be performed on them. 1.3.1   the primitive types Java’s primitive  types are integer,  ﬂoating-point, Boolean, and character. Java has eight primitive types, shown in Figure 1.2. The most common is the integer, which is speciﬁed by the keyword int. Unlike with many other languages, the range of integers is not machine-dependent. Rather, it is the same in any Java implementation, regardless of the underlying computer architecture. Java also allows entities of types byte, short, and long. These are known as integral types. Floating-point numbers are represented by the types float and double. double has more signiﬁcant digits, so use of it is recommended over use of float. The char type is used to represent single characters. A char occupies 16 bits to represent the Unicode standard. The Unicode standard contains over 30,000 distinct coded characters covering the principal written The Unicode standard contains over  30,000 distinct  coded characters  covering the principal written  languages.

1.3 primitive types languages. The low end of Unicode is identical to ASCII. The ﬁnal primitive type is boolean, which is either true or false. 1.3.2   constants Integer constants  can be represented in either  decimal, octal, or  hexadecimal notation. Integer constants can be represented in either decimal, octal, or hexadecimal notation. Octal notation is indicated by a leading 0; hexadecimal is indicated by a leading 0x or 0X. The following are all equivalent ways of representing the integer 37: 37, 045, 0x25. Octal integers are not used in this text. However, we must be aware of them so that we use leading 0s only when we intend to. We use hexadecimals in only one place (Section 12.1), and we will revisit them at that point. A string constant consists of a  sequence of characters enclosed by  double quotes. A character constant is enclosed with a pair of single quotation marks, as in 'a'. Internally, this character sequence is interpreted as a small number. The output routines later interpret that small number as the corresponding character. A string constant consists of a sequence of characters enclosed within double quotation marks, as in "Hello". There are some special sequences, known as escape sequences, that are used (for instance, how does one represent a single quotation mark?). In this text we use '\n', '\\', '\'', and '\"', which mean, respectively, the newline character, backslash character, single quotation mark, and double quotation mark. 1.3.3   declaration and initialization  of primitive types A variable is named  by using an  identiﬁer. Any variable, including those of a primitive type, is declared by providing its name, its type, and optionally, its initial value. The name must be an identiﬁer. An identiﬁer may consist of any combination of letters, digits, and the underscore character; it may not start with a digit, however. Reserved words, such Primitive Type What It Stores Range byte 8-bit integer –128 to 127 short 16-bit integer –32,768 to 32,767 int 32-bit integer –2,147,483,648 to 2,147,483,647 long 64-bit integer –263 to 263 – 1 float 32-bit ﬂoating-point 6 signiﬁcant digits ( 10–46, 1038 ) double 64-bit ﬂoating-point 15 signiﬁcant digits ( 10–324, 10308 ) char Unicode character boolean Boolean variable false and true figure 1.2 The eight primitive  types in Java Escape sequences are used to represent certain character constants.

chapter 1 primitive java as int, are not allowed. Although it is legal to do so, you should not reuse identiﬁer names that are already visibly used (for example, do not use main as the name of an entity). Java is casesensitive. Java is case-sensitive, meaning that Age and age are different identiﬁers. This text uses the following convention for naming variables: All variables start with a lowercase letter and new words start with an uppercase letter. An example is the identiﬁer minimumWage. Here are some examples of declarations: int num3;                     // Default initialization double minimumWage = 4.50;    // Standard initialization int x = 0, num1 = 0;          // Two entities are declared int num2 = num1; A variable should be declared near its ﬁrst use. As will be shown, the placement of a declaration determines its scope and meaning. 1.3.4   terminal input and output Basic formatted terminal I/O is accomplished by nextLine and println. The standard input stream is System.in, and the standard output stream is System.out. The basic mechanism for formatted I/O uses the String type, which is discussed in Section 2.3. For output, + combines two Strings. If the second argument is not a String, a temporary String is created for it if it is a primitive type. These conversions to String can also be deﬁned for objects (Section 3.4.3). For input, we associate a Scanner object with System.in. Then a String or a primitive type can be read. A more detailed discussion of I/O, including a treatment of formatted ﬁles, is in Section 2.6. 1.4 basic operators This section describes some of the operators available in Java. These operators are used to form expressions. A constant or entity by itself is an expression, as are combinations of constants and variables with operators. An expression followed by a semicolon is a simple statement. In Section 1.5, we examine other types of statements, which introduce additional operators.

1.4 basic operators 1.4.1   assignment operators A simple Java program that illustrates a few operators is shown in Figure 1.3. The basic assignment operator is the equals sign. For example, on line 16 the variable a is assigned the value of the variable c (which at that point is 6). Subsequent changes to the value of c do not affect a. Assignment operators can be chained, as in z=y=x=0. Java provides a  host of assignment operators, including =, +=, -=, *=, and /=. Another assignment operator is the +=, whose use is illustrated on line 18 of the ﬁgure. The += operator adds the value on the right-hand side (of the += operator) to the variable on the left-hand side. Thus, in the ﬁgure, c is incremented from its value of 6 before line 18, to a value of 14. Java provides various other assignment operators, such as -=, *=, and /=, which alter the variable on the left-hand side of the operator via subtraction, multiplication, and division, respectively. figure 1.3 Program that  illustrates operators public class OperatorTest {     // Program to illustrate basic operators     // The output is as follows:     // 12 8 6     // 6 8 6     // 6 8 14     // 22 8 14     // 24 10 33     public static void main( String [ ] args )     {         int a = 12, b = 8, c = 6;         System.out.println( a + " " + b + " " + c );         a = c;         System.out.println( a + " " + b + " " + c );         c += b;         System.out.println( a + " " + b + " " + c );         a = b + c;         System.out.println( a + " " + b + " " + c );         a++;         ++b;         c = a++ + ++b;         System.out.println( a + " " + b + " " + c );     } }

chapter 1 primitive java The type conversion  operator is used to  generate a temporary entity of a new  type. 1.4.2   binary arithmetic operators Java provides several binary arithmetic operators, including +, -, *, /, and %. Line 20 in Figure 1.3 illustrates one of the binary arithmetic operators that are typical of all programming languages: the addition operator (+). The + operator causes the values of b and c to be added together; b and c remain unchanged. The resulting value is assigned to a. Other arithmetic operators typically used in Java are -, *, /, and %, which are used, respectively, for subtraction, multiplication, division, and remainder. Integer division returns only the integral part and discards any remainder. As is typical, addition and subtraction have the same precedence, and this precedence is lower than the precedence of the group consisting of the multiplication, division, and mod operators; thus 1+2*3 evaluates to 7. All of these operators associate from left to right (so 3-2-2 evaluates to –1). All operators have precedence and associativity. The complete table of operators is in Appendix A. 1.4.3   unary operators Several unary operators are deﬁned,  including -. In addition to binary arithmetic operators, which require two operands, Java provides unary operators, which require only one operand. The most familiar of these is the unary minus, which evaluates to the negative of its operand. Thus -x returns the negative of x. Autoincrement and  autodecrement add 1 and subtract 1,  respectively. The  operators for doing  this are ++ and --. There are two  forms of incrementing and decrementing: preﬁx and  postﬁx. Java also provides the autoincrement operator to add 1 to a variable— denoted by ++ —and the autodecrement operator to subtract 1 from a variable— denoted by --. The most benign use of this feature is shown on lines 22 and 23 of Figure 1.3. In both lines, the autoincrement operator ++ adds 1 to the value of the variable. In Java, however, an operator applied to an expression yields an expression that has a value. Although it is guaranteed that the variable will be incremented before the execution of the next statement, the question arises: What is the value of the autoincrement expression if it is used in a larger expression?  In this case, the placement of the ++ is crucial. The semantics of ++x is that the value of the expression is the new value of x. This is called the preﬁx increment. In contrast, x++ means the value of the expression is the original value of x. This is called the postﬁx increment. This feature is shown in line 24 of Figure 1.3. a and b are both incremented by 1, and c is obtained by adding the original value of a to the incremented value of b. 1.4.4   type conversions The type conversion operator is used to generate a temporary entity of a new type. Consider, for instance,

1.5 conditional statements double quotient; int x = 6; int y = 10; quotient = x / y;    // Probably wrong! The ﬁrst operation is the division, and since x and y are both integers, the result is integer division, and we obtain 0. Integer 0 is then implicitly converted to a double so that it can be assigned to quotient. But we had intended quotient to be assigned 0.6. The solution is to generate a temporary variable for either x or y so that the division is performed using the rules for double. This would be done as follows: quotient = ( double ) x / y; Note that neither x nor y are changed. An unnamed temporary is created, and its value is used for the division. The type conversion operator has higher precedence than division does, so x is type-converted and then the division is performed (rather than the conversion coming after the division of two ints being performed). 1.5 conditional statements This section examines statements that affect the ﬂow of control: conditional statements and loops. As a consequence, new operators are introduced. 1.5.1   relational and equality operators The basic test that we can perform on primitive types is the comparison. This is done using the equality and inequality operators, as well as the relational operators (less than, greater than, and so on). In Java, the  equality operators are == and !=. In Java, the equality operators are == and !=. For example, leftExpr==rightExpr evaluates to true if leftExpr and rightExpr are equal; otherwise, it evaluates to false. Similarly, leftExpr!=rightExpr evaluates to true if leftExpr and rightExpr are not equal and to false otherwise. The relational operators are <, <=, >, and >=. The relational operators are <, <=, >, and >=. These have natural meanings for the built-in types. The relational operators have higher precedence than the equality operators. Both have lower precedence than the arithmetic operators

chapt er reference types Chapter 1 examined the Java primitive types. All types that are not one of the eight primitive types are reference types, including important entities such as strings, arrays, and ﬁle streams.  In this chapter, we will see  n What a reference type and value is n How reference types differ from primitive types n Examples of reference types, including strings, arrays, and streams n How exceptions are used to signal erroneous behavior 2.1 what is a reference? Chapter 1 described the eight primitive types, along with some of the operations that these types can perform. All other types in Java are reference types, including strings, arrays, and ﬁle streams. So what is a reference? A reference variable (often abbreviated as simply reference) in Java is a variable that somehow stores the memory address where an object resides.

chapter 2 reference types As an example, in Figure 2.1 are two objects of type Point. It happens, by chance, that these objects are stored in memory locations 1000 and 1024, respectively. For these two objects, there are three references: point1, point2, and point3. point1 and point3 both reference the object stored at memory location 1000; point2 references the object stored at memory location 1024. Both point1 and point3 store the value 1000, while point2 stores the value 1024. Note that the actual locations, such as 1000 and 1024, are assigned by the runtime system at its discretion (when it ﬁnds available memory). Thus these values are not useful externally as numbers. However, the fact that point1 and point3 store identical values is useful: It means they are referencing the same object. A reference will always store the memory address where some object is residing, unless it is not currently referencing any object. In this case, it will store the null reference, null. Java does not allow references to primitive variables. There are two broad categories of operations that can be applied to reference variables. One allows us to examine or manipulate the reference value. For instance, if we change the stored value of point1 (which is 1000), we could have it reference another object. We can also compare point1 and point3 and determine if they are referencing the same object. The other category of operations applies to the object being referenced; perhaps we could examine or change the internal state of one of the Point objects. For instance, we could examine some of Point’s x and y coordinates. Before we describe what can be done with references, let us see what is not allowed. Consider the expression point1*point2. Since the stored values of point1 and point2 are 1000 and 1024, respectively, their product would be 1024000. However, this is a meaningless calculation that could not have any possible use. Reference variables store addresses, and there is no logical meaning that can be associated with multiplying two addresses. ( 0, 0 ) ( 5, 12 ) point2 = 1024 point1 = 1000 point3 = 1000 point1 point2 point3 (at 1000) ( 0, 0 ) (at 1024) ( 5, 12 ) figure 2.1 An illustration of a  reference. The Point object stored at  memory location  1000 is referenced  by both point1 and  point3. The Point object stored at  memory location  1024 is referenced  by point2. The  memory locations  where the variables  are stored are  arbitrary.

2.1 what is a reference? Similarly, point1++ has no Java meaning; it suggests that point1—1000— should be increased to 1001, but in that case it might not be referencing a valid Point object. Many languages (e.g., C++) deﬁne the pointer, which behaves like a reference variable. However, pointers in C++ are much more dangerous because arithmetic on stored addresses is allowed. Thus, in C++, point1++ has a meaning. Because C++ allows pointers to primitive types, one must be careful to distinguish between arithmetic on addresses and arithmetic on the objects being referenced. This is done by explicitly dereferencing the pointer. In practice, C++’s unsafe pointers tend to cause numerous programming errors. Some operations are performed on references themselves, while other operations are performed on the objects being referenced. In Java, the only operators that are allowed for reference types (with one exception made for Strings) are assignment via = and equality comparison via == or !=. Figure 2.2 illustrates the assignment operator for reference variables. By assigning point3 the stored value of point2, we have point3 reference the same object that point2 was referencing. Now, point2==point3 is true because point2 and point3 both store 1024 and thus reference the same object. point1!=point2 is also true because point1 and point2 reference different objects. The other category of operations deals with the object that is being referenced. There are only three basic actions that can be done: 1. Apply a type conversion (Section 1.4.4). 2. Access an internal ﬁeld or call a method via the dot operator (.) (Section 2.2.1). 3. Use the instanceof operator to verify that the stored object is of  a certain type (Section 3.6.3). The next section illustrates in more detail the common reference operations. ( 0, 0 ) ( 5, 12 ) point2 = 1024 point1 = 1000 point3 = 1024 point1 point2 point3 (at 1000) ( 0, 0 ) (at 1024) ( 5, 12 ) figure 2.2 The result of  point3=point2: point3 now  references the  same object as  point2.

chapter 2 reference types 2.2 basics of objects and references In Java, an object is an instance of any  of the nonprimitive  types. In Java, an object is an instance of any of the nonprimitive types. Objects are treated differently from primitive types. Primitive types, as already shown, are handled by value, meaning that the values assumed by the primitive variables are stored in those variables and copied from primitive variable to primitive variable during assignments. As shown in Section 2.1, reference variables store references to objects. The actual object is stored somewhere in memory, and the reference variable stores the object’s memory address. Thus a reference variable simply represents a name for that part of memory. This means that primitive variables and reference variables behave differently. This section examines these differences in more detail and illustrates the operations that are allowed for reference variables. 2.2.1   the dot operator (.) The dot operator (.) is used to select a method that is applied to an object. For instance, suppose we have an object of type Circle that deﬁnes an area method. If theCircle references a Circle, then we can compute the area of the referenced Circle (and save it to a variable of type double) by doing this: double theArea = theCircle.area( ); It is possible that theCircle stores the null reference. In this case, applying the dot operator will generate a NullPointerException when the program runs. Generally, this will cause abnormal termination. The dot operator can also be used to access individual components of an object, provided arrangements have been made to allow internal components to be viewable. Chapter 3 discusses how these arrangements are made. Chapter 3 also explains why it is generally preferable to not allow direct access of individual components. 2.2.2   declaration of objects We have already seen the syntax for declaring primitive variables. For objects, there is an important difference. When we declare a reference variable, we are simply providing a name that can be used to reference an object that is stored in memory. However, the declaration by itself does not provide that object. For example, suppose there is an object of type Button that we want to add into an existing Panel p, using the method add (all this is provided in the Java library). Consider the statements

2.2 basics of objects and references Button b;            // b may reference a Button object b.setLabel( "No" );  // Label the button b refers to "No" p.add( b );          // and add it to Panel p When a reference  type is declared, no  object is allocated.  At that point, the  reference is to null. To create the  object, use new. All seems well with these statements until we remember that b is the name of some Button object but no Button has been created yet. As a result, after the declaration of b the value stored by the reference variable b is null, meaning b is not yet referring to a valid Button object. Consequently, the second line is illegal because we are attempting to alter an object that does not exist. In this scenario, the compiler will probably detect the error, stating that “b is uninitialized.” In other cases, the compiler will not notice, and a run-time error will result in the cryptic NullPointerException error message. The new keyword is  used to construct an object. The (only common) way to allocate an object is to use the new keyword. new is used to construct an object. One way to do this is as follows: Button b;            // b may reference a Button object b = new Button( );   // Now b refers to an allocated object b.setLabel( "No" );  // Label the Button b refers to "No" p.add( b );          // and add it to Panel p Parentheses are  required when new is used. Note that parentheses are required after the object name. It is also possible to combine the declaration and object construction, as in Button b = new Button( );  b.setLabel( "No" );  // Label the Button b refers to "No" p.add( b );          // and add it to Panel p The construction  can specify an initial state of the  object. Many objects can also be constructed with initial values. For instance, it happens that the Button can be constructed with a String that speciﬁes the label: Button b = new Button( "No" );  p.add( b );          // Add it to Panel p 2.2.3   garbage collection Java uses garbage collection. With garbage collection,  unreferenced  memory is automatically reclaimed. Since all objects must be constructed, we might expect that when they are no longer needed, we must explicitly destroy them. In Java, when a constructed object is no longer referenced by any object variable, the memory it consumes will automatically be reclaimed and therefore be made available. This technique is known as garbage collection. The runtime system (i.e., the Java Virtual Machine) guarantees that as long as it is possible to access an object by a reference, or a chain of references, the object will never be reclaimed. Once the object is unreachable by a

chapter 2 reference types chain of references, it can be reclaimed at the discretion of the runtime system if memory is low. It is possible that if memory does not run low, the virtual machine will not attempt to reclaim these objects. 2.2.4   the meaning of = lhs and rhs stand  for left-hand side and right-hand side, respectively. Suppose we have two primitive variables lhs and rhs where lhs and rhs stand for left-hand side and right-hand side, respectively. Then the assignment statement lhs = rhs; has a simple meaning: The value stored in rhs is copied to the primitive variable lhs. Subsequent changes to either lhs or rhs do not affect the other.   For objects, = is a  reference assignment, rather than  an object copy. For objects, the meaning of = is the same: Stored values are copied. If lhs and rhs are references (of compatible types), then after the assignment statement, lhs will refer to the same object that rhs does. Here, what is being copied is an address. The object that lhs used to refer to is no longer referred to by lhs. If lhs was the only reference to that object, then that object is now unreferenced and subject to garbage collection. Note that the objects are not copied. Here are some examples. First, suppose we want two Button objects. Then suppose we try to obtain them ﬁrst by creating noButton. Then we attempt to create yesButton by modifying noButton as follows: Button noButton = new Button( "No" ); Button yesButton = noButton; yesButton.setLabel( "Yes" ); p.add( noButton ); p.add( yesButton ); This does not work because only one Button object has been constructed. Thus the second statement simply states that yesButton is now another name for the constructed Button at line 1. That constructed Button is now known by two names. On line 3, the constructed Button has its label changed to Yes, but this means that the single Button object, known by two names, is now labeled Yes. The last two lines add that Button object to the Panel p twice. The fact that yesButton never referred to its own object is immaterial in this example. The problem is the assignment. Consider Button noButton = new Button( "No" ); Button yesButton = new Button( ); yesButton = noButton; yesButton.setLabel( "Yes" ); p.add( noButton ); p.add( yesButton );

2.2 basics of objects and references The consequences are the same. Here, there are two Button objects that have been constructed. At the end of the sequence, the ﬁrst object is being referenced by both noButton and yesButton, while the second object is unreferenced. At ﬁrst glance, the fact that objects cannot be copied seems like a severe limitation. Actually, it is not, although this does take a little getting used to. (Some objects do need to be copied. For those, if a clone method is available, it should be used. However, clone is not used in this text.) 2.2.5   parameter passing Call-by-value means that for reference types, the  formal parameter  references the  same object as  does the actual  argument. Because of call-by-value, the actual arguments are sent into the formal parameters using normal assignment. If the parameter is a reference type, then we know that normal assignment means that the formal parameter now references the same object as does the actual argument. Any method applied to the formal parameter is thus also being applied to the actual argument. In other languages, this is known as call-by-reference parameter passing. Using this terminology for Java would be somewhat misleading because it implies that the parameter passing is different. In reality, the parameter passing has not changed; rather, it is the parameters that have changed, from nonreference types to reference types. As an example, suppose we pass yesButton as a parameter to the clearButton routine that is deﬁned as follows:     public static void clearButton( Button b )     {         b.setLabel( "No" );         b = null;     } Then, as Figure 2.3 shows, b references the same object as yesButton, and changes made to the state of this object by methods invoked through b will be seen when clearButton returns. Changes to the value of b (i.e., which object it references) will not have any affect on yesButton. 2.2.6   the meaning of == For reference  types, == is true  only if the two references reference  the same object. For primitive types, == is true if the stored values are identical. For reference types, its meaning is different but is perfectly consistent with the previous discussion.

chapter 2 reference types Two reference types are equal via == if they refer to the same stored object (or they are both null). Consider, for example, the following: Button a = new Button( "Yes" ); Button b = new Button( "Yes" ); Button c = b; Here, we have two objects. The ﬁrst is known by the name a, and the second is known by two names: b and c. b==c is true. However, even though a and b are referencing objects that seem to have the same value, a==b is false, since they reference different objects. Similar rules apply for !=. The equals method  can be used to test  whether two references reference  objects that have  identical states. Sometimes it is important to know if the states of the objects being referenced are identical. All objects can be compared by using equals, but for many objects (including Button) equals returns false unless the two references are referencing the same object (in other words, for some objects equals is no more than the == test). We will see an example of where equals is useful when the String type is discussed in Section 2.3. 2.2.7   no operator overloading for objects Except for the single exception described in the next section, new operators, such as +, -, *, and / cannot be deﬁned to work for objects. Thus there is no < operator available for any object. Instead, a named method, such as lessThan, must be deﬁned for this task. (a) (b) (c) (d) yesButton b yesButton b yesButton yesButton b = null Yes No No No figure 2.3 The result of call-byvalue. (a) b is a copy  of yesButton; (b) after  b.setLabel("No"): changes to the state  of the object  referenced by b are  reflected in the object  referenced by  yesButton because  these are the same  object; (c) after  b=null: change to the  value of b does not  affect the value of  yesButton; (d) after  the method returns, b is out of scope.

2.3 strings 2.3 strings The String behaves like a reference  type. Strings in Java are handled with the String reference type. The language does make it appear that the String type is a primitive type because it provides the + and += operators for concatenation. However, this is the only reference type for which any operator overloading is allowed. Otherwise, the String behaves like any other reference type. 2.3.1   basics of string manipulation Strings are  immutable; that is, a  String object will  not be changed. There are two fundamental rules about a String object. First, with the exception of the concatenation operators, it behaves like an object. Second, the String is immutable. This means that once a String object is constructed, its contents may not be changed. Because a String is immutable, it is always safe to use the = operator with it. Thus a String may be declared as follows: String empty   = ""; String message = "Hello"; String repeat  = message; After these declarations, there are two String objects. The ﬁrst is the empty string, which is referenced by empty. The second is the String "Hello" which is referenced by both message and repeat. For most objects, being referenced by both message and repeat could be a problem. However, because Strings are immutable, the sharing of String objects is safe, as well as efﬁcient. The only way to change the value that the string repeat refers to is to construct a new String and have repeat reference it. This has no effect on the String that message references. 2.3.2   string concatenation Java does not allow operator overloading for reference types. However, a special language exemption is granted for string concatenation. String concatenation is performed  with + (and +=). The operator +, when at least one operand is a String, performs concatenation. The result is a reference to a newly constructed String object. For example, "this" + " that" // Generates "this that" "abc" + 5 // Generates "abc5" 5 + "abc" // Generates "5abc" "a" + "b" + "c" // Generates "abc"

chapter 2 reference types Single-character strings should not be replaced with character constants; Exercise 2.7 asks you to show why. Note that operator + is left-associative, and thus "a" + 1 + 2        // Generates "a12" 1 + 2 + "a"        // Generates "3a" 1 + ( 2 + "a" )    // Generates "12a" Also, operator += is provided for the String. The effect of str+=exp is the same as str=str+exp. Speciﬁcally, this means that str will reference the newly constructed String generated by str+exp. 2.3.3   comparing strings Use equals and  compareTo to perform string  comparison. Since the basic assignment operator works for Strings, it is tempting to assume that the relational and equality operators also work. This is not true. In accordance with the ban on operator overloading, relational operators (<, >, <=, and >=) are not deﬁned for the String type. Further, == and != have the typical meaning for reference variables. For two String objects lhs and rhs, for example, lhs==rhs is true only if lhs and rhs refer to the same String object. Thus, if they refer to different objects that have identical contents, lhs==rhs is false. Similar logic applies for !=. To compare two String objects for equality, we use the equals method. lhs.equals(rhs) is true if lhs and rhs reference Strings that store identical values. A more general test can be performed with the compareTo method. lhs.compareTo(rhs) compares two String objects, lhs and rhs. It returns a negative number, zero, or a positive number, depending on whether lhs is lexicographically less than, equal to, or greater than rhs, respectively. 2.3.4   other String methods Use length, charAt, and substring to  compute string  length, get a single  character, and get  a substring,  respectively.  The length of a String object (an empty string has length zero) can be obtained with the method length. Since length is a method, parentheses are required. Two methods are deﬁned to access individual characters in a String. The method charAt gets a single character by specifying a position (the ﬁrst position is position 0). The method substring returns a reference to a newly constructed String. The call is made by specifying the starting point and the ﬁrst nonincluded position. Here is an example of these three methods:

2.4 arrays String greeting = "hello"; int len    = greeting.length( );          // len is 5 char ch    = greeting.charAt( 1 );        // ch  is 'e' String sub = greeting.substring( 2, 4 );  // sub is "ll" 2.3.5   converting other types to strings toString converts  primitive types (and  objects) to Strings. String concatenation provides a lazy way to convert any primitive to a String. For instance, ""+45.3 returns the newly constructed String "45.3". There are also methods to do this directly. The method toString can be used to convert any primitive type to a String. As an example, Integer.toString(45) returns a reference to the newly constructed String "45". All reference types also provide an implementation of toString of varying quality. In fact, when operator + has only one String argument, the nonString argument is converted to a String by automatically applying an appropriate toString. For the integer types, an alternative form of Integer.toString allows the speciﬁcation of a radix. Thus System.out.println( "55 in base 2: " + Integer.toString( 55, 2 ) ); prints out the binary representation of 55. The int value that is represented by a String can be obtained by calling the method Integer.parseInt. This method generates an exception if the String does not represent an int. Exceptions are discussed in Section 2.5. Similar ideas work for a doubles. Here are some examples: int    x = Integer.parseInt( "75" ); double y = Double.parseDouble( "3.14" ); 2.4 arrays An array stores a  collection of identically typed entities. An aggregate is a collection of entities stored in one unit. An array is the basic mechanism for storing a collection of identically typed entities. In Java the array is not a primitive type. Instead, it behaves very much like an object. Thus many of the rules for objects also apply to arrays. The array indexing  operator [] provides access to any  object in the array. Each entity in the array can be accessed via the array indexing operator []. We say that the [] operator indexes the array, meaning that it speciﬁes which object is to be accessed. Unlike C and C++, bounds-checking is performed automatically. In Java, arrays are always indexed starting at zero. Thus an array a of three items stores a[0], a[1], and a[2]. The number of items that can be stored in an

chapter 2 reference types Arrays are indexed  starting at zero. The  number of items  stored in the array is  obtained by the  length ﬁeld. No  parentheses are  used. array a can always be obtained by a.length. Note that there are no parentheses. A typical array loop would use for( int i = 0; i < a.length; i++ ) 2.4.1   declaration, assignment, and methods An array is an object, so when the array declaration int [ ] array1; To allocate an array,  use new. is given, no memory is yet allocated to store the array. array1 is simply a name (reference) for an array, and at this point is null. To have 100 ints, for example, we use new: array1 = new int [ 100 ]; Now array1 references an array of 100 ints. There are other ways to declare arrays. For instance, in some contexts int [ ] array2 = new int [ 100 ]; is acceptable. Also, initializer lists can be used, as in C or C++, to specify initial values. In the next example, an array of four ints is allocated and then referenced by array3. int [ ] array3 = { 3, 4, 10, 6 }; The brackets can go either before or after the array name. Placing them before makes it easier to see that the name is an array type, so that is the style used here. Declaring an array of reference types (rather than primitive types) uses the same syntax. Note, however, that when we allocate an array of reference types, each reference initially stores a null reference. Each also must be set to reference a constructed object. For instance, an array of ﬁve buttons is constructed as Button [ ] arrayOfButtons; arrayOfButtons = new Button [ 5 ]; for( int i = 0; i < arrayOfButtons.length; i++ )     arrayOfButtons[ i ] = new Button( ); Figure 2.4 illustrates the use of arrays in Java. The program in Figure 2.4 repeatedly chooses numbers between 1 and 100, inclusive. The output is the number of times that each number has occurred. The import directive at line 1 will be discussed in Section 3.8.1.

2.4 arrays Always be sure to  declare the correct  array size. Off-byone errors are  common. Line 14 declares an array of integers that count the occurrences of each number. Because arrays are indexed starting at zero, the +1 is crucial if we want to access the item in position DIFF_NUMBERS. Without it we would have an array whose indexible range was 0 to 99, and thus any access to index 100 would be out-of-bounds. The loop at lines 15 and 16 initializes the array entries to zero; this is actually unnecessary, since by default, array elements are initialized to zero for primitive and null for references. The rest of the program is relatively straightforward. It uses the Random object deﬁned in the java.util library (hence the import directive at line 1). The nextInt method repeatedly gives a (somewhat) random number in the range that includes zero but stops at one less than the parameter to nextInt; thus by adding 1, we get a number in the desired range. The results are output at lines 25 and 26. Since an array is a reference type, = does not copy arrays. Instead, if lhs and rhs are arrays, the effect of figure 2.4 Simple demonstration  of arrays import java.util.Random; public class RandomNumbers {     // Generate random numbers (from 1-100)     // Print number of occurrences of each number     public static final int DIFF_NUMBERS     =     100;     public static final int TOTAL_NUMBERS    = 1000000;     public static void main( String [ ] args )     {         // Create array; initialize to 0s         int [ ] numbers = new int [ DIFF_NUMBERS + 1 ];         for( int i = 0; i < numbers.length; i++ )             numbers[ i ] = 0;         Random r = new Random( );         // Generate the numbers         for( int i = 0; i < TOTAL_NUMBERS; i++ )             numbers[ r.nextInt( DIFF_NUMBERS ) + 1 ]++;         // Output the

chapt er objects and classes This chapter begins the discussion of object-oriented programming. A fundamental component of object-oriented programming is the speciﬁcation, implementation, and use of objects. In Chapter 2, we saw several examples of objects, including strings and ﬁles, that are part of the mandatory Java library. We also saw that these objects have an internal state that can be manipulated by applying the dot operator to select a method. In Java, the state and functionality of an object is given by deﬁning a class. An object is then an instance of a class. In this chapter, we will see n How Java uses the class to achieve encapsulation and information hiding n How classes are implemented and automatically documented n How classes are grouped into packages 3.1 what is object-oriented  programming? Object-oriented programming emerged as the dominant paradigm of the mid1990s. In this section we discuss some of the things that Java provides in the way

chapter 3 objects and classes of object-oriented support and mention some of the principles of object-oriented programming. Objects are entities that have structure and state.  Each object deﬁnes  operations that may  access or manipulate that state. At the heart of object-oriented programming is the object. An object is a data type that has structure and state. Each object deﬁnes operations that may access or manipulate that state. As we have already seen, in Java an object is distinguished from a primitive type, but this is a particular feature of Java rather than the object-oriented paradigm. In addition to performing general operations, we can do the following: n Create new objects, possibly with initialization n Copy or test for equality n Perform I/O on these objects An object is an  atomic unit: Its  parts cannot be  dissected by the  general users of  the object. Also, we view the object as an atomic unit that the user ought not to dissect. Most of us would not even think of ﬁddling around with the bits that represent a ﬂoating-point number, and we would ﬁnd it completely ridiculous to try to increment some ﬂoating-point object by altering its internal representation ourselves. The atomicity principle is known as information hiding. The user does not get direct access to the parts of the object or their implementations; they can be accessed only indirectly by methods supplied with the object. We can view each object as coming with the warning, “Do not open—no userserviceable parts inside.” In real life, most people who try to ﬁx things that have such a warning wind up doing more harm than good. In this respect, programming mimics the real world. The grouping of data and the operations that apply to them to form an aggregate, while hiding implementation details of the aggregate, is known as encapsulation. Encapsulation is  the grouping of  data and the operations that apply to  them to form an  aggregate, while  hiding the implementation of the  aggregate. An important goal of object-oriented programming is to support code reuse. Just as engineers use components over and over in their designs, programmers should be able to reuse objects rather than repeatedly reimplementing them. When we have an implementation of the exact object that we need to use, reuse is a simple matter. The challenge is to use an existing object when the object that is needed is not an exact match but is merely very similar. Object-oriented languages provide several mechanisms to support this goal. One is the use of generic code. If the implementation is identical except for the basic type of the object, there is no need to completely rewrite code: Instead, we write the code generically so that it works for any type. For instance, the logic used to sort an array of objects is independent of the types of objects being sorted, so a generic algorithm could be used. Information hiding makes implementation details, including components of  an object, inaccessible.

3.2 a simple example The inheritance mechanism allows us to extend the functionality of an object. In other words, we can create new types with restricted (or extended) properties of the original type. Inheritance goes a long way toward our goal of code reuse. Another important object-oriented principle is polymorphism. A polymorphic reference type can reference objects of several different types. When methods are applied to the polymorphic type, the operation that is appropriate to the actual referenced object is automatically selected. In Java, this is implemented as part of inheritance. Polymorphism allows us to implement classes that share common logic. As is discussed in Chapter 4, this is illustrated in the Java libraries. The use of inheritance to create these hierarchies distinguishes object-oriented programming from the simpler object-based programming. In Java, generic algorithms are implemented as part of inheritance. Chapter 4 discusses inheritance and polymorphism. In this chapter, we describe how Java uses classes to achieve encapsulation and information hiding. A class in Java  consists of ﬁelds that store data and  methods that are  applied to  instances of the  class. An object in Java is an instance of a class. A class is similar to a C structure or Pascal/Ada record, except that there are two important enhancements. First, members can be both functions and data, known as methods and ﬁelds, respectively. Second, the visibility of these members can be restricted. Because methods that manipulate the object’s state are members of the class, they are accessed by the dot member operator, just like the ﬁelds. In objectoriented terminology, when we make a call to a method we are passing a message to the object. Types discussed in Chapter 2, such as String, ArrayList, Scanner, and FileReader, are all classes implemented in the Java library. 3.2 a simple example Functionality is  supplied as additional members;  these methods manipulate the  object’s state. Recall that when you are designing the class, it is important to be able to hide internal details from the class user. This is done in two ways. First, the class can deﬁne functionality as class members, called methods. Some of these methods describe how an instance of the structure is created and initialized, how equality tests are performed, and how output is performed. Other methods would be speciﬁc to the particular structure. The idea is that the internal data ﬁelds that represent an object’s state should not be manipulated directly by the class user but instead should be manipulated only through use of the methods. This idea can be strengthened by hiding members from the user. To do this, we can specify that they be stored in a private section. The compiler will enforce the rule that members in the private section are inaccessible by methods that are not in the class of the object. Generally speaking, all data members should be private.

chapter 3 objects and classes Public members  are visible to nonclass routines; private members are  not. Figure 3.1 illustrates a class declaration for an IntCell object.1 The declaration consists of two parts: public and private. Public members represent the portion that is visible to the user of the object. Since we expect to hide data, generally only methods and constants would be placed in the public section. In our example, we have methods that read from and write to the IntCell object. The private section contains the data; this is invisible to the user of the object. The storedValue member must be accessed through the publicly visible routines read and write; it cannot be accessed directly by main. Another way of viewing this is shown in Figure 3.2. Members that are  declared private are not visible to  nonclass routines. Figure 3.3 shows how IntCell objects are used. Since read and write are members of the IntCell class, they are accessed by using the dot member operator. The storedValue member could also be accessed by using the dot member operator, but since it is private, the access at line 14 would be illegal if it were not commented out. A ﬁeld is a member  that stores data; a  method is a member that performs  an action. Here is a

chapt er inheritance As mentioned in Chapter 3, an important goal of object-oriented programming is code reuse. Just as engineers use components over and over in their designs, programmers should be able to reuse objects rather than repeatedly reimplement them. In an object-oriented programming language, the fundamental mechanism for code reuse is inheritance. Inheritance allows us to extend the functionality of an object. In other words, we can create new types with restricted (or extended) properties of the original type, in effect forming a hierarchy of classes. Inheritance is more than simply code reuse, however. By using inheritance correctly, it enables the programmer to more easily maintain and update code, both of which are essential in large commercial applications. Understanding of the use of inheritance is essential in order to write signiﬁcant Java programs, and it is also used by Java to implement generic methods and classes. In this chapter, we will see n General principles of inheritance, including polymorphism n How inheritance is implemented in Java n How a collection of classes can be derived from a single abstract  class n The interface, which is a special kind of a class

chapter 4 inheritance n How Java implements generic programming using inheritance n How Java 5 implements generic programming using generic classes 4.1 what is inheritance? In an IS-A relationship, we say the  derived class is a (variation of the)  base class. Inheritance is the fundamental object-oriented principle that is used to reuse code among related classes. Inheritance models the IS-A relationship. In an IS-A relationship, we say the derived class is a (variation of the) base class. For example, a Circle IS-A Shape and a Car IS-A Vehicle. However, an Ellipse IS-NOT-A Circle. Inheritance relationships form hierarchies. For instance, we can extend Car to other classes, since a ForeignCar IS-A Car (and pays tariffs) and a DomesticCar IS-A Car (and does not pay tariffs), and so on. In a HAS-A relationship, we say the  derived class has a (instance of the)  base class. Composition is used to  model HAS-A relationships. Another type of relationship is a HAS-A (or IS-COMPOSED-OF) relationship. This type of relationship does not possess the properties that would be natural in an inheritance hierarchy. An example of a HAS-A relationship is that a car HAS-A steering wheel. HAS-A relationships should not be modeled by inheritance. Instead, they should use the technique of composition, in which the components are simply made private data ﬁelds. As we will see in forthcoming chapters, the Java language itself makes extensive use of inheritance in implementing its class libraries. 4.1.1   creating new classes Our inheritance discussion will center around an example. Figure 4.1 shows a typical class. The Person class is used to store information about a person; in our case we have private data that includes the name, age, address, and phone number, along with some public methods that can access and perhaps change this information. We can imagine that in reality, this class is signiﬁcantly more complex, storing perhaps 30 data ﬁelds with 100 methods. Now suppose we want to have a Student class or an Employee class or both. Imagine that a Student is similar to a Person, with the addition of only a few extra data members and methods. In our simple example, imagine that the difference is that a Student adds a gpa ﬁeld and a getGPA accessor. Similarly, imagine that the Employee has all of the same components as a Person but also has a salary ﬁeld and methods to manipulate the salary. One option in designing these classes is the classic copy-and-paste: We copy the Person class, change the name of the class and constructors, and then add the new stuff. This strategy is illustrated in Figure 4.2.

4.1 what is inheritance? Copy-and-paste is a weak design option, fraught with signiﬁcant liabilities. First, there is the problem that if you copy garbage, you wind up with more garbage. This makes it very hard to ﬁx programming errors that are detected, especially when they are detected late. Second is the related issue of maintenance and versioning. Suppose we decide in the second version that it is better to store names in last name, ﬁrst name format, rather than as a single ﬁeld. Or perhaps it is better to store addresses using a special Address class. In order to maintain consistency, these should be done for all classes. Using copy-and-paste, these design changes have to be done in numerous places. Third, and more subtle, is the fact that using copy-and-paste, Person, Student, and Employee are three separate entities with zero relationship between figure 4.1 The Person class  stores name, age,  address, and phone  number. class Person {     public Person( String n, int ag, String ad, String p )       { name = n; age = ag; address = ad; phone = p; }     public String toString( )       { return getName( ) + " " + getAge( ) + " "                           + getPhoneNumber( ); }     public String getName( )       { return name; }     public int getAge( )       { return age; }     public String getAddress( )       { return address; }     public String getPhoneNumber( )       { return phone; }     public void setAddress( String newAddress )       { address = newAddress; }     public void setPhoneNumber( String newPhone )       { phone = newPhone; }     private String name;     private int    age;     private String address;     private String phone; }

chapter 4 inheritance each other, in spite of their similarities. So, for instance, if we have a routine that accepted a Person as a parameter, we could not send in a Student. We would thus have to copy and paste all of those routines to make them work for these new types. Inheritance solves all three of these problems. Using inheritance, we would say that a Student IS-A Person. We would then specify the changes that a Student has relative to Person. There are only three types of changes that are allowed: figure 4.2 The Student class  stores name, age,  address, phone  number,  and gpa via  copy-and-paste. class Student {     public Student( String n, int ag, String ad, String p,                     double g )       { name = n; age = ag; address = ad; phone = p; gpa = g; }     public String toString( )       { return getName( ) + " " + getAge( ) + " "              + getPhoneNumber( ) + " " + getGPA( ); }     public String getName( )       { return name; }     public int getAge( )       { return age; }     public String getAddress( )       { return address; }     public String getPhoneNumber( )       { return phone; }     public void setAddress( String newAddress )       { address = newAddress; }     public void setPhoneNumber( String newPhone )       { phone = newPhone; }     public double getGPA( )       { return gpa; }     private String name;     private int    age;     private String address;     private String phone;     private double gpa }

4.1 what is inheritance? 1. Student can add new ﬁelds (e.g., gpa). 2. Student can add new methods (e.g., getGPA). 3. Student can override existing methods (e.g., toString). Two changes are speciﬁcally not allowed because they would violate the notion of an IS-A relationship: 1. Student cannot remove ﬁelds. 2. Student cannot remove methods. Finally, the new class must specify its own constructors; this is likely to involve some syntax that we will discuss in Section 4.1.6. Figure 4.3 shows the Student class. The data layout for the Person and Student classes is shown in Figure 4.4. It illustrates that the memory footprint of any Student object includes all ﬁelds that would be contained in a Person figure 4.3 Inheritance used to  create Student class class Student extends Person {     public Student( String n, int ag, String ad, String p,                     double g )       {         /* OOPS! Need some syntax; see Section 4.1.6 */          gpa = g; }     public String toString( )       { return getName( ) + " " + getAge( ) + " "              + getPhoneNumber( ) + " " + getGPA( ); }     public double getGPA( )       { return gpa; }     private double gpa; } figure 4.4 Memory layout with  inheritance. Light  shading indicates  fields that are private,  and accessible only by  methods of the class.  Dark shading in the  Student class  indicates fields that  are not accessible in  the Student class but  are nonetheless  present. Person Class Student Class name address age phone name address age phone gpa

chapter 4 inheritance object. However, because those ﬁelds are declared private by Person, they are not accessible by Student class methods. That is why the constructor is problematic at this point: We cannot touch the data ﬁelds in any Student method and instead can only manipulate the inherited private ﬁelds by using public Person methods. Of course, we could make the inherited ﬁelds public, but that would generally be a terrible design decision. It would embolden the implementors of the Student and Employee classes to access the inherited ﬁelds directly. If that was done, and modiﬁcations such as a change in the Person’s data representation of the name or address were made to the Person class, we would now have to track down all of the dependencies, which would bring us back to the copy-and-paste liabilities. Inheritance allows  us to derive classes  from a base class without disturbing  the implementation  of the base class. As we can see, except for the constructors, the code is relatively simple. We have added one data ﬁeld, added one new method, and overridden an existing method. Internally, we have memory for all of the inherited ﬁelds, and we also have implementations of all original methods that have not been overridden. The amount of new code we have to write for Student would be roughly the same, regardless of how small or large the Person class was, and we have the beneﬁt of direct code reuse and easy maintenance. Observe also that we have done so without disturbing the implementation of the existing class. The extends clause is used to declare  that a class is  derived from  another class. Let us summarize the syntax so far. A derived class inherits all the properties of a base class. It can then add data members, override methods, and add new methods. Each derived class is a completely new class. A typical layout for inheritance is shown in Figure 4.5 and uses an extends clause. An extends clause declares that a class is derived from another class. A derived class extends a base class. Here is a brief description of a derived class: n Generally all data is private, so we add additional data fields in the  derived class by specifying them in the private section. figure 4.5 The general layout  of inheritance public class Derived extends Base {     // Any members that are not listed are inherited unchanged     // except for constructor.         // public members     // Constructor(s) if default is not acceptable     // Base methods whose definitions are to change in Derived     // Additional public methods         // private members     // Additional data fields (generally private)     // Additional private methods }

4.1 what is inheritance? n Any base class methods that are not specified in the derived class  are inherited unchanged, with the exception of the constructor. The  special case of the constructor is discussed in Section 4.1.6. n Any base class method that is declared in the derived class’s public  section is overridden. The new definition will be applied to objects of  the derived class. n Public base class methods may not be overridden in the private  section of the derived class, because that would be tantamount to  removing methods and would violate the IS-A relationship. n Additional methods can be added in the derived class. 4.1.2   type compatibility The direct code reuse described in the preceding paragraph is a signiﬁcant gain. However, the more signiﬁcant gain is indirect code reuse. This gain comes from the fact that a Student IS-A Person and an Employee IS-A Person. Each derived class is a completely new  class that nonetheless has some  compatibility with  the class from  which it was  derived. Because a Student IS-A Person, a Student object can be accessed by a Person reference. The following code is thus legal:         Student s = new Student( "Joe", 26, "1 Main St",                                     "202-555-1212", 4.0 );         Person p = s;         System.out.println( "Age is " + p.getAge( ) ); This is legal because the static type (i.e., compile-time type) of p is Person. Thus p may reference any object that IS-A Person, and any method that we invoke through the p reference is guaranteed to make sense, since once a method is deﬁned for Person, it cannot be removed by a derived class. You might ask why this is a big deal. The reason is that this applies not only to assignment, but also to parameter passing. A method whose formal parameter is a Person can receive anything that IS-A Person, including Student and Employee. So consider the following code written in any class:     public static boolean isOlder( Person p1, Person p2 )     {         return p1.getAge( ) > p2.getAge( );     } Consider the following declarations, in which constructor arguments are missing to save space:         Person   p = new Person( ... );         Student  s = new Student( ... );         Employee e = new Employee( ... ); A derived class  inherits all data  members from the  base class and may  add more data  members. The derived class  inherits all methods  from the base  class. It may accept  or redeﬁne them.  It also can deﬁne  new methods.

chapter 4 inheritance The single isOlder routine can be used for all of the following calls: isOlder(p,p), isOlder(s,s), isOlder(e,e), isOlder(p,e), isOlder(p,s), isOlder(s,p), isOlder(s,e), isOlder(e,p), isOlder(e,s). All in all, we now have leveraged one non-class routine to work for nine different cases. In fact there is no limit to the amount of reuse this gets us. As soon as we use inheritance to add a fourth class into the hierarchy, we now have 4 times 4, or 16 different methods, without changing isOlder at all! The reuse is even more signiﬁcant if a method were to take three Person references as parameters. And imagine the huge code reuse if a method takes an array of Person references. Thus, for many people, the type compatibility of derived classes with their base classes is the most important thing about inheritance because it leads to massive indirect code reuse. And as isOlder illustrates, it also makes it very easy to add in new types that automatically work with existing methods. 4.1.3   dynamic dispatch and polymorphism There is the issue of overriding methods: If the type of the reference and the class of the object being referenced (in the example above, these are Person and Student, respectively) disagree, and they have different implementations, whose implementation is to be used? As an example, consider the following fragment:         Student s = new Student( "Joe", 26, "1 Main St",                                     "202-555-1212", 4.0 );         Employee e = new Employee( "Boss", 42, "4 Main St.",                                "203-555-1212", 100000.0 );         Person p = null;         if( getTodaysDay( ).equals( "Tuesday" ) )              p = s;         else             p = e;         System.out.println( "Person is " + p.toString( ) ); A polymorphic variable can reference  objects of several  different types.  When operations  are applied to the  polymorphic variable, the operation  appropriate to the  referenced object  is automatically  selected. Here the static type of p is Person. When we run the program, the dynamic type (i.e., the type of the object actually being referenced) will be either Student or Employee. It is impossible to deduce the dynamic type until the program runs. Naturally, however, we would want the dynamic type to be used, and that is what happens in Java. When this code fragment is run, the toString method that is used will be the one appropriate for the dynamic type of the controlling object reference. This is an important object-oriented principle known as polymorphism. A reference variable that is polymorphic can reference objects of several different types. When operations are applied to the reference, the operation that is appropriate to the actual referenced object is automatically selected.

4.1 what is inheritance? All reference types are polymorphic in Java. This is also known as dynamic dispatch or late binding (or sometimes dynamic binding). A derived class is type-compatible with its base class, meaning that a reference variable of the base class type may reference an object of the derived class, but not vice versa. Sibling classes (that is, classes derived from a common class) are not type-compatible. 4.1.4   inheritance hierarchies If X IS-A Y, then X is a subclass of Y and Y is a superclass of X. These  relationships are  transitive. As mentioned earlier, the use of inheritance typically generates a hierarchy of classes. Figure 4.6 illustrates a possible Person hierarchy. Notice that Faculty is indirectly, rather than directly, derived from Person⎯so faculty are people too! This fact is transparent to the user of the classes because IS-A relationships are transitive. In other words, if X IS-A Y and Y IS-A  Z, then X IS-A Z. The Person hierarchy illustrates the typical design issues of factoring out commonalities into base classes and then specializing in the derived classes. In this hierarchy, we say that the derived class is a subclass of the base class and the base class is a superclass of the derived class. These relationships are transitive, and furthermore, the instanceof operator works with subclasses. Thus if obj is of type Undergrad (and not null), then obj instanceof Person is true. 4.1.5   visibility rules We know that any member that is declared with private visibility is accessible only to methods of the class. Thus as we have seen, any private members in the base class are not accessible to the derived class. figure 4.6 The Person hierarchy Undergrad Employee Student Person Graduate Faculty Staff

chapter 4 inheritance Occasionally we want the derived class to have access to the base class members. There are two basic options. The ﬁrst is to use either public or package visible access (if the base and derived classes are in the same package), as appropriate. However, this allows access by other classes in addition to derived classes. A protected class member is visible to  the derived class  and also classes in  the same package. If we want to restrict access to only derived classes, we can make members protected. A protected class member is visible to methods in a derived class and also methods in classes in the same package, but not to anyone else.1 Declaring data members as protected or public violates the spirit of encapsulation and information hiding and is generally done only as a matter of programming expediency. Typically, a better alternative is to write accessor and mutator methods. However, if a protected declaration allows you to avoid convoluted code, then it is not unreasonable to use it. In this text, protected data members are used for precisely this reason. Protected methods are also used in this text. This allows a derived class to inherit an internal method without making it accessible outside the class hierarchy. Notice that in toy code, in which all classes are in the default unnamed package, protected members are visible. If no constructor is  written, then a single zero-parameter  default constructor  is generated that  calls the base class  zero-parameter constructor for the  inherited portion,  and then applies  the default initialization for any additional data ﬁelds. 4.1.6   the constructor and super Each derived class should deﬁne its constructors. If no constructor is written, then a single zero-parameter default constructor is generated. This constructor will call the base class zero-parameter constructor for the inherited portion and then apply the default initialization for any additional data ﬁelds (meaning 0 for primitive types, and null for reference types). Constructing a derived class object by ﬁrst constructing the inherited portion is standard practice. In fact, it is done by default, even if an explicit derived class constructor is given. This is natural because the encapsulation 1. The rule for protected visibility is quite complex. A protected member of class B is visible to all methods in any classes that are in the same package as B. It is also visible to methods in any class D that is in a different package than B if D extends B, but only if accessed through a reference that is type-compatible with D (including an implicit or explicit this). Speciﬁcally, it is NOT VISIBLE in class D if accessed through a reference of type B. The following example illustrates this. class Demo extends java.io.FilterInputStream {       // FilterInputStream has protected data field named in     public void foo( )      {         java.io.FilterInputStream b = this;  // legal         System.out.println( in );            // legal         System.out.println( this.in );       // legal         System.out.println( b.in );          // illegal      } }

4.1 what is inheritance? viewpoint tells us that the inherited portion is a single entity, and the base class constructor tells us how to initialize this single entity. Base class constructors can be explicitly called by using the method super. Thus the default constructor for a derived class is in reality     public Derived( )     {         super( );     } super is used to call  the base class constructor. The super method can be called with parameters that match a base class constructor. As an example, Figure 4.7 illustrates the implementation of the Student constructor. The super method can be used only as the ﬁrst line of a constructor. If it is not provided, then an automatic call to super with no parameters is generated. 4.1.7   final methods and classes A final method is  invariant over the  inheritance hierarchy and may not be  overridden. As described earlier, the derived class either overrides or accepts the base class methods. In many cases, it is clear that a particular base class method should be invariant over the hierarchy, meaning that a derived class should not override it. In this case, we can declare that the method is final and cannot be overridden. Declaring invariant methods final is not only good programming practice. It also can lead to more efﬁcient code. It is good programming practice because in addition to declaring your intentions to the reader of the program and documentation, you prevent the accidental overriding of a method that should not be overridden. figure 4.7 A constructor for new  class Student; uses  super class Student extends Person {     public Student( String n, int ag, String ad, String p,                     double g )       { super( n, ag, ad, p );  gpa = g; }     // toString and getAge omitted     private double gpa; }

chapter 4 inheritance To see why using final may make for more efﬁcient code, suppose base class Base declares a ﬁnal method f and suppose Derived extends Base. Consider the routine     void doIt( Base obj )     {         obj.f( );      } Static binding could  be used when the  method is invariant  over the inheritance  hierarchy. Since f is a ﬁnal method, it does not matter whether obj actually references a Base or Derived object; the deﬁnition of f is invariant, so we know what f does. As a result, a compile-time decision, rather than a run-time decision, could be made to resolve the method call. This is known as static binding. Because binding is done during compilation rather than at run time, the program should run faster. Whether this is noticeable would depend on how many times we avoid making the run-time decision while executing the program. Static methods  have no controlling  object and thus are  resolved at compile time using  static binding. A corollary to this observation is that if f is a trivial method, such as a single ﬁeld accessor, and is declared final, the compiler could replace the call to f with its inline deﬁnition. Thus the method call would be replaced by a single line that accesses a data ﬁeld, thereby saving time. If f is not declared final, then this is impossible, since obj could be referencing a derived class object, for which the deﬁnition of f could be different.2 Static methods are not ﬁnal methods, but have no controlling object and thus are resolved at compile time using static binding. A ﬁnal class may  not be extended. A  leaf class is a ﬁnal  class. Similar to the ﬁnal method is the ﬁnal class. A ﬁnal class cannot be extended. As a result, all of its methods are automatically ﬁnal methods. As an example, the String class is a ﬁnal class. Notice that the fact that a class has only ﬁnal methods does not imply that it is a ﬁnal class. Final classes are also known as leaf classes because in the inheritance hierarchy, which resembles a tree, ﬁnal classes are at the fringes, like leaves. In the Person class, the trivial accessors and mutators (those starting with get and set) are good candidates for ﬁnal methods, and they are declared as such in the online code. 2. In the preceding two paragraphs, we say that static binding and inline optimizations “could be” done because although compile-time decisions would appear to make sense, Section 8.4.3.3 of the language speciﬁcation makes clear that inline optimizations for trivial ﬁnal methods can be done, but this optimization must be done by the virtual machine at run time, rather than the compiler at compile time. This ensures that dependent classes do not get out of sync as a result of the optimization.

4.1 what is inheritance? 4.1.8   overriding a method The derived class  method must have  the same return  type and signature  and may not add  exceptions to the  throws list. Methods in the base class are overridden in the derived class by simply providing a derived class method with the same signature.3 The derived class method must have the same return type and may not add exceptions to the throws list.4 The derived class may not reduce visibility, as that would violate the spirit of an IS-A relationship. Thus you may not override a public method with a package-visible method. Sometimes the derived class method wants to invoke the base class method. Typically, this is known as partial overriding. That is, we want to do what the base class does, plus a little more, rather than doing something entirely different. Calls to a base class method can be accomplished by using super. Here is an example: public class Workaholic extends Worker {     public void doWork( )     {         super.doWork( );   // Work like a Worker         drinkCoffee( );    // Take a break         super.doWork( );   // Work like a Worker some more     } } A more typical example is the overriding of standard methods, such as toString. Figure 4.8 illustrates this use in the Student and Employee classes. 4.1.9   type compatibility revisited Figure 4.9 illustrates the typical use of polymorphism with arrays. At line 17, we create an array of four Person references, which will each be initialized to null. The values of these references can be set at lines 19 to 24, and we know that all the assignments are legal because of the ability of a base type reference to refer to objects of a derived type. The printAll routine simply steps through the array and calls the toString method, using dynamic dispatch. The test at line 7 is important because, as we have seen, some of the array references could be null. 3. If a different signature is used, you simply have overloaded the method, and now there are two methods with different signatures available for the compiler to choose from.  4. Java 5 loosens this requirement and allows the return type of the derived class’s method to be slightly different as long as it is “compatible.” The new rule is discussed in Section 4.1.11. Partial overriding involves calling a  base class method  by using super.

chapter 4 inheritance In the example, suppose that prior to completing the printing, we want to give p[3]—which we know is an employee—a raise? Since p[3] is an Employee, it might seem that         p[3].raise( 0.04 ); would be legal. But it is not. The problem is that the static type of p[3] is a Person, and raise is not deﬁned for Person. At compile time, only (visible) members of the static type of the reference can appear to the right of the dot operator. We can change the static type by using a cast:         ((Employee) p[3]).raise( 0.04 ); The above code makes the static type of the reference to the left of the dot operator an Employee. If this is impossible (for instance, p[3] is in a completely figure 4.8 The complete Student and Employee classes,  using both forms of  super class Student extends Person {     public Student( String n, int ag, String ad, String p,                     double g )       { super( n, ag, ad, p ); gpa = g; }     public String toString( )       { return super.toString( ) + getGPA( ); }     public double getGPA( )       { return gpa; }     private double gpa; } class Employee extends Person {     public Employee( String n, int ag, String ad,                      String p, double s )       { super( n, ag, ad, p ); salary = s; }     public String toString( )       { return super.toString( ) + " $" + getSalary( ); }     public double getSalary( )       { return salary; }     public void raise( double percentRaise )       { salary *= ( 1 + percentRaise ); }     private double salary; }

4.1 what is inheritance? different hierarchy), the compiler will complain. If it is possible for the cast to make sense, the program will compile, and so the above code will successfully give a 4% raise to p[3]. This construct, in which we change the static type of an expression from a base class to a class farther down in the inheritance hierarchy is known as a downcast. What if p[3] was not an Employee? For instance, what if we used the following?         ((Employee) p[1]).raise( 0.04 ); // p[1] is a Student In that case the program would compile, but the Virtual Machine would throw a ClassCastException, which is a run-time exception that signals a programming error. Casts are always double-checked at run time to ensure that the programmer (or a malicious hacker) is not trying to subvert Java’s strong typing system. The safe way of doing these types of calls is to use instanceof ﬁrst:         if( p[3] instanceof Employee )             ((Employee) p[3]).raise( 0.04 ); figure 4.9 An illustration of  polymorphism with  arrays class PersonDemo {     public static void printAll( Person [ ] arr )     {         for( int i = 0; i < arr.length; i++ )         {             if( arr[ i ] != null )             {                 System.out.print( "[" + i + "] " );                 System.out.println( arr[ i ].toString( ) );             }         }     }     public static void main( String [ ] args )     { Person [ ] p = new Person[ 4 ];         p[0] = new Person( "joe", 25, "New York",                            "212-555-1212" );         p[1] = new Student( "jill", 27, "Chicago",                             "312-555-1212", 4.0 );         p[3] = new Employee( "bob", 29, "Boston",                              "617-555-1212", 100000.0 );         printAll( p );     } } A downcast is a  cast down the  inheritance hierarchy. Casts are  always veriﬁed at  runtime by the  Virtual Machine.

chapter 4 inheritance 4.1.10   compatibility of array types One of the difﬁculties in language design is how to handle inheritance for aggregate types. In our example, we know that Employee IS-A Person. But is it true that Employee[] IS-A Person[]? In other words, if a routine is written to accept Person[] as a parameter, can we pass an Employee[] as an argument? Arrays of subclasses are typecompatible with  arrays of superclasses. This is  known as  covariant arrays. At ﬁrst glance, this seems like a no-brainer, and Employee[] should be type-compatible with Person[]. However, this issue is trickier than it seems. Suppose that in addition to Employee, Student IS-A Person. Suppose the Employee[] is type-compatible with Person[]. Then consider this sequence of assignments: Person[] arr = new Employee[ 5 ]; // compiles: arrays are compatible arr[ 0 ] = new Student( ... );    // compiles: Student IS-A Person If an incompatible  type is inserted into  the array, the Virtual  Machine will throw  an ArrayStoreException. Both assignments compile, yet arr[0] is actually a referencing an Employee, and Student IS-NOT-A Employee. Thus we have type confusion. The runtime system cannot throw a ClassCastException since there is no cast. The easiest way to avoid this problem is to specify that the arrays are not type-compatible. However, in Java the arrays are type-compatible. This is known as a covariant array type. Each array keeps track of the type of object it is allowed to store. If an incompatible type is inserted into the array, the Virtual Machine will throw an ArrayStoreException. In Java 5, the subclass method’s  return type only  needs to be typecompatible with  (i.e., it may be a  subclass of) the  superclass method’s return  type. This is known  as a covariant return type. 4.1.11   covariant return types Prior to Java 5, when a method was overridden, the subclass method was required to have the same return type as the superclass method. Java 5 relaxes this rule. In Java 5, the subclass method’s return type only needs to be type-compatible with (i.e., it may be a subclass of) the superclass method’s return type. This is known as a covariant return type. As an example, suppose class Person has a makeCopy method public Person makeCopy( ); that returns a copy of the Person. Prior to Java 5, if class Employee overrode this method, the return type would have to be Person. In Java 5, the method may be overridden as public Employee makeCopy( );

4.2 designing hierarchies 4.2 designing hierarchies Suppose we have a Circle class, and for any non-null Circle c, c.area() returns the area of Circle c. Additionally, suppose we have a Rectangle class, and for any non-null Rectangle r, r.area() returns the area of Rectangle r. Possibly we have other classes such as Ellipse, Triangle, and Square, all with area methods. Suppose we have an array that contains references to these objects, and we want to compute the total area of all the objects. Since they all have an area method for all classes, polymorphism is an attractive option, yielding code such as the following:     public static double totalArea( WhatType [ ] arr )     {         double total = 0.0;         for( int i = 0; i < arr.length; i++ )             if( arr[ i ] != null )                 total += arr[ i ].area( );         return total;     } For this code to work, we need to decide the type declaration for WhatType. None of Circle, Rectangle, etc., will work, since there is no IS-A relationship. Thus we need to deﬁne a type, say Shape, such that Circle IS-A Shape, Rectangle IS-A Shape, etc. A possible hierarchy is illustrated in Figure 4.10. Additionally, in order for arr[i].area() to make sense, area must be a method available for Shape. figure 4.10 The hierarchy of  shapes used in an  inheritance example Shape Circle Rectangle

chapter 4 inheritance This suggests a class for Shape, as shown in Figure 4.11. Once we have the Shape class, we can provide others, as shown in Figure 4.12. These classes also include a perimeter method.  The code in Figure 4.12, with classes that extend the simple Shape class in Figure 4.11 that returns –1 for area, can now be used polymorphically, as shown in Figure 4.13. Too many  instanceof operators is a symptom  of poor objectoriented design. A huge beneﬁt of this design is that we can add a new class to the hierarchy without disturbing implementations. For instance, suppose we want to add triangles into the mix. All we need to do is have Triangle extend Shape, override area appropriately, and now Triangle objects can be included in any Shape[] object. Observe that this involves the following: n NO CHANGES to the Shape class n NO CHANGES to the Circle, Rectangle, or other existing classes n NO CHANGES to the totalArea method making it difﬁcult to break existing code in the process of adding new code. Notice also the lack of any instanceof tests, which is typical of good polymorphic code. 4.2.1   abstract methods and classes Although the code in the previous example works, improvements are possible in the Shape class written in Figure 4.11. Notice that the Shape class itself, and the area method in particular, are placeholders: The Shape’s area method is never intended to be called directly. It is there so that the compiler and run-time system can conspire to use dynamic dispatch and call an appropriate area method. In fact, examining main, we see that Shape objects themselves are not supposed to be created either. The class exists simply as a common superclass for the others.5 figure 4.11 A possible Shape class public class Shape {     public double area( )     {         return -1;     } } 5. Declaring a private Shape constructor DOES NOT solve the second problem: The constructor is needed by the subclasses.

4.2 designing hierarchies The programmer has attempted to signal that calling Shape’s area is an error by returning –1, which is an obviously impossible area. But this is a value that might be ignored. Furthermore, this is a value that will be returned if, when extending Shape, area is not overridden. This failure to override could occur because of a typographical error: An Area function is written instead of area, making it difﬁcult to track down the error at run time. figure 4.12 Circle and Rectangle classes public class Circle extends Shape {     public Circle( double rad )       { radius = rad; }     public double area( )       { return Math.PI * radius * radius; }     public double perimeter( )       { return 2 * Math.PI * radius; }     public String toString( )       { return "Circle: " + radius; }     private double radius; } public class Rectangle extends Shape {     public Rectangle( double len, double wid )       { length = len; width = wid; }     public double area( )       { return length * width; }     public double perimeter( )       { return 2 * ( length + width ); }     public String toString( )       { return "Rectangle: " + length + " " + width; }     public double getLength( )       { return length; }     public double getWidth( )       { return width; }     private double length;     private double width; }

chapter 4 inheritance Abstract methods  and classes represent placeholders. A better solution for area is to throw a runtime exception (a good one is UnsupportedOperationException) in the Shape class. This is preferable to returning –1 because the exception will not be ignored. However, even that solution resolves the problem at runtime. It would be better to have syntax that explicitly states that area is a placeholder and does not need any implementation at all, and that further, Shape is a placeholder class and cannot be constructed, even though it may declare constructors, and will have a default constructor if none are declared. If this syntax were available, then the compiler could, at compile time, declare as illegal any attempts to construct a Shape instance. It could also declare as illegal any classes, such as Triangle, for which there are attempts to construct instances, even though area has not been overridden. This exactly describes abstract methods and abstract classes. An abstract method has no meaningful  deﬁnition and is  thus always deﬁned  in the derived class. An abstract method is a method that declares functionality that all derived class objects must eventually implement. In other words, it says what these objects can do. However, it does not provide a default implementation. Instead, each object must provide its own implementation. figure 4.13 A sample  program that  uses the  shape hierarchy class ShapeDemo {     public static double totalArea( Shape [ ] arr )     {         double total = 0;         for( Shape s : arr )             if( s != null )                 total += s.area( );         return total;     }     public static void printAll( Shape [ ] arr )     {         for( Shape s : arr )             System.out.println( s );     }     public static void main( String [ ] args )     {          Shape [ ] a = { new Circle( 2.0 ), new Rectangle( 1.0, 3.0 ), null };         System.out.println( "Total area = " + totalArea( a ) );         printAll( a );     } }

4.2 designing hierarchies A class that has at least one abstract method is an abstract class. Java requires that all abstract classes explicitly be declared as such. When a derived class fails to override an abstract method with an implementation, the method remains abstract in the derived class. As a result, if a class that is not intended to be abstract fails to override an abstract method, the compiler will detect the inconsistency and report an error. An example of how we can make Shape abstract is shown in Figure 4.14. No changes are required to any of the other code in Figures 4.12 and 4.13. Observe that an abstract class can have methods that are not abstract, as is the case with semiperimeter. An abstract class can also declare both static and instance ﬁelds. Like nonabstract classes, these ﬁelds would typically be private, and the instance ﬁelds would be initialized by constructors. Although abstract classes cannot be created, these constructors will be called when the derived classes use super. In a more extensive example, the Shape class could include the coordinates of the object’s extremities, which would be set by constructors, and it could provide implementation of methods, such as positionOf, that are independent of the actual type of object; positionOf would be a ﬁnal method. A class with at least  one abstract  method must be an  abstract class. As mentioned earlier, the existence of at least one abstract method makes the base class abstract and disallows creation of it. Thus a Shape object cannot itself be created; only the derived objects can. However, as usual, a Shape variable can reference any concrete derived object, such as a Circle or Rectangle. Thus         Shape a, b;         a = new Circle( 3.0 );      // Legal         b = new Shape( );           // Illegal Before continuing, let us summarize the four types of class methods: 1. Final methods. The virtual machine may choose at run time to perform inline optimization, thus avoiding dynamic dispatch. We use a ﬁnal method only when the method is invariant over the inheritance hierarchy (that is, when the method is never redeﬁned). figure 4.14 An abstract Shape class. Figures 4.12  and 4.13 are  unchanged. public abstract class Shape {     public abstract double area( );     public abstract double perimeter( );     public double semiperimeter( )       { return perimeter( ) / 2; } }

chapter 4 inheritance 2. Abstract methods. Overriding is resolved at run time. The base class provides no implementation and is abstract. The absence of a default requires either that the derived classes provide an implementation or that the classes themselves be abstract. 3. Static methods. Overridding is resolved at compile time because there is no controlling object. 4. Other methods. Overriding is resolved at run time. The base class provides a default implementation that may be either overridden by the derived classes or accepted unchanged by the derived classes. 4.2.2   designing for the future Consider the following implementation for the Square class: public class Square extends Rectangle {     public Square( double side )       { super( side, side ); } } Since obviously a square is a rectangle whose length and width are the same, it would seem reasonable to have Square extend Rectangle, and thus avoid rewriting methods such as area and perimeter. While it is true that because toString is not overridden, Squares will always be output as Rectangles with identical lengths and widths, that can be repaired by providing a toString method for Square. Thus the Square class can be made slick and we can reuse Rectangle code. But is this a reasonable design? To answer that question, we must go back to the fundamental rule of inheritance. The extends clause is appropriate only if it is true that Square IS-A Rectangle. From a programming perspective, this does not simply mean that a square must geometrically be a type of rectangle; rather, it means that any operation that a Rectangle can perform must also be able to be supported by a Square. Most important, however, this is not a static decision, meaning that we do not simply look at the current set of operations supported by Rectangle. Rather, we must ask if it is reasonable to assume that in the future it is possible that operations might be added to the Rectangle class that would not make sense for a Square. If that is the case, then the argument that a Square IS-A Rectangle is weakened considerably. For instance, suppose the Rectangle class has a stretch method whose contract provides that stretch lengths the larger dimension of the Rectangle, while leaving the smaller dimension in tact. Clearly the operation cannot be made available to a Square, since doing so would destroy squareness.

4.3 multiple inheritance Multiple inheritance is used to derive a  class from several  base classes. Java  does not allow multiple inheritance. If we know that the Rectangle class has a stretch method, then it is probably not a good design to have Square extend Rectangle. If Square already extends Rectangle and then later on we want to add stretch to Rectangle, there are two basic ways to proceed. Option #1 would be to have Square override stretch with an implementation that throws an exception:  public void stretch( double factor ) { throw new UnsupportedOperationException( ); } With this design, at least squares will never lose their squareness. Option #2 would be to redesign the entire hierarchy to have Square no longer extend Rectangle. This is known as refactoring. Depending on how complicated the entire hierarchy is, this could be an incredibly messy task. However, some development tools can automate much of the process. The best plan, especially for a large hierarchy is to think about these issues at the time of design, and ask what the hierarchy might reasonably look like in the future. Often this is easy to say, but very challenging to do. A similar philosophy occurs when deciding what exceptions should be listed in the throws list of a method. Because of the IS-A relationship, when a method is overridden, new checked exceptions cannot be added to the throws list. The overriding implementation can reduce, or subset, the original list of checked exceptions, but can never add to it. As such, in determining the throws list of a method, the designer should look not only at the exceptions that can be thrown in the current implementation of the method, but also what exceptions might reasonably be thrown by the method in the future (should the implementation change) and what exceptions might be thrown by overriding implementations provided by future subclasses. 4.3 multiple inheritance All the inheritance examples seen so far derived one class from a single base class. In multiple inheritance, a class may be derived from more than one base class. For instance, we may have a Student class and an Employee class. A StudentEmployee could then be derived from both classes. Although multiple inheritance sounds attractive, and some languages (including C++) support it, it is wrought with subtleties that make design difficult. For instance, the two base classes may contain two methods that have the same signature but different implementations. Alternatively, they may have two identically named ﬁelds. Which one should be used?

chapter 4 inheritance For example, suppose that in the previous StudentEmployee example, Person is a class with data ﬁeld name and method toString. Suppose, too, that Student extends Person and overrides toString to include the year of graduation. Further, suppose that Employee extends Person but does not override toString; instead, it declares that it is final. 1. Since StudentEmployee inherits the data members from both Student and Employee, do we get two copies of name? 2. If StudentEmployee does not override toString, which toString method should be used? When many classes are involved, the problems are even larger. It appears, however, that the typical multiple inheritance problems can be traced to conﬂicting implementations or conﬂicting data ﬁelds. As a result, Java does not allow multiple inheritance of implementations. However, allowing multiple inheritance for the purposes of typecompatibility can be very useful, as long as we can ensure that there are no implementation conﬂicts. Returning to our Shape example, suppose our hierarchy contains many shapes such as Circle, Square, Ellipse, Rectangle, Triangle. Suppose that for some, but not all of these shapes, we have a stretch method, as described in Section 4.2.2 that lengthens the longest dimension, leaving all others unchanged. We could reasonably envision that the stretch method is written for Ellipse, Rectangle, and Triangle, but not Circle or Square. We would like a method to stretch all the shapes in an array:     public static void stretchAll( WhatType [ ] arr, factor )     {         for( WhatType s : arr )             s.stretch( factor );     } The idea is that stretchAll would work for arrays of Ellipse, arrays of Rectangle, arrays of Triangle, or even an array that contained Ellipses, Rectangles, and Triangles. For this code to work, we need to decide the type declaration for WhatType. One possibility is that WhatType can be Shape, as long as Shape has an abstract stretch method. We could then override stretch for each type of Shape, having Circle and Square throw UnsupportedOperationExceptions. But as we discussed in Section 4.2.2, this solution seems to violate the notion of an IS-A relationship, and moreover, it does not generalize out to more complicated cases.

4.3 multiple inheritance Another idea would be to try to deﬁne an abstract class Stretchable as follows: abstract class Stretchable {     public abstract void stretch( double factor ); } We could use Stretchable as our missing type in the stretchAll method. At that point, we would try to have Rectangle, Ellipses, and Triangle extend Stretchable, and provide the stretch method: // Does not work public class Rectangle extends Shape, Stretchable {     public void stretch( double factor )       { ... }     public void area( )       { ... }     ... } The picture that we would have at this point is shown in Figure 4.15. In principle this would be ﬁne, but then we have multiple inheritance, which we have previously said was illegal, because of concerns that we might inherit conﬂicting implementations. As it stands now, only the Shape class has an implementation; Stretchable is purely abstract, so one could argue that the compiler should be willing to grant leniency. But it is possible that after everything is compiled, Stretchable could be altered to provide an Shape Circle Square Rectangle Ellipse Triangle Stretchable figure 4.15 Inheriting multiple classes. This does not work unless either Shape or Stretchable is specifically designated as being implementation-free

chapter 4 inheritance implementation, at which point there would be a problem. What we would like is more than a campaign promise; we need some syntax that forces Stretchable to remain implementation-free forever. If this was possible, then the compiler could allow the inheritance from two classes that has a hierarchy in the style of Figure 4.15. This syntax is precisely what an interface is. 4.4 the interface The interface is an  abstract class that  contains no implementation details. The interface in Java is the ultimate abstract class. It consists of public abstract methods and public static ﬁnal ﬁelds, only. A class is said to implement the interface if it provides deﬁnitions for all of the abstract methods in the interface. A class that implements the interface behaves as if it had extended an abstract class speciﬁed by the interface. In principle, the main difference between an interface and an abstract class is that although both provide a speciﬁcation of what the subclasses must do, the interface is not allowed to provide any implementation details either in the form of data ﬁelds or implemented methods. The practical effect of this is that multiple interfaces do not suffer the same potential problems as multiple inheritance because we cannot have conﬂicting implementations. Thus, while a class may extend only one other class, it may implement more than one interface. 4.4.1   specifying an interface Syntactically, virtually nothing is easier than specifying an interface. The interface looks like a class declaration, except that it uses the keyword interface. It consists of a listing of the methods that must be implemented. An example is the Stretchable interface, shown in Figure 4.16. The Stretchable interface speciﬁes one method that every subclass must implement: Stretch. Note that we do not have to specify that these methods are public and abstract. Since these modiﬁers are required for interface methods, they can and usually are omitted. figure 4.16 The Stretchable interface /** 2  * Interface that defines stretch method to lengthen the longest 3  * dimension of a Shape  */ public interface Stretchable {     void stretch( double factor ); }

4.4 the interface 4.4.2   implementing an interface The implements clause is used to  declare that a class  implements an  interface. The class  must implement all  interface methods  or it remains  abstract. A class implements an interface by 1. Declaring that it implements the interface 2. Deﬁning implementations for all the interface methods An example is shown in Figure 4.17. Here, we complete the Rectangle class, which we used in Section 4.2. Line 1 shows that when implementing an interface, we use implements instead of extends. We can provide any methods that we want, but we must provide at least those listed in the interface. The interface is implemented at lines 5 to 14. Notice that we must implement the exact method speciﬁed in the interface. A class that implements an interface can be extended if it is not ﬁnal. The extended class automatically implements the interface. As we see from our example, a class that implements an interface may still extend one other class. The extends clause must precede the implements clause. 4.4.3   multiple interfaces As we mentioned earlier, a class may implement multiple interfaces. The syntax for doing so is simple. A class implements multiple interfaces by 1. Listing the interfaces (comma separated) that it implements 2. Deﬁning implementations for all of the interface methods The interface is the ultimate in abstract classes and represents an elegant solution to the multiple inheritance problem. figure 4.17 The Rectangle class  (abbreviated), which  implements the  Stretchable interface public class Rectangle extends Shape implements Stretchable {     /* Remainder of class unchanged from Figure 4.12 */     public void stretch( double factor )     {         if( factor <= 0 )             throw new IllegalArgumentException( );         if( length > width )             length *= factor;         else             width *= factor;     } }

chapter 4 inheritance 4.4.4   interfaces are abstract classes Because an interface is an abstract class, all the rules of inheritance apply. Speciﬁcally, 1. The IS-A relationship holds. If class C implements interface I, then C IS-A I and is type-compatible with I. If a class C implements interfaces I1, I2, and I3, then C IS-A I1, C IS-A I2, and C IS-A I3, and is type-compatible with I1, I2, and I3. 2. The instanceof operator can be used to determine if a reference is type-compatible with an interface. 3. When a class implements an interface method, it may not reduce visibility. Since all interface methods are public, all implementations must be public. 4. When a class implements an interface method, it may not add checked exceptions to the throws list. If a class implements multiple interfaces in which the same method occurs with a different throws list, the throws list of the implementation may list only checked exceptions that are in the intersection of the throws lists of the interface methods. 5. When a class implements an interface method, it must implement the exact signature (not including the throws list); otherwise, it inherits an abstract version of the interface method and has provided a nonabstract overloaded, but different, method. 6. A class may not implement two interfaces that contain a method with the same signature and incompatible return types, since it would be impossible to provide both methods in one class. 7. If a class fails to implement any methods in an interface, it must be declared abstract. 8. Interfaces can extend other interfaces (including multiple interfaces). 4.5 fundamental inheritance in java Two important places where inheritance is used in Java are the Object class and the hierarchy of exceptions. 4.5.1   the Object class Java speciﬁes that if a class does not extend another class, then it implicitly extends the class Object (deﬁned in java.lang). As a result, every class is either a direct or indirect subclass of Object.

4.5 fundamental inheritance in java The Object class contains several methods, and since it is not abstract, all have implementations. The most commonly used method is toString, which we have already seen. If toString is not written for a class, an implementation is provided that concatenates the name of the class, an @, and the class’s “hash code.” Other important methods are equals and the hashCode, which we will discuss in more detail in Chapter 6, and a set of somewhat tricky methods that advanced Java programmers need to know about. 4.5.2   the hierarchy of exceptions As described in Section 2.5, there are several types of exceptions. The root of the hierarchy, a portion of which is shown in Figure 4.18, is Throwable, which NullPointerException ArrayIndexOutOfBoundsException ArithmeticException UnsupportedOperationException NoSuchMethodException InvalidArgumentException java.util.NoSuchElementException java.util.ConcurrentModificationException java.util.EmptyStackException ClassCastException OutOfMemoryError InternalError UnknownError java.io.IOException java.io.FileNotFoundException RuntimeException Error Throwable Exception figure 4.18 The hierarchy of  exceptions (partial  list)

chapter 4 inheritance deﬁnes a set of printStackTrace methods, provides a toString implementation, a pair of constructors, and little else. The hierarchy is split off into Error, RuntimeException, and checked exceptions. A checked exception is any Exception that is not a RuntimeException. For the most part, each new class extends another exception class, providing only a pair of constructors. It is possible to provide more, but none of the standard exceptions bother to do so. In weiss.util, we implement three of the standard java.util exceptions. One such implementation, which shows that new exception classes typically provide little more than constructors, is shown in Figure 4.19. 4.5.3   i/o: the decorator pattern I/O in Java looks fairly complex to use but works nicely for doing I/O with different sources, such as the terminal, ﬁles, and Internet sockets. Because it is designed to be extensible, there are lots of classes—over 50 in all. It is cumbersome to use for trivial tasks; for instance, reading a number from the terminal requires substantial work. Input is done through the use of stream classes. Because Java was designed for Internet programming, most of the I/O centers around byteoriented reading and writing. figure 4.19 NoSuchElementException, implemented in  weiss.util package weiss.util; public class NoSuchElementException extends RuntimeException {     /**      * Constructs a NoSuchElementException with      * no detail message.      */     public NoSuchElementException( )     {     }     /*      * Constructs a NoSuchElementException with      * a detail message.      * @param msg the detail message.      */     public NoSuchElementException( String msg )     {         super( msg );     } }

4.5 fundamental inheritance in java Byte-oriented I/O is done with stream classes that extend InputStream or OutputStream. InputStream and OutputStream are abstract classes and not interfaces, so there is no such thing as a stream open for both input and output. These classes declare an abstract read and write method for single-byte I/O, respectively, and also a small set of concrete methods such as close and block I/O (which can be implemented in terms of calls to single-byte I/O). Examples of these classes include FileInputStream and FileOutputStream, as well as the hidden SocketInputStream and SocketOutputStream. (The socket streams are produced by methods that return an object statically typed as InputStream or OutputStream.) InputStreamReader and OutputStreamWriter classes are bridges that allow  the programmer to  cross over from  the Stream to  Reader and Writer hierarchies. Character-oriented I/O is done with classes that extend the abstract classes Reader and Writer. These also contain read and write methods. There are not as many Reader and Writer classes as InputStream and OutputStream classes. However, this is not a problem, because of the InputStreamReader and OutputStreamWriter classes. These are called bridges because they cross over from the Stream to Reader and Writer hierarchies. An InputStreamReader is constructed with any InputStream and creates an object that IS-A Reader. For instance, we can create a Reader for ﬁles using         InputStream fis = new FileInputStream( "foo.txt" );         Reader fin = new InputStreamReader( fis ); It happens that there is a FileReader convenience class that does this already; Figure 4.20 provides a plausible implementation.  From a Reader, we can do limited I/O; the read method returns one character. If we want to read one line instead, we need a class called BufferedReader. Like other Reader objects, a BufferedReader is constructed from any other Reader, but it provides both buffering and a readLine method. Thus, continuing the previous example,         BufferedReader bin = new BufferedReader( fin ); Wrapping an InputStream inside an InputStreamReader inside a BufferedReader works for any InputStream, including System.in or sockets. Figure 4.21, which mimics Figure 2.17, illustrates the use of this pattern to read two numbers from the standard input. figure 4.20 The FileReader convenience class class FileReader extends InputStreamReader {     public FileReader( String name ) throws FileNotFoundException       { super( new FileInputStream( name ) ); } }

chapter 4 inheritance The wrapping idea is an example of a commonly used Java design pattern, which we will see again in Section 4.6.2. Similar to the BufferedReader is the PrintWriter, which allows us to do println operations. The OutputStream hierarchy includes several wrappers, such as DataOutput-Stream, ObjectOutputStream, and GZIPOutputStream. DataOutputStream allows us to write primitives in binary form (rather than human-readable text form); for instance, a call to writeInt writes the 4 bytes that represent a 32-bit integer. Writing data that way avoids conversions to text form, resulting in time and (sometimes) space savings. ObjectOutputStream allows us to write an entire object including all its components, its component’s components, etc., to a stream. The object and all its components must figure 4.21 A program that  demonstrates the  wrapping of streams  and readers import java.io.InputStreamReader; import java.io.BufferedReader; import java.io.IOException; import java.util.Scanner; import java.util.NoSuchElementException; class MaxTest {     public static void main( String [ ] args )     {         BufferedReader in = new BufferedReader( new                              InputStreamReader( System.in ) );         System.out.println( "Enter 2 ints on one line: " );         try         {             String oneLine = in.readLine( );             if( oneLine == null )                 return;             Scanner str = new Scanner( oneLine );             int x = str.nextInt( );             int y = str.nextInt( );             System.out.println( "Max: " + Math.max( x, y ) );         }         catch( IOException e )           { System.err.println( "Unexpected I/O error" ); }         catch( NoSuchElementException e )           { System.err.println( "Error: need two ints" ); }     } }

4.5 fundamental inheritance in java implement the Serializable interface. There are no methods in the interface; one must simply declare that a class is serializable.6 The GZIPOutputStream wraps an OutputStream and compresses the writes prior to sending it to the OutputStream. In addition, there is a BufferedOutputStream class. Similar wrappers are found on the InputStream side. As an example, suppose we have an array of serializable Person objects. We can write the objects, as a unit, compressed as follows: Person [ ] p = getPersons( );   // populate the array FileOutputStream fout = new FileOutputStream( "people.gzip" ); BufferedOutputStream bout = new BufferedOutputStream( fout ); GZIPOutputStream gout = new GZIPOutputStream( bout ); ObjectOutputStream oout = new ObjectOutputStream( gout ); oout.writeObject( p ); oout.close( ); Later on, we could read everything back: FileInputStream fin = new FileInputStream( "people.gzip" ); BufferedInputStream bin = new BufferedInputStream( fin ); GZIPInputStream gin = new GZIPInputStream( bin ); ObjectInputStream oin = new ObjectInputStream( gin ); Person [ ] p = (Person[ ]) oin.readObject( ); oin.close( ); The online code expands this example by having each Person store a name, a birth date, and the two Person objects that represent the parents. The idea of nesting  wrappers in order  to add functionality  is known as the  decorator pattern. The idea of nesting wrappers in order to add functionality is known as the decorator pattern. By doing this, we have numerous small classes that are combined to provide a powerful interface. Without this pattern, each different I/O source would have to have functionality for compression, serialization, character, and byte I/O, and so on. With the pattern, each source is only responsible for minimal, basic I/O, and then the extra features are added on by the decorators.  6. The reason for this is that serialization, by default, is insecure. When an object is written out in an ObjectOutputStream, the format is well known, so its private members can be read by a malicious user. Similarly, when an object is read back in, the data on the input stream is not checked for correctness, so it is possible to read a corrupt object. There are advanced techniques that can be used to ensure security and integrity when serialization is used, but that is beyond the scope of this text. The designers of the serialization library felt that serialization should not be the default because correct use requires knowledge of these issues, and so they placed a small roadblock in the way.

chapter 4 inheritance 4.6 implementing generic  components using inheritance Generic programming allows us to  implement typeindependent logic. Recall that an important goal of object-oriented programming is the support of code reuse. An important mechanism that supports this goal is the generic mechanism: If the implementation is identical except for the basic type of the object, a generic implementation can be used to describe the basic functionality. For instance, a method can be written to sort an array of items; the logic is independent of the types of objects being sorted, so a generic method could be used. In Java, genericity  is obtained by using  inheritance. Unlike many of the newer languages (such as C++, which uses templates to implement generic programming), before version 1.5 Java did not support generic implementations directly. Instead, generic programming was implemented using the basic concepts of inheritance. This section describes how generic methods and classes can be implemented in Java using the basic principles of inheritance.  Direct support for generic methods and classes was announced by Sun in June 2001 as a future language addition. Finally, in late 2004, Java 5 was released and provided support for generic methods and classes. However, using generic classes requires an understanding of the pre-Java 5 idioms for generic programming. As a result, an understanding of how inheritance is used to implement generic programs is essential, even in Java 5. 4.6.1   using Object for genericity The basic idea in Java is that we can implement a generic class by using an appropriate superclass, such as Object. Consider the IntCell class shown in Figure 3.2. Recall that the IntCell supports the read and write methods. We can, in principle, make this a generic MemoryCell class that stores any type of Object by replacing instances of int with Object. The resulting MemoryCell class is shown in Figure 4.22. There are two details that must be considered when we use this strategy. The ﬁrst is illustrated in Figure 4.23, which depicts a main that writes a "37" to a MemoryCell object and then reads from the MemoryCell object. To access a speciﬁc method of the object we must downcast to the correct type. (Of course in this example, we do not need the downcast, since we are simply invoking the toString method at line 9, and this can be done for any object.) A second important detail is that primitive types cannot be used. Only reference types are compatible with Object. A standard workaround to this problem is discussed momentarily.

4.6 implementing generic components using inheritance MemoryCell is a fairly small example. A larger example that is typical of generic code reuse, Figure 4.24 shows a simpliﬁed generic ArrayList class as it would be written before Java 5; the online code ﬁlls in some additional methods. 4.6.2   wrappers for primitive types When we implement algorithms, often we run into a language typing problem: We have an object of one type, but the language syntax requires an object of a different type. This technique illustrates the basic theme of a wrapper class. One typical use is to store a primitive type, and add operations that the primitive type either does not support or does not support correctly. A second example was seen in the I/O system, in which a wrapper stores a reference to an object and forwards requests to the object, embellishing the result somehow (for instance, with buffering or compression). A similar concept is an adapter class (in fact, wrapper and adapter are often used interchangeably). An figure 4.22 A generic MemoryCell class (pre-Java 5) // MemoryCell class //  Object read( )         -->  Returns the stored value //  void write( Object x ) -->  x is stored public class MemoryCell {         // Public methods     public Object read( )         { return storedValue; }     public void write( Object x ) { storedValue = x; }         // Private internal data representation     private Object storedValue; } figure 4.23 Using the generic  MemoryCell class  (pre-Java 5) public class TestMemoryCell {     public static void main( String [ ] args )     {         MemoryCell m = new MemoryCell( );         m.write( "37" );         String val = (String) m.read( );         System.out.println( "Contents are: " + val );     } } A wrapper class stores an entity  (the wrapee) and  adds operations  that the original  type does not support correctly. An  adapter class is  used when the  interface of a class  is not exactly what  is needed.

chapter 4 inheritance figure 4.24 A simplified ArrayList, with add, get, and size (pre-Java 5) /**  * The SimpleArrayList implements a growable array of Object.  * Insertions are always done at the end.  */ public class SimpleArrayList {     /**      * Returns the number of items in this collection.      * @return the number of items in this collection.      */     public int size( )     {         return theSize;     }     /**      * Returns the item at position idx.      * @param idx the index to search in.      * @throws ArrayIndexOutOfBoundsException if index is bad.      */     public Object get( int idx )     {         if( idx < 0 || idx >= size( ) )             throw new ArrayIndexOutOfBoundsException( );         return theItems[ idx ];     }     /**      * Adds an item at the end of this collection.      * @param x any object.      * @return true (as per java.util.ArrayList).      */     public boolean add( Object x )     {         if( theItems.length == size( ) )         {             Object [ ] old = theItems;             theItems = new Object[ theItems.length * 2 + 1 ];             for( int i = 0; i < size( ); i++ )                 theItems[ i ] = old[ i ];         }         theItems[ theSize++ ] = x;          return true;     }     private static final int INIT_CAPACITY = 10;     private int         theSize = 0;     private Object [ ] theItems = new Object[ INIT_CAPACITY ]; }

4.6 implementing generic components using inheritance adapter class is typically used when the interface of a class is not exactly what is needed, and provides a wrapping effect, while changing the interface. In Java, we have already seen that although every reference type is compatible with Object, the eight primitive types are not. As a result, Java provides a wrapper class for each of the eight primitive types. For instance, the wrapper for the int type is Integer. Each wrapper object is immutable (meaning its state can never change), stores one primitive value that is set when the object is constructed, and provides a method to retrieve the value. The wrapper classes also contain a host of static utility methods. As an example, Figure 4.25 shows how we can use the Java 5 ArrayList to store integers. Note carefully that we cannot use ArrayList<int>. 4.6.3   autoboxing/unboxing The code in Figure 4.25 is annoying to write because using the wrapper class requires creation of an Integer object prior to the call to add, and then the extraction of the int value from the Integer, using the intValue method. Prior to Java 1.4, this is required because if an int is passed in a place where an Integer object is required, the compiler will generate an error message, and if the result of an Integer object is assigned to an int, the compiler will generate an error message. This resulting code in Figure 4.25 accurately reﬂects the distinction between primitive types and reference types, yet, it does not cleanly express the programmer’s intent of storing ints in the collection.  Java 5 rectiﬁes this situation. If an int is passed in a place where an Integer is required, the compiler will insert a call to the Integer constructor behind the scenes. This is known as auto-boxing. And if an Integer is passed in a figure 4.25 An illustration of the  Integer wrapper class  using Java 5 generic  ArrayList import java.util.ArrayList; public class BoxingDemo {     public static void main( String [ ] args )     {         ArrayList<Integer> arr = new ArrayList<Integer>( );         arr.add( new Integer( 46 ) );         Integer wrapperVal = arr.get( 0 );         int val = wrapperVal.intValue( );         System.out.println( "Position 0: " + val );      } }

chapter 4 inheritance place where an int is required, the compiler will insert a call to the intValue method behind the scenes. This is known as auto-unboxing. Similar behavior occurs for the seven other primitive/wrapper pairs. Figure 4.26 illustrates the use of autoboxing and unboxing. Note that the entities referenced in the ArrayList are still Integer objects; int cannot be substituted for Integer in the ArrayList instantiations. 4.6.4   adapters: changing an interface The adapter pattern is used to change the interface of an existing class to conform to another. Sometimes it is used to provide a simpler interface, either with fewer methods or easier-to-use methods. Other times it is used simply to change some method names. In either case, the implementation technique is similar. We have already seen one example of an adapter: the bridge classes InputStreamReader and OutputStreamWriter that convert byte-oriented streams into character-oriented streams. As another example, our MemoryCell class in Section 4.6.1 uses read and write. But what if we wanted the interface to use get and put instead? There are two reasonable alternatives. One is to cut and paste a completely new class. The other is to use composition, in which we design a new class that wraps the behavior of an existing class. We use this technique to implement the new class, StorageCell, in Figure 4.27. Its methods are implemented by calls to the wrapped MemoryCell. It is tempting to use inheritance instead of composition, but inheritance supplements the interface (i.e., it adds additional methods, but leaves the originals). If that is the appropriate behavior, then indeed inheritance may be preferable to composition. figure 4.26 Autoboxing and  unboxing import java.util.ArrayList; public class BoxingDemo {     public static void main( String [ ] args )     {         ArrayList<Integer> arr = new ArrayList<Integer>( );         arr.add( 46 );         int val = arr.get( 0 );         System.out.println( "Position 0: " + val );      } } The adapter pattern is used to change  the interface of an  existing class to  conform to another.

4.6 implementing generic components using inheritance 4.6.5   using interface types for genericity Using Object as a generic type works only if the operations that are being performed can be expressed using only methods available in the Object class. Consider, for example, the problem of ﬁnding the maximum item in an array of items. The basic code is type-independent, but it does require the ability to compare any two objects and decide which is larger and which is smaller. For instance, here is the basic code for ﬁnding the maximum BigInteger in an array:     public static BigInteger findMax( BigInteger [ ] arr )     {         int maxIndex = 0;         for( int i = 1; i < arr.length; i++ ) if( arr[i].compareTo( arr[ maxIndex ] < 0 ) maxIndex = i;         return arr[ maxIndex ];     } Finding the maximum item in an array of String, where maximum is taken to be lexicographically (i.e. alphabetically last) is the same basic code. figure 4.27 An adapter class that  changes the  MemoryCell interface  to use get and put // A class for simulating a memory cell. public class StorageCell {     public Object get( )       { return m.read( ); }     public void put( Object x )       { m.write( x ); }     private MemoryCell m = new MemoryCell( ); }

chapter 4 inheritance     public static String findMax( String [ ] arr )     {         int maxIndex = 0;         for( int i = 1; i < arr.length; i++ ) if( arr[i].compareTo( arr[ maxIndex ] < 0 ) maxIndex = i;         return arr[ maxIndex ];     } If we want the findMax code to work for both types, or even others that happen to also have a compareTo method, then we should be able to do so, as long as we can identify a unifying type. As it turns out, the Java language deﬁnes the Comparable interface, that contains a single compareTo method. Many library classes implement the interface. We can also have own own classes implement the interface. Figure 4.28 shows the basic hierarchy. Older versions of Java require that compareTo’s parameter be listed as type Object; newer versions (since Java 5) will have Comparable be a generic interface, which we discuss in Section 4.7. With this interface, we can simply write the findMax routine to accept an array of Comparable. The older, pre-generic style for findMax is shown in Figure 4.29, along with a test program. It is important to mention a few caveats. First, only objects that implement the Comparable interface can be passed as elements of the Comparable array. Objects that have a compareTo method but do not declare that they implement Comparable are not Comparable, and do not have the requisite IS-A relationship. Second, if a Comparable array were to have two objects that are incompatible (e.g., a Date and a BigInteger), the compareTo method would throw a ClassCastException. This is the expected (indeed, required) behavior. Third, as before, primitives cannot be passed as Comparables, but the wrappers work because they implement the Comparable interface. Comparable Date String BigInteger figure 4.28 Three classes that  all implement the  Comparable interface

4.6 implementing generic components using inheritance Fourth, it is not required that the interface be a standard library interface. Finally, this solution does not always work, because it might be impossible to declare that a class implements a needed interface. For instance, the class might be a library class, while the interface is a user-deﬁned interface. And if the class is ﬁnal, we can’t even create a new class. Section 4.8 offers another solution for this problem, which is the function object. The function object uses interfaces also, and is perhaps one of the central themes encountered in the Java library. figure 4.29 A generic findMax routine, with demo  using shapes and  strings (pre-Java 5) import java.math.BigInteger; class FindMaxDemo {     /**      * Return max item in a.      * Precondition: a.length > 0      */     public static Comparable findMax( Comparable [ ] a )     {         int maxIndex = 0;         for( int i = 1; i < a.length; i++ )             if( a[ i ].compareTo( a[ maxIndex ] ) > 0 )                 maxIndex = i;         return a[ maxIndex ];     }     /**      * Test findMax on BigInteger and String objects.      */     public static void main( String [ ] args )     {         BigInteger [ ] bi1 = { new BigInteger( "8764" ),                                new BigInteger( "29345" ),                                new BigInteger( "1818" ) };         String [ ] st1 = { "Joe", "Bob", "Bill", "Zeke" };         System.out.println( findMax( bi1 ) );         System.out.println( findMax( st1 ) );     } }

chapter 4 inheritance 4.7 implementing generic components  using java 5 generics We have already seen that Java 5 supports generic classes and that these classes are easy to use. However, writing generic classes requires a little more work. In this section, we illustrate the basics of how generic classes and methods are written. We do not attempt to cover all the constructs of the language, which are quite complex and sometimes tricky. Instead, we show the syntax and idioms that are used throughout this book. 4.7.1   simple generic classes and interfaces Figure 4.30 shows a generic version of the MemoryCell class previously depicted in Figure 4.22. Here, we have changed the name to GenericMemoryCell because neither class is in a package and thus the names cannot be the same. When a generic  class is speciﬁed,  the class declaration includes one or  more type parameters, enclosed in  angle brackets <> after the class  name. When a generic class is speciﬁed, the class declaration includes one or more type parameters enclosed in angle brackets <> after the class name. Line 1 shows that the GenericMemoryCell takes one type parameter. In this instance, there are no explicit restrictions on the type parameter, so the user can create types such as GenericMemoryCell<String> and GenericMemoryCell<Integer> but not GenericMemoryCell<int>. Inside the GenericMemoryCell class declaration, we can declare ﬁelds of the generic type and methods that use the generic type as a parameter or return type. Interfaces can also  be declared as  generic. Interfaces can also be declared as generic. For example, prior to Java 5 the Comparable interface was not generic, and its compareTo method took an Object as the parameter. As a result, any reference variable passed to the compareTo method would compile, even if the variable was not a sensible type, and only at runtime would the error be reported as a ClassCastException. In Java 5, the Comparable class is generic, as shown in Figure 4.31. The String class, for figure 4.30 Generic implementation of the  MemoryCell class public class GenericMemoryCell<AnyType> {     public AnyType read( )        { return storedValue; }     public void write( AnyType x )       { storedValue = x; }     private AnyType storedValue; }

4.7 implementing generic components using java 5 generics instance, now implements Comparable<String> and has a compareTo method that takes a String as a parameter. By making the class generic, many of the errors that were previously only reported at runtime become compile-time errors. 4.7.2   wildcards with bounds In Figure 4.13 we saw a static method that computes the total area in an array of Shapes. Suppose we want to rewrite the method so that it works with a parameter that is ArrayList<Shape>. Because of the enhanced for loop, the code should be identical, and the resulting code is shown in Figure 4.32. If we pass an ArrayList<Shape>, the code works. However, what happens if we pass an ArrayList<Square>? The answer depends on whether an ArrayList<Square> IS-A ArrayList<Shape>. Recall from Section 4.1.10 that the technical term for this is whether we have covariance. Generic collections  are not covariant. In Java, as we mentioned in Section 4.1.10, arrays are covariant. So Square[] IS-A Shape[]. On the one hand, consistency would suggest that if arrays are covariant, then collections should be covariant too. On the other hand, as we saw in Section 4.1.10, the covariance of arrays leads to code that compiles but then generates a runtime exception (an ArrayStoreException). Because the entire reason to have generics is to generate compiler errors rather than runtime exceptions for type mismatches, generic collections are not covariant. As a result, we cannot pass an ArrayList<Square> as a parameter to the method in Figure 4.32. figure 4.31 Comparable interface,  Java 5 version which  is generic package java.lang; public interface Comparable<AnyType> {     public int compareTo( AnyType other ); } figure 4.32 totalArea method that  does not work if  passed an  ArrayList<Square> public static double totalArea( ArrayList<Shape> arr ) {     double total = 0;     for( Shape s : arr )         if( s != null )             total += s.area( );     return total; }

chapter 4 inheritance What we are left with is that generics (and the generic collections) are not covariant (which makes sense) but arrays are. Without additional syntax, users would tend to avoid collections because the lack of covariance makes the code less ﬂexible. Wildcards are used  to express subclasses (or  superclasses) of  parameter types. Java 5 makes up for this with wildcards. Wildcards are used to express subclasses (or superclasses) of parameter types. Figure 4.33 illustrates the use of wildcards with a bound to write a totalArea method that takes as parameter an ArrayList<T>, where T IS-A Shape. Thus, ArrayList<Shape> and ArrayList<Square> are both acceptable parameters. Wildcards can also be used without a bound (in which case extends Object is presumed) or with super instead of extends (to express superclass rather than subclass); there are also some other syntax uses that we do not discuss here.  4.7.3   generic static methods In some sense, the totalArea method in Figure 4.33 is generic, since it works for different types. But there is no speciﬁc type parameter list, as was done in the GenericMemoryCell class declaration. Sometimes the speciﬁc type is important perhaps because one of the following reasons apply: The generic  method looks much  like the generic  class in that the  type parameter list  uses the same syntax. The type list in  a generic method  precedes the return  type. 1. The type is used as the return type 2. The type is used in more than one parameter type 3. The type is used to declare a local variable If so, then an explicit generic method with type parameters must be declared. For instance, Figure 4.34 illustrates a generic static method that performs a sequential search for value x in array arr. By using a generic method instead of a nongeneric method that uses Object as the parameter types, we can get compile-time errors if searching for an Apple in an array of Shapes. figure 4.33 totalArea method  revised with wildcards  that works if passed  an ArrayList<Square> public static double totalArea( ArrayList<? extends Shape> arr ) {     double total = 0;     for( Shape s : arr )         if( s != null )             total += s.area( );     return total; }

4.7 implementing generic components using java 5 generics The generic method looks much like the generic class in that the type parameter list uses the same syntax. The type parameters in a generic method precede the return type. 4.7.4   type bounds The type bound is  speciﬁed inside the  angle brackets <>. Suppose we want to write a findMax routine. Consider the code in Figure 4.35. This code cannot work because the compiler cannot prove that the call to compareTo at line 6 is valid; compareTo is guaranteed to exist only if AnyType is Comparable. We can solve this problem by using a type bound. The type bound is speciﬁed inside the angle brackets <>, and it speciﬁes properties that the parameter types must have. A naive attempt is to rewrite the signature as public static <AnyType extends Comparable> ... This is naive because as we know, the Comparable interface is now generic. Although this code would compile, a better attempt would be public static <AnyType extends Comparable<AnyType>> ... However, this attempt is not satisfactory. To see the problem, suppose Shape implements Comparable<Shape>. Suppose Square extends Shape. Then all figure 4.34 Generic static method  to search an array public static <AnyType> boolean contains( AnyType [ ] arr, AnyType x ) {     for( AnyType val : arr )         if( x.equals( val ) )             return true;     return false; } figure 4.35 Generic static method  to find largest element  in an array that does  not work public static <AnyType> AnyType findMax( AnyType [ ] a ) {     int maxIndex = 0;     for( int i = 1; i < a.length; i++ )         if( a[ i ].compareTo( a[ maxIndex ] ) > 0 )             maxIndex = i;     return a[ maxIndex ]; }

chapter 4 inheritance we know is that Square implements Comparable<Shape>. Thus, a Square IS-A Comparable<Shape>, but it IS-NOT-A Comparable<Square>! As a result, what we need to say is that AnyType IS-A Comparable<T> where T is a superclass of AnyType. Since we do not need to know the exact type T, we can use a wildcard. The resulting signature is public static <AnyType extends Comparable<? super AnyType>> Figure 4.36 shows the implementation of findMax. The compiler will accept arrays of types T only such that T implements the Comparable<S> interface, where T IS-A S. Certainly the bounds declaration looks like a mess. Fortunately, we won’t see anything more complicated than this idiom. 4.7.5   type erasure Generic classes are  converted by the  compiler to nongeneric classes by  a process known as  type erasure. Generic types, for the most part, are constructs in the Java language but not in the Virtual Machine. Generic classes are converted by the compiler to nongeneric classes by a process known as type erasure. The simpliﬁed version of what happens is that the compiler generates a raw class with the same name as the generic class with the type parameters removed. The type variables are replaced with their bounds, and when calls are made to generic methods that have an erased return type, casts are inserted automatically. If a generic class is used without a type parameter, the raw class is used. Generics do not  make the code  faster. They do  make the code  more type-safe at  compile time. One important consequence of type erasure is that the generated code is not much different than the code that programmers have been writing before generics and in fact is not any faster. The signiﬁcant beneﬁt is that the programmer does not have to place casts in the code, and the compiler will do signiﬁcant type checking. 4.7.6   restrictions on generics There are numerous restrictions on generic types. Every one of the restrictions listed here is required because of type erasure. figure 4.36 Generic static method  to find largest element  in an array. Illustrates  a bounds on the type  parameter public static <AnyType extends Comparable<? super AnyType>> AnyType findMax( AnyType [ ] a ) {     int maxIndex = 0;     for( int i = 1; i < a.length; i++ )         if( a[ i ].compareTo( a[ maxIndex ] ) > 0 )             maxIndex = i;     return a[ maxIndex ]; }

4.7 implementing generic components using java 5 generics primitive types Primitive types cannot be used for a  type parameter. Primitive types cannot be used for a type parameter. Thus ArrayList<int> is illegal. You must use wrapper classes. instanceof tests instanceof tests  and type casts  work only with the  raw type. instanceof tests and type casts work only with the raw type. Thus, if ArrayList<Integer> list1 = new ArrayList<Integer>( ); list1.add( 4 ); Object list = list1; ArrayList<String> list2 = (ArrayList<String>) list; String s = list2.get( 0 ); was legal. then at runtime the typecast would succeed since all types are ArrayList. Eventually, a runtime error would result at the last line because the call to get would try to return a String but could not. static contexts Static methods and  ﬁelds cannot refer  to the class’s type  variables. Static  ﬁelds are shared  among the  class’s generic  instantiations. In a generic class, static methods and ﬁelds cannot refer to the class’s type variables since after erasure, there are no type variables. Further, since there is really only one raw class, static ﬁelds are shared among the class’s generic instantiations. instantiation of generic types It is illegal to create an instance of a generic type. If T is a type variable, the statement T obj = new T( );         // Right-hand side is illegal It is illegal to  create an instance  of a generic type. is illegal. T is replaced by its bounds, which could be Object (or even an abstract class), so the call to new cannot make sense. generic array objects It is illegal to create an array of a  generic type. It is illegal to create an array of a generic type. If T is a type variable, the statement T [ ] arr = new T[ 10 ];  // Right-hand side is illegal is illegal. T would be replaced by its bounds, which would probably be Object, and then the cast (generated by type erasure) to T[] would fail because Object[] IS-NOT-A T[]. Figure 4.37 shows a generic version of SimpleArrayList previously seen in Figure 4.24. The only tricky part is the code at line 38. Because we cannot create arrays of generic objects, we must create an array of Object and then use a typecast. This typecast will generate a compiler warning about an unchecked type conversion. It is impossible to implement the generic collection classes with arrays without getting this warning. If clients want their code to compile without warnings, they should use only generic collection types, not generic array types.

chapter 4 inheritance figure 4.37 SimpleArrayList class using generics /**  * The GenericSimpleArrayList implements a growable array.  * Insertions are always done at the end.  */ public class GenericSimpleArrayList<AnyType> {     /**      * Returns the number of items in this collection.      * @return the number of items in this collection.      */     public int size( )     {         return theSize;     }     /**      * Returns the item at position idx.      * @param idx the index to search in.      * @throws ArrayIndexOutOfBoundsException if index is bad.      */     public AnyType get( int idx )     {         if( idx < 0 || idx >= size( ) )             throw new ArrayIndexOutOfBoundsException( );         return theItems[ idx ];     }     /**      * Adds an item at the end of this collection.      * @param x any object.      * @return true.      */     public boolean add( AnyType x )     {         if( theItems.length == size( ) )         {             AnyType [ ] old = theItems;               theItems = (AnyType [])new Object[size( )*2 + 1];             for( int i = 0; i < size( ); i++ )                 theItems[ i ] = old[ i ];         }         theItems[ theSize++ ] = x;          return true;     }     private static final int INIT_CAPACITY = 10;     private int theSize;     private AnyType [ ] theItems; }

4.8 the functor (function objects) arrays of parameterized types Instantiation of  arrays of parameterized types is  illegal. Instantiation of arrays of parameterized types is illegal. Consider the following code: ArrayList<String> [ ] arr1 = new ArrayList<String>[ 10 ]; Object [ ] arr2 = arr1; arr2[ 0 ] = new ArrayList<Double>( ); Normally, we would expect that the assignment at line 3, which has the wrong type, would generate an ArrayStoreException. However, after type erasure, the array type is ArrayList[], and the object added to the array is ArrayList, so there is no ArrayStoreException. Thus, this code has no casts, yet it will eventually generate a ClassCastException, which is exactly the situation that generics are supposed to avoid.  4.8 the functor (function objects) In Sections 4.6 and 4.7, we saw how interfaces can be used to write generic algorithms. As an example, the method in Figure 4.36 can be used to ﬁnd the maximum item in an array. However, the findMax method has an important limitation. That is, it works only for objects that implement the Comparable interface and are able to provide compareTo as the basis for all comparison decisions. There are many situations in which this is not feasible. As an example, consider the SimpleRectangle class in Figure 4.38. The SimpleRectangle class does not have a compareTo function, and consequently cannot implement the Comparable interface. The main reason for this is that because there are many plausible alternatives, it is difﬁcult to decide on a good meaning for compareTo. We could base the comparison on area, perimeter, length, width, and so on. Once we write compareTo, we are stuck with it. What if we want to have findMax work with several different comparison alternatives? The solution to the problem is to pass the comparison function as a second parameter to findMax, and have findMax use the comparison function instead of assuming the existence of compareTo. Thus findMax will now have two parameters: an array of Object of an arbitrary type (which need not have compareTo deﬁned), and a comparison function. The main issue left is how to pass the comparison function. Some languages allow parameters to be functions. However, this solution often has efﬁciency problems and is not available in all object-oriented languages. Java does not allow functions to be passed as parameters; we can only pass

chapter 4 inheritance primitive values and references. So we appear not to have a way of passing a function. Functor is another  name for a function object. However, recall that an object consists of data and functions. So we can embed the function in an object and pass a reference to it. Indeed, this idea works in all object-oriented languages. The object is called a function object, and is sometimes also called a functor. The function object  class contains a  method speciﬁed  by the generic  algorithm. An  instance of the  class is passed to  the algorithm. The function object often contains no data. The class simply contains a single method, with a given name, that is speciﬁed by the generic algorithm (in this case, findMax). An instance of the class is then passed to the algorithm, which in turn calls the single method of the function object. We can design different comparison functions by simply declaring new classes. Each new class contains a different implementation of the agreed-upon single method. In Java, to implement this idiom we use inheritance, and speciﬁcally we make use of interfaces. The interface is used to declare the signature of the agreed-upon function. As an example, Figure 4.39 shows the Comparator interface, which is part of the standard java.util package. Recall that to illustrate how the Java library is implemented, we will reimplement a portion of java.util as weiss.util. Before Java 5, this class was not generic. The interface says that any (nonabstract) class that claims to be a Comparator must provide an implementation of the compare method; thus any object that is an instance of such a class has a compare method that it can call. figure 4.38 The SimpleRectangle class, which does not  implement the  Comparable interface // A simple rectangle class. public class SimpleRectangle {     public SimpleRectangle( int len, int wid )       { length = len; width = wid; }     public int getLength( )       { return length; }     public int getWidth( )       { return width; }     public String toString( )       { return "Rectangle " + getLength( ) + " by "                             + getWidth( ); }     private int length;     private int width; }

4.8 the functor (function objects) Using this interface, we can now pass a Comparator as the second parameter to findMax. If this Comparator is cmp, we can safely make the call cmp.compare(o1,o2) to compare any two objects as needed. A wildcard  used in the Comparator parameter to indicate that the Comparator knows how to compare objects that are the same type or supertypes of those in the array. It is up to the caller of findMax to pass an appropriately implemented instance of Comparator as the actual argument. An example is shown in Figure 4.40. findMax now takes two parameters. The second parameter is the function object. As shown on line 11, findMax expects that the function object implements a method named compare, and it must do so, since it implements the Comparator interface. Once findMax is written, it can be called in main. To do so, we need to pass to findMax an array of SimpleRectangle objects and a function object that implements the Comparator interface. We implement OrderRectByWidth, a new class that contains the required compare method. The compare method returns an integer indicating if the ﬁrst rectangle is less than, equal to, or greater than the second rectangle on the basis of widths. main simply passes an instance of OrderRectByWidth to findMax.7 Both main and OrderRectByWidth are shown in 7. The trick of implementing compare by subtracting works for ints as long as both are the same sign. Otherwise, there is a possibility of overﬂow. This simplifying trick is also why we use SimpleRectangle, rather than Rectangle (which stored widths as doubles). figure 4.39 The Comparator interface, originally  defined in java.util and rewritten for the  weiss.util package package weiss.util; /**  * Comparator function object interface.  */ public interface Comparator<AnyType> { /**  * Return the result of comparing lhs and rhs.  * @param lhs first object.  * @param rhs second object.  * @return < 0 if lhs is less than rhs,  *           0 if lhs is equal to rhs,  *         > 0 if lhs is greater than rhs.  */     int compare( AnyType lhs, AnyType rhs ); }

chapter 4 inheritance Figure 4.41. Observe that the OrderRectByWidth object has no data members. This is usually true of function objects. The function object technique is an illustration of a pattern that we see over and over again, not just in Java, but in any language that has objects. In Java, this pattern is used over and over and over again and represents perhaps the single dominant idiomatic use of interfaces. figure 4.40 The generic findMax algorithm, using a  function object public class Utils {     // Generic findMax with a function object. // Precondition: a.length > 0.     public static <AnyType> AnyType     findMax( AnyType [ ] a, Comparator<? super AnyType> cmp )     {         int maxIndex = 0;         for( int i = 1; i < a.length; i++ )             if( cmp.compare( a[ i ], a[ maxIndex ] ) > 0 )                 maxIndex = i;         return a[ maxIndex ];     } } figure 4.41 An example of a  function object class OrderRectByWidth implements Comparator<SimpleRectangle> {     public int compare( SimpleRectangle r1, SimpleRectangle r2 )       { return( r1.getWidth() - r2.getWidth() ); } } public class CompareTest {     public static void main( String [ ] args )     {         SimpleRectangle [ ] rects = new SimpleRectangle[ 4 ];         rects[ 0 ] = new SimpleRectangle( 1, 10 );         rects[ 1 ] = new SimpleRectangle( 20, 1 );         rects[ 2 ] = new SimpleRectangle( 4, 6 );         rects[ 3 ] = new SimpleRectangle( 5, 5 );         System.out.println( "MAX WIDTH: " +               Utils.findMax( rects, new OrderRectByWidth( ) ) );     } }

4.8 the functor (function objects) 4.8.1   nested classes Generally speaking, when we write a class, we expect, or at least hope, for it to be useful in many contexts, not just the particular application that is being worked on. An annoying feature of the function object pattern, especially in Java, is the fact that because it is used so often, it results in the creation of numerous small classes, that each contain one method, that are used perhaps only once in a program, and that have limited applicability outside of the current application. This is annoying for at least two reasons. First, we might have dozens of function object classes. If they are public, by rule they are scattered in separate ﬁles. If they are package visible, they might all be in the same ﬁle, but we still have to scroll up and down to ﬁnd their deﬁnitions, which is likely to be far removed from the one or perhaps two places in the entire program where they are instantiated as function objects. It would be preferable if each function object class could be declared as close as possible to its instantiation. Second, once a name is used, it cannot be reused in the package without possibilities of name collisions. Although packages solve some namespace problems, they do not solve them all, especially when the same class name is used twice in the default package. With a nested class, we can solve some of these problems. A nested class is a class declaration that is placed inside another class declaration—the outer class—using the keyword static. A nested class is considered a member of the outer class. As a result, it can be public, private, package visible, or protected, and depending on the visibility, may or may not be accessible by methods that are not part of the outer class. Typically, it is private and thus inaccessible from outside the outer class. Also, because a nested class is a member of the outer class, its methods can access private static members of the outer class, and can access private instance members when given a reference to an outer object. Figure 4.42 illustrates the use of a nested class in conjunction with the function object pattern. The static in front of the nested class declaration of OrderRectByWidth is essential; without it, we have an inner class, which behaves differently and is discussed later in this text (in Chapter 15). Occasionally, a nested class is public. In Figure 4.42, if OrderRectByWidth was declared public, the class CompareTestInner1.OrderRectByWidth could be used from outside of the CompareTestInner1 class. 4.8.2   local classes In addition to allowing class declarations inside of classes, Java also allows class declarations inside of methods. These classes are called local classes. This is illustrated in Figure 4.43. A nested class is a  class declaration  that is placed inside  another class declaration—the outer  class—using the  keyword static. A nested class is a  part of the outer  class and can be  declared with a visibility speciﬁer. All  outer class members are visible to  the nested class’s  methods.

chapter 4 inheritance figure 4.42 Using a nested class to hide the OrderRectByWidth class declaration import java.util.Comparator; class CompareTestInner1 {     private static class OrderRectByWidth implements Comparator<SimpleRectangle>     {         public int compare( SimpleRectangle r1, SimpleRectangle r2 )           { return r1.getWidth( ) - r2.getWidth( ); }     }     public static void main( String [ ] args )     {         SimpleRectangle [ ] rects = new SimpleRectangle[ 4 ];         rects[ 0 ] = new SimpleRectangle( 1, 10 );         rects[ 1 ] = new SimpleRectangle( 20, 1 );         rects[ 2 ] = new SimpleRectangle( 4, 6 );         rects[ 3 ] = new SimpleRectangle( 5, 5 );         System.out.println( "MAX WIDTH: " +               Utils.findMax( rects, new OrderRectByWidth( ) ) );     } } figure 4.43 Using a local class to hide the OrderRectByWidth class declaration further class CompareTestInner2 {     public static void main( String [ ] args )     {         SimpleRectangle [ ] rects = new SimpleRectangle[ 4 ];         rects[ 0 ] = new SimpleRectangle( 1, 10 );         rects[ 1 ] = new SimpleRectangle( 20, 1 );         rects[ 2 ] = new SimpleRectangle( 4, 6 );         rects[ 3 ] = new SimpleRectangle( 5, 5 );         class OrderRectByWidth implements Comparator<SimpleRectangle>         {             public int compare( SimpleRectangle r1, SimpleRectangle r2 )               { return r1.getWidth( ) - r2.getWidth( ); }         }         System.out.println( "MAX WIDTH: " +              Utils.findMax( rects, new OrderRectByWidth( ) ) );     } }

4.8 the functor (function objects) An anonymous class is a class that  has no name. Java also allows  class declarations  inside of methods.  Such classes are  known as local classes and may  not be declared  with either a visibility modiﬁer or the  static modiﬁer. Note that when a class is declared inside a method, it cannot be declared private or static. However, the class is visible only inside of the method in which it was declared. This makes it easier to write the class right before its ﬁrst (perhaps only) use and avoid pollution of namespaces. An advantage of declaring a class inside of a method is that the class’s methods (in this case, compare) has access to local variables of the function that are declared prior to the class. This can be important in some applications. There is a technical rule: In order to access local variables, the variables must be declared final. We will not be using these types of classes in the text. 4.8.3   anonymous classes One might suspect that by placing a class immediately before the line of code in which it is used, we have declared the class as close as possible to its use. However, in Java, we can do even better. Figure 4.44 illustrates the anonymous inner class. An anonymous class is a class that has no name. The syntax is that instead of writing new Inner(), and providing the implementation of Inner as a named class, we write new Interface(), and then provide the implementation of the interface (everything from the opening to closing brace) immediately after the new expression. figure 4.44 Using an anonymous class to implement the function object class CompareTestInner3 {     public static void main( String [ ] args )     {         SimpleRectangle [ ] rects = new SimpleRectangle[ 4 ];         rects[ 0 ] = new SimpleRectangle( 1, 10 );         rects[ 1 ] = new SimpleRectangle( 20, 1 );         rects[ 2 ] = new SimpleRectangle( 4, 6 );         rects[ 3 ] = new SimpleRectangle( 5, 5 );         System.out.println( "MAX WIDTH: " +             Utils.findMax( rects, new Comparator<SimpleRectangle>( )             {                 public int compare( SimpleRectangle r1, SimpleRectangle r2 )                   { return r1.getWidth( ) - r2.getWidth( ); }             }         ) );     } }

chapter 4 inheritance Anonymous classes  introduce signiﬁcant language  complications. Anonymous classes  are often used to  implement function  objects. Instead of implementing an interface anonymously, it is also possible to extend a class anonymously, providing only the overridden methods. The syntax looks very daunting, but after a while, one gets used to it. It complicates the language signiﬁcantly, because the anonymous class is a class. As an example of the complications that are introduced, since the name of a constructor is the name of a class, how does one deﬁne a constructor for an anonymous class? The answer is that you cannot do so. The anonymous class is in practice very useful, and its use is often seen as part of the function object pattern in conjunction with event handling in user interfaces. In event handling, the programmer is required to specify, in a function, what happens when certain events occur. 4.8.4   nested classes and generics When a nested class is declared inside a generic class, the nested class cannot refer to the parameter types of the generic outer class. However, the nested class can itself be generic and can reuse the parameter type names of the generic outer class. Examples of syntax include the following: class Outer<AnyType> {     public static class Inner<AnyType>     {     }     public static class OtherInner     {         // cannot use AnyType here     } } Outer.Inner<String> i1 = new Outer.Inner<String>( ); Outer.OtherInner    i2 = new Outer.OtherInner( ); Notice that in the declarations of i1 and i2, Outer has no parameter types. 4.9 dynamic dispatch details Dynamic dispatch  is not important for  static, ﬁnal, or private methods. A common myth is that all methods and all parameters are bound at run time. This is not true. First, there are some cases in which dynamic dispatch is never used or is not an issue: n Static methods, regardless of how the method is invoked n Final methods

4.9 dynamic dispatch details Static overloading means that the  parameters to a  method are always  deduced statically,  at compile time. n Private methods (since they are invoked only from inside the class  and are thus implicitly ﬁnal) In other scenarios, dynamic dispatch is meaningfully used. But what exactly does dynamic dispatch mean? In Java, the parameters to a method  are always deduced  statically, at compile  time. Dynamic dispatch means that the method that is appropriate for the object being operated on is the one that is used. However, it does not mean that the absolute best match is performed for all parameters. Speciﬁcally, in Java, the parameters to a method are always deduced statically, at compile time. For a concrete example, consider the code in Figure 4.45. In the whichFoo method, a call is made to foo. But which foo is called? We expect the answer to depend on the run-time types of arg1 and arg2. Because parameters are always matched at compile time, it does not matter what type arg2 is actually referencing. The foo that is matched will be     public void foo( Base x ) The only issue is whether the Base or Derived version is used. That is the decision that is made at run time, when the object that arg1 references is known. Dynamic dispatch means that once  the signature of an  instance method is  ascertained, the  class of the method  can be determined  at run time based  on the dynamic  type of the invoking  object. The precise methodology used is that the compiler deduces, at compile time, the best signature, based on the static types of the parameters and the methods that are available for the static type of the controlling reference. At that point, the signature of the method is set. This step is called static overloading. The only remaining issue is which class’s version of that method is used. This is done by having the Virtual Machine deduce the runtime type of this object. Once the runtime type is known, the Virtual Machine walks up the inheritance hierarchy, looking for the last overridden version of the method; this is the ﬁrst method of the appropriate signature that the Virtual Machine ﬁnds as it walks up toward Object.8 This second step is called dynamic dispatch. Static overloading can lead to subtle errors when a method that is supposed to be overridden is instead overloaded. Figure 4.46 illustrates a common programming error that occurs when implementing the equals method. The equals method is deﬁned in class Object and is intended to return true if two objects have identical states. It takes an Object as a parameter, and the Object provides a default implementation that returns true only if the two objects are the same. In other words, in class Object, the implementation of equals is roughly     public boolean equals( Object other )       { return this == other; } 8. If no such method is found, perhaps because only part of the program was recompiled, then the Virtual Machine throws a NoSuchMethodException.

chapter 4 inheritance When overridding equals, the parameter must be of type Object; otherwise, overloading is being done. In Figure 4.46, equals is not overridden; instead it is (unintentionally) overloaded. As a result, the call to sameVal will return false, which appears surprising, since the call to equals returns true and sameVal calls equals. figure 4.45 An illustration of static  binding for  parameters class Base {     public void foo( Base x )       { System.out.println( "Base.Base" ); }     public void foo( Derived x )       { System.out.println( "Base.Derived" ); } } class Derived extends Base {     public void foo( Base x )       { System.out.println( "Derived.Base" ); }     public void foo( Derived x )       { System.out.println( "Derived.Derived" ); } } class StaticParamsDemo {     public static void whichFoo( Base arg1, Base arg2 )     {         // It is guaranteed that we will call foo( Base )         // Only issue is which class's version of foo( Base )         // is called; the dynamic type of arg1 is used         // to decide.         arg1.foo( arg2 );     }     public static void main( String [] args )     {         Base b = new Base( );         Derived d = new Derived( );         whichFoo( b, b );         whichFoo( b, d );         whichFoo( d, b );         whichFoo( d, d );     } }

4.9 dynamic dispatch details The problem is that the call in sameVal is this.equals(other). The static type of this is SomeClass. In SomeClass, there are two versions of equals: the listed equals that takes a SomeClass as a parameter, and the inherited equals that takes an Object. The static type of the parameter (other) is Object, so the best match is the equals that takes an Object. At run time, the virtual machine searches for that equals, and ﬁnds the one in class Object. And since this and other are different objects, the equals method in class Object returns false. Thus, equals must be written to take an Object as a parameter, and typically a downcast will be required after a veriﬁcation that the type is appropriate. One way of doing that is to use an instanceof test, but that is safe only for ﬁnal classes. Overriding equals is actually fairly tricky in the presence of inheritance, and is discussed in Section 6.7. figure 4.46 An illustration of  overloading equals instead of overriding  equals. Here, the call  to the sameVal returns false! final class SomeClass {     public SomeClass( int i )       { id = i; }     public boolean sameVal( Object other )       { return other instanceof SomeClass && equals( other ); }     /**      * This is a bad implementation!      * other has the wrong type, so this does      * not override Object's equals.      */     public boolean equals( SomeClass other )       { return other != null && id == other.id; }     private int id; } class BadEqualsDemo {     public static void main( String [ ] args )     {         SomeClass obj1 = new SomeClass( 4 );         SomeClass obj2 = new SomeClass( 4 );         System.out.println( obj1.equals( obj2 ) );   // true         System.out.println( obj1.sameVal( obj2 ) );  // false     } }

chapt er algorithm analysis I n Part One we examined how object-oriented  programming can help in the design and implementation of large systems. We did not examine performance issues. Generally, we use a computer because we need to process a large amount of data. When we run a program on large amounts of input, we must be certain that the program terminates within a reasonable amount of time. Although the amount of running time is somewhat dependent on the programming language we use, and to a smaller extent the methodology we use (such as procedural versus object-oriented), often those factors are unchangeable constants of the design. Even so, the running time is most strongly correlated with the choice of algorithms. An algorithm is a clearly speciﬁed set of instructions the computer will follow to solve a problem. Once an algorithm is given for a problem and determined to be correct, the next step is to determine the amount of resources, such as time and space, that the algorithm will require. This step is called algorithm analysis. An algorithm that requires several hundred gigabytes of main memory is not useful for most current machines, even if it is completely correct.

chapter 5 algorithm analysis In this chapter, we show the following: n How to estimate the time required for an algorithm n How to use techniques that drastically reduce the running time of an  algorithm n How to use a mathematical framework that more rigorously describes  the running time of an algorithm n How to write a simple binary search routine 5.1 what is algorithm analysis? More data means  that the program  takes more time. The amount of time that any algorithm takes to run almost always depends on the amount of input that it must process. We expect, for instance, that sorting 10,000 elements requires more time than sorting 10 elements. The running time of an algorithm is thus a function of the input size. The exact value of the function depends on many factors, such as the speed of the host machine, the quality of the compiler, and in some cases, the quality of the program. For a given program on a given computer, we can plot the running time function on a graph. Figure 5.1 illustrates such a plot for four programs. The curves represent four common functions encountered in algorithm analysis: linear, , quadratic, and cubic. The input size N ranges from 1 to 100 figure 5.1 Running times for  small inputs Linear O(N log N) Quadratic Cubic Running Time (microseconds) Input Size (N ) O( N log N )

5.1 what is algorithm analysis? items, and the running times range from 0 to 10 microseconds. A quick glance at Figure 5.1 and its companion, Figure 5.2, suggests that the linear, O(N log N), quadratic, and cubic curves represent running times in order of decreasing preference. Of the common  functions encountered in algorithm  analysis, linear represents the most  efﬁcient algorithm. An example is the problem of downloading a ﬁle over the Internet. Suppose there is an initial 2-sec delay (to set up a connection), after which the download proceeds at 160 K/sec. Then if the ﬁle is N kilobytes, the time to download is described by the formula  . This is a linear function. Downloading an 8,000K ﬁle takes approximately 52 sec, whereas downloading a ﬁle twice as large (16,000K) takes about 102 sec, or roughly twice as long. This property, in which time essentially is directly proportional to amount of input, is the signature of a linear algorithm, which is the most efﬁcient algorithm. In contrast, as these ﬁrst two graphs show, some of the nonlinear algorithms lead to large running times. For instance, the linear algorithm is much more efﬁcient than the cubic algorithm. In this chapter we address several important questions: n Is it always important to be on the most efficient curve? n How much better is one curve than another? n How do you decide which curve a particular algorithm lies on? n How do you design algorithms that avoid being on less-efﬁcient  curves? figure 5.2 Running times for  moderate inputs 0.8 0.6 0.4 0.2 Linear O(N log N) Quadratic Cubic Running Time (milliseconds) Input Size (N ) T N ( ) N 160 ⁄ + =

chapter 5 algorithm analysis A cubic function is a function whose dominant term is some constant times  . As an example,   is a cubic function. Similarly, a quadratic function has a dominant term that is some constant times , and a linear function has a dominant term that is some constant times N. The expression represents a function whose dominant term is N times the logarithm of N. The logarithm is a slowly growing function; for instance, the logarithm of 1,000,000 (with the typical base 2) is only 20. The logarithm grows more slowly than a square or cube (or any) root. We discuss the logarithm in more depth in Section 5.5.  The growth rate of  a function is most  important when N is sufﬁciently large. Either of two functions may be smaller than the other at any given point, so claiming, for instance, that   does not make sense. Instead, we measure the functions’ rates of growth. This is justiﬁed for three reasons. First, for cubic functions such as the one shown in Figure 5.2, when N is 1,000 the value of the cubic function is almost entirely determined by the cubic term. In the function , for N = 1,000, the value of the function is 10,001,040,080, of which 10,000,000,000 is due to the   term. If we were to use only the cubic term to estimate the entire function, an error of about 0.01 percent would result. For sufﬁciently large N, the value of a function is largely determined by its dominant term (the meaning of the term sufﬁciently large varies by function). The second reason we measure the functions’ growth rates is that the exact value of the leading constant of the dominant term is not meaningful across different machines (although the relative values of the leading constant for identically growing functions might be). For instance, the quality of the compiler could have a large inﬂuence on the leading constant. The third reason is that small values of N generally are not important. For N = 20, Figure 5.1 shows that all algorithms terminate within 5 μs. The difference between the best and worst algorithm is less than a blink of the eye. Big-Oh notation is  used to capture the  most dominant  term in a function. We use Big-Oh notation to capture the most dominant term in a function and to represent the growth rate. For instance, the running time of a quadratic algorithm is speciﬁed as   (pronounced “order en-squared”). Big-Oh notation also allows us to establish a relative order among functions by comparing dominant terms. We discuss Big-Oh notation more formally in Section 5.4. For small values of N (for instance, those less than 40), Figure 5.1 shows that one curve may be initially better than another, which doesn’t hold for larger values of N. For example, initially the quadratic curve is better than the curve, but as N gets sufﬁciently large, the quadratic algorithm loses its advantage. For small amounts of input, making comparisons between functions is difﬁcult because leading constants become very signiﬁcant. The function N + 2,500 is larger than   when N is less than 50. Eventually, the linear function is always less than the quadratic function. Most important, for small input sizes the running times are generally inconsequential, so we need not worry about them. For instance, Figure 5.1 shows that when N is less than 25, all four N 3 10N 3 N 2 40N + + + N 2 O( N log N ) F N (  ) G N (  ) < 10N 3 N 2 40N + + + 10N 3 O N 2 (  ) O( N log N ) N 2

5.1 what is algorithm analysis? algorithms run in less than 10 μs. Consequently, when input sizes are very small, a good rule of thumb is to use the simplest algorithm. Figure 5.2 clearly demonstrates the differences between the various curves for large input sizes. A linear algorithm solves a problem of size 10,000 in a small fraction of a second. The   algorithm uses roughly 10 times as much time. Note that the actual time differences depend on the constants involved and thus might be more or less. Depending on these constants, an   algorithm might be faster than a linear algorithm for fairly large input sizes. For equally complex algorithms, however, linear algorithms tend to win out over   algorithms. Quadratic algorithms are impractical for input sizes  exceeding a few  thousand. This relationship is not true, however, for the quadratic and cubic algorithms. Quadratic algorithms are almost always impractical when the input size is more than a few thousand, and cubic algorithms are impractical for input sizes as small as a few hundred. For instance, it is impractical to use a naive sorting algorithm for 1,000,000 items, because most simple sorting algorithms (such as bubble sort and selection sort) are quadratic algorithms. The sorting algorithms discussed in Chapter 8 run in subquadratic time—that is, better than O(N2)—thus making sorting large arrays practical. Cubic algorithms  are impractical for  input sizes as small  as a few hundred. The most striking feature of these curves is that the quadratic and cubic algorithms are not competitive with the others for reasonably large inputs. We can code the quadratic algorithm in highly efﬁcient machine language and do a poor job coding the linear algorithm, and the quadratic algorithm will still lose badly. Even the most clever programming tricks cannot make an inefﬁcient algorithm fast. Thus, before we waste effort attempting to optimize code, we need to optimize the algorithm. Figure 5.3 arranges functions that commonly describe algorithm running times in order of increasing growth rate. O( N log N ) O( N log N ) O( N log N ) figure 5.3 Functions in order of  increasing growth rate Function Name Constant Logarithmic Log-squared Linear N log N Quadratic Cubic Exponential c log N N log2 N N log N N 2 N 3 2N

chapter 5 algorithm analysis 5.2 examples of algorithm  running times In this section we examine three problems. We also sketch possible solutions and determine what kind of running times the algorithms will exhibit, without providing detailed programs. The goal in this section is to provide you with some intuition about algorithm analysis. In Section 5.3 we provide more details on the process, and in Section 5.4 we formally approach an algorithm analysis problem. We look at the following problems in this section: minimum element in an array Given an array of N items, ﬁnd the smallest item. closest points in the plane Given N points in a plane (that is, an x-y coordinate system), ﬁnd the pair of points that are closest together. colinear points in the plane Given N points in a plane (that is, an x-y coordinate system), determine if any three form a straight line. The minimum element problem is fundamental in computer science. It can be solved as follows: 1. Maintain a variable min that stores the minimum element. 2. Initialize min to the ﬁrst element. 3. Make a sequential scan through the array and update min as appropriate. The running time of this algorithm will be  , or linear, because we will repeat a ﬁxed amount of work for each element in the array. A linear algorithm is as good as we can hope for. This is because we have to examine every element in the array, a process that requires linear time. The closest points problem is a fundamental problem in graphics that can be solved as follows: 1. Calculate the distance between each pair of points. 2. Retain the minimum distance. This calculation is expensive, however, because there are   pairs of points.1 Thus there are roughly   pairs of points. Examining all these 1. Each of N points can be paired with   points for a total of   pairs. However, this pairing double-counts pairs A, B and B, A, so we must divide by 2. O( N ) N N – ( ) 2 ⁄ N 2 N – N N – ( )

5.3 the maximum contiguous subsequence sum problem pairs and ﬁnding the minimum distance among them takes quadratic time. A better algorithm runs in   time and works by avoiding the computation of all distances. There is also an algorithm that is expected to take time. These last two algorithms use subtle observations to provide faster results and are beyond the scope of this text. The colinear points problem is important for many graphics algorithms. The reason is that the existence of colinear points introduces a degenerate case that requires special handling. It can be directly solved by enumerating all groups of three points. This solution is even more computationally expensive than that for the closest points problem because the number of different groups of three points is   (using reasoning similar to that used for the closest points problem). This result tells us that the direct approach will yield a cubic algorithm. There is also a more clever strategy (also beyond the scope of this text) that solves the problem in quadratic time (and further improvement is an area of continuously active research). In Section 5.3 we look at a problem that illustrates the differences among linear, quadratic, and cubic algorithms. We also show how the performance of these algorithms compares to a mathematical prediction. Finally, after discussing the basic ideas, we examine Big-Oh notation more formally. 5.3 the maximum contiguous  subsequence sum problem In this section, we consider the following problem: maximum contiguous subsequence sum problem Given (possibly negative) integers  , ﬁnd (and identify the sequence corresponding to) the maximum value of   . The maximum contiguous subsequence sum is zero if all the integers are negative. Programming details are considered after the algorithm design. As an example, if the input is {–2, 11, –4, 13, –5, 2}, then the answer is 20, which represents the contiguous subsequence encompassing items 2 through 4 (shown in boldface type). As a second example, for the input { 1, –3, 4, –2, –1, 6 }, the answer is 7 for the subsequence encompassing the last four items. In Java, arrays begin at zero, so a Java program would represent the input as a sequence   to  . This is a programming detail and not part of the algorithm design. Always consider  emptiness. Before the discussion of the algorithms for this problem, we need to comment on the degenerate case in which all input integers are negative. The problem statement gives a maximum contiguous subsequence sum of 0 for this case. One might wonder why we do this, rather than just returning the largest (that is, O( N log N ) O( N ) N N – ( ) N – ( ) 6 ⁄ A1 A2 … AN , , , Ak k i = j ∑ A0 AN –

chapter 5 algorithm analysis the smallest in magnitude) negative integer in the input. The reason is that the empty subsequence, consisting of zero integers, is also a subsequence, and its sum is clearly 0. Because the empty subsequence is contiguous, there is always a contiguous subsequence whose sum is 0. This result is analogous to the empty set being a subset of any set. Be aware that emptiness is always a possibility and that in many instances it is not a special case at all. There are lots of  drastically different  algorithms (in terms  of efﬁciency) that  can be used to  solve the maximum  contiguous subsequence sum  problem. The maximum contiguous subsequence sum problem is interesting mainly because there are so many algorithms to solve it—and the performance of these algorithms varies drastically. In this section, we discuss three such algorithms. The ﬁrst is an obvious exhaustive search algorithm, but it is very inefﬁcient. The second is an improvement on the ﬁrst, which is accomplished by a simple observation. The third is a very efﬁcient, but not obvious, algorithm. We prove that its running time is linear. In Chapter 7 we present a fourth algorithm, which has   running time. That algorithm is not as efﬁcient as the linear algorithm, but it is much more efﬁcient than the other two. It is also typical of the kinds of algorithms that result in   running times. The graphs shown in Figures 5.1 and 5.2 are representative of these four algorithms. 5.3.1   the obvious O(N3) algorithm A brute force algorithm is generally  the least efﬁcient  but simplest  method to code. The simplest algorithm is a direct exhaustive search, or a brute force algorithm, as shown in Figure 5.4. Lines 9 and 10 control a pair of loops that iterate over all possible subsequences. For each possible subsequence, the value of its sum is computed at lines 12 to 15. If that sum is the best sum encountered, we update the value of maxSum, which is eventually returned at line 25. Two ints—seqStart and seqEnd (which are static class ﬁelds)—are also updated whenever a new best sequence is encountered. The direct exhaustive search algorithm has the merit of extreme simplicity; the less complex an algorithm is, the more likely it is to be programmed correctly. However, exhaustive search algorithms are usually not as efﬁcient as possible. In the remainder of this section we show that the running time of the algorithm is cubic. We count the number of times (as a function of the input size) the expressions in Figure 5.4 are evaluated. We require only a BigOh result, so once we have found a dominant term, we can ignore lower order terms and leading constants. The running time of the algorithm is entirely dominated by the innermost for loop in lines 14 and 15. Four expressions there are repeatedly executed: 1. The initialization k = i 2. The test k <= j 3. The increment thisSum += a[ k ] 4. The adjustment k++ O( N log N ) O( N log N )

5.3 the maximum contiguous subsequence sum problem A mathematical  analysis is used to  count the number  of times that certain statements are  executed. The number of times expression 3 is executed makes it the dominant term among the four expressions. Note that each initialization is accompanied by at least one test. We are ignoring constants, so we may disregard the cost of the initializations; the initializations cannot be the single dominating cost of the algorithm. Because the test given by expression 2 is unsuccessful exactly once per loop, the number of unsuccessful tests performed by expression 2 is exactly equal to the number of initializations. Consequently, it is not dominant. The number of successful tests at expression 2, the number of increments performed by expression 3, and the number of adjustments at expression 4 are all identical. Thus the number of increments (i.e., the number of times that line 15 is executed) is a dominant measure of the work performed in the innermost loop. The number of times line 15 is executed is exactly equal to the number of ordered triplets (i, j, k) that satisfy  2 The reason is that the index i runs over the entire array, j runs from i to the end of the array, and k runs from i to j. A quick and dirty estimate is that the number of triplets is somewhat less than , or  , because i, j, and k can each assume one of N values. The     /**      * Cubic maximum contiguous subsequence sum algorithm.      * seqStart and seqEnd represent the actual best sequence.      */     public static int maxSubsequenceSum( int [ ] a )     {         int maxSum = 0;         for( int i = 0; i < a.length; i++ )             for( int j = i; j < a.length; j++ )             {                 int thisSum = 0;                 for( int k = i; k <= j; k++ )                     thisSum += a[ k ];                 if( thisSum > maxSum )                 {                     maxSum = thisSum;                     seqStart = i;                     seqEnd   = j;                 }             }         return maxSum;     } figure 5.4 A cubic maximum  contiguous subsequence sum  algorithm 2. In Java, the indices run from 0 to  . We have used the algorithmic equivalent 1 to N to simplify the analysis. i k j N. ≤≤ ≤ ≤ N – N N N × × N 3

chapter 5 algorithm analysis additional restriction   reduces this number. A precise calculation is somewhat difﬁcult to obtain and is performed in Theorem 5.1. The most important part of Theorem 5.1 is not the proof, but rather the result. There are two ways to evaluate the number of triplets. One is to evaluate the sum  . We could evaluate this sum inside out (see Exercise 5.12). Instead, we will use an alternative. The result of Theorem 5.1 is that the innermost for loop accounts for cubic running time. The remaining work in the algorithm is inconsequential because it is done, at most, once per iteration of the inner loop. Put another way, the cost of lines 17 to 22 is inconsequential because that part of the code is done only as often as the initialization of the inner for loop, rather than as often as the repeated body of the inner for loop. Consequently, the algorithm is  . We do not need  precise calculations for a Big-Oh  estimate. In many  cases, we can use  the simple rule of  multiplying the size  of all the nested  loops. Note carefully that consecutive loops do not  multiply. The previous combinatorial argument allows us to obtain precise calculations on the number of iterations in the inner loop. For a Big-Oh calculation, this is not really necessary; we need to know only that the leading term is some constant times  . Looking at the algorithm, we see a loop that is potentially of size N inside a loop that is potentially of size N inside another loop that is potentially of size N. This conﬁguration tells us that the triple loop has the potential for   iterations. This potential is only about six times higher than what our precise calculation of what actually occurs. Constants are ignored anyway, so we can adopt the general rule that, when we have nested loops, we should multiply the cost of the innermost statement by the size of each loop in the nest to obtain an upper bound. In most cases, the i k j ≤ ≤ k i = j ∑ j i = N ∑ i = N ∑ Theorem 5.1 The number of integer-ordered triplets (i, j, k) that satisfy  is . Proof Place the following N + 2 balls in a box: N balls numbered 1 to N, one unnumbered  red ball, and one unnumbered blue ball. Remove three balls from the box. If a red ball  is drawn, number it as the lowest of the numbered balls drawn. If a blue ball is drawn,  number it as the highest of the numbered balls drawn. Notice that if we draw both a  red and blue ball, then the effect is to have three balls identically numbered. Order  the three balls. Each such order corresponds to a triplet solution to the equation in  Theorem 5.1. The number of possible orders is the number of distinct ways to draw  three balls without replacement from a collection of N + 2 balls. This is similar to the  problem of selecting three points from a group of N that we evaluated in Section 5.2,  so we immediately obtain the stated result. i k j N ≤≤ ≤ ≤ N N + ( ) N + ( ) 6 ⁄ O N 3 ( ) N 3 N N N × ×

5.3 the maximum contiguous subsequence sum problem If we remove  another loop, we  have a linear  algorithm. The algorithm is  tricky. It uses a  clever observation  to step quickly over  large numbers of  subsequences that  cannot be the best. upper bound will not be a gross overestimate.3 Thus a program with three nested loops, each running sequentially through large portions of an array, is likely to exhibit   behavior. Note that three consecutive (nonnested) loops exhibit linear behavior; it is nesting that leads to a combinatoric explosion. Consequently, to improve the algorithm, we need to remove a loop. 5.3.2   an improved O(N 2) algorithm When we remove a  nested loop from  an algorithm, we  generally lower the  running time. When we can remove a nested loop from the algorithm, we generally lower the running time. How do we remove a loop? Obviously, we cannot always do so. However, the previous algorithm has many unnecessary computations. The inefﬁciency that the improved algorithm corrects is the unduly expensive computation in the inner for loop in Figure 5.4. The improved algorithm makes use of the fact that  . In other words, suppose we have just calculated the sum for the subsequence i, ...,  . Then computing the sum for the subsequence i, ..., j should not take long because we need only one more addition. However, the cubic algorithm throws away this information. If we use this observation, we obtain the improved algorithm shown in Figure 5.5. We have two rather than three nested loops, and the running time is O(N 2). 5.3.3   a linear algorithm To move from a quadratic algorithm to a linear algorithm, we need to remove yet another loop. However, unlike the reduction illustrated in Figures 5.4 and 5.5, where loop removal was simple, getting rid of another loop is not so easy. The problem is that the quadratic algorithm is still an exhaustive search; that is, we are trying all possible subsequences. The only difference between the quadratic and cubic algorithms is that the cost of testing each successive subsequence is a constant   instead of linear  . Because a quadratic number of subsequences are possible, the only way we can attain a subquadratic bound is to ﬁnd a clever way to eliminate from consideration a large number of subsequences, without actually computing their sum and testing to see if that sum is a new maximum. This section shows how this is done. First, we eliminate a large number of possible subsequences from consideration. Clearly, the best subsequence can never start with a negative number, so if a[i] is negative we can skip the inner loop and advance i. More generally, the best subsequence can never start with a negative subsubsequence.  So, let   be the subsequence encompassing elements from i to j, and let  be its sum. 3. Exercise 5.21 illustrates a case in which the multiplication of loop sizes yields an overestimate in the Big-Oh result. O N 3 ( ) Ak k i = j ∑ A j Ak k i = j – ∑ + = j – O 1 (  ) O( N ) Ai j , Si j ,

chapter 5 algorithm analysis An illustration of the sums generated by i, j, and q is shown on the ﬁrst two lines in Figure 5.6. Theorem 5.2 demonstrates that we can avoid examining several subsequences by including an additional test: If thisSum is less than 0, we can break from the inner loop in Figure 5.5. Intuitively, if a subsequence’s sum is negative, then it cannot be part of the maximum contiguous subsequence. The reason is that we can get a larger contiguous subsequence figure 5.5 A quadratic maximum  contiguous subsequence sum  algorithm /**  * Quadratic maximum contiguous subsequence sum algorithm.  * seqStart and seqEnd represent the actual best sequence.  */     public static int maxSubsequenceSum( int [ ] a )     {         int maxSum = 0;         for( int i = 0; i < a.length; i++ )         {             int thisSum = 0;             for( int j = i; j < a.length; j++ )             {                 thisSum += a[ j ];                 if( thisSum > maxSum )                 {                     maxSum = thisSum;                     seqStart = i;                     seqEnd   = j;                 }             }         }         return maxSum;     } Theorem 5.2 Let   be any sequence with  . If  , then   is not the maximum contiguous subsequence. Proof The sum of A’s elements from i to q is the sum of A’s elements from i to j added to  the sum of A’s elements from   to q. Thus we have  . Because  , we know that  . Thus   is not a maximum contiguous subsequence. Ai j , Si j , < q j > Ai q , j + Si q , Si j , S j 1 q , + + = Si j , < Si q , S j + q , < Ai q ,

5.3 the maximum contiguous subsequence sum problem by not including it. This observation by itself is not sufﬁcient to reduce the running time below quadratic. A similar observation also holds: All contiguous subsequences that border the maximum contiguous subsequence must have negative (or 0) sums (otherwise, we would include them). This observation also does not reduce the running time to below quadratic. However, a third observation, illustrated in Figure 5.7, does, and we formalize it with Theorem 5.3. figure 5.6 The subsequences  used in Theorem 5.2 <Sj + 1, q Sj + 1, q < 0 i                        j  j + 1                       q figure 5.7 The subsequences  used in Theorem 5.3.  The sequence from p to q has a sum that is,  at most, that of the  subsequence from i to q. On the left-hand  side, the sequence  from i to q is itself not  the maximum (by  Theorem 5.2). On the  right-hand side, the  sequence from i to q has already been  seen. <=Si, q Si, q >=0 i                        j  j + 1                       q <=Si, q Si, q >=0 i                                                       q     j p – 1  p p – 1  p For any i, let   be the ﬁrst sequence, with  . Then, for any   and  ,   either is not a maximum contiguous subsequence or is equal to an  already seen maximum contiguous subsequence. Theorem 5.3 If  , then Theorem 5.2 applies. Otherwise, as in Theorem 5.2, we have  . Since j is the lowest index for which  , it follows that  . Thus  . If   (shown on the left-hand side in Figure 5.7), then  Theorem 5.2 implies that   is not a maximum contiguous subsequence, so neither  is  . Otherwise, as shown on the right-hand side in Figure 5.7, the subsequence   has a sum equal to, at most, that of the already seen subsequence  . Proof Ai j , Si j , < i p j ≤ ≤ p q ≤ Ap q , p i = Si q , Si p – , Sp q , + = Si j , < Si p – , ≥ Sp q , Si q , ≤ q j > Ai q , Ap q , Ap q , Ai q ,

chapter 5 algorithm analysis Theorem 5.3 tells us that, when a negative subsequence is detected, not only can we break the inner loop, but also we can advance i to j+1. Figure 5.8 shows that we can rewrite the algorithm using only a single loop. Clearly, the running time of this algorithm is linear: At each step in the loop, we advance j, so the loop iterates at most N times. The correctness of this algorithm is much less obvious than for the previous algorithms, which is typical. That is, algorithms that use the structure of a problem to beat an exhaustive search generally require some sort of correctness proof. We proved that the algorithm (although not the resulting Java program) is correct using a short mathematical argument. The purpose is not to make the discussion entirely mathematical, but rather to give a ﬂavor of the techniques that might be required in advanced work. figure 5.8 A linear maximum  contiguous subsequence sum  algorithm /**  * Linear maximum contiguous subsequence sum algorithm.  * seqStart and seqEnd represent the actual best sequence.  */     public static int maximumSubsequenceSum( int [ ] a )     {         int maxSum = 0;         int thisSum = 0;         for( int i = 0, j = 0; j < a.length; j++ )         {             thisSum += a[ j ];             if( thisSum > maxSum )             {                 maxSum = thisSum;                 seqStart = i;                 seqEnd   = j;             }             else if( thisSum < 0 )             {                 i = j + 1;                 thisSum = 0;             }         }         return maxSum;     } If we detect a negative sum, we can  move i all the way  past j. If an algorithm is  complex, a correctness proof is  required.

5.4 general big-oh rules 5.4 general big-oh rules Now that we have the basic ideas of algorithm analysis, we can adopt a slightly more formal approach. In this section, we outline the general rules for using Big-Oh notation. Although we use Big-Oh notation almost exclusively throughout this text, we also deﬁne three other types of algorithm notation that are related to Big-Oh and used occasionally later on in the text. deﬁnition: (Big-Oh)   is   if there are positive constants c and such that   when  . deﬁnition: (Big-Omega)   is   if there are positive constants c and   such that   when  . deﬁnition: (Big-Theta)   is   if and only if   is   and  is  . deﬁnition: (Little-Oh)   is   if and only if   is   and  is not  .4 The ﬁrst deﬁnition, Big-Oh notation, states that there is a point   such that for all values of N that are past this point,   is bounded by some multiple of  . This is the sufﬁciently large N mentioned earlier. Thus, if the running time   of an algorithm is  , then, ignoring constants, we are guaranteeing that at some point we can bound the running time by a quadratic function. Notice that if the true running time is linear, then the statement that the running time is   is technically correct because the inequality holds. However,   would be the more precise claim. If we use the traditional inequality operators to compare growth rates, then the ﬁrst deﬁnition says that the growth rate of   is less than or equal to that of  . The second deﬁnition,  , called Big-Omega, says that the growth rate of   is greater than or equal to that of  . For instance, we might say that any algorithm that works by examining every possible subsequence in the maximum subsequence sum problem must take   time because a quadratic number of subsequences are possible. This is a lower-bound argument that is used in more advanced analysis. Later in the text, we will see one example of this argument and demonstrate that any general-purpose sorting algorithm requires   time. 4. Our deﬁnition for Little-Oh is not precisely correct for some unsual functions, but is the simplest to express the basic concepts used throughout this text. T N (  ) O F N (  ) (  ) N0 T N (  ) cF N (  ) ≤ N N0 ≥ T N (  ) Ω F N (  ) (  ) N0 T N (  ) cF N (  ) ≥ N N0 ≥ T N (  ) Θ F N (  ) (  ) T N (  ) O F N (  ) (  ) T N (  ) Ω F N (  ) (  ) T N (  ) o F N (  ) (  ) T N (  ) O F N (  ) (  ) T N (  ) Θ F N (  ) (  ) N 0 T N (  ) F N (  ) T N (  ) O N 2 (  ) O N 2 (  ) O( N ) T N (  ) F N (  ) T N (  ) Ω F N (  ) (  ) = T N (  ) F N (  ) Ω N 2 (  ) Ω N log N (  ) Big-Oh is similar to  less than or equal  to, when growth  rates are being  considered. Big-Omega is similar to greater than  or equal to, when  growth rates are  being considered.

chapter 5 algorithm analysis Big-Theta is similar  to equal to, when  growth rates are  being considered. The third deﬁnition,  , called Big-Theta, says that the growth rate of   equals the growth rate of  . For instance, the maximum subsequence algorithm shown in Figure 5.5 runs in   time. In other words, the running time is bounded by a quadratic function, and this bound cannot be improved because it is also lower-bounded by another quadratic function. When we use Big-Theta notation, we are providing not only an upper bound on an algorithm but also assurances that the analysis that leads to the upper bound is as good (tight) as possible. In spite of the additional precision offered by Big-Theta, however, Big-Oh is more commonly used, except by researchers in the algorithm analysis ﬁeld. Little-Oh is similar  to less than, when  growth rates are  being considered. The ﬁnal deﬁnition,  , called Little-Oh, says that the growth rate of   is strictly less than the growth rate of  . This function is different from Big-Oh because Big-Oh allows the possibility that the growth rates are the same. For instance, if the running time of an algorithm is , then it is guaranteed to be growing at a slower rate than quadratic (that is, it is a subquadratic algorithm). Thus a bound of   is a better bound than  . Figure 5.9 summarizes these four deﬁnitions. Throw out leading  constants, lowerorder terms, and  relational symbols  when using Big-Oh. A couple of stylistic notes are in order. First, including constants or loworder terms inside a Big-Oh is bad style. Do not say   or . In both cases, the correct form is  . Second, in any analysis that requires a Big-Oh answer, all sorts of shortcuts are possible. Lower-order terms, leading constants, and relational symbols are all thrown away. Now that the mathematics have formalized, we can relate it to the analysis of algorithms. The most basic rule is that the running time of a loop is at most the running time of the statements inside the loop (including tests) times the number of iterations. As shown earlier, the initialization and testing of the loop condition is usually no more dominant than are the statements encompassing the body of the loop. T N (  ) Θ F N (  ) (  ) = T N (  ) F N (  ) Θ N 2 (  ) T N (  ) o F N (  ) (  ) = T N (  ) F N (  ) o N 2 (  ) o N 2 (  ) Θ N 2 (  ) figure 5.9 Meanings of the  various growth  functions Mathematical Expression Relative Rates of Growth Growth of   is   growth of  . Growth of   is   growth of  . Growth of   is    growth of  . Growth of   is    growth of  . T N (  ) O F N (  ) (  ) = T N (  ) ≤ F N (  ) T N (  ) Ω F N (  ) (  ) = T N (  ) ≥ F N (  ) T N (  ) Θ F N (  ) (  ) = T N (  ) = F N (  ) T N (  ) o F N (  ) (  ) = T N (  ) < F N (  ) T N (  ) O 2N 2 (  ) = T N (  ) O N 2 N + (  ) = T N (  ) O N 2 (  ) =

5.4 general big-oh rules A worst-case bound is a guarantee over all inputs  of some size. The running time of statements inside a group of nested loops is the running time of the statements (including tests in the innermost loop) multiplied by the sizes of all the loops. The running time of a sequence of consecutive loops is equal to the running time of the dominant loop. The time difference between a nested loop in which both indices run from 1 to N and two consecutive loops that are not nested but run over the same indices is the same as the space difference between a two-dimensional array and two one-dimensional arrays. The ﬁrst case is quadratic. The second case is linear because N+N is 2N, which is still  . Occasionally, this simple rule can overestimate the running time, but in most cases it does not. Even if it does, BigOh does not guarantee an exact asymptotic answer—just an upper bound. In an average-case bound, the running  time is measured  as an average over  all of the possible  inputs of size N. The analyses we have performed thus far involved use of a worst-case bound, which is a guarantee over all inputs of some size. Another form of analysis is the average-case bound, in which the running time is measured as an average over all of the possible inputs of size N. The average might differ from the worst case if, for example, a conditional statement that depends on the particular input causes an early exit from a loop. We discuss average-case bounds in more detail in Section 5.8. For now, simply note that the fact that one algorithm has a better worstcase bound than another algorithm implies nothing about their relative averagecase bounds. However, in many cases average-case and worst-case bounds are closely correlated. When they are not,  the bounds are treated separately. The last Big-Oh item we examine is how the running time grows for each type of curve, as illustrated in Figures 5.1 and 5.2. We want a more quantitative answer to this question: If an algorithm takes   time to solve a problem of size N, how long does it take to solve a larger problem? For instance, how long does it take to solve a problem when there is 10 times as much input? The answers are shown in Figure 5.10. However, we want to answer the question without running the program and hope our analytical answers will agree with the observed behavior. O( N ) T( N ) figure 5.10 Observed running  times (in seconds) for  various maximum  contiguous subsequence sum  algorithms Figure 5.4 Figure 5.5 Figure 7.20 Figure 5.8 0.000001 0.000000 0.000001 0.000000 0.000288 0.000019 0.000014 0.000005 1,000 0.223111 0.001630 0.000154 0.000053 10,000 0.133064  0.001630 0.000533 100,000 NA 13.17 0.017467 0.005571 1,000,000 NA NA 0.185363 0.056338 N O N 3 (  ) O N 2 (  ) O(N log N ) O(N )

chapter 5 algorithm analysis We begin by examining the cubic algorithm. We assume that the running time is reasonably approximated by  . Consequently, . Mathematical manipulation yields If the  size of the  input increases by  a factor of f, the  running time of a  cubic program  increases by a factor of roughly f 3. Thus the running time of a cubic program increases by a factor of 1,000 (assuming N is sufﬁciently large) when the amount of input is increased by a factor of 10. This relationship is roughly conﬁrmed by the increase in running time from N = 100 to 1,000 shown in Figure 5.10. Recall that we do not expect an exact answer—just a reasonable approximation. We would also expect that for N = 100,000, the running time would increase another 1,000-fold. The result would be that a cubic algorithm requires roughly 60 hours (21/2 days) of computation time. In general, if the amount of the input increases by a factor of f, then the cubic algorithm’s running time increases by a factor of  . If the size of the  input increases by  a factor of f, the  running time of a  quadratic program  increases by a factor of roughly f 2. We can perform similar calculations for quadratic and linear algorithms. For the quadratic algorithm, we assume that  . It follows that . When we expand, we obtain So when the input size increases by a factor of 10, the running time of a quadratic program increases by a factor of approximately 100. This relationship is also conﬁrmed in Figure 5.10. In general, an f-fold increase in input size yields an  -fold increase in running time for a quadratic algorithm. If the size of the  input increases by a  factor of f, then the  running time of a linear program also  increases by a factor  of f. This is the preferred running time  for an algorithm. Finally, for a linear algorithm, a similar calculation shows that a 10-fold increase in input size results in a 10-fold increase in running time. Again, this relationship has been conﬁrmed experimentally in Figure 5.10. Note, however, that for a linear program, the term sufﬁciently large could mean a somewhat higher input size than for the other programs in the event of signiﬁcant overhead used in all cases. For a linear program, this term could still be signiﬁcant for moderate input sizes. The analysis used here does not work when there are logarithmic terms. When an   algorithm is presented with 10 times as much input, the running time increases by a factor slightly larger than 10. Speciﬁcally, we have  . When we expand, we obtain Here c′ = 10clog10. As N gets very large, the ratio   gets closer and closer to 10 because   gets smaller and smaller with increasing N. Consequently, if the algorithm is competitive with a linear algorithm for very large N, it is likely to remain competitive for slightly larger N. Does all this mean that quadratic and cubic algorithms are useless? The answer is no. In some cases, the most efﬁcient algorithms known are quadratic or T( N ) cN 3 = T 10N (  ) c 10N (  ) 3 = T 10N ( ) 1000cN 3 1000T( N ) = = f 3 T N (  ) cN 2 = T 10N (  ) c 10N ( )2 = T 10N (  ) 100cN 2 100T N (  ) = = f 2 O( N log N ) T 10N (  ) c 10N ( ) 10N (  ) log = T 10N (  ) 10cN 10N (  ) log 10cN N log 10cN log + 10T N (  ) c′N + = = = T 10N ( ) T N (  ) ⁄ c′N T N (  ) ⁄ log ( ) N log ⁄ ≈

5.5 the logarithm cubic. In others, the most efﬁcient algorithm is even worse (exponential). Furthermore, when the amount of input is small, any algorithm will do. Frequently the algorithms that are not asymptotically efﬁcient are nonetheless easy to program. For small inputs, that is the way to go. Finally, a good way to test a complex linear algorithm is to compare its output with an exhaustive search algorithm. In Section 5.8 we discuss some other limitations of the Big-Oh model. 5.5 the logarithm The list of typical growth rate functions includes several entries containing the logarithm. A logarithm is the exponent that indicates the power to which a number (the base) is raised to produce a given number. In this section we look in more detail at the mathematics behind the logarithm. In Section 5.6 we show its use in a simple algorithm. The logarithm of N (to the base 2) is  the value X such  that 2 raised to the  power of X equals  N. By default, the  base of the logarithm is 2. We begin with the formal deﬁnition and then follow with more intuitive viewpoints. deﬁnition: For any  ,   if  . In this deﬁnition, B is the base of the logarithm. In computer science, when the base is omitted, it defaults to 2, which is natural for several reasons, as we show later in the chapter. We will prove one mathematical theorem, Theorem 5.4, to show that, as far as Big-Oh notation is concerned, the base is unimportant, and also to show how relations that involve logarithms can be derived. In the rest of the text, we use base 2 logarithms exclusively. An important fact about the logarithm is that it grows slowly. Because 210 = 1,024, log 1,024 = 10. Additional calculations show that the logarithm of 1,000,000 is roughly 20, and the logarithm of 1,000,000,000 is only 30. Consequently, performance of an algorithm is much closer to a linear   algorithm than to a quadratic   algorithm for even moderately large amounts of input. Before we look at a realistic algorithm whose running time includes the logarithm, let us look at a few examples of how the logarithm comes into play. B N , > N B log K = BK N = The base does not matter. For any constant  ,  . Theorem 5.4 Let  . Then  . Let  . Then  . Thus  . Hence, we have , which implies that  . Therefore  , thus completing the  proof. Proof B > N B log O(log N ) = N B log K = BK N = C B log = 2C B = BK 2C ( )K N = = 2CK N = log N CK C N B log = = N B log N log ( ) B log ( ) ⁄ = O( N log N ) O( N ) O N 2 (  )

chapter 5 algorithm analysis bits in a binary number How many bits are required to represent N consecutive integers? The number of bits  required to represent numbers is  logarithmic. A 16-bit short integer represents the 65,536 integers in the range –32,768 to 32,767. In general, B bits are sufﬁcient to represent   different integers. Thus the number of bits B required to represent N consecutive integers satisﬁes the equation  . Hence, we obtain  , so the minimum number of bits is  . (Here   is the ceiling function and represents the smallest integer that is at least as large as X. The corresponding ﬂoor function  represents the largest integer that is at least as small as X.) repeated doubling Starting from X = 1, how many times should X be doubled before it is at least as large as N? The repeated doubling principle holds that, starting at 1,  we can repeatedly  double only logarithmically many  times until we  reach N. Suppose we start with $1 and double it every year. How long would it take to save a million dollars? In this case, after 1 year we would have $2; after 2 years, $4; after 3 years, $8 and so on. In general, after K years we would have   dollars, so we want to ﬁnd the smallest K satisfying  . This is the same equation as before, so  . After 20 years, we would have over a million dollars. The repeated doubling principle holds that, starting from 1, we can repeatedly double only   times until we reach N. repeated halving Starting from X = N, if N is repeatedly halved, how many iterations must be applied to make N smaller than or equal to 1? If the division rounds up to the nearest integer (or is real, not integer, division), we have the same problem as with repeated doubling, except that we are going in the opposite direction. Once again the answer is   iterations. If the division rounds down, the answer is  . We can show the difference by starting with  . Two divisions are necessary, unless the division rounds down, in which case only one is needed. Many of the algorithms examined in this text will have logarithms, introduced because of the repeated halving principle, which holds that, starting at N, we can halve only logarithmically many times. In other words, an algorithm is   if it takes constant ( ) time to cut the problem size by a constant fraction (which is usually  ). This condition follows directly from the fact that there will be   iterations of the loop. Any constant fraction will do because the fraction is reﬂected in the base of the logarithm, and Theorem 5.4 tells us that the base does not matter. All of the remaining occurrences of logarithms are introduced (either directly or indirectly) by applying Theorem 5.5. This theorem concerns the Nth harmonic number, which is the sum of the reciprocals of the ﬁrst N positive integers, and states that the Nth harmonic number, HN, satisﬁes 2B 2B N ≥ B log N ≥ log N X X 2K 2K N ≥ K log N = log N log N log N X = O(log N ) O 1 (  ) 2O(log N ) The repeated halving principle holds  that, starting at N, we can halve only  logarithmically many times.  This process is  used to obtain logarithmic routines  for searching. The Nth harmonic number is the sum  of the reciprocals  of the ﬁrst N positive integers. The  growth rate of the  harmonic number is  logarithmic.

5.6 static searching problem . The proof uses calculus, but you do not need to understand the proof to use the theorem. The next section shows how the repeated halving principle leads to an efﬁcient searching algorithm. 5.6 static searching problem An important use of computers is looking up data. If the data are not allowed to change (e.g., it is stored on a CD-ROM), we say that the data are static. A static search accesses data that are never altered. The static searching problem is naturally formulated as follows. static searching problem Given an integer X and an array A, return the position of X in A or an indication that it is not present. If X occurs more than once, return any occurrence. The array A is never altered. An example of static searching is looking up a person in the telephone book. The efﬁciency of a static searching algorithm depends on whether the array being searched is sorted. In the case of the telephone book, searching by name is fast, but searching by phone number is hopeless (for humans). In this section, we examine some solutions to the static searching problem. 5.6.1   sequential search A sequential search steps through the  data sequentially  until a match is  found. When the input array is not sorted, we have little choice but to do a linear sequential search, which steps through the array sequentially until a match is found. The complexity of the algorithm is analyzed in three ways. First, we provide the cost of an unsuccessful search. Then, we give the worst-case cost of a successful search. Finally, we ﬁnd the average cost of a successful search. HN Θ log N (  ) = Let  . Then  . A more precise estimate is  . Theorem 5.5 The intuition of the proof is that a discrete sum is well approximated by the (continuous) integral. The proof uses a construction to show that the sum   can be  bounded above and below by  , with appropriate limits. Details are left as Exercise  5.24. Proof HN 1 i⁄ i = N ∑ = HN Θ log N (  ) = N ln 0.577 + HN x d x----- ∫

chapter 5 algorithm analysis Analyzing successful and unsuccessful searches separately is typical. Unsuccessful searches usually are more time consuming than are successful searches (just think about the last time you lost something in your house). For sequential searching, the analysis is straightforward. A sequential search  is linear. An unsuccessful search requires the examination of every item in the array, so the time will be  . In the worst case, a successful search, too, requires the examination of every item in the array because we might not ﬁnd a match until the last item. Thus the worst-case running time for a successful search is also linear. On average, however, we search only half of the array. That is, for every successful search in position i, there is a corresponding successful search in position   (assuming we start numbering from 0). However,   is still  . As mentioned earlier in the chapter, all these Big-Oh terms should correctly be Big-Theta terms. However, the use of BigOh is more popular. 5.6.2   binary search If the input array is  sorted, we can use  the binary search, which we perform  from the middle of  the array rather  than the end. If the input array has been sorted, we have an alternative to the sequential search, the binary search, which is performed from the middle of the array rather than the end. We keep track of low and high, which delimit the portion of the array in which an item, if present, must reside. Initially, the range is from 0 to  . If low is larger than high, we know that the item is not present, so we return NOT_FOUND. Otherwise, at line 15, we let mid be the halfway point of the range (rounding down if the range has an even number of elements) and compare the item we are searching for with the item in position mid.5 If we ﬁnd a match, we are done and can return. If the item we are searching for is less than the item in position mid, then it must reside in the range low to mid-1. If it is greater, then it must reside in the range mid+1 to high. In Figure 5.11, lines 17 to 20 alter the possible range, essentially cutting it in half. By the repeated halving principle, we know that the number of iterations will be  . The binary search  is logarithmic  because the search  range is halved in  each iteration.  For an unsuccessful search, the number of iterations in the loop is . The reason is that we halve the range in each iteration (rounding down if the range has an odd number of elements); we add 1 because the ﬁnal range encompasses zero elements. For a successful search, the worst case is iterations because in the worst case we get down to a range of only one element. The average case is only one iteration better because half of the elements require the worst case for their search, a quarter of the elements save one iteration, and only one in   elements will save i iterations from the worst case. O( N ) N – i – N 2 ⁄ O( N ) 5. Note that if low and high are large enough, their sum will overﬂow an int, yielding an incorrect negative value for mid. Exercise 5.27 discusses this in more detail. N – O(log N ) log N + log N 2i

5.6 static searching problem The mathematics involves computing the weighted average by calculating the sum of a ﬁnite series. The bottom line, however, is that the running time for each search is  . In Exercise 5.26, you are asked to complete the calculation. For reasonably large values of N, the binary search outperforms the sequential search. For instance, if N is 1,000, then on average a successful sequential search requires about 500 comparisons. The average binary search, using the previous formula, requires  , or eight iterations for a successful search. Each iteration uses 1.5 comparisons on average (sometimes 1; other times, 2), so the total is 12 comparisons for a successful search. The binary search wins by even more in the worst case or when searches are unsuccessful. Optimizing the  binary search can  cut the number of  comparisons roughly in half. If we want to make the binary search even faster, we need to make the inner loop tighter. A possible strategy is to remove the (implicit) test for a successful search from that inner loop and shrink the range down to one item in all cases. Then we can use a single test outside of the loop to determine if the item is in the array or cannot be found, as shown in Figure 5.12. If the item we are searching for in Figure 5.12 is not larger than the item in the mid position, then it is in the range that includes the mid position. When we break the loop, the subrange is 1, and we can test to see whether we have a match. figure 5.11 Basic binary search  that uses three-way  comparisons /**      * Performs the standard binary search      * using two comparisons per level.      * @return index where item is found, or NOT_FOUND.      */     public static <AnyType extends Comparable<? super AnyType>>                   int binarySearch( AnyType [ ] a, AnyType x )     {         int low = 0;         int high = a.length - 1;         int mid;         while( low <= high )         {             mid = ( low + high ) / 2;             if( a[ mid ].compareTo( x ) < 0 )                 low = mid + 1;             else if( a[ mid ].compareTo( x ) > 0 )                 high = mid - 1;             else                 return mid;         }         return NOT_FOUND;     // NOT_FOUND = -1     } O(log N ) log N –

chapter 5 algorithm analysis In the revised algorithm, the number of iterations is always ⎡log N⎤ because we always shrink the range in half, possibly by rounding down. Thus, the number of comparisons used is always ⎡log N⎤ + 1. Binary search is surprisingly tricky to code. Exercise 5.9 illustrates some common errors. Notice that for small N, such as values smaller than 6, the binary search might not be worth using. It uses roughly the same number of comparisons for a typical successful search, but it has the overhead of line 18 in each iteration. Indeed, the last few iterations of the binary search progress slowly. One can adopt a hybrid strategy in which the binary search loop terminates when the range is small and applies a sequential scan to ﬁnish. Similarly, people search a phone book nonsequentially. Once they have narrowed the range to a column, they perform a sequential scan. The scan of a telephone book is not sequential, but it also is not a binary search. Instead it is more like the algorithm discussed in the next section. figure 5.12 Binary search using  two-way comparisons /**      * Performs the standard binary search      * using one comparison per level.      * @return index where item is found or NOT_FOUND.      */ public static <AnyType extends Comparable<? super AnyType>>                   int binarySearch( AnyType [ ] a, AnyType x )     { if( a.length == 0 )             return NOT_FOUND;         int low = 0;         int high = a.length - 1;         int mid;         while( low < high )         {             mid = ( low + high ) / 2;             if( a[ mid ].compareTo( x ) < 0 )                 low = mid + 1;             else                 high = mid;         }         if( a[ low ].compareTo( x ) == 0 )             return low;         return NOT_FOUND;     }

5.6 static searching problem 5.6.3   interpolation search The binary search is very fast at searching a sorted static array. In fact, it is so fast that we would rarely use anything else. A static searching method that is sometimes faster, however, is an interpolation search, which has better Big-Oh performance on average than binary search but has limited practicality and a bad worst case. For an interpolation search to be practical, two assumptions must be satisﬁed: 1. Each access must be very expensive compared to a typical instruction. For example, the array might be on a disk instead of in memory, and each comparison requires a disk access. 2. The data must not only be sorted, it must also be fairly uniformly distributed. For example, a phone book is fairly uniformly distributed. If the input items are {1, 2, 4, 8, 16,  }, the distribution is not uniform. These assumptions are quite restrictive, so you might never use an interpolation search. But it is interesting to see that there is more than one way to solve a problem and that no algorithm, not even the classic binary search, is the best in all situations. The interpolation search requires that we spend more time to make an accurate guess regarding where the item might be. The binary search always uses the midpoint. However, searching for Hank Aaron in the middle of the phone book would be silly; somewhere near the start clearly would be more appropriate. Thus, instead of mid, we use next to indicate the next item that we will try to access. Here’s an example of what might work well. Suppose that the range contains 1,000 items, the low item in the range is 1,000, the high item in the range is 1,000,000, and we are searching for an item of value 12,000. If the items are uniformly distributed, then we expect to ﬁnd a match somewhere near the twelfth item. The applicable formula is The subtraction of 1 is a technical adjustment that has been shown to perform well in practice. Clearly, this calculation is more costly than the binary search calculation. It involves an extra division (the division by 2 in the binary search is really just a bit shift, just as dividing by 10 is easy for humans), multiplication, and four subtractions. These calculations need to be done using ﬂoating-point operations. One iteration may be slower than the complete binary search. However, if the cost of these calculations is insigniﬁcant when compared to the cost of accessing an item, speed is immaterial; we care only about the number of iterations. … next low x a low [ ] – a high [ ] a low [ ] – ------------------------------------------ high low – – (  ) × + =

chapter 5 algorithm analysis Interpolation search has a better Big-Oh  bound on average  than does binary  search, but has limited practicality and  a bad worst case. In the worst case, where data is not uniformly distributed, the running time could be linear and every item might be examined. In Exercise 5.25 you are asked to construct such a case. However, if we assume that the items are reasonably distributed, as with a phone book, the average number of comparisons has been shown to be  . In other words, we apply the logarithm twice in succession. For N = 4 billion,   is about 32 and  is roughly 5. Of course, there are some hidden constants in the Big-Oh notation, but the extra logarithm can lower the number of iterations considerably, so long as a bad case does not crop up. Proving the result rigorously, however, is quite complicated. 5.7 checking an algorithm analysis Once we have performed an algorithm analysis, we want to determine whether it is correct and as good as we can possibly make it. One way to do this is to code the program and see if the empirically observed running time matches the running time predicted by the analysis. When N increases by a factor of 10, the running time goes up by a factor of 10 for linear programs, 100 for quadratic programs, and 1,000 for cubic programs. Programs that run in   take slightly more than 10 times as long to run under the same circumstances. These increases can be hard to spot if the lower-order terms have relatively large coefﬁcients and N is not large enough. An example is the jump from N = 10 to N = 100 in the running time for the various implementations of the maximum contiguous subsequence sum problem. Differentiating linear programs from   programs, based purely on empirical evidence, also can be very difﬁcult. Another commonly used trick to verify that some program is  is to compute the values   for a range of N (usually spaced out by factors of 2), where   is the empirically observed running time. If  is a tight answer for the running time, then the computed values converge to a positive constant. If   is an overestimate, the values converge to zero. If  is an underestimate, and hence wrong, the values diverge. As an example, suppose that we write a program to perform N random searches using the binary search algorithm. Since each search is logarithmic, we expect the total running time of the program to be  . Figure 5.13 shows the actual observed running time for the routine for various input sizes on a real (but extremely slow) computer. The last column is most likely the converging column and thus conﬁrms our analysis, whereas the increasing numbers for   suggest that   is an underestimate, and the quickly decreasing values for   suggest that   is an overestimate. O N log log (  ) log N log N log O( N log N ) O( N log N ) O F N (  ) (  ) T N ( ) F N (  ) ⁄ T N (  ) F N (  ) F N (  ) F N (  ) O( N log N ) T N ⁄ O( N ) T N 2 ⁄ O N 2 (  )

5.8 limitations of big-oh analysis Note in particular that we do not have deﬁnitive convergence. One problem is that the clock that we used to time the program ticks only every 10 ms. Note also that there is not a great difference between   and  . Certainly an   algorithm is much closer to being linear than being quadratic. 5.8 limitations of big-oh analysis Worst-case is  sometimes uncommon and can be  safely ignored. At  other times, it is  very common and  cannot be ignored. Big-Oh analysis is a very effective tool, but it does have limitations. As already mentioned, its use is not appropriate for small amounts of input. For small amounts of input, use the simplest algorithm. Also, for a particular algorithm, the constant implied by the Big-Oh may be too large to be practical. For example, if one algorithm’s running time is governed by the formula   and another has a running time of 1000N, then the ﬁrst algorithm would most likely be better, even though its growth rate is larger. Large constants can come into play when an algorithm is excessively complex. They also come into play because our analysis disregards constants and thus cannot differentiate between things like memory access (which is cheap) and disk access (which typically is many thousand times more expensive). Our analysis assumes inﬁnite memory, but in applications involving large data sets, lack of sufﬁcient memory can be a severe problem. Sometimes, even when constants and lower-order terms are considered, the analysis is shown empirically to be an overestimate. In this case, the analysis needs to be tightened (usually by a clever observation). Or the average-case figure 5.13 Empirical running time  for N binary searches  in an N-item array CPU Time T (microseconds) T/N T/N2 T / (N log N ) 10,000 1,000 0.1000000 0.0000100 0.0075257 20,000 2,000 0.1000000 0.0000050 0.0069990 40,000 4,400 0.1100000 0.0000027 0.0071953 80,000 9,300 0.1162500 0.0000015 0.0071373 160,000 19,600 0.1225000 0.0000008 0.0070860 320,000 41,700 0.1303125 0.0000004 0.0071257 640,000 87,700 0.1370313 0.0000002 0.0071046 N O( N ) O( N log N ) O( N log N ) 2N log N

chapter 5 algorithm analysis running time bound may be signiﬁcantly less than the worst-case running time bound, and so no improvement in the bound is possible. For many complicated algorithms the worst-case bound is achievable by some bad input, but in practice it is usually an overestimate. Two examples are the sorting algorithms Shellsort and quicksort (both described in Chapter 8). Average-case analysis is almost  always much more  difﬁcult than worstcase analysis.  However, worst-case bounds are usually easier to obtain than their averagecase counterparts. For example, a mathematical analysis of the average-case running time of Shellsort has not been obtained. Sometimes, merely deﬁning what average means is difﬁcult. We use a worst-case analysis because it is expedient and also because, in most instances, the worst-case analysis is very meaningful. In the course of performing the analysis, we frequently can tell whether it will apply to the average case.

chapt er the collections api Many algorithms require the use of a proper representation of data to achieve efﬁciency. This representation and the operations that are allowed for it are known as a data structure. Each data structure allows arbitrary insertion but differs in how it allows access to members in the group. Some data structures allow arbitrary access and deletions, whereas others impose restrictions, such as allowing access to only the most recently or least recently inserted item in the group. As part of Java, a supporting library known as the Collections API is provided. Most of the Collections API resides in java.util. This API provides a collection of data structures. It also provides some generic algorithms, such as sorting. The Collections API makes heavy use of inheritance. Our primary goal is to describe, in general terms, some examples and applications of data structures. Our secondary goal is to describe the basics of the Collections API, so that we can use it in Part Three. We do not discuss the theory behind an efﬁcient Collections API implementation until Part Four, at which point we provide simpliﬁed implementations of some core Collections API components. But delaying the discussion of the Collections API’s implementation until after we use it is not a problem. We do not need to know how something is implemented so long as we know that it is implemented.

chapter 6 the collections api In this chapter, we show n Common data structures, their allowed operations, and their running times n Some applications of the data structures n The organization of the Collections API, and its integration with the  rest of the language 6.1 introduction A data structure is  a representation of  data and the operations allowed on  that data. Data structures allow us to achieve an important object-oriented programming goal: component reuse. The data structures described in this section (and implemented later, in Part Four) have recurring uses. When each data structure has been implemented once, it can be used over and over in various applications. A data structure is a representation of data and the operations allowed on that data. Many, but by no means all, of the common data structures store a collection of objects and then provide methods to add a new object to, remove an existing object from, or access a contained object in the collection. Data structures  allow us to achieve  component reuse. In this chapter, we examine some of the fundamental data structures and their applications. Using a high-level protocol, we describe typical operations that are usually supported by the data structures and brieﬂy describe their uses. When possible, we give an estimate of the cost of implementing these operations efﬁciently. This estimate is often based on analogy with noncomputer applications of the data structure. Our high-level protocol usually supports only a core set of basic operations. Later, when describing the basics of how the data structures can be implemented (in general there are multiple competing ideas), we can more easily focus on language-independent algorithmic details if we restrict the set of operations to a minimum core set. As an example, Figure 6.1 illustrates a generic protocol that many data structures tend to follow. We do not actually use this protocol directly in any code. However, an inheritance-based hierarchy of data structures could use this class as a starting point. The Collections  API is the one  library for data  structures and  algorithms that is  guaranteed to be  available. Then we give a description of the Collections API interface that is provided for these data structures. By no means does the Collections API represent the best way of doing things. However, it represents the one library for data structures and algorithms guaranteed to be available. Its use also illustrates some of the core issues that must be dealt with once the theory is taken care of. We defer consideration of efﬁcient implementation of data structures to Part IV. At that point we will provide, as part of package weiss.nonstandard,

6.2 the iterator pattern some competing implementations for data structures that follow the simple protocols developed in this chapter. We will also provide one implementation for the basic Collections API components described in the chapter, in package weiss.util. Thus we are separating the interface of the Collections API (that is, what it does, which we describe in the chapter) from its implementation (that is, how it is done, which we describe in Part Four). This approach—the separation of the interface and implementation—is part of the object-oriented paradigm. The user of the data structure needs to see only the available operations, not the implementation. Recall this is the encapsulation and information-hiding part of object-oriented programming. The rest of this chapter is organized as follows: First, we discuss the basics of the iterator pattern, which is used throughout the Collections API. Then we discuss the interface for containers and iterators in the Collections API. Next we describe some Collections API algorithms, and ﬁnally, we examine some other data structures many of which are supported in the Collections API. 6.2 the iterator pattern The Collections API makes heavy use of a common technique known as the iterator pattern. So before we begin our discussion of the Collections API, we examine the ideas behind the iterator pattern. An iterator object  controls iteration  of a collection. Consider the problem of printing the elements in a collection. Typically, the collection is an array, so assuming that the object v is an array, its contents are easily printed with code like the following:1 figure 6.1 A generic protocol for  many data structures package weiss.nonstandard; // SimpleContainer protocol public interface SimpleContainer<AnyType> {     void insert( AnyType x ); void remove( AnyType x ); AnyType find( AnyType x ); boolean isEmpty( ); void makeEmpty( ); } 1. The enhanced for loop added in Java 5 is simply additional syntax. The compiler expands the enhanced for loop to obtain the code shown here.

chapter 6 the collections api for( int i = 0; i < v.length; i++ )     System.out.println( v[ i ] ); In this loop, i is an iterator object, because it is the object that is used to control the iteration. However, using the integer i as an iterator constrains the design: We can only store the collection in an array-like structure. A more ﬂexible alternative is to design an iterator class that encapsulates a position inside of a collection. The iterator class provides methods to step through the collection. When we program  to an interface, we  write code that  uses the most  abstract methods.  These methods  will be applied to  the actual concrete types. The key is the concept of programming to an interface: We want the code that performs access of the container to be as independent of the type of the container as possible. This is done by using only methods that are common to all containers and their iterators. There are many different possible iterator designs. If we replace int i with IteratorType itr, then the loop above expresses for( itr = v.first( ); itr.isValid( ); itr.advance( ) )     System.out.println( itr.getData( ) ); This suggests an iterator class that contains methods such as isValid, advance, getData, and so on. We describe two designs, outside the Collections API context, that lead to the Collections API iterator design. We discuss the speciﬁcs of the Collections iterators in Section 6.3.2, deferring implementations to Part IV. 6.2.1   basic iterator design iterator returns  an appropriate  iterator for the collection. The ﬁrst iterator design uses only three methods. The container class is required to provide an iterator method. iterator returns an appropriate iterator for the collection. The iterator class has only two methods, hasNext and next. hasNext returns true if the iteration has not yet been exhausted. next returns the next item in the collection (and in the process, advances the current position). This iterator interface is similar to the interface provided in the Collections API. To illustrate the implementation of this design, we outline the collection class and provide an iterator class, MyContainer and MyContainerIterator, respectively. Their use is shown in Figure 6.2. The data members and iterator method for MyContainer are written in Figure 6.3. To simplify matters, we omit the constructors, and methods such as add, size, etc. The ArrayList class from earlier chapters can be reused to provide an implementation of these methods. We also avoid use of generics for now.

6.2 the iterator pattern The iterator is constructed with a  reference to the  container that it  iterates over. The iterator method in class MyContainer simply returns a new iterator; notice that the iterator must have information about the container that it is iterating over. Thus the iterator is constructed with a reference to the MyContainer. Figure 6.4 shows the MyContainerIterator. The iterator keeps a variable (current) that represents the current position in the container, and a reference to the container. The implementation of the constructor and two methods is straightforward. The constructor initializes the container reference, hasNext simply compares the current position with the container size, and next uses the current position to index the array (and then advances the current position). The better design  would put more  functionality in the  iterator.  A limitation of this iterator design is the relatively limited interface. Observe that it is impossible to reset the iterator back to the beginning, and that the next method couples access of an item with advancing. The next, hasNext design is what is used in the Java Collections API; many people feel that the API should have provided a more ﬂexible iterator. It is certainly possible to put more functionality in the iterator, while leaving the figure 6.2 A main method, to  illustrate iterator  design 1     public static void main( String [ ] args )     {         MyContainer v = new MyContainer( );         v.add( "3" );         v.add( "2" );         System.out.println( "Container contents: " );         MyContainerIterator itr = v.iterator( );         while( itr.hasNext( ) )             System.out.println( itr.next( ) );     } figure 6.3 The MyContainer class, design 1 package weiss.ds; public class MyContainer {     Object [ ] items;     int size;     public MyContainerIterator iterator( )       { return new MyContainerIterator( this ); }     // Other methods }

chapter 6 the collections api MyContainer class implementation completely unchanged. On the other hand, doing so illustrates no new principles. Note that in the implementation of MyContainer, the data members items and size are package visible, rather than being private. This unfortunate relaxation of the usual privacy of data members is necessary because these data members need to be accessed by MyContainerIterator. Similarly, the MyContainerIterator constructor is package visible, so that it can be called by MyContainer. 6.2.2   inheritance-based iterators and factories The iterator designed so far manages to abstract the concept of iteration into an iterator class. This is good, because it means that if the collection changes from an array-based collection to something else, the basic code such as lines 10 and 11 in Figure 6.2 does not need to change. While this is a signiﬁcant improvement, changes from an array-based collection to something else require that we change all the declarations of the iterator. For instance, in Figure 6.2, we would need to change line 9. We discuss an alternative in this section. An inheritancebased iteration  scheme deﬁnes an  iterator interface.  Clients program to  this interface. Our basic idea is to deﬁne an interface Iterator. Corresponding to each different kind of container is an iterator that implements the Iterator protocol. In our example, this gives three classes: MyContainer, Iterator, and MyContainerIterator. The relationship that holds is MyContainerIterator IS-A Iterator. The reason we do this is that each container can now create an appropriate iterator, but pass it back as an abstract Iterator. figure 6.4 Implementation of the  MyContainerIterator, design 1 // An iterator class that steps through a MyContainer. package weiss.ds; public class MyContainerIterator {     private int current = 0;     private MyContainer container;     MyContainerIterator( MyContainer c )       { container = c; }     public boolean hasNext( )       { return current < container.size; }     public Object next( )       { return container.items[ current++ ]; } }

6.2 the iterator pattern Figure 6.5 shows MyContainer. In the revised MyContainer, the iterator method returns a reference to an Iterator object; the actual type turns out to be a MyContainerIterator. Since MyContainerIterator IS-A Iterator, this is safe to do.  A factory method creates a new  concrete instance  but returns it using  a reference to the  interface type. Because iterator creates and returns a new Iterator object, whose actual type is unknown, it is commonly known as a factory method. The iterator interface, which serves simply to establish the protocol by which all subclasses of Iterator can be accessed, is shown in Figure 6.6. There are only two changes to the implementation of MyContainerIterator, shown in Figure 6.7 and both changes are at line 5. First, the implements clause has been added. Second, MyContainerIterator no longer needs to be a public class.  Nowhere in main is there any mention  of the actual iterator type. Figure 6.8 demonstrates how the inheritance-based iterators are used. At line 9, we see the declaration of itr: It is now a reference to an Iterator. Nowhere in main is there any mention of the actual MyContainerIterator type. The fact that a MyContainerIterator exists is not used by any clients of the MyContainer class. This is a very slick design and illustrates nicely the idea of hiding an implementation and programming to an interface. The implementation can be made even slicker by use of nested classes, and a Java feature known as inner classes. Those implementation details are deferred until Chapter 15. figure 6.5 The MyContainer class, design 2 package weiss.ds; public class MyContainer {     Object [ ] items;     int size;     public Iterator iterator( )       { return new MyContainerIterator( this ); }     // Other methods not shown. } figure 6.6 The Iterator interface, design 2 package weiss.ds; public interface Iterator {     boolean hasNext( );     Object next( ); }

chapter 6 the collections api 6.3 collections api:  containers and iterators This section describes the basics of the Collections API iterators and how they interact with containers. We know that an iterator is an object that is used to traverse a collection of objects. In the Collections API such a collection is abstracted by the Collection interface, and the iterator is abstracted by the Iterator interface. The Collections API iterators are somewhat inﬂexible, in that they provide few operations. These iterators use an inheritance model described in Section 6.2.2. figure 6.7 Implementation of the  MyContainerIterator, design 2 // An iterator class that steps through a MyContainer. package weiss.ds; class MyContainerIterator implements Iterator {     private int current = 0;     private MyContainer container;     MyContainerIterator( MyContainer c )       { container = c; }     public boolean hasNext( )       { return current < container.size; }     public Object next( )       { return container.items[ current++ ]; } } figure 6.8 A main method, to  illustrate iterator  design 2     public static void main( String [ ] args )     {         MyContainer v = new MyContainer( );         v.add( "3" );         v.add( "2" );         System.out.println( "Container contents: " );         Iterator itr = v.iterator( );         while( itr.hasNext( ) )             System.out.println( itr.next( ) );     }

6.3 collections api: containers and iterators 6.3.1   the Collection interface The Collection interface represents a group of  objects, known as  its elements. The Collection interface represents a group of objects, known as its elements. Some implementations, such as lists, are unsorted; others, such as sets and maps, may be sorted. Some implementations allow duplicates; others do not. Starting with Java 5, the Collection interface and the entire Collections API make use of generics. All containers support the following operations. boolean isEmpty( ) Returns true if the container contains no elements and false otherwise. int size( ) Returns the number of elements in the container. boolean add( AnyType x ) Adds item x to the container. Returns true if this operation succeeds and false otherwise (e.g., if the container does not allow duplicates and x is already in the container). boolean contains( Object x ) Returns true if x is in the container and false otherwise. boolean remove( Object x ) Removes item x from the container. Returns true if x was removed and false otherwise. void clear( ) Makes the container empty. Object [ ] toArray( ) <OtherType> OtherType [ ] toArray ( OtherType [ ] arr )  Returns an array that contains references to all items in the container. java.util.Iterator<AnyType> iterator( ) Returns an Iterator that can be used to begin traversing all locations in the container. Because Collection is generic, it allows objects of only a speciﬁc type (AnyType) to be in the collection. Thus, the parameter to add is AnyType. The parameter to contains and remove should be AnyType also; however, for backward compatibility it is Object. Certainly, if contains or remove are called with a parameter that is not of type AnyType, the return value will be false. The method toArray returns an array that contains references to the items that are in the collection. In some cases, it can be faster to manipulate this array than to use an iterator to manipulate the collection; however, the cost of

chapter 6 the collections api doing so is extra space. The most common place where using the array would be useful is when the collection is being accessed several times or via nested loops. If the array is being accessed only once, sequentially, it is unlikely that using toArray will make things faster; it can make things slower while also costing extra space. One version of toArray returns the array in a type that is Object[]. The other version allows the user to specify the exact type of the array by passing a parameter containing the array (thus avoiding the costs of casting during the subsequent manipulation). If the array is not large enough, a sufﬁciently large array is returned instead; however, this should never be needed. The following snippet shows how to obtain an array from a Collection<String> coll: String [ ] theStrings = new String[ coll.size( ) ]; coll.toArray( theStrings ); At this point, the array can be manipulated via normal array indexing. The one-parameter version of toArray is generally the one that you would want to use because the runtime costs of casting are avoided. Finally, the iterator method returns an Iterator<AnyType>, which can be used to traverse the collection. Figure 6.9 illustrates a speciﬁcation of the Collection interface. The actual Collection interface in java.util contains some additional methods, but we will be content with this subset. By convention, all implementations supply both a zero-parameter constructor that creates an empty collection and a constructor that creates a collection that refers to the same elements as another collection. This is basically a shallow-copy of a collection. However, there is no syntax in the language that forces the implementation of these constructors. The Collection interface extends Iterable, which means that the enhanced for loop can be applied to it. Recall that the Iterable interface requires the implementation of an iterator method that returns a java.util.Iterator. The compiler will expand the enhanced for loop with appropriate calls to methods in java.util.Iterator. At line 41, we see the iterator method required by the Iterable interface. However, we remark that we are taking advantage of covariant return types (Section 4.1.11), because the return type for the iterator method at line 41 is actually weiss.util.Iterator, which is our own class that extends java.util.Iterator, and is shown in Section 6.3.2.  The Collections API also codiﬁes the notion of an optional interface method. For instance, suppose we want an immutable collection: Once it is constructed, its state should never change. An immutable collection appears incompatible with Collection, since add and remove do not make sense for immutable collections.

6.3 collections api: containers and iterators figure 6.9 A sample specification of the Collection interface package weiss.util; /**  * Collection interface; the root of all 1.5 collections.  */ public interface Collection<AnyType> extends Iterable<AnyType>, java.io.Serializable {     /**      * Returns the number of items in this collection.      */     int size( );     /**      * Tests if this collection is empty.      */     boolean isEmpty( );     /**      * Tests if some item is in this collection.      */     boolean contains( Object x );     /**      * Adds an item to this collection.      */     boolean add( AnyType x );     /**      * Removes an item from this collection.      */     boolean remove( Object x );     /**      * Change the size of this collection to zero.      */     void clear( );     /**      * Obtains an Iterator object used to traverse the collection.      */     Iterator<AnyType> iterator( );     /**      * Obtains a primitive array view of the collection.      */     Object [ ] toArray( );     /**      * Obtains a primitive array view of the collection.      */     <OtherType> OtherType [ ] toArray( OtherType [ ] arr ); }

chapter 6 the collections api However, there is an existing loophole: Although the implementor of the immutable collection must implement add and remove, there is no rule that says these methods must do anything. Instead, the implementor can simply throw a run-time UnsupportedOperationException. In doing so, the implementor has technically implemented the interface, while not really providing add and remove. By convention, interface methods that document that they are optional can be implemented in this manner. If the implementation chooses not to implement an optional method, then it should document that fact. It is up to the client user of the API to verify that the method is implemented by consulting the documentation, and if the client ignores the documentation and calls the method anyway, the run-time UnsupportedOperationException is thrown, signifying a programming error. Optional methods are somewhat controversial, but they do not represent any new language additions. They are simply a convention. We will eventually implement all methods. The most interesting of these methods is iterator, which is a factory method that creates and returns an Iterator object. The operations that can be performed by an Iterator are described in Section 6.3.2. 6.3.2   Iterator interface An iterator is an  object that allows  us to iterate  through all objects  in a collection.  As described in Section 6.2, an iterator is an object that allows us to iterate through all objects in a collection. The technique of using an iterator class was discussed in the context of read-only vectors in Section 6.2. The Iterator interface in the Collections API is small and contains only three methods: boolean hasNext( ) Returns true if there are more items to view in this iteration. AnyType next( )  Returns a reference to the next object not yet seen by this iterator. The object becomes seen, and thus advances the iterator. void remove( )  Removes the last item viewed by next. This can be called only once between calls to next. The Iterator interface contains only  three methods:  next, hasNext, and  remove. Each collection deﬁnes its own implementation of the Iterator interface, in a class that is invisible to users of the java.util package. The iterators also expect a stable container. An important problem that occurs in the design of containers and iterators is to decide what happens if the state of a container is modiﬁed while an iteration is in progress. The Collections

6.3 collections api: containers and iterators API takes a strict view: Any external structural modiﬁcation of the container (adds, removes, and so on) will result in a ConcurrentModificationException by the iterator methods when one of the methods is called. In other words, if we have an iterator, and then an object is added to the container, and then we invoke the next method on the iterator, the iterator will detect that it is now invalid, and an exception will be thrown by next. The Iterator methods throw an  exception if its  container has  been structurally  modiﬁed. This means that it is impossible to remove an object from a container when we have seen it via an iterator, without invalidating the iterator. This is one reason why there is a remove method in the iterator class. Calling the iterator remove causes the last seen object to be removed from the container. It invalidates all other iterators that are viewing this container, but not the iterator that performed the remove. It is also likely to be more efﬁcient than the container’s remove method, at least for some collections. However, remove cannot be called twice in a row. Furthermore, remove preserves the semantics of next and hasNext, because the next unseen item in the iteration remains the same. This version of remove is listed as an optional method, so the programmer needs to check that it is implemented. The design of remove has been criticized as poor, but we will use it at one point in the text. Figure 6.10 provides a sample speciﬁcation of the Iterator interface. (Our iterator class extends the standard java.util version in order to allow figure 6.10 A sample specification of Iterator package weiss.util; /**  * Iterator interface.  */ public interface Iterator<AnyType> extends java.util.Iterator<AnyType> {     /**      * Tests if there are items not yet iterated over.      */     boolean hasNext( );     /**      * Obtains the next (as yet unseen) item in the collection.      */     AnyType next( );     /**      * Remove the last item returned by next.      * Can only be called once after next.      */     void remove( ); }

chapter 6 the collections api the enhanced for loop to work.) As an example of using the Iterator, the routines in Figure 6.11 print each element in any container. If the container is an ordered set, its elements are output in sorted order. The ﬁrst implementation uses an iterator directly, and the second implementation uses an enhanced for loop. The enhanced for loop is simply a compiler substitution. The compiler, in effect, generates the ﬁrst version (with java.util.Iterator) from the second. 6.4 generic algorithms The Collections class contains a set  of static methods  that operate on  Collection objects. The Collections API provides a few general purpose algorithms that operate on all of the containers. These are static methods in the Collections class (note that this is a different class than the Collection interface). There are also some static methods in the Arrays class that manipulate arrays (sorting, searching, etc.). Most of those methods are overloaded—a generic version, and once for each of the primitive types (except boolean). We examine only a few of the algorithms, with the intention of showing the general ideas that pervade the Collections API, while documenting the speciﬁc algorithms that will be used in Part Three. The material in  Section 4.8 is an  essential prerequisite to this section. Some of the algorithms make use of function objects. Consequently, the material in Section 4.8 is an essential prerequisite to this section. figure 6.11 Print the contents of  any Collection. // Print the contents of Collection c (using iterator directly) public static <AnyType> void printCollection( Collection<AnyType> c ) { Iterator<AnyType> itr = c.iterator( );  while( itr.hasNext( ) ) System.out.print( itr.next( ) + " " ); System.out.println( ); } // Print the contents of Collection c (using enhanced for loop) public static <AnyType> void printCollection( Collection<AnyType> c ) { for( AnyType val : c ) System.out.print( val + " " ); System.out.println( ); }

6.4 generic algorithms 6.4.1   Comparator function objects Many Collections API classes and routines require the ability to order objects. There are two ways to do this. One possibility is that the objects implement the Comparable interface and provide a compareTo method. The other possibility is that the comparison function is embedded as the compare method in an object that implements the Comparator interface. Comparator is deﬁned in java.util; a sample implementation was shown in Figure 4.39 and is repeated in Figure 6.12. 6.4.2   the Collections class Although we will not make use of the Collections class in this text, it has two methods that are thematic of how generic algorithms for the Collections API are written. We write these methods in the Collections class implementation that spans Figures 6.13 and 6.14. reverseOrder is a  factory method  that creates a  Comparator representing the  reverse natural  order. Figure 6.13 begins by illustrating the common technique of declaring a private constructor in classes that contain only static methods. This prevents instantiation of the class. It continues by providing the reverseOrder method. This is a factory method that returns a Comparator that provides the reverse of the natural ordering for Comparable objects. The returned object, created at line 20, is an instance of the ReverseComparator class written in lines 23 to 29. In the ReverseComparator class, we use the compareTo method. This is an figure 6.12 The Comparator interface, originally defined in java.util and rewritten for the weiss.util package package weiss.util; /**  * Comparator function object interface.  */ public interface Comparator<AnyType> {     /**      * Return the result of comparing lhs and rhs.      * @param lhs first object.      * @param rhs second object.      * @return < 0 if lhs is less than rhs,      *           0 if lhs is equal to rhs,      *         > 0 if lhs is greater than rhs.      * @throws ClassCastException if objects cannot be compared.      */     int compare( AnyType lhs, AnyType rhs ) throws ClassCastException; }

chapter 6 the collections api example of the type of code that might be implemented with an anonymous class. We have a similar declaration for the default comparator; since the standard API does not provide a public method to return this, we declare our method to be package-visible. Figure 6.14 illustrates the max method, which returns the largest element in any Collection. The one-parameter max calls the two-parameter max figure 6.13 The Collections class (part 1): private constructor and reverseOrder package weiss.util; /**  * Instanceless class contains static methods that operate on collections.  */ public class Collections {     private Collections( )     {     }     /*      * Returns a comparator that imposes the reverse of the      * default ordering on a collection of objects that      * implement the Comparable interface.      * @return the comparator.      */     public static <AnyType> Comparator<AnyType> reverseOrder( )     {         return new ReverseComparator<AnyType>( );     }     private static class ReverseComparator<AnyType> implements Comparator<AnyType>     {         public int compare( AnyType lhs, AnyType rhs )         {             return - ((Comparable)lhs).compareTo( rhs );         }     }     static class DefaultComparator<AnyType extends Comparable<? super AnyType>>                  implements Comparator<AnyType>     {         public int compare( AnyType lhs, AnyType rhs )         {             return lhs.compareTo( rhs );         }     }

6.4 generic algorithms by supplying the default comparator. The funky syntax in the type parameter list is used to ensure that the type erasure of max generates Object (rather than Comparable). This is important because earlier versions of Java figure 6.14 The Collections class (part 2): max /**  * Returns the maximum object in the collection,      * using default ordering      * @param coll the collection.      * @return the maximum object.      * @throws NoSuchElementException if coll is empty.      * @throws ClassCastException if objects in collection      *         cannot be compared.      */     public static <AnyType extends Object & Comparable<? super AnyType>>     AnyType max( Collection<? extends AnyType> coll )     {         return max( coll, new DefaultComparator<AnyType>( ) );     }     /**      * Returns the maximum object in the collection.      * @param coll the collection.      * @param cmp the comparator.      * @return the maximum object.      * @throws NoSuchElementException if coll is empty.      * @throws ClassCastException if objects in collection      *         cannot be compared.      */     public static <AnyType>     AnyType max( Collection<? extends AnyType> coll, Comparator<? super AnyType> cmp )     {         if( coll.size( ) == 0 )             throw new NoSuchElementException( ); Iterator<? extends AnyType> itr = coll.iterator( );         AnyType maxValue = itr.next( );         while( itr.hasNext( ) )         { AnyType current = itr.next( );             if( cmp.compare( current, maxValue ) > 0 )                 maxValue = current;         }         return maxValue; } }

chapter 6 the collections api used Object as the return type, and we want to ensure backward compatability. The two-parameter max combines the iterator pattern with the function object pattern to step through the collection, and at line 75 uses calls to the function object to update the maximum item. 6.4.3   binary search The Collections API implementation of the binary search is the static method Arrays.binarySearch. There are actually seven overloaded versions—one for each of the primitive types except boolean, plus two more overloaded versions that work on Objects (one works with a comparator, one uses the default comparator). We will implement the Object versions (using generics); the other seven are mindless copy-and-paste. binarySearch uses  binary search and  returns the index  of the matched  item or a negative  number if the item  is not found. As usual, for binary search the array must be sorted; if it is not, the results are undeﬁned (verifying that the array is sorted would destroy the logarithmic time bound for the operation). If the search for the item is successful, the index of the match is returned. If the search is unsuccessful, we determine the ﬁrst position that contains a larger item, add 1 to this position, and then return the negative of the value. Thus, the return value is always negative, because it is at most -1 (which occurs if the item we are searching for is smaller than all other items) and is at least -a.length-1 (which occurs if the item we are searching for is larger than all other items). The implementation is shown in Figure 6.15. As was the case for the max routines, the two-parameter binarySearch calls the three-parameter binarySearch (see lines 17 and 18). The three-parameter binary search routine mirrors the implementation in Figure 5.12. In Java 5, the two-parameter version does not use generics. Instead, all types are Object. But our generic implementation seems to make more sense. The three-parameter version is generic in Java 5. We use the binarySearch method in Section 10.1. 6.4.4   sorting The Arrays class  contains a set of  static methods  that operate on  arrays. The Collections API provides a set of overloaded sort methods in the Arrays class. Simply pass an array of primitives, or an array of Objects that implement Comparable, or an array of Objects and a Comparator. We have not provided a sort method in our Arrays class. void sort( Object [ ] arr ) Rearranges the elements in the array to be in sorted order, using the natural order.

6.4 generic algorithms figure 6.15 Implementation of the binarySearch method in Arrays class package weiss.util; /**  * Instanceless class that contains static methods  * to manipulate arrays.  */ public class Arrays {     private Arrays( ) { }     /**      * Search sorted array arr using default comparator      */ public static <AnyType extends Comparable<AnyType>> int     binarySearch( AnyType [ ] arr, AnyType x )     {         return binarySearch( arr, x,                   new Collections.DefaultComparator<AnyType>( ) );     }     /**      * Performs a search on sorted array arr using a comparator.      * If arr is not sorted, results are undefined.      * @param arr the array to search.      * @param x the object to search for.      * @param cmp the comparator.      * @return if x is found, returns the index where it is found.      *   otherwise, the return value is a negative number equal      *   to -( p + 1 ), where p is the first position greater      *   than x. This can range from -1 down to -(arr.length+1).      * @throws ClassCastException if items are not comparable.      */     public static <AnyType> int     binarySearch( AnyType [ ] arr, AnyType x, Comparator<? super AnyType> cmp )     {         int low = 0, mid = 0;         int high = arr.length;         while( low < high )         {             mid = ( low + high ) / 2;             if( cmp.compare( x, arr[ mid ] ) > 0 )                 low = mid + 1;             else                 high = mid;         }         if( low == arr.length || cmp.compare( x, arr[ low ] ) != 0 )             return - ( low + 1 );         return low;     } }

chapter 6 the collections api void sort( Object [ ] arr, Comparator cmp ) Rearranges the elements in the array to be in sorted order, using the order speciﬁed by the comparator. In Java 5, these methods have been written as generic methods. The generic  sorting algorithms are required to run in   time. 6.5 the List interface A list is a collection of items in  which the items  have a position. A list is a collection of items in which the items have a position. The most obvious example of a list is an array. In an array, items are placed in position 0, 1, etc. The List interface extends the Collection interface and abstracts the notion of a position. The interface in java.util adds numerous methods to the Collection interface. We are content to add the three shown in Figure 6.16. The List interface  extends the  Collection interface and abstracts  the notion of a  position. The ﬁrst two methods are get and set, which are similar to the methods that we have already seen in ArrayList. The third method returns a more ﬂexible iterator, the ListIterator. O( N log N ) figure 6.16 A sample List interface package weiss.util; /**  * List interface. Contains much less than java.util  */ public interface List<AnyType> extends Collection<AnyType> {     AnyType get( int idx );     AnyType set( int idx, AnyType newVal );     /**      * Obtains a ListIterator object used to traverse      * the collection bidirectionally.      * @return an iterator positioned      *          prior to the requested element.      * @param pos the index to start the iterator.      *          Use size() to do complete reverse traversal.      *          Use 0 to do complete forward traversal.      * @throws IndexOutOfBoundsException if pos is not      *          between 0 and size(), inclusive.      */     ListIterator<AnyType> listIterator( int pos ); }

6.5 the List interface 6.5.1   the ListIterator interface ListIterator is a  bidirectional version of Iterator. As shown in Figure 6.17, ListIterator is just like an Iterator, except that it is bidirectional. Thus we can both advance and retreat. Because of this, the listIterator factory method that creates it must be given a value that is logically equal to the number of elements that have already been visited in the forward direction. If this value is zero, the ListIterator is initialized at the front, just like an Iterator. If this value is the size of the List, the iterator is initialized to have processed all elements in the forward direction. Thus in this state, hasNext returns false, but we can use hasPrevious and previous to traverse the list in reverse.  Figure 6.18 illustrates that we can use itr1 to traverse a list in the forward direction, and then once we reach the end, we can traverse the list backwards. It also illustrates itr2, which is positioned at the end, and simply processes the ArrayList in reverse. Finally, it shows the enhanced for loop. One difﬁculty with the ListIterator is that the semantics for remove must change slightly. The new semantics are that remove deletes from the List the last figure 6.17 A sample  ListIterator interface package weiss.util; /**  * ListIterator interface for List interface.  */ public interface ListIterator<AnyType> extends Iterator<AnyType> {     /**      * Tests if there are more items in the collection      * when iterating in reverse.      * @return true if there are more items in the collection      *  when traversing in reverse.      */     boolean hasPrevious( );     /**      * Obtains the previous item in the collection.      * @return the previous (as yet unseen) item in the collection      *  when traversing in reverse.      */     AnyType previous( );     /**      * Remove the last item returned by next or previous.      * Can only be called once after next or previous.      */     void remove( ); }

chapter 6 the collections api object returned as a result of calling either next or previous, and remove can only be called once between calls to either next or previous. To override the javadoc output that is generated for remove, remove is listed in the ListIterator interface. The interface in Figure 6.17 is only a partial interface. There are some additional methods in the ListIterator that we do not discuss in the text, but which are used throughout as exercises. These methods include add and set, which allow the user to make changes to the List at the current location held by the iterator. figure 6.18 A sample program that illustrates bidirectional iteration import java.util.ArrayList; import java.util.ListIterator; class TestArrayList {     public static void main( String [ ] args )     {         ArrayList<Integer> lst = new ArrayList<Integer>( );         lst.add( 2 ); lst.add( 4 );         ListIterator<Integer> itr1 = lst.listIterator( 0 );         ListIterator<Integer> itr2 = lst.listIterator( lst.size( ) );         System.out.print( "Forward: " );         while( itr1.hasNext( ) )             System.out.print( itr1.next( ) + " " );         System.out.println( );         System.out.print( "Backward: " );         while( itr1.hasPrevious( ) )             System.out.print( itr1.previous( ) + " " );         System.out.println( );         System.out.print( "Backward: " );         while( itr2.hasPrevious( ) )             System.out.print( itr2.previous( ) + " " );         System.out.println( ); System.out.print( "Forward: "); for( Integer x : lst ) System.out.print( x + " " ); System.out.println( );     } }

6.5 the List interface 6.5.2   LinkedList class There are two basic List implementations in the Collections API. One implementation is the ArrayList, which we have already seen. The other is a LinkedList, which stores items internally in a different manner than ArrayList, yielding performance trade-offs. A third version is Vector, which is like ArrayList, but is from an older library, and is present mostly for compatibility with legacy (old) code. Using Vector is no longer in vogue. The LinkedList class implements  a linked list. The ArrayList may be appropriate if insertions are performed only at the high end of the array (using add), for the reasons discussed in Section 2.4.3. The ArrayList doubles the internal array capacity if an insertion at the high end would exceed the internal capacity. Although this gives good Big-Oh performance, especially if we add a constructor that allows the caller to suggest initial capacity for the internal array, the ArrayList is a poor choice if insertions are not made at the end, because then we must move items out of the way. The linked list is  used to avoid large  amounts of data  movement. It  stores items with  an additional one  reference per item  overhead. In a linked list, we store items noncontiguously rather than in the usual contiguous array. To do this, we store each object in a node that contains the object and a reference to the next node in the list, as shown in Figure 6.19. In this scenario, we maintain references to both the ﬁrst and last node in the list. To be more concrete, a typical node looks like this: class ListNode {     Object   data;   // Some element     ListNode next; } At any point, we can add a new last item x by doing this: last.next = new ListNode( ); // Attach a new ListNode last = last.next;            // Adjust last  last.data = x;               // Place x in the node last.next = null;            // It's the last; adjust next figure 6.19 A simple linked list first last A0 A1 A2 A3

chapter 6 the collections api Now an arbitrary item can no longer be found in one access. Instead, we must scan down the list. This is similar to the difference between accessing an item on a compact disk (one access) or a tape (sequential). While this may appear to make linked lists less attractive than arrays, they still have advantages. First, an insertion into the middle of the list does not require moving all of the items that follow the insertion point. Data movement is very expensive in practice, and the linked list allows insertion with only a constant number of assignment statements. The basic tradeoff between  ArrayList and  LinkedList is that  get is not efﬁcient  for LinkedList, while insertion and  removal from the  middle of a container is more efﬁciently supported  by the LinkedList. Comparing ArrayList and LinkedList, we see that insertions and deletions toward the middle of the sequence are inefﬁcient in the ArrayList but may be efﬁcient for a LinkedList. However, an ArrayList allows direct access by the index, but a LinkedList should not. It happens that in the Collections API, get and set are part of the List interface, so LinkedList supports these operations, but does so very slowly. Thus, the LinkedList can always be used unless efﬁcient indexing is needed. The ArrayList may still be a better choice if insertions occur only at the end. To access items in the list, we need a reference to the corresponding node, rather than an index. The reference to the node would typically be hidden inside an iterator class. Access to the list  is done through an  iterator class. Because LinkedList performs adds and removes more efﬁciently, it has more operations than the ArrayList. Some of the additional operations available for LinkedList are the following: void addLast( AnyType element )  Appends element at the end of this LinkedList. void addFirst( AnyType element )  Appends element to the front of this LinkedList. AnyType getFirst( )  AnyType element( ) Returns the ﬁrst element in this LinkedList. element was added in Java 5. AnyType getLast( )  Returns the last element in this LinkedList. AnyType removeFirst( )  AnyType remove( ) Removes and returns the ﬁrst element from this LinkedList. remove was added in Java 5. AnyType removeLast( )  Removes and returns the last element from this LinkedList. We implement the LinkedList class in Part Four.

6.5 the List interface 6.5.3   running time for Lists In Section 6.5.2 we saw that for some operations, ArrayList is a  better choice than LinkedList, and for other operations the reverse is true. In this section, rather than discuss the times informally, we will analyze the running times in terms of Big-Oh. Initially, we concentrate on the following subset of operations: n add (at the end) n add (at the front) n remove (at the end) n remove (at the front) n get and set n contains ArrayList costs For the ArrayList, adding at the end simply involves placing an item at the next available array slot, and incrementing the current size. Occasionally we have to resize the capacity of the array, but as this is an extremely rare operation, one can argue that it does not affect the running time. Thus the cost of adding at the end of an ArrayList does not depend on the number of items stored in the ArrayList and is  Similarly, removing from the end of the ArrayList simply involves decrementing the current size, and is   get and set on the ArrayList become array indexing operations, which are typically taken to be constant-time,  operations. Needless to say, when we are discussing the cost of a single operation on a collection, it is hard to envision anything better than   constant time, per operation. To do better than this would require that as the collection gets larger, operations actually get faster, which would be very unusual. However, not all operations are   on an ArrayList. As we have seen, if we add at the front of the ArrayList, then every element in the ArrayList must be shifted one index higher. Thus if there are N elements in the ArrayList, adding at the front is an   operation. Similarly, removing from the front of the ArrayList requires shifting all the elements one index lower, which is also an   operation. And a contains on an ArrayList is an   operation, because we potentially have to sequentially examine every item in the ArrayList. Needless to say,   per operation is not as good as   per operation. In fact, when one considers that the contains operation is   and is basically an exhaustive search, one can argue that   per operation for a basic collection operation is about as bad as it gets. O 1 ( ). O 1 ( ). O 1 (  ) O 1 ( ), O 1 (  ) O( N ) O( N ) O( N ) O( N ) O 1 (  ) O( N ) O( N )

chapter 6 the collections api LinkedList costs If we look at the LinkedList operations, we can see that adding at either the front or the end is an   operation. To add at the front, we simply create a new node and add it at the front, updating first. This operation does not depend on knowing how many subsequent nodes are in the list. To add at the end, we simply create a new node and add it at the end, adjusting last. Removing the ﬁrst item in the linked list is likewise an   operation, because we simply advance first to the next node in the list. Removing the last item in the linked list appears to also be  , since we need to move last to the next-to-last node, and update a next link. However, getting to the next-to-last node is not easy in the linked list, as drawn in Figure 6.19. In the classic linked list, where each node stores a link to its next node, having a link to the last node provides no information about the next-to-last node. The obvious idea of maintaining a third link to the next-to-last node doesn’t work because it too would need to be updated during a remove. Instead, we have every node maintain a link to its previous node in the list. This is shown in Figure 6.20 and is know as a doubly linked list. In a doubly linked list, add and remove operations at either end take  time. As we know, there is a trade-off, however, because get and set are no longer efﬁcient. Instead of direct access through an array, we have to follow links. In some cases we can optimize by starting at the end instead of the front, but if the get or set is to an item that is near the middle of the list, it must take   time. contains in a linked list is the same as an ArrayList: the basic algorithm is a sequential search that potentially examines every item, and thus is an  operation. comparison of ArrayList and LinkedList costs Figure 6.21 compares the running time of single operations in the ArrayList and LinkedList. To see the differences between using ArrayList and LinkedList in a larger routine, we look at some methods that operate on a List. First, suppose we construct a List by adding items at the end. O 1 (  ) O 1 (  ) O 1 (  ) figure 6.20 A doubly linked list b a c d first last O 1 (  ) O( N ) O( N )

6.5 the List interface public static void makeList1( List<Integer> lst, int N )     {         lst.clear( );         for( int i = 0; i < N; i++ )             lst.add( i );     } Irregardless of whether an ArrayList or LinkedList is passed as a parameter, the running time of makeList1 is   because each call to add, being at the end of the list, takes constant time. On the other hand, if we construct a List by adding items at the front,     public static void makeList2( List<Integer> lst, int N )     {         lst.clear( );         for( int i = 0; i < N; i++ )             lst.add( 0, i );     } the running time is   for a LinkedList, but O( N 2 ) for an ArrayList, because in an ArrayList, adding at the front is an   operation. The next routine attempts to compute the sum of the numbers in a List:     public static int sum( List<Integer> lst )     {         int total = 0;         for( int i = 0; i < N; i++ )             total += lst.get( i );     } Here, the running time is   for an ArrayList, but O( N 2 ) for a LinkedList, because in a LinkedList, calls to get are   operations. Instead, use an enhanced for loop, which will be make the running time   for any List, because the iterator will efﬁciently advance from one item to the next. figure 6.21 Single-operation costs for ArrayList and LinkedList ArrayList LinkedList add/remove at end add/remove at front get/set contains O 1 (  ) O 1 (  ) O( N ) O 1 (  ) O 1 (  ) O( N ) O( N ) O( N ) O( N ) O( N ) O( N ) O( N ) O( N ) O( N )

chapter 6 the collections api 6.5.4   removing from and adding  to the middle of a List The List interface contains two operations: void add( int idx, AnyType x ); void remove( int idx ); that allow the adding of an item to a speciﬁed index and removing of an item from a speciﬁed index. For an ArrayList, these operations are in general  , because of the item shifting that is required. For a LinkedList, in principle one would expect that if we know where the change is being made, then we should be able to do it efﬁciently by splicing links in the linked list. For instance, it is easy to see that in principle, removing a single node from a doubly linked list requires changing some links in the preceding and succeeding nodes. However, these operations are still   in a LinkedList because it takes   time to ﬁnd the node.  This is precisely why the Iterator provides a remove method. The idea is that often an item is being removed only after we have examined it and decided to discard it. This is similar to the idea of picking up items from the ﬂoor: as you search the ﬂoor, if you see an item, you immediately pick it up because you are already there.  As an example, we provide a routine that removes all even-valued items in a list. Thus if the list contains 6, 5, 1, 4, 2, then after the method is invoked it will contain 5, 1. There are several possible ideas for an algorithm that deletes items from the list as they are encountered. Of course, one idea is to construct a new list containing all the odd numbers, and then clear the original list and copy the odd numbers back into it. But we are more interested in writing a clean version that avoids making a copy, and instead removes items from the list as they are encountered. This is almost certainly a losing strategy for an ArrayList, since removing from almost anywhere in an ArrayList is expensive. (It is possible to design a different algorithm for ArrayList that works in place, but let us not worry about that now.) In a LinkedList, there is some hope, since as we know, removing from a known position can be done efﬁciently by rearranging some links. Figure 6.22 shows the ﬁrst attempt. On an ArrayList, as expected, the remove is not efﬁcient, so the routine takes quadratic time. A LinkedList exposes two problems. First, the call to get is not efﬁcient, so the routine takes quadratic time. Additionally, the call to remove is equally inefﬁcient, because as we have seen, it is expensive to get to position i. O( N ) O( N ) O( N )

6.5 the List interface Figure 6.23 shows one attempt to rectify the problem. Instead of using get, we use an iterator to step through the list. This is efﬁcient. But then we use the Collection’s remove method to remove an even-valued item. This is not an efﬁcient operation because the remove method has to search for the item again, which takes linear time. But if we run the code, we ﬁnd out that the situation is even worse: the program generates a ConcurrentModificationException because when an item is removed, the underlying iterator used by the enhanced for loop is invalidated. (The code in Figure 6.22 explains why: we cannot expect the enhanced for loop to understand that it must advance only if an item is not removed.) Figure 6.24 shows an idea that works: after the iterator ﬁnds an evenvalued item, we can use the iterator to remove the value it has just seen. For a LinkedList, the call to the iterator’s remove method is only constant time, because the iterator is at (or near) the node that needs to be removed. Thus, for a LinkedList, the entire routine takes linear time, rather than quadratic time. figure 6.22 Removes the even  numbers in a list;  quadratic on all  types of lists public static void removeEvensVer1( List<Integer> lst ) { int i = 0; while( i < lst.size( ) ) if( lst.get( i ) % 2 == 0 )     lst.remove( i ); else     i++; } figure 6.23 Removes the even  numbers in a list;  doesn’t work  because of  ConcurrentModificationException public static void removeEvensVer2( List<Integer> lst ) { for( Integer x : lst )     if( x % 2 == 0 )         lst.remove( x ); } figure 6.24 Removes the even  numbers in a list;  quadratic on  ArrayList, but linear  time for LinkedList public static void removeEvensVer3( List<Integer> lst ) {  Iterator<Integer> itr = lst.iterator( );  while( itr.hasNext( ) )  if( itr.next( ) % 2 == 0 )  itr.remove( ); 8  }

chapter 6 the collections api For an ArrayList, even though the iterator is at the point that needs to be removed, the remove is still expensive, because array items must be shifted, so as expected, the entire routine still takes quadratic time for an ArrayList. If we run the code in Figure 6.24, passing a LinkedList<Integer>, it takes 0.015 sec. for a 400,000 item list, and 0.031 sec. for an 800,000 item LinkedList, and is clearly a linear-time routine, because the running time increases by the same factor as the input size. When we pass an ArrayList<Integer>, the routine takes about 1  minutes for a 400,000 item ArrayList, and about ﬁve minutes for an 800,000 item ArrayList; the four-fold increase in running time when the input increases by only a factor of two is consistent with quadratic behavior. A similar situation occurs for add. The Iterator interface does not provide an add method, but ListIterator does. We have not shown that method in Figure 6.17. Exercise 6.23 asks you to use it. 6.6 stacks and queues In this section, we describe two containers: the stack and queue. In principle, both have very simple interfaces (but not in the Collections API) and very efﬁcient implementations. Even so, as we will see, they are very useful data structures. 6.6.1   stacks A stack restricts  access to the most  recently inserted  item. A stack is a data structure in which access is restricted to the most recently inserted item. It behaves very much like the common stack of bills, stack of plates, or stack of newspapers. The last item added to the stack is placed on the top and is easily accessible, whereas items that have been in the stack for a while are more difﬁcult to access. Thus the stack is appropriate if we expect to access only the top item; all other items are inaccessible. In a stack, the three natural operations of insert, remove, and find are renamed push, pop, and top. These basic operations are illustrated in Figure 6.25. The interface shown in Figure 6.26 illustrates the typical protocol. It is similar to the protocol previously seen in Figure 6.1. By pushing items and then popping them, we can use the stack to reverse the order of things. Stack operations  take a constant  amount of time. Each stack operation should take a constant amount of time, independent of the number of items in the stack. By analogy, ﬁnding today’s newspaper in a stack of newspapers is fast, no matter how deep the stack is. However, arbitrary access in a stack is not efﬁciently supported, so we do not list it as an option in the protocol. 4-

6.6 stacks and queues What makes the stack useful are the many applications for which we need to access only the most recently inserted item. An important use of stacks is in compiler design. 6.6.2   stacks and computer languages Compilers check your programs for syntax errors. Often, however, a lack of one symbol (e.g., a missing comment-ender */ or }) causes the compiler to spill out a hundred lines of diagnostics without identifying the real error; this is especially true when using anonymous classes. A useful tool in this situation is a program that checks whether everything is balanced, that is, every { corresponds to a }, every [ to a ], and so on. The sequence [()] is legal but [(]) is not—so simply counting the numbers of each symbol is insufﬁcient. (Assume for now that we are processing only a sequence of tokens and will not worry about problems such as the character constant '{' not needing a matching '}'.) push pop, top Stack figure 6.25 The stack model:  Input to a stack is by  push, output is by top, and deletion is by pop. figure 6.26 Protocol for the stack // Stack protocol package weiss.nonstandard; public interface Stack<AnyType> {     void    push( AnyType x ); // insert     void    pop( );            // remove     AnyType top( );            // find     AnyType topAndPop( );      // find + remove     boolean isEmpty( );     void    makeEmpty( ); }

chapter 6 the collections api A stack can be  used to check for  unbalanced symbols. A stack is useful for checking unbalanced symbols because we know that when a closing symbol such as ) is seen, it matches the most recently seen unclosed (. Therefore, by placing opening symbols on a stack, we can easily check that a closing symbol makes sense. Speciﬁcally, we have the following algorithm. 1. Make an empty stack. 2. Read symbols until the end of the ﬁle. a. If the token is an opening symbol, push it onto the stack. b. If it is a closing symbol and if the stack is empty, report an error. c. Otherwise, pop the stack. If the symbol popped is not the corresponding opening symbol, report an error. 3. At the end of the ﬁle, if the stack is not empty, report an error. In Section 11.1 we will develop this algorithm to work for (almost) all Java programs. Details include error reporting, and processing of comments, strings, and character constants, as well as escape sequences. The stack is used  to implement  method calls in  most programming languages. The algorithm to check balanced symbols suggests a way to implement method calls. The problem is that, when a call is made to a new method, all the variables local to the calling method need to be saved by the system; otherwise, the new method would overwrite the calling routine’s variables. Furthermore, the current location in the calling routine must be saved so that the new method knows where to go after it is done. The reason that this problem is similar to balancing symbols is because a method call and a method return are essentially the same as an open parenthesis and a closed parenthesis, so the same ideas should apply. This indeed is the case: As discussed in Section 7.3, the stack is used to implement method calls in most programming languages. The operator precedence parsing algorithm uses a  stack to evaluate  expressions. A ﬁnal important application of the stack is the evaluation of expressions in computer languages. In the expression 1+2*3, we see that at the point that the * is encountered, we have already read the operator + and the operands 1 and 2. Does * operate on 2, or 1+2? Precedence rules tell us that * operates on 2, which is the most recently seen operand. After the 3 is seen, we can evaluate 2*3 as 6 and then apply the + operator. This process suggests that operands and intermediate results should be saved on a stack. It also suggests that the operators be saved on the stack (since the + is held until the higher precedence * is evaluated). An algorithm that uses this strategy is operator precedence parsing, and is described in Section 11.2. 6.6.3   queues The queue restricts access to  the least recently  inserted item. Another simple data structure is the queue, which restricts access to the least recently inserted item. In many cases being able to ﬁnd and/or remove the most-recently inserted item is important. But in an equal number of cases, it is not only unimportant, it is actually the wrong thing to do. In a multiprocessing

6.7 sets system, for example, when jobs are submitted to a printer, we expect the least recent or most senior job to be printed ﬁrst. This order is not only fair but it is also required to guarantee that the ﬁrst job does not wait forever. Thus you can expect to ﬁnd printer queues on all large systems. The basic operations supported by queues are the following: n enqueue, or insertion at the back of the line n dequeue, or removal of the item from the front of the line n getFront, or access of the item at the front of the line Queue operations  take a constant  amount of time. Figure 6.27 illustrates these queue operations. Historically, dequeue and getFront have been combined into one operation; we do this by having dequeue return a reference to the item that it has removed. Because the queue operations and the stack operations are restricted similarly, we expect that they should also take a constant amount of time per query. This is indeed the case. All of the basic queue operations take  time. We will see several applications of queues in the case studies. 6.6.4   stacks and queues in the collections api The Collections  API provides a  Stack class but no  queue class. Java  5 adds a Queue interface. The Collections API provides a Stack class but no queue class. The Stack methods are push, pop, and peek. However, the Stack class extends Vector and is slower than it needs to be; like Vector, its use is no longer in vogue and can be replaced with List operations. Before Java 1.4, the only java.util support for queue operations was to use a LinkedList (e.g., addLast, removeFirst, and getFirst). Java 5 adds a Queue interface, part of which is shown in Figure 6.28. However, we still must use LinkedList methods. The new methods are add, remove, and element. 6.7 sets A Set contains no  duplicates. A Set is a container that contains no duplicates. It supports all of the Collection methods. Most importantly, recall that as we discussed in Section 6.5.3, contains for a List is inefﬁcient, regardless of whether the List is an ArrayList or a figure 6.27 The queue model:  Input is by enqueue, output is by getFront, and deletion is by  dequeue. enqueue dequeue getFront Queue O 1 (  )

chapter 6 the collections api LinkedList. A library implementation of Set is expected to efﬁciently support contains. Similarly, the Collection remove method (which has as a parameter a speciﬁed object, not a speciﬁed index) for a List is inefﬁcient because it is implied that the ﬁrst thing remove must do is to ﬁnd the item being removed; essentially this makes remove at least as difﬁcult as contains. For a Set, remove is expected to also be efﬁciently implemented. And ﬁnally, add is expected to have an efﬁcient implementation. There is no Java syntax that can be used to specify that an operation must meet a time constraint or may not contain duplicates; thus Figure 6.29 illustrates that the Set interface does little more than declare a type. figure 6.28 Possible Queue interface package weiss.util; /**  * Queue interface.  */ public interface Queue<AnyType> extends Collection<AnyType> {     /**      * Returns but does not remove the item at the "front"      * of the queue.      * @return the front item of null if the queue is empty.      * @throws NoSuchElementException if the queue is empty.      */     AnyType element( );     /**      * Returns and removes the item at the "front"      * of the queue.      * @return the front item.      * @throws NoSuchElementException if the queue is empty.      */     AnyType remove( ); } figure 6.29 Possible Set interface package weiss.util; /**  * Set interface.  */ public interface Set<AnyType> extends Collection<AnyType> { }

6.7 sets The SortedSet is an  ordered container.  It allows no  duplicates. A SortedSet is a Set that maintains (internally) its items in sorted order. Objects that are added into the SortedSet must either be comparable, or a Comparator has to be provided when the container is instantiated. A SortedSet supports all of the Set methods, but its iterator is guaranteed to step through items in its sorted order. The SortedSet also allows us to ﬁnd the smallest and largest item. The interface for our subset of SortedSet is shown in Figure 6.30. 6.7.1   the TreeSet class The TreeSet is an  implementation of  SortedSet. The SortedSet is implemented by a TreeSet. The underlying implementation of the TreeSet is a balanced-binary search tree and is discussed in Chapter 19. By default, ordering uses the default comparator. An alternate ordering can be speciﬁed by providing a comparator to the constructor. As an example, Figure 6.31 illustrates how a SortedSet that stores strings is constructed. The call to printCollection will output elements in decreasing sorted order. The SortedSet, like all Sets, does not allow duplicates. Two items are considered equal if the comparator’s compare method returns 0. figure 6.30 Possible SortedSet interface package weiss.util; /**  * SortedSet interface.  */ public interface SortedSet<AnyType> extends Set<AnyType> {     /**      * Return the comparator used by this SortedSet.      * @return the comparator or null if the      * default comparator is used.      */     Comparator<? super AnyType> comparator( );     /**      * Find the smallest item in the set.      * @return the smallest item.      * @throws NoSuchElementException if the set is empty.      */     AnyType first( );     /**      * Find the largest item in the set.      * @return the largest item.      * @throws NoSuchElementException if the set is empty.      */ AnyType last( ); }

chapter 6 the collections api In Section 5.6, we examined the static searching problem and saw that if the items are presented to us in sorted order, then we can support the find operation in logarithmic worst-case time. This is static searching because, once we are presented with the items, we cannot add or remove items. The SortedSet allows us to add and remove items. We are hoping that the worst-case cost of the contains, add, and remove operations is   because that would match the bound obtained for the static binary search. Unfortunately, for the simplest implementation of the TreeSet, this is not the case. The average case is logarithmic, but the worst case is   and occurs quite frequently. However, by applying some algorithmic tricks, we can obtain a more complex structure that does indeed have cost per operation. The Collections API TreeSet is guaranteed to have this performance, and in Chapter 19, we discuss how to obtain it using the binary search tree and its variants, and provide an implementation of the TreeSet, with an iterator. We can also use a  binary search tree  to access the Kth smallest item in  logarithmic time. We mention in closing that although we can ﬁnd the smallest and largest item in a SortedSet in   time, ﬁnding the Kth smallest item, where K is a parameter, is not supported in the Collections API. However, it is possible to perform this operation in   time, while preserving the running time of the other operations, if we do more work. 6.7.2   the HashSet class The HashSet implements the Set interface. It does  not require a  comparator. In addition to the TreeSet, the Collections API provides a HashSet class that implements the Set interface. The HashSet differs from the TreeSet in that it cannot be used to enumerate items in sorted order, nor can it be used to obtain the smallest or largest item. Indeed, the items in the HashSet do not have to be comparable in any way. This means that the HashSet is less powerful than the TreeSet. If being able to enumerate the items in a Set in sorted order is not important, then it is often preferable to use the HashSet because not having to maintain sorted order allows the HashSet to obtain faster performance. To do so, elements placed in the HashSet must provide hints to the HashSet algorithms. figure 6.31 An illustration of the TreeSet, using reverse order public static void main( String [] args )     {         Set<String> s = new TreeSet<String>( Collections.reverseOrder( ) );         s.add( "joe" );         s.add( "bob" );         s.add( "hal" );         printCollection( s );    // Figure 6.11 } O(log N ) O( N ) O(log N ) O(log N ) O(log N )

6.7 sets This is done by having each element implement a special hashCode method; we describe this method later in this subsection. Figure 6.32 illustrates the use of the HashSet. It is guaranteed that if we iterate through the entire HashSet, we will see each item once, but the order that the items are visited is unknown. It is almost certain that the order will not be the same as the order of insertion, nor will it be any kind of sorted order. Like all Sets, the HashSet does not allow duplicates. Two items are considered equal if the equals method says so. Thus, any object that is inserted into the HashSet must have a properly overridden equals method. Recall that in Section 4.9, we discussed that it is essential that equals is overridden (by providing a new version that takes an Object as parameter) rather than overloaded. implementing equals and hashCode equals must be  symmetric; this is  tricky when inheritance is involved. Overriding equals is very tricky when inheritance is involved. The contract for equals states that if p and q are not null, p.equals(q) should return the same value as q.equals(p). This does not occur in Figure 6.33. In that example, clearly b.equals(c) returns true, as expected. a.equals(b) also returns true, because BaseClass’s equals method is used, and that only compares the x components. However, b.equals(a) returns false, because DerivedClass’s equals method is used, and the instanceof test will fail (a is not an instance of DerivedClass) at line 29. Solution 1 is to not  override equals below the base  class. Solution 2 is  to require identically typed objects  using getClass. There are two standard solutions to this problem. One is to make the equals method ﬁnal in BaseClass. This avoids the problem of conﬂicting equals. The other solution is to strengthen the equals test to require that the types are identical, and not simply compatible, since the one-way compatibility is what breaks equals. In this example, a BaseClass and DerivedClass object would never be declared equal. Figure 6.34 shows a correct implementation. Line 8 contains the idiomatic test. getClass returns a special object of type Class (note the capital C) that represents information about any object’s class. getClass is a ﬁnal method in the Object class. If when invoked on two different objects it returns the same Class instance, then the two objects have identical types. figure 6.32 An illustration of the  HashSet, where items  are output in some  order public static void main( String [] args )     {         Set<String> s = new HashSet<String>( );         s.add( "joe" );         s.add( "bob" );         s.add( "hal" );         printCollection( s );    // Figure 6.11 }

chapter 6 the collections api figure 6.33 An illustration of a  broken implementation  of equals class BaseClass {     public BaseClass( int i )       { x = i; }     public boolean equals( Object rhs )     {         // This is the wrong test (ok if final class)         if( !( rhs instanceof BaseClass ) )             return false;         return x == ( (BaseClass) rhs ).x;     }     private int x; } class DerivedClass extends BaseClass {     public DerivedClass( int i, int j )     {         super( i );         y = j;     }     public boolean equals( Object rhs )     {         // This is the wrong test.         if( !( rhs instanceof DerivedClass ) )             return false;         return super.equals( rhs ) &&                y == ( (DerivedClass) rhs ).y;     }     private int y; } public class EqualsWithInheritance {     public static void main( String [ ] args )     {         BaseClass a = new BaseClass( 5 );         DerivedClass b = new DerivedClass( 5, 8 );         DerivedClass c = new DerivedClass( 5, 8 );         System.out.println( "b.equals(c): " + b.equals( c ) );         System.out.println( "a.equals(b): " + a.equals( b ) );         System.out.println( "b.equals(a): " + b.equals( a ) );     } }

6.7 sets The hashCode method must be  overridden, if  equals is overridden, or the HashSet will not work. When using a HashSet, we must also override the special hashCode method that is speciﬁed in Object; hashCode returns an int. Think of hashCode as providing a trusted hint of where the items are stored. If the hint is wrong, the item is not found, so if two objects are equal, they should provide identical hints. The contract for hashCode is that if two objects are declared equal by the equals method, then the hashCode method must return the same value for them. If this contract is violated, the HashSet will fail to ﬁnd objects, even if equals declares that there is a match. If equals declares the objects are not equal, the hashCode method should return a different value for them, but this is not required. However, it is very beneﬁcial for HashSet performance if hashCode rarely produces identical results for unequal objects. How hashCode and HashSet interact is discussed in Chapter 20. figure 6.34 Correct implementation of  equals class BaseClass {     public BaseClass( int i )       { x = i; }     public boolean equals( Object rhs )     {         if( rhs == null || getClass( ) != rhs.getClass( ) )             return false;         return x == ( (BaseClass) rhs ).x;     }     private int x; } class DerivedClass extends BaseClass {     public DerivedClass( int i, int j )     {         super( i );         y = j;     }     public boolean equals( Object rhs )     {           // Class test not needed; getClass() is done           // in superclass equals         return super.equals( rhs ) &&                y == ( (DerivedClass) rhs ).y;     }     private int y; }

chapter 6 the collections api Figure 6.35 illustrates a SimpleStudent class in which two SimpleStudents are equal if they have the same name (and are both SimpleStudents). This could be overridden using the techniques in Figure 6.34 as needed, or this method could be declared final. If it was declared final, then the test that is present allows only two identically typed SimpleStudents to be declared equal. If, with a ﬁnal equals, we replace the test at line 40 with an instanceof test, then any two objects in the hierarchy can be declared equal if their names match. The hashCode method at lines 47 and 48 simply uses the hashCode of the name ﬁeld. Thus if two SimpleStudent objects have the same name (as declared by equals) they will have the same hashCode, since, presumably, the implementors of String honored the contract for hashCode. The accompanying test program is part of a larger test that illustrates all the basic containers. Observe that if hashCode is unimplemented, all three SimpleStudent objects will be added to the HashSet because the duplicate will not be detected. It turns out that on average, the HashSet operations can be performed in constant time. This seems like an astounding result because it means that the cost of a single HashSet operation does not depend on whether the HashSet contains 10 items or 10,000 items. The theory behind the HashSet is fascinating and is described in Chapter 20. 6.8 maps A Map is used to  store a collection  of entries that  consists of keys and their values. The Map maps keys  to values.  A Map is used to store a collection of entries that consists of keys and their values. The Map maps keys to values. Keys must be unique, but several keys can map to the same value. Thus, values need not be unique. There is a SortedMap interface that maintains the map logically in key-sorted order. Not surprisingly, there are two implementations: the HashMap and TreeMap. The HashMap does not keep keys in sorted order, whereas the TreeMap does. For simplicity, we do not implement the SortedMap interface but we do implement HashMap and TreeMap. The Map can be implemented as a Set instantiated with a pair (see Section 3.9), whose comparator or equals/hashCode implementation refers only to the key. The Map interface does not extend Collection; instead it exists on its own. A sample interface that contains the most important methods is shown in Figures 6.36 and 6.37. Most of the methods have intuitive semantics. put is used to add a key/ value pair, remove is used to remove a key/value pair (only the key is speciﬁed), and get returns the value associated with a key. null values are allowed, which complicates issues for get, because the return value from get will not

6.8 maps /**  * Test program for HashSet.  */ class IteratorTest {     public static void main( String [ ] args )     {         List<SimpleStudent> stud1 = new ArrayList<SimpleStudent>( );         stud1.add( new SimpleStudent( "Bob", 0 ) );         stud1.add( new SimpleStudent( "Joe", 1 ) );         stud1.add( new SimpleStudent( "Bob", 2 ) ); // duplicate    // Will only have 2 items, if hashCode is            // implemented. Otherwise will have 3 because            // duplicate will not be detected.         Set<SimpleStudent>  stud2 = new HashSet<SimpleStudent>( stud1 );         printCollection( stud1 ); // Bob Joe Bob (unspecified order)         printCollection( stud2 ); // Two items in unspecified order     } } /**  * Illustrates use of hashCode/equals for a user-defined class.  * Students are ordered on basis of name only.  */ class SimpleStudent implements Comparable<SimpleStudent> {     String name;     int id;     public SimpleStudent( String n, int i )       { name = n; id = i; }     public String toString( )       { return name + " " + id; }     public boolean equals( Object rhs )     {         if( rhs == null || getClass( ) != rhs.getClass( ) )             return false;         SimpleStudent other = (SimpleStudent) rhs;         return name.equals( other.name );     }     public int hashCode( )       { return name.hashCode( ); } } figure 6.35 Illustrates the equals and hashCode methods for use in HashSet

chapter 6 the collections api figure 6.36 A sample Map interface (part 1) package weiss.util; /**  * Map interface.  * A map stores key/value pairs.  * In our implementations, duplicate keys are not allowed.  */ public interface Map<KeyType,ValueType> extends java.io.Serializable {     /**      * Returns the number of keys in this map.      */     int size( );     /**      * Tests if this map is empty.      */     boolean isEmpty( );     /**      * Tests if this map contains a given key.      */     boolean containsKey( KeyType key );     /**      * Returns the value that matches the key or null      * if the key is not found. Since null values are allowed,      * checking if the return value is null may not be a      * safe way to ascertain if the key is present in the map.      */     ValueType get( KeyType key );     /**      * Adds the key/value pair to the map, overriding the      * original value if the key was already present.      * Returns the old value associated with the key, or      * null if the key was not present prior to this call.      */     ValueType put( KeyType key, ValueType value );     /**      * Removes the key and its value from the map.      * Returns the previous value associated with the key,      * or null if the key was not present prior to this call.      */     ValueType remove( KeyType key );

6.8 maps figure 6.37 A sample Map interface (part 2) /**  * Removes all key/value pairs from the map.   */     void clear( );     /**      * Returns the keys in the map.      */     Set<KeyType> keySet( );     /**      * Returns the values in the map. There may be duplicates.      */     Collection<ValueType> values( );     /**      * Return a set of Map.Entry objects corresponding to      * the key/value pairs in the map.      */      Set<Entry<KeyType,ValueType>> entrySet( );     /**      * Interface used to access the key/value pairs in a map.      * From a map, use entrySet().iterator to obtain an iterator      * over a Set of pairs. The next() method of this iterator      * yields objects of type Map.Entry<KeyType,ValueType>.      */     public interface Entry<KeyType,ValueType> extends java.io.Serializable     {         /**          * Returns this pair's key.          */         KeyType getKey( );         /**          * Returns this pair's value.          */         ValueType getValue( );         /**          * Change this pair's value.          * @return the old value associated with this pair.          */         ValueType setValue( ValueType newValue );     } }

chapter 6 the collections api distinguish between a failed search and a successful search that returns null for the value. containsKey can be used if null values are known to be in the map. The Map interface does not provide an iterator method or class. Instead it returns a Collection that can be used to view the contents of the map. The keySet method gives a Collection that contains all the keys. Since duplicate keys are not allowed, the result of keySet is a Set, for which we can obtain an iterator. If the Map is a SortedMap, the Set is a SortedSet. Similarly, the values method returns a Collection that contains all the values. This really is a Collection, since duplicate values are allowed. Map.Entry abstracts the  notion of a pair in  the map. Finally, the entrySet method returns a collection of key/value pairs. Again, this is a Set, because the pairs must have different keys. The objects in the Set returned by the entrySet are pairs; there must be a type that represents key/value pairs. This is speciﬁed by the Entry interface that is nested in the Map interface. Thus the type of object that is in the entrySet is Map.Entry. Figure 6.38 illustrates the use of the Map with a TreeMap. An empty map is created at line 23 and then populated with a series of put calls at lines 25 to 29. The last call to put simply replaces a value with “unlisted”. Lines 31 and 32 print the result of a call to get, which is used to obtain the value for the key "Jane Doe". More interesting is the printMap routine that spans lines 8 to 19. In printMap, at line 12, we obtain a Set containing Map.Entry pairs. From the Set, we can use an enhanced for loop to view the Map.Entrys, and we can obtain the key and value information using getKey and getValue, as shown on lines 16 and 17. keySet, values, and  entrySet return  views. Returning to main, we see that keySet returns a set of keys (at line 37) that can be printed at line 38 by calling printCollection (in Figure 6.11); similarly at lines 41 and 42, values returns a collection of values that can be printed. More interesting, the key set and value collection are views of the map, so changes to the map are immediately reﬂected in the key set and value collection, and removals from the key set or value set become removals from the underlying map. Thus line 44 removes not only the key from the key set but also the associated entry from the map. Similarly, line 45 removes an entry from the map. Thus the printing at line 49 reﬂects a map with two entries removed. Views themselves are an interesting concept and we will discuss speciﬁcs of how they are implemented later when we implement the map classes. Some further examples of views are discussed in Section 6.10. Figure 6.39 illustrates another use of the map, in a method that returns items in a list that appear more than once. In this code, a map is being used internally to group the duplicates together: the key of the map is an item, and the value is the number of times the item has occurred. Lines 8–12 illustrate the typical idea seen in building up a map this way. If the item has never been

6.8 maps figure 6.38 An illustration using the Map interface import java.util.Map; import java.util.TreeMap; import java.util.Set; import java.util.Collection; public class MapDemo {     public static <KeyType,ValueType>     void printMap( String msg, Map<KeyType,ValueType> m )     {         System.out.println( msg + ":" );         Set<Map.Entry<KeyType,ValueType>> entries = m.entrySet( );         for( Map.Entry<KeyType,ValueType> thisPair : entries )         {             System.out.print( thisPair.getKey( ) + ": " );             System.out.println( thisPair.getValue( ) );         }     }     public static void main( String [ ] args )     {         Map<String,String> phone1 = new TreeMap<String,String>( );         phone1.put( "John Doe", "212-555-1212" );         phone1.put( "Jane Doe", "312-555-1212" );         phone1.put( "Holly Doe", "213-555-1212" );          phone1.put( "Susan Doe", "617-555-1212" );          phone1.put( "Jane Doe", "unlisted" );         System.out.println( "phone1.get(\"Jane Doe\"): " +                              phone1.get( "Jane Doe" ) );         System.out.println( "\nThe map is: " );         printMap( "phone1", phone1 );         System.out.println( "\nThe keys are: " );         Set<String> keys = phone1.keySet( );         printCollection( keys );         System.out.println( "\nThe values are: " );         Collection<String> values = phone1.values( );         printCollection( values );         keys.remove( "John Doe" );         values.remove( "unlisted" );         System.out.println( "After John Doe and 1 unlisted are removed" ); System.out.println( "\nThe map is: " );         printMap( "phone1", phone1 );     } }

chapter 6 the collections api placed in the map, we do so with a count of 1. Otherwise, we update the count. Note the judicious use of autoboxing and unboxing. Then at lines 15–17, we use an iterator to traverse through the entry set, obtaining keys that appear with a count of two or more in the map. 6.9 priority queues The priority queue supports access of  the minimum item  only. Although jobs sent to a printer are generally placed on a queue, that might not always be the best thing to do. For instance, one job might be particularly important, so we might want to allow that job to be run as soon as the printer is available. Conversely, when the printer ﬁnishes a job, and several 1-page jobs and one 100-page job are waiting, it might be reasonable to print the long job last, even if it is not the last job submitted. (Unfortunately, most systems do not do this, which can be particularly annoying at times.) Similarly, in a multiuser environment the operating system scheduler must decide which of several processes to run. Generally, a process is allowed to run only for a ﬁxed period of time. A poor algorithm for such a procedure involves use of a queue. Jobs are initially placed at the end of the queue. The scheduler repeatedly takes the ﬁrst job from the queue, runs it until either it ﬁnishes or its time limit is up, and places it at the end of the queue if it does not ﬁnish. Generally, this strategy is not appropriate because short jobs must figure 6.39 A typical use of a map  public static List<String> listDuplicates( List<String> coll ) {     Map<String,Integer> count = new TreeMap<String,Integer>( );     List<String> result = new ArrayList<String>( );     for( String word : coll )     {         Integer occurs = count.get( word );         if( occurs == null )             count.put( word, 1 );         else             count.put( word, occurs + 1 );     }     for( Map.Entry<String,Integer> e : count.entrySet( ) )         if( e.getValue( ) >= 2 )            result.add( e.getKey( ) );     return result; }

6.9 priority queues wait and thus seem to take a long time to run. Clearly, users who are running an editor should not see a visible delay in the echoing of typed characters. Thus short jobs (that is, those using fewer resources) should have precedence over jobs that have already consumed large amounts of resources. Furthermore, some resource-intensive jobs, such as jobs run by the system administrator, might be important and should also have precedence. If we give each job a number to measure its priority, then the smaller number (pages printed, resources used) tends to indicate greater importance. Thus we want to be able to access the smallest item in a collection of items and remove it from the collection. To do so, we use the findMin and deleteMin operations. The data structure that supports these operations is the priority queue and supports access of the minimum item only. Figure 6.40 illustrates the basic priority queue operations. Although the priority queue is a fundamental data structure, before Java 5 there was no implementation of it in the Collections API. A SortedSet was not sufﬁcient because it is important for a priority queue to allow duplicate items. In Java 5, the PriorityQueue is a class that implements the Queue interface. Thus insert, findMin, and deleteMin are expressed via calls to add, element, and remove. The PriorityQueue can be constructed either with no parameters, a comparator, or another compatible collection. Throughout the text, we often use the terms insert, findMin, and deleteMin to describe the priority queue methods. Figure 6.41 illustrates the use of the priority queue. The binary heap implements the  priority queue in  logarithmic time  per operation with  little extra space. As the priority queue supports only the deleteMin and findMin operations, we might expect performance that is a compromise between the constant-time queue and the logarithmic time set. Indeed, this is the case. The basic priority queue supports all operations in logarithmic worst-case time, uses only an array, supports insertion in constant average time, is simple to implement, and is known as a binary heap. This structure is one of the most elegant data structures known. In Chapter 21, we provide details on the implementation of the binary heap. An alternate implementation that supports an additional decreaseKey operation is the pairing heap, described in Chapter 23. Because there are many efﬁcient implementations of priority queues, it is unfortunate insert Priority Queue deleteMin findMin figure 6.40 The priority queue  model: Only the  minimum element is  accessible.

chapter 6 the collections api that the library designers did not choose to make PriorityQueue an interface. Nonetheless, the PriorityQueue implementation in Java 5 is sufﬁcient for most priority queue applications.  An important use  of priority queues  is event-driven  simulation. An important application of the priority queue is event-driven simulation. Consider, for example, a system such as a bank in which customers arrive and wait in line until one of K tellers is available. Customer arrival is governed by a probability distribution function, as is the service time (the amount of time it takes a teller to provide complete service to one customer). We are interested in statistics such as how long on average a customer has to wait or how long a line might be. With certain probability distributions and values of K, we can compute these statistics exactly. However, as K gets larger, the analysis becomes considerably more difﬁcult, so the use of a computer to simulate the operation of the bank is appealing. In this way, the bank’s ofﬁcers can determine how many tellers are needed to ensure reasonably smooth service. An event-driven simulation consists of processing events. The two events here are (1) a customer arriving and (2) a customer departing, thus freeing up a teller. At any point we have a collection of events waiting to happen. To run the simulation, we need to determine the next event; this is the event whose time of occurrence is minimum. Hence, we use a priority queue that extracts the event of minimum time to process the event list efﬁciently. We present a complete discussion and implementation of event-driven simulation in Section 13.2. figure 6.41 A routine to  demonstrate the  PriorityQueue import java.util.PriorityQueue; public class PriorityQueueDemo {     public static <AnyType extends Comparable<? super AnyType>>     void dumpPQ( String msg, PriorityQueue<AnyType> pq )     {         System.out.println( msg + ":" );         while( !pq.isEmpty( ) )             System.out.println( pq.remove( ) );     }     // Do some inserts and removes (done in dumpPQ).     public static void main( String [ ] args )     {         PriorityQueue<Integer> minPQ = new PriorityQueue<Integer>( );         minPQ.add( 4 );         minPQ.add( 3 );         minPQ.add( 5 );         dumpPQ( "minPQ", minPQ );     } }

6.10 views in the collections api 6.10 views in the collections api In Section 6.8, we saw an illustration of methods that return “views” of a map. Speciﬁcially, keySet returns a view representing a Set of all keys in the map; values returns a view representing a Collection of all values in the map; entrySet returns a view representing a Set of all entries in the map. Changes to the map will be reﬂected in any view, and changes to any view will reﬂect on the map and also the other views. This behavior was demonstrated in Figure 6.38 by using the remove method on the key set and value collection. There are many other examples of views in the Collections API. In this section, we discuss two such uses of views. 6.10.1   the subList method for Lists The subList method takes two parameters representing list indices and returns a view of a List whose range includes the ﬁrst index and excludes the last index. Thus,     System.out.println( theList.subList( 3, 8 ) ); prints the ﬁve items in the sublist. Because subList is a view, nonstructural changes to the sublist reﬂect back in the original, and vice-versa. However, as is the case with iterators, a structural modiﬁcation to the original list invalidates the sublist. Finally, perhaps most importantly, because the sublist is a view, and not a copy of a portion of the original list, the cost of invoking subList is O( 1 ), and the operations on the sublist retain their same efﬁciency. 6.10.2   the headSet, subSet, and tailSet methods for  SortedSets The SortedSet class has methods that return views of the Set: SortedSet<AnyType> subSet(AnyType fromElement, AnyTypet toElement); SortedSet<AnyType> headSet(AnyType toElement); SortedSet<AnyType> tailSet(AnyType fromElement); fromElement and toElement in effect partition the SortedSet into three subsets: the headSet, subSet (middle part), and tailSet. Figure 6.42 illustrates this by partitioning a number line. In these methods, toElement is not included in any range, but fromElement is. In Java 6, there are additional overloads of these methods that allow the caller to control whether fromElement and toElement are included in any particular range.

chapter 6 the collections api For instance, in a Set<String> words, the number of words starting with the letter 'x' is given by:     words.subSet( "x", true, "y", false ).size( )   The rank of a value val in any Set s (i.e. if val is the third largest, it has rank 3)  is given by     s.tailSet( val ).size( ) Because the subsets are views, changes to the subsets reﬂect back in the original, and vice-versa, and structural modiﬁcations to either are reﬂected in the other. As is the case with lists, because the subsets are views, and not copies of a portion of the original set, the cost of invoking headSet, tailSet, or subSet is comparable to any of the other set operations, which will be , and the add, contains, and remove operations (but not size!) on the subset retain their same efﬁciency.

chapt er recursion A method that is partially deﬁned in terms of itself is called recursive. Like many languages, Java supports recursive methods. Recursion, which is the use of recursive methods, is a powerful programming tool that in many cases can yield both short and efﬁcient algorithms. In this chapter we explore how recursion works, thus providing some insight into its variations, limitations, and uses. We begin our discussion of recursion by examining the mathematical principle on which it is based: mathematical induction. Then we give examples of simple recursive methods and prove that they generate correct answers. In this chapter, we show n The four basic rules of recursion n Numerical applications of recursion, leading to implementation of an encryption algorithm n A general technique called divide and conquer n A general technique called dynamic programming, which is similar to recursion but uses tables instead of recursive method calls n A general technique called backtracking, which amounts to a careful exhaustive search

chapter 7 recursion 7.1 what is recursion? A recursive method  is a method that  directly or indirectly makes a call  to itself. A recursive method is a method that either directly or indirectly makes a call to itself. This action may seem to be circular logic: How can a method F solve a problem by calling itself? The key is that the method F calls itself on a different, generally simpler, instance. The following are some examples. n Files on a computer are generally stored in directories. Users may create subdirectories that store more ﬁles and directories. Suppose that we want to examine every ﬁle in a directory D, including all ﬁles in all subdirectories (and subsubdirectories, and so on). We do so by recursively examining every ﬁle in each subdirectory and then examining all ﬁles in the directory D (discussed in Chapter 18). n Suppose that we have a large dictionary. Words in dictionaries are deﬁned in terms of other words. When we look up the meaning of a word, we might not always understand the deﬁnition, so we might have to look up words in the deﬁnition. Likewise, we might not understand some of those, so we might have to continue this search for a while. As the dictionary is ﬁnite, eventually either we come to a point where we understand all the words in some deﬁnition (and thus understand that deﬁnition and can retrace our path through the other deﬁnitions), we ﬁnd that the deﬁnitions are circular and that we are stuck, or some word we need to understand is not deﬁned in the dictionary. Our recursive strategy to understand words is as follows: If we know the meaning of a word, we are done; otherwise, we look the word up in the dictionary. If we understand all the words in the deﬁnition, we are done. Otherwise, we ﬁgure out what the deﬁnition means by recursively looking up the words that we do not know. This procedure terminates if the dictionary is well deﬁned, but it can loop indefinitely if a word is circularly deﬁned. n Computer languages are frequently deﬁned recursively. For instance, an arithmetic expression is an object, or a parenthesized expression, or two expressions added to each other, and so on. Recursion is a powerful problem-solving tool. Many algorithms are most easily expressed in a recursive formulation. Furthermore, the most efﬁcient solutions to many problems are based on this natural recursive formulation. But you must be careful not to create circular logic that would result in inﬁnite loops. In this chapter we discuss the general conditions that must be satisﬁed by recursive algorithms and give several practical examples. It shows that sometimes algorithms that are naturally expressed recursively must be rewritten without recursion.

7.2 background: proofs by mathematical induction 7.2 background: proofs by  mathematical induction Induction is an  important proof  technique used to  establish theorems  that hold for positive integers. In this section we discuss proof by mathematical induction. (Throughout this chapter we omit the word mathematical when describing this technique.) Induction is commonly used to establish theorems that hold for positive integers. We start by proving a simple theorem, Theorem 7.1. This particular theorem can be easily established by using other methods, but often a proof by induction is the simplest mechanism. Obviously, the theorem is true for N = 1 because both the left-hand and righthand sides evaluate to 1. Further checking shows that it is true for 2 ≤N ≤10. However, the fact that the theorem holds for all N that are easy to check by hand does not imply that it holds for all N. Consider, for instance, numbers of the form . The ﬁrst ﬁve numbers (corresponding to   are 3, 5, 17, 257, and 65,537. These numbers are all prime. Indeed, at one time mathematicians conjectured that all numbers of this form are prime. That is not the case. We can easily check by computer that  . In fact, no other prime of the form   is known. A proof by induction shows that the  theorem is true for  some simple cases  and then shows  how to extend the  range of true cases  indeﬁnitely. A proof by induction is carried out in two steps. First, as we have just done, we show that the theorem is true for the smallest cases. We then show that if the theorem is true for the ﬁrst few cases, it can be extended to include the next case. For instance, we show that a theorem that is true for all must be true for  . Once we have shown how to extend the range of true cases, we have shown that it is true for all cases. The reason is that we can extend the range of true cases indeﬁnitely. We use this technique to prove Theorem 7.1.  For any integer  , the sum of the ﬁrst N integers, given by  , equals  . Theorem 7.1 N ≥ i i = N ∑ … N + + + = N N + ( ) 2 ⁄ 22k + k 4) ≤ ≤ + 6,700,417 × ˙ = 22k + N k ≤ ≤ N k + ≤ ≤ Clearly, the theorem is true for  . Suppose that the theorem is true for all  . Then . (7.1) (continued on next page) Proof of Theorem 7.1 N = N k ≤ ≤ i i = k + ∑ k + ( ) i i = k ∑ + =

chapter 7 recursion In a proof by induction, the basis is the  easy case that can  be shown by hand. Why does this constitute a proof? First, the theorem is true for N = 1, which is called the basis. We can view it as being the basis for our belief that the theorem is true in general. In a proof by induction, the basis is the easy case that can be shown by hand. Once we have established the basis, we use inductive hypothesis to assume that the theorem is true for some arbitrary k and that, under this assumption, if the theorem is true for k, then it is true for k + 1. In our case, we know that the theorem is true for the basis N = 1, so we know that it also is true for N = 2. Because it is true for N = 2, it must be true for N = 3. And as it is true for N = 3, it must be true for N = 4. Extending this logic, we know that the theorem is true for every positive integer beginning with N = 1. The inductive hypothesis assumes that the  theorem is true for  some arbitrary case  and that, under this  assumption, it is  true for the next  case. Let us apply proof by induction to a second problem, which is not quite as simple as the ﬁrst. First, we examine the sequence of numbers  ,  , ,  ,  , and so on. Each member represents the sum of the ﬁrst N squares, with alternating signs. The sequence evaluates to 1, 3, 6, 10, and 15. Thus, in general, the sum seems to be equal to the sum of the ﬁrst N integers, which, as we know from Theorem 7.1, would be  . Theorem 7.2  proves this result. Proof of Theorem 7.1 (continued from previous page) By assumption, the theorem is true for k, so we may replace the sum on the righthand side of Equation 7.1 with  , obtaining  (7.2) Algebraic manipulation of the right-hand side of Equation 7.2 now yields This result conﬁrms the theorem for the case  . Thus by induction, the theorem is  true for all integers  . k k + ( ) 2 ⁄ i i = k + ∑ k + ( ) k k + ( ) 2 ⁄ ( ) + = i i = k + ∑ k + ( ) k + ( ) 2 ⁄ = k + N ≥ – – + – – + – – + + N N + ( ) 2 ⁄ Theorem 7.2 The sum   is  . Proof The proof is by induction. Basis: Clearly, the theorem is true for  . (continued on next page) – ( )N i – i2 i N = ∑ N2 N – ( )2 N – ( )2 … – + – = N N + ( ) 2 ⁄ N =

7.3 basic recursion 7.3 basic recursion A recursive method  is deﬁned in terms  of a smaller  instance of itself.  There must be  some base case  that can be computed without  recursion. Proofs by induction show us that, if we know that a statement is true for a smallest case and can show that one case implies the next case, then we know the statement is true for all cases. Sometimes mathematical functions are deﬁned recursively. For instance, let S(N) be the sum of the ﬁrst N integers. Then S(1) = 1, and we can write S(N) = S(N – 1) + N. Here we have deﬁned the function S in terms of a smaller instance of itself. The recursive deﬁnition of S(N) is virtually identical to the closed form S(N) = N(N + 1) / 2, with the exception that the recursive deﬁnition is only deﬁned for positive integers and is less directly computable. (continued from previous page) Inductive hypothesis: First we assume that the theorem is true for k: . Then we must show that it is true for  , namely, that We write (7.3) If we rewrite the right-hand side of Equation 7.3, we obtain and a substitution yields (7.4) If we apply the inductive hypothesis, then we can replace the summation on the righthand side of Equation 7.4, obtaining (7.5) Simple algebraic manipulation of the right-hand side of Equation 7.5 then yields which establishes the theorem for  . Thus, by induction, the theorem is true  for all  . Proof of Theorem 7.2 – ( )k i – i2 i k = ∑ k k + ( ) ------------------- = k + – ( )k i – + i2 i k + = ∑ k + ( ) k + ( ) --------------------------------- = – ( )k i – + i2 i k + = ∑ k + ( )2 k2 k – ( )2 … – + – = – ( )k i – + i2 i k + = ∑ k + ( )2 (k k 1) – … + ( ) – – = – ( )k i – + i2 i k + = ∑ k + ( )2 – ( )k i – i2 i k = ∑ ( ) – = – ( )k i – + i2 i k + = ∑ k + ( )2 k k + ( ) 2 ⁄ – = – ( )k i – + i2 i k + = ∑ k + ( ) k + ( ) 2 ⁄ = N k + = N ≥

chapter 7 recursion Sometimes writing a formula recursively is easier than writing it in closed form. Figure 7.1 shows a straightforward implementation of the recursive function. If N = 1, we have the basis, for which we know that S(1) = 1. We take care of this case at lines 4 and 5. Otherwise, we follow the recursive deﬁnition S(N) = S(N – 1) + N precisely at line 7. It is hard to imagine that we could implement the recursive method any more simply than this, so the natural question is, does it actually work? The answer, except as noted shortly, is that this routine works. Let us examine how the call to s(4) is evaluated. When the call to s(4) is made, the test at line 4 fails. We then execute line 7, where we evaluate s(3). Like any other method, this evaluation requires a call to s. In that call we get to line 4, where the test fails; thus we go to line 7. At this point we call s(2). Again, we call s, and now n is 2. The test at line 4 still fails, so we call s(1) at line 7. Now we have n equal to 1, so s(1) returns 1. At this point s(2) can continue, adding the return value from s(1) to 2; thus s(2) returns 3. Now s(3) continues, adding the value of 3 returned by s(2) to n, which is 3; thus s(3) returns 6. This result enables completion of the call to s(4), which ﬁnally returns 10. Note that, although s seems to be calling itself, in reality it is calling a clone of itself. That clone is simply another method with different parameters. At any instant only one clone is active; the rest are pending. It is the computer’s job, not yours, to handle all the bookkeeping. If there were too much bookkeeping even for the computer, then it would be time to worry. We discuss these details later in the chapter. The base case is an  instance that can  be solved without  recursion. Any  recursive call must  make progress  toward a base case. A base case is an instance that can be solved without recursion. Any recursive call must progress toward the base case in order to terminate eventually. We thus have our ﬁrst two (of four) fundamental rules of recursion. 1. Base case: Always have at least one case that can be solved without using recursion. 2. Make progress: Any recursive call must progress toward a base case. Our recursive evaluation routine does have a few problems. One is the call s(0), for which the method behaves poorly.1 This behavior is natural because figure 7.1 Recursive evaluation  of the sum of the first  N integers     // Evaluate the sum of the first n integers     public static long s( int n )     {         if( n == 1 )             return 1;         else             return s( n - 1 ) + n;     } 1. A call to s(-1) is made, and the program eventually crashes because there are too many pending recursive calls. The recursive calls are not progressing toward a base case.

7.3 basic recursion the recursive deﬁnition of S(N) does not allow for N < 1. We can ﬁx this problem by extending the deﬁnition of S(N) to include N = 0. Because there are no numbers to add in this case, a natural value for S(0) would be 0. This value makes sense because the recursive deﬁnition can apply for S(1), as S(0) + 1 is 1. To implement this change, we just replace 1 with 0 on lines 4 and 5. Negative N also causes errors, but this problem can be ﬁxed in a similar manner (and is left for you to do as Exercise 7.2). A second problem is that if the parameter n is large, but not so large that the answer does not ﬁt in an int, the program can crash or hang. Our system, for instance, cannot handle N ≥8,882. The reason is that, as we have shown, the implementation of recursion requires some bookkeeping to keep track of the pending recursive calls, and for sufﬁciently long chains of recursion, the computer simply runs out of memory. We explain this condition in more detail later in the chapter. This routine also is somewhat more time consuming than an equivalent loop because the bookkeeping also uses some time. Needless to say, this particular example does not demonstrate the best use of recursion because the problem is so easy to solve without recursion. Most of the good uses of recursion do not exhaust the computer’s memory and are only slightly more time consuming than nonrecursive implementations. However, recursion almost always leads to more compact code. 7.3.1   printing numbers in any base A good example of how recursion simpliﬁes the coding of routines is number printing. Suppose that we want to print out a nonnegative number N in decimal form but that we do not have a number output function available. However, we can print out one digit at a time. Consider, for instance, how we would print the number 1369. First we would need to print 1, then 3, then 6, and then 9. The problem is that obtaining the ﬁrst digit is a bit sloppy: Given a number n, we need a loop to determine the ﬁrst digit of n. In contrast is the last digit, which is immediately available as n%10 (which is n for n less than 10). Recursion provides a nifty solution. To print out 1369, we print out 136, followed by the last digit, 9. As we have mentioned, printing out the last digit using the % operator is easy. Printing out all but the number represented by eliminating the last digit also is easy, because it is the same problem as printing out n/10. Thus, it can be done by a recursive call. The code shown in Figure 7.2 implements this printing routine. If n is smaller than 10, line 6 is not executed and only the one digit n%10 is printed; otherwise, all but the last digit are printed recursively and then the last digit is printed.

chapter 7 recursion Note how we have a base case (n is a one-digit integer), and because the recursive problem has one less digit, all recursive calls progress toward the base case. Thus we have satisﬁed the ﬁrst two fundamental rules of recursion. To make our printing routine useful, we can extend it to print in any base between 2 and 16.2 This modiﬁcation is shown in Figure 7.3. We introduced a String to make the printing of a through f easier. Each digit is now output by indexing to the DIGIT_TABLE string. The printInt routine is not robust. If base is larger than 16, the index to DIGIT_TABLE could be out of bounds. If base is 0, an arithmetic error results when division by 0 is attempted at line 8. Failure to progress  means that the program does not work. The most interesting error occurs when base is 1. Then the recursive call at line 8 fails to make progress because the two parameters to the recursive call are identical to the original call. Thus the system makes recursive calls until it eventually runs out of bookkeeping space (and exits less than gracefully). A driver routine tests the validity of  the ﬁrst call and  then calls the  recursive routine. We can make the routine more robust by adding an explicit test for base. The problem with this strategy is that the test would be executed during each of the recursive calls to printInt, not just during the ﬁrst call. Once base is valid in the ﬁrst call, to retest it is silly because it does not change in the course of the recursion and thus must still be valid. One way to avoid this inefﬁciency is to set up a driver routine. A driver routine tests the validity of base and then calls the recursive routine, as shown in Figure 7.4. The use of driver routines for recursive programs is a common technique. 2. Java’s toString method can take any base, but many languages do not have this built-in capability. figure 7.2 A recursive routine for  printing N in decimal  form // Print n in base 10, recursively. // Precondition: n >= 0. public static void printDecimal( long n ) {         if( n >= 10 )             printDecimal( n / 10 );         System.out.print( (char) ('0' + ( n % 10 ) ) ); } figure 7.3 A recursive routine for  printing N in any base private static final String DIGIT_TABLE = "0123456789abcdef"; // Print n in any base, recursively. // Precondition: n >= 0, base is valid. public static void printInt( long n, int base ) {         if( n >= base )             printInt( n / base, base );         System.out.print( DIGIT_TABLE.charAt( (int) ( n % base ) ) ); }

7.3 basic recursion 7.3.2   why it works In Theorem 7.3 we show, somewhat rigorously, that the printDecimal algorithm works. Our goal is to verify that the algorithm is correct, so the proof is based on the assumption that we have made no syntax errors. Recursive algorithms can be  proven correct with  mathematical induction. The proof of Theorem 7.3 illustrates an important principle. When designing a recursive algorithm, we can always assume that the recursive calls work (if they progress toward the base case) because, when a proof is performed, this assumption is used as the inductive hypothesis. At ﬁrst glance such an assumption seems strange. However, recall that we always assume that method calls work, and thus the assumption that the recursive call works is really no different. Like any method, a recursive routine needs to combine solutions from calls to other methods to obtain a solution. However, other methods may include easier instances of the original method. figure 7.4 A robust numberprinting program public final class PrintInt {     private static final String DIGIT_TABLE = "0123456789abcdef";     private static final int    MAX_BASE    = DIGIT_TABLE.length( );     // Print n in any base, recursively     // Precondition: n >= 0, 2 <= base <= MAX_BASE     private static void printIntRec( long n, int base )     {         if( n >= base )             printIntRec( n / base, base );         System.out.print( DIGIT_TABLE.charAt( (int) ( n % base ) ) );     }     // Driver routine     public static void printInt( long n, int base )     {         if( base <= 1 || base > MAX_BASE )             System.err.println( "Cannot print in base " + base );         else         {             if( n < 0 )             {                 n = -n;                 System.out.print( "-" );             }             printIntRec( n, base );         }     } }

chapter 7 recursion This observation leads us to the third fundamental rule of recursion. 3. “You gotta believe”: Always assume that the recursive call works. The third fundamental rule of  recursion: Always  assume that the  recursive call  works. Use this  rule to design your  algorithms. Rule 3 tells us that when we design a recursive method, we do not have to attempt to trace the possibly long path of recursive calls. As we showed earlier, this task can be daunting and tends to make the design and veriﬁcation more difﬁcult. A good use of recursion makes such a trace almost impossible to understand. Intuitively, we are letting the computer handle the bookkeeping that, were we to do ourselves, would result in much longer code. This principle is so important that we state it again: Always assume that the recursive call works. 7.3.3   how it works Recall that the implementation of recursion requires additional bookkeeping on the part of the computer. Said another way, the implementation of any method requires bookkeeping, and a recursive call is not particularly special (except that it can overload the computer’s bookkeeping limitations by calling itself too many times). To see how a computer might handle recursion, or, more generally, any sequence of method calls, consider how a person might handle a super busy day. Imagine you are editing a ﬁle on your computer and the phone rings. When the house phone rings you have to stop editing the ﬁle to deal with the phone. You might want to write down on a piece of paper what you were doing to the ﬁle, just in case the phone call takes a long time and you can’t remember. Now imagine that while you are on the phone with your spouse, the cell phone rings. You put your spouse on hold, leaving the phone on a Theorem 7.3 The algorithm printDecimal shown in Figure 7.2 correctly prints n in base 10. Proof Let k be the number of digits in n. The proof is by induction on k. Basis: If  , then no recursive call is made, and line 7 correctly outputs the one  digit of n. Inductive Hypothesis: Assume that printDecimal works correctly for all   digit  integers. We show that this assumption implies correctness for any   digit integer n. Because  , the if statement at line 5 is satisﬁed for a   digit integer n. By the inductive hypothesis, the recursive call at line 6 prints the ﬁrst k digits of n. Then line 7 prints the ﬁnal digit. Thus if any k digit integer can be printed, then so can  a   digit integer. By induction, we conclude that printDecimal works for all k, and  thus all n. k = k ≥ k + k ≥ k + k +

7.3 basic recursion The bookkeeping in  a procedural or  object-oriented language is done by  using a stack of  activation records. Recursion is a  natural by-product. counter. You better write down on a piece of paper that you put the home phone down (and where you left the phone!). While you are on the cell phone, someone knocks on the door. You want to tell your cell phone partner to wait while you deal with the door. So you put the cell phone down, and you better write down another note on a piece of paper that you put the cell phone down (and also where you left the cell phone!). At this point, you have made three notes to yourself, with the cell phone note being the most recent. When you open the door, the burglar alarm goes off because you forgot to deactivate it. So you have to tell the person at the door to wait. You make another note to yourself, while you deactivate the burglar alarm. Although you are overwhelmed, you can now ﬁnish handling all the tasks that were started in reverse order: the person at the door, the cell phone conversation, the home phone conversation, and the ﬁle edit. You just have to go back through your stack of notes that you made to yourself. Note the important phrasing: you are maintaining a “stack.” Java, like other languages such as C++, implements methods by using an internal stack of activation records. An activation record contains relevant information about the method, including, for instance, the values of the parameters and local variables. The actual contents of the activation record is system dependent. Method calling and  method return  sequences are  stack operations. The stack of activation records is used because methods return in reverse order of their invocation. Recall that stacks are great for reversing the order of things. In the most popular scenario, the top of the stack stores the activation record for the currently active method. When method G is called, an activation record for G is pushed onto the stack, which makes G the currently active method. When a method returns, the stack is popped and the activation record that is the new top of the stack contains the restored values. As an example, Figure 7.5 shows a stack of activation records that occurs in the course of evaluating s(4). At this point, we have the calls to main, s(4), and s(3) pending and we are actively processing s(2). The space overhead is the memory used to store an activation record for each currently active method. Thus, in our earlier example where s(8883) crashes, the system has room for roughly 8,883 activation records. (Note that main generates an activation record itself.) The pushing and popping of the internal stack also represents the overhead of executing a method call. s (2) s (3) s (4) main ( ) TOP: figure 7.5 A stack of activation  records

chapter 7 recursion Recursion can  always be removed  by using a stack.  This is occasionally  required to save  space. The close relation between recursion and stacks tells us that recursive programs can always be implemented iteractively with an explicit stack. Presumably our stack will store items that are smaller than an activation record, so we can also reasonably expect to use less space. The result is slightly faster but longer code. Modern optimizing compilers have lessened the costs associated with recursion to such a degree that, for the purposes of speed, removing recursion from an application that uses it well is rarely worthwhile. 7.3.4   too much recursion can be dangerous Do not use recursion as a substitute  for a simple loop. In this text we give many examples of the power of recursion. However, before we look at those examples, you should recognize that recursion is not always appropriate. For instance, the use of recursion in Figure 7.1 is poor because a loop would do just as well. A practical liability is that the overhead of the recursive call takes time and limits the value of n for which the program is correct. A good rule of thumb is that you should never use recursion as a substitute for a simple loop. The i th Fibonacci number is the sum  of the two previous  Fibonacci numbers. A much more serious problem is illustrated by an attempt to calculate the Fibonacci numbers recursively. The Fibonacci numbers   are deﬁned as follows:   and  ; the ith Fibonacci number equals the sum of the (ith – 1) and (ith – 2) Fibonacci numbers; thus  . From this deﬁnition we can determine that the series of Fibonacci numbers continues: 1, 2, 3, 5, 8, 13, 21, 34, 55, 89,  . Do not do redundant work recursively; the program  will be incredibly  inefﬁcient. The Fibonacci numbers have an incredible number of properties, which seem always to crop up. In fact, one journal, The Fibonacci Quarterly, exists solely for the purpose of publishing theorems involving the Fibonacci numbers. For instance, the sum of the squares of two consecutive Fibonacci numbers is another Fibonacci number. The sum of the ﬁrst N Fibonacci numbers is one less than   (see Exercise 7.9 for some other interesting identities). Because the Fibonacci numbers are recursively deﬁned, writing a recursive routine to determine FN seems natural. This recursive routine, shown in Figure 7.6, works but has a serious problem. On our relatively fast machine, it takes nearly a minute to compute F40, an absurd amount of time considering that the basic calculation requires only 39 additions. F0 F1 … Fi , , , F0 = F1 = Fi Fi – Fi – + = … FN + figure 7.6 A recursive routine for  Fibonacci numbers: A  bad idea // Compute the Nth Fibonacci number. // Bad algorithm.     public static long fib( int n )     {         if( n <= 1 )     return n;         else             return fib( n - 1 ) + fib( n - 2 );     }

7.3 basic recursion The underlying problem is that this recursive routine performs redundant calculations. To compute fib(n), we recursively compute fib(n-1). When the recursive call returns, we compute fib(n-2) by using another recursive call. But we have already computed fib(n-2) in the process of computing fib(n-1), so the call to fib(n-2) is a wasted, redundant calculation. In effect, we make two calls to fib(n-2) instead of only one. Normally, making two method calls instead of one would only double the running time of a program. However, here it is worse than that: Each call to fib(n-1) and each call to fib(n-2) makes a call to fib(n-3); thus there are actually three calls to fib(n-3). In fact, it keeps getting worse: Each call to fib(n-2) or fib(n-3) results in a call to fib(n-4), so there are ﬁve calls to fib(n-4). Thus we get a compounding effect: Each recursive call does more and more redundant work. The recursive  routine fib is  exponential. Let C(N) be the number of calls to fib made during the evaluation of fib(n). Clearly C(0) = C(1) = 1 call. For N ≥2, we call fib(n), plus all the calls needed to evaluate fib(n-1) and fib(n-2) recursively and independently. Thus . By induction, we can easily verify that for N ≥3 the solution to this recurrence is C(N) = FN + 2 + FN – 1– 1. Thus the number of recursive calls is larger than the Fibonacci number we are trying to compute, and it is exponential. For N = 40, F40 = 102,334,155, and the total number of recursive calls is more than 300,000,000. No wonder the program takes forever. The explosive growth of the number of recursive calls is illustrated in Figure 7.7. The fourth fundamental rule of  recursion: Never  duplicate work by  solving the same  instance of a problem in separate  recursive calls. This example illustrates the fourth and ﬁnal basic rule of recursion. 4. Compound interest rule: Never duplicate work by solving the same instance of a problem in separate recursive calls. 7.3.5   preview of trees The tree is a fundamental structure in computer science. Almost all operating systems store ﬁles in trees or tree-like structures. Trees are also used in compiler design, text processing, and searching algorithms. We discuss trees in detail in Chapters 18 and 19. We also make use of trees in Sections 11.2.4 (expression trees) and 12.1 (Huffman codes). One deﬁnition of the tree is recursive: Either a tree is empty or it consists of a root and zero or more nonempty subtrees  , each of whose C N ( ) C N – ( ) C N – ( ) + + = F1 F2 F0 F3 F1 F4 F1 F2 F0 F5 F1 F2 F0 F3 F1 figure 7.7 A trace of the  recursive calculation  of the Fibonacci  numbers T1 T2 … Tk , , ,

chapter 7 recursion A tree consists of a  set of nodes and a  set of directed  edges that connect  them. Parents and children are naturally  deﬁned. A directed  edge connects the  parent to the child. roots are connected by an edge from the root, as illustrated in Figure 7.8. In certain instances (most notably, the binary trees discussed in Chapter 18), we may allow some of the subtrees to be empty.  Nonrecursively, then, a tree consists of a set of nodes and a set of directed edges that connect pairs of nodes. Throughout this text we consider only rooted trees. A rooted tree has the following properties. n One node is distinguished as the root. n Every node c, except the root, is connected by an edge from exactly one other node p. Node p is c’s parent, and c is one of p’s children. n A unique path traverses from the root to each node. The number of edges that must be followed is the path length. Parents and children are naturally deﬁned. A directed edge connects the parent to the child. A leaf has no  children. Figure 7.9 illustrates a tree. The root node is A: A’s children are B, C, D, and E. Because A is the root, it has no parent; all other nodes have parents. For instance, B’s parent is A. A node that has no children is called a leaf. The leaves in this tree are C, F, G, H, I, and K. The length of the path from A to K is 3 (edges); the length of the path from A to A is 0 (edges). 7.3.6   additional examples Perhaps the best way to understand recursion is to consider examples. In this section, we look at four more examples of recursion. The ﬁrst two are easily implemented nonrecursively, but the last two show off some of the power of recursion. T1 root T2 T3 Tk . . .  figure 7.8 A tree viewed  recursively

7.3 basic recursion factorials Recall that N! is the product of the ﬁrst N integers. Thus we can express N! as N times  . Combined with the base case 1! = 1, this information immediately provides all that we need for a recursive implementation. It is shown in Figure 7.10. binary search In Section 5.6.2 we described the binary search. Recall that in a binary search, we perform a search in a sorted array A by examining the middle element. If we have a match, we are done. Otherwise, if the item being searched for is smaller than the middle element, we search in the subarray that is to the left of the middle element. Otherwise, we search in the subarray that is to the right of the middle element. This procedure presumes that the subarray is not empty; if it is, the item is not found. This description translates directly into the recursive method shown in Figure 7.11. The code illustrates a thematic technique in which the public driver routine makes an initial call to a recursive routine and passes on the return value. Here, the driver sets the low and high points of the subarray, namely, 0 and a.length-1. In the recursive method, the base case at lines 18 and 19 handles an empty subarray. Otherwise, we follow the description given previously by making a G B F H D A C I E J K figure 7.9 A tree N – ( )! figure 7.10 Recursive implementation of the  factorial method // Evaluate n! public static long factorial( int n )     {         if( n <= 1 )    // base case     return 1;         else             return n * factorial( n - 1 );     }

chapter 7 recursion recursive call on the appropriate subarray (line 24 or 26) if a match has not been detected. When a match is detected, the matching index is returned at line 28. Note that the running time, in terms of Big-Oh, is unchanged from the nonrecursive implementation because we are performing the same work. In practice, the running time would be expected to be slightly larger because of the hidden costs of recursion. drawing a ruler Figure 7.12 shows the result of running a Java program that draws ruler markings. Here, we consider the problem of marking 1 inch. In the middle is the longest mark. In Figure 7.12, to the left of the middle is a miniaturized version of the ruler and to the right of the middle is a second miniaturized version. This result suggests a recursive algorithm that ﬁrst draws the middle line and then draws the left and right halves. figure 7.11 A binary search  routine, using  recursion /**  * Performs the standard binary search using two comparisons  * per level. This is a driver that calls the recursive method.  * @return index where item is found or NOT_FOUND if not found.  */     public static <AnyType extends Comparable<? super AnyType>>     int binarySearch( AnyType [ ] a, AnyType x )     {         return binarySearch( a, x, 0, a.length -1 );     }     /**      * Hidden recursive routine.      */     private static <AnyType extends Comparable<? super AnyType>> int binarySearch( AnyType [ ] a, AnyType x, int low, int high )     {         if( low > high )             return NOT_FOUND;         int mid = ( low + high ) / 2;         if( a[ mid ].compareTo( x ) < 0 )             return binarySearch( a, x, mid + 1, high );         else if( a[ mid ].compareTo( x ) > 0 )             return binarySearch( a, x, low, mid - 1 );         else             return mid; }

7.3 basic recursion You do not have to understand the details of drawing lines and shapes in Java to understand this program. You simply need to know that a Graphics object is something that gets drawn to. The drawRuler method in Figure 7.13 is our recursive routine. It uses the drawLine method, which is part of the Graphics class. The method drawLine draws a line from one (x, y) coordinate to another (x, y) coordinate, where coordinates are offset from the top-left corner. Our routine draws markings at level different heights; each recursive call is one level deeper (in Figure 7.12, there are eight levels). It ﬁrst disposes of the base case at lines 4 and 5. Then the midpoint mark is drawn at line 9. Finally, the two miniatures are drawn recursively at lines 11 and 12. In the online code, we include extra code to slow down the drawing. In that way, we can see the order in which the lines are drawn by the recursive algorithm. fractal star Shown in Figure 7.14(a) is a seemingly complex pattern called a fractal star, which we can easily draw by using recursion. The entire canvas is initially gray (not shown); the pattern is formed by drawing white squares onto the gray background. The last square drawn is over the center. Figure 7.14(b) shows the drawing immediately before the last square is added. Thus prior to the last square being drawn, four miniature versions have been drawn, one in each of the four quadrants. This pattern provides the information needed to derive the recursive algorithm. figure 7.12 A recursively drawn  ruler figure 7.13 A recursive method  for drawing a ruler     // Java code to draw Figure 7.12.     void drawRuler( Graphics g, int left, int right, int level )     {         if( level < 1 )             return;         int mid = ( left + right ) / 2;         g.drawLine( mid, 80, mid, 80 - level * 5 );         drawRuler( g, left, mid - 1, level - 1 );         drawRuler( g, mid + 1, right, level - 1 );     }

chapter 7 recursion As with the previous example, the method drawFractal uses a Java library routine. In this case, fillRect draws a rectangle; its upper left-hand corner and dimensions must be speciﬁed. The code is shown in Figure 7.15. The (a) (b) figure 7.15 Code for drawing the  fractal star outline  shown in Figure 7.14     // Draw picture in Figure 7.14.     void drawFractal( Graphics g, int xCenter,                       int yCenter, int boundingDim )     {         int side = boundingDim / 2;         if( side < 1 )             return;           // Compute corners.         int left =   xCenter - side / 2;         int top =    yCenter - side / 2;         int right =  xCenter + side / 2;         int bottom = yCenter + side / 2;           // Recursively draw four quadrants.         drawFractal( g, left, top, boundingDim / 2 );         drawFractal( g, left, bottom, boundingDim / 2 );         drawFractal( g, right, top, boundingDim / 2 );          drawFractal( g, right, bottom, boundingDim / 2 );           // Draw central square, overlapping quadrants.         g.fillRect( left, top, right - left, bottom - top );     } figure 7.14 (a) A fractal star outline drawn by the code shown in Figure 7.15; (b) The same star immediately before the last square is added.

7.4 numerical applications parameters to drawFractal include the center of the fractal and the overall dimension. From this, we can compute, at line 5, the size of the large central square. After handling the base case at lines 7 and 8, we compute the boundaries of the central rectangle. We can then draw the four miniature fractals at lines 17 to 20. Finally, we draw the central square at line 23. Note that this square must be drawn after the recursive calls. Otherwise, we obtain a different picture (in Exercise 7.35, you are asked to describe the difference). 7.4 numerical applications In this section we look at three problems drawn primarily from number theory. Number theory used to be considered an interesting but useless branch of mathematics. However, in the last 30 years, an important application for number theory has emerged: data security. We begin the discussion with a small amount of mathematical background and then show recursive algorithms to solve three problems. We can combine these routines in conjunction with a fourth algorithm that is more complex (described in Chapter 9), to implement an algorithm that can be used to encode and decode messages. To date, nobody has been able to show that the encryption scheme described here is not secure. Here are the four problems we examine. 1. Modular exponentiation: Compute XN(mod P). 2. Greatest common divisor: Compute gcd(A, B). 3. Multiplicative inverse: Solve AX ≡ 1(mod P) for X. 4. Primality testing: Determine whether N is prime (deferred to Chapter 9). The integers we expect to deal with are all large, requiring at least 100 digits each. Therefore we must have a way to represent large integers, along with a complete set of algorithms for the basic operations of addition, subtraction, multiplication, division, and so on. Java provides the BigInteger class for this purpose. Implementing it efﬁciently is no trivial matter, and in fact there is extensive literature on the subject. We use long numbers to simplify our code presentation. The algorithms described here work with large objects but still execute in a reasonable amount of time. 7.4.1   modular arithmetic The problems in this section, as well as the implementation of the hash table data structure (Chapter 20), require the use of the Java % operator. The % operator, denoted as operator%, computes the remainder of two integral types. For example, 13%10 evaluates to 3, as does 3%10, and 23%10. When we compute the

chapter 7 recursion remainder of a division by 10, the possible results range from 0 to 9.3 This range makes operator% useful for generating small integers. If two numbers A and B give the same remainder when divided by N, we say that they are congruent modulo N, written as A ≡ B (mod N). In this case, it must be true that N divides A – B. Furthermore, the converse is true: If N divides A – B, then A ≡ B (mod N). Because there are only N possible remainders—0, 1, ..., N – 1— we say that the integers are divided into congruence classes modulo N. In other words, every integer can be placed in one of N classes, and those in the same class are congruent to each other, modulo N. We use three important theorems in our algorithms (we leave the proof of these facts as Exercise 7.10). 1. If A ≡ B (mod N), then for any C, A + C ≡B + C(mod N). 2. If A ≡ B (mod N), then for any D, AD ≡BD(mod N). 3. If A ≡ B (mod N), then for any positive P, AP ≡BP(mod N). These theorems allow certain calculations to be done with less effort. For instance, suppose that we want to know the last digit in 33335555. Because this number has more than 15,000 digits, it is expensive to compute the answer directly. However, what we want is to determine 33335555(mod 10). As 3333 ≡3(mod 10), we need only to compute 35555(mod 10). As 34 = 81, we know that 34 ≡ 1(mod 10), and raising both sides to the power of 1388 tells us that 35552 ≡1(mod 10). If we multiply both sides by 33 = 27, we obtain 35555 ≡ 27 ≡ 7(mod 10), thereby completing the calculation.  7.4.2   modular exponentiation In this section we show how to compute XN(mod P) efﬁciently. We can do so by initializing result to 1 and then repeatedly multiplying result by X, applying the % operator after every multiplication. Using operator% in this way instead of just the last multiplication makes each multiplication easier because it keeps result smaller. After N multiplications, result is the answer that we are looking for. However, doing N multiplications is impractical if N is a 100-digit BigInteger. In fact, if N is 1,000,000,000, it is impractical on all but the fastest machines. A faster algorithm is based on the following observation. That is, if N is even, then 3. If n is negative, n%10 ranges from 0 to –9. XN X X ⋅ ( ) N 2 ⁄ =

7.4 numerical applications and if N is odd, then (Recall that   is the largest integer that is smaller than or equal to X.) As before, to perform modular exponentiation, we apply a % after every multiplication. The recursive algorithm shown in Figure 7.16 represents a direct implementation of this strategy. Lines 8 and 9 handle the base case: X0 is 1, by definition.4 At line 11, we make a recursive call based on the identity stated in the preceding paragraph. If N is even, this call computes the desired answer; if N is odd, we need to multiply by an extra X (and use operator%). Exponentiation can  be done in logarithmic number of  multiplications. This algorithm is faster than the simple algorithm proposed earlier. If M(N) is the number of multiplications used by power, we have M(N) ≤ M(⎣N/2⎦) + 2. The reason is that if N is even, we perform one multiplication, plus those done recursively, and that if N is odd, we perform two multiplications, plus those done recursively. Because M(0) = 0, we can show that M(N) < 2 log N. The logarithmic factor can be obtained without direct calculation by application of the halving principle (see Section 5.5), which tells us the number of recursive invocations of power. Moreover, an average value of M(N) is (3/2)log N, as in each recursive step N is equally likely to be even or odd. If N is a 100-digit number, in the worst case only about 665 multiplications (and typically only 500 on average) are needed. 4. We deﬁne 00 = 1 for the purposes of this algorithm. We also assume that N is nonnegative and P is positive. XN = X XN – ⋅ X X X ⋅ ( ) N 2 ⁄ ⋅ = X figure 7.16 Modular exponentiation routine /**  * Return x^n (mod p)  * Assumes x, n >= 0, p > 0, x < p, 0^0 = 1  * Overflow may occur if p > 31 bits.  */     public static long power( long x, long n, long p )     {         if( n == 0 )             return 1;         long tmp = power( ( x * x ) % p, n / 2, p );         if( n % 2 != 0 )             tmp = ( tmp * x ) % p;         return tmp;     }

chapter 7 recursion 7.4.3   greatest common divisor  and multiplicative inverses The greatest common divisor (gcd) of  two integers is the  largest integer that  divides both of  them. Given two nonnegative integers A and B, their greatest common divisor, gcd(A, B), is the largest integer D that divides both A and B. For instance, gcd(70, 25) is 5. In other words, the greatest common divisor (gcd) is the largest integer that divides two given integers. We can easily verify that gcd(A, B) ≡gcd(A – B, B). If D divides both A and B, it must also divide A – B; and if D divides both A – B and B, then it must also divide A. This observation leads to a simple algorithm in which we repeatedly subtract B from A, transforming the problem into a smaller one. Eventually A becomes less than B, and then we can switch roles for A and B and continue from there. At some point B will become 0. Then we know that gcd(A, 0) ≡A, and as each transformation preserves the gcd of the original A and B, we have our answer. This algorithm is called Euclid’s algorithm and was ﬁrst described more than 2,000 years ago. Although correct, it is unusable for large numbers because a huge number of subtractions are likely to be required. A computationally efﬁcient modiﬁcation is that the repeated subtractions of B from A until A is smaller than B is equivalent to the conversion of A to precisely A mod B. Thus gcd(A, B) ≡gcd(B, A mod B). This recursive deﬁnition, along with the base case in which B = 0, is used directly to obtain the routine shown in Figure 7.17. To visualize how it works, note that in the previous example we used the following sequence of recursive calls to deduce that the gcd of 70 and 25 is 5: gcd(70, 25) ⇒gcd(25, 20) ⇒gcd(20, 5) ⇒ gcd(5, 0) ⇒ 5. The number of recursive calls used is proportional to the logarithm of A, which is the same order of magnitude as the other routines that we have presented in this section. The reason is that, in two recursive calls, the problem is reduced at least in half. The proof of this is left for you to do as Exercise 7.11. figure 7.17 Computation of  greatest common  divisor /**  * Return the greatest common divisor.  */     public static long gcd( long a, long b )     {         if( b == 0 )             return a;         else             return gcd( b, a % b );     }

7.4 numerical applications The greatest common divisor and  multiplicative inverse can also be  calculated in logarithmic time by  using a variant of  Euclid’s algorithm. The gcd algorithm is used implicitly to solve a similar mathematical problem. The solution 1 ≤X < N to the equation AX ≡1(mod N) is called the multiplicative inverse of A, mod N. Also assume that 1 ≤A < N. For example, the inverse of 3, mod 13 is 9; that is, 3 ⋅ 9 mod 13 yields 1. The ability to compute multiplicative inverses is important because equations such as 3i ≡7(mod 13) are easily solved if we know the multiplicative inverse. These equations arise in many applications, including the encryption algorithm discussed at the end of this section. In this example, if we multiply by the inverse of 3 (namely 9), we obtain i ≡63(mod 13), so i = 11 is a solution. If AX ≡ 1(mod N), then AX + NY = 1(mod N) is true for any Y. For some Y, the left-hand side must be exactly 1. Thus the equation AX + NY = 1 is solvable if and only if A has a multiplicative inverse. Given A and B, we show how to ﬁnd X and Y satisfying AX + BY = 1 We assume that 0 ≤⎥B⎪< ⎥A⎪and then extend the gcd algorithm to compute X and Y. First, we consider the base case, B ≡0. In this case we have to solve AX = 1, which implies that both A and X are 1. In fact, if A is not 1, there is no multiplicative inverse. Hence A has a multiplicative inverse modulo N only if gcd(A, N) = 1. Otherwise, B is not zero. Recall that gcd(A, B) ≡gcd(B, A mod B). So we let A = BQ + R. Here Q is the quotient and R is the remainder, and thus the recursive call is gcd(B, R). Suppose that we can recursively solve Since , we have which means that BX1 RY1 + = R A BQ – = BX1 A BQ – ( )Y1 + = AY1 B X1 QY1 – ( ) + =

chapter 7 recursion Thus   and   is a solution to  . We code this observation directly as fullGcd in Figure 7.18. The method inverse just calls fullGcd, where X and Y are static class variables. The only detail left is that the value given for X may be negative. If it is, line 35 of inverse will make it positive. We leave a proof of that fact for you to do as Exercise 7.14. The proof can be done by induction. X Y1 = Y X1 A B ⁄ Y1 – = AX BY + = figure 7.18 A routine for  determining multiplicative inverse // Internal variables for fullGcd private static long x; private static long y; /**  * Works back through Euclid’s algorithm to find  * x and y such that if gcd(a,b) = 1,  * ax + by = 1.  */ private static void fullGcd( long a, long b ) {         long x1, y1;         if( b == 0 )         {             x = 1;             y = 0;         }         else         {             fullGcd( b, a % b );             x1 = x; y1 = y;             x = y1;             y = x1 - ( a / b ) * y1;         }     }     /**      * Solve ax == 1 (mod n), assuming gcd( a, n ) = 1.      * @return x.      */     public static long inverse( long a, long n )     {         fullGcd( a, n );         return x > 0 ? x : x + n;     }

7.4 numerical applications 7.4.4   the rsa cryptosystem Number theory is  used in cryptography because factoring appears to  be a much harder  process than  multiplication. For centuries, number theory was thought to be a completely impractical branch of mathematics. Recently, however, it has emerged as an important ﬁeld because of its applicability to cryptography. The problem we consider has two parts. Suppose that Alice wants to send a message to Bob but that she is worried that the transmission may be compromised. For instance, if the transmission is over a phone line and the phone is tapped, somebody else may be reading the message. We assume that, even if there is eavesdropping on the phone line, there is no maliciousness (i.e., damage to the signal)—Bob gets whatever Alice sends. Encryption is used  to transmit messages so that they  cannot be read by  other parties. A solution to this problem is to use encryption, an encoding scheme to transmit messages that cannot be read by other parties. Encryption consists of two parts. First, Alice encrypts the message and sends the result, which is no longer plainly readable. When Bob receives Alice’s transmission, he decrypts it to obtain the original. The security of the algorithm is based on the fact that nobody else besides Bob should be able to perform the decryption, including Alice (if she did not save the original message). The RSA cryptosystem is a popular  encryption method. Thus Bob must provide Alice with a method of encryption that only he knows how to reverse. This problem is extremely challenging. Many proposed algorithms can be compromised by subtle code-breaking techniques. One method, described here, is the RSA cryptosystem (named after the initials of its authors), an elegant implementation of an encryption strategy. Here we give only a high-level overview of encryption, showing how the methods written in this section interact in a practical way. The references contain pointers to more detailed descriptions, as well as proofs of the key properties of the algorithm. First, however, note that a message consists of a sequence of characters and that each character is just a sequence of bits. Thus a message is a sequence of bits. If we break the message into blocks of B bits, we can interpret the message as a series of very large numbers. Thus the basic problem is reduced to encrypting a large number and then decrypting the result. computation of the rsa constants The RSA algorithm begins by having the receiver determine some constants. First, two large primes p and q are randomly chosen. Typically, these would be at least 100 or so digits each. For the purposes of this example, suppose that p = 127 and q = 211. Note that Bob is the receiver and thus is performing these computations. Note, also, that primes are plentiful. Bob can thus keep trying random numbers until two of them pass the primality test (discussed in Chapter 9).

chapter 7 recursion Next, Bob computes   and  , which for this example gives N = 26,797 and   = 26,460. Bob continues by choosing any such that gcd(e, N ′) = 1. In mathematical terms, he chooses any e that is relatively prime to N ′. Bob can keep trying different values of e by using the routine shown in Figure 7.17 until he ﬁnds one that satisﬁes the property. Any prime e would work, so ﬁnding e is at least as easy as ﬁnding a prime number. In this case, e = 13,379 is one of many valid choices. Next, d, the multiplicative inverse of e, mod N ′ is computed by using the routine shown in Figure 7.18. In this example, d = 11,099. Once Bob has computed all these constants, he does the following. First, he destroys p, q, and N ′. The security of the system is compromised if any one of these values is discovered. Bob then tells anybody who wants to send him an encrypted message the values of e and N, but he keeps d secret. encryption and decryption algorithms To encrypt an integer M, the sender computes Me(mod N) and sends it. In our case, if M = 10,237, the value sent is 8,422. When an encrypted integer R is received, all Bob has to do is compute Rd(mod N). For R = 8,422, he gets back the original M = 10,237 (which is not accidental). Both encryption and decryption can thus be carried out by using the modular exponentiation routine given in Figure 7.16. The algorithm works because the choices of e, d, and N guarantee (via a number theory proof beyond the scope of this text) that Med = M(mod N), so long as M and N share no common factors. As the only factors of N are two 100-digit primes, it is virtually impossible for that to occur.5 Thus decryption of the encrypted text gets the original back. What makes the scheme seem secure is that knowledge of d is apparently required in order to decode. Now N and e uniquely determine d. For instance, if we factor N, we get p and q and can then reconstruct d. The caveat is that factoring is apparently very hard to do for large numbers. Thus the security of the RSA system is based on the belief that factoring large numbers is intrinsically very difﬁcult. So far it has held up well. In public key cryptography, each participant publishes  the code others  can use to send  encrypted messages but keeps  the decrypting code  secret. This general scheme is known as public key cryptography, by which anybody who wants to receive messages publishes encryption information for anybody else to use but keeps the decryption code secret. In the RSA system, e and N would be computed once by each person and listed in a publicly readable place. The RSA algorithm is widely used to implement secure e-mail, as well as secure Internet transactions. When you access a Web page via the https protocol, a secure transaction is being performed via cryptography. The method 5. You are more likely to win a typical state lottery 13 weeks in a row. However, if M and N have a common factor, the system is compromised because the gcd will be a factor of N. N pq = N′ p – ( ) q – ( ) = N′ e >

7.5 divide-and-conquer algorithms actually employed is more complex than described here. One problem is that the RSA algorithm is somewhat slow for sending large messages. In practice, RSA is  used to encrypt the  key used by a  single-key encryption algorithm, such  as DES. A faster method is called DES. Unlike the RSA algorithm, DES is a singlekey algorithm, meaning that the same key serves both to encode and decode. It is like the typical lock on your house door. The problem with single-key algorithms is that both parties need to share the single key. How does one party ensure that the other party has the single key? That problem can be solved by using the RSA algorithm. A typical solution is that, say, Alice will randomly generate a single key for DES encryption. She then encrypts her message by using DES, which is much faster than using RSA. She transmits the encrypted message to Bob. For Bob to decode the encrypted message, he needs to get the DES key that Alice used. A DES key is relatively short, so Alice can use RSA to encrypt the DES key and then send it in a second transmission to Bob. Bob next decrypts Alice’s second transmission, thus obtaining the DES key, at which point he can decode the original message. These types of protocols, with enhancements, form the basis of most practical encryption implementations. 7.5 divide-and-conquer algorithms A divide-andconquer algorithm  is a recursive algorithm that is generally very efﬁcient. An important problem-solving technique that makes use of recursion is divide and conquer. A divide-and-conquer algorithm is an efﬁcient recursive algorithm that consist of two parts: n Divide, in which smaller problems are solved recursively (except, of course, base cases) In divide and conquer, the recursion  is the divide, and  the overhead is the  conquer. n Conquer, in which the solution to the original problem is then formed from the solutions to the subproblems Traditionally, routines in which the algorithm contains at least two recursive calls are called divide-and-conquer algorithms, whereas routines whose text contains only one recursive call are not. Consequently, the recursive routines presented so far in this chapter are not divide-and-conquer algorithms. Also, the subproblems usually must be disjoint (i.e., essentially nonoverlapping), so as to avoid the excessive costs seen in the sample recursive computation of the Fibonacci numbers. In this section we give an example of the divide-and-conquer paradigm. First we show how to use recursion to solve the maximum subsequence sum problem. Then we provide an analysis to show that the running time is O(N log N). Although we have already used a linear algorithm for this

chapter 7 recursion problem, the solution here is thematic of others in a wide range of applications, including the sorting algorithms, such as mergesort and quicksort, discussed in Chapter 8. Consequently, learning the technique is important. Finally, we show the general form for the running time of a broad class of divide-and-conquer algorithms. 7.5.1   the maximum contiguous  subsequence sum problem In Section 5.3 we discussed the problem of ﬁnding, in a sequence of numbers, a contiguous subsequence of maximum sum. For convenience, we restate the problem here. maximum contiguous subsequence sum problem Given (possibly negative) integers  , ﬁnd (and identify the sequence corresponding to) the maximum value of  . The maximum contiguous subsequence sum is zero if all the integers are negative. The maximum contiguous subsequence sum problem can be solved  with a divide-andconquer algorithm.  We presented three algorithms of various complexity. One was a cubic algorithm based on an exhaustive search: We calculated the sum of each possible subsequence and selected the maximum. We described a quadratic improvement that takes advantage of the fact that each new subsequence can be computed in constant time from a previous subsequence. Because we have O(N2) subsequences, this bound is the best that can be achieved with an approach that directly examines all subsequences. We also gave a linear-time algorithm that works by examining only a few subsequences. However, its correctness is not obvious. Let us consider a divide-and-conquer algorithm. Suppose that the sample input is {4, –3, 5, –2, –1, 2, 6, –2}. We divide this input into two halves, as shown in Figure 7.19. Then the maximum contiguous subsequence sum can occur in one of three ways. n Case 1: It resides entirely in the ﬁrst half. n Case 2: It resides entirely in the second half. n Case 3: It begins in the ﬁrst half but ends in the second half. We show how to ﬁnd the maximums for each of these three cases more efﬁciently than by using an exhaustive search. We begin by looking at case 3. We want to avoid the nested loop that results from considering all N / 2 starting points and N / 2 ending points independently. We can do so by replacing two nested loops by two consecutive loops. The consecutive loops, each of size N / 2, combine to require only linear A1 A2 … AN , , , Ak k i = j ∑

7.5 divide-and-conquer algorithms work. We can make this substitution because any contiguous subsequence that begins in the ﬁrst half and ends in the second half must include both the last element of the ﬁrst half and the ﬁrst element of the second half. Figure 7.19 shows that for each element in the ﬁrst half, we can calculate the contiguous subsequence sum that ends at the rightmost item. We do so with a right-to-left scan, starting from the border between the two halves. Similarly, we can calculate the contiguous subsequence sum for all sequences that begin with the ﬁrst element in the second half. We can then combine these two subsequences to form the maximum contiguous subsequence that spans the dividing border. In this example, the resulting sequence spans from the ﬁrst element in the ﬁrst half to the next-to-last element in the second half. The total sum is the sum of the two subsequences, or 4 + 7 = 11. This analysis shows that case 3 can be solved in linear time. But what about cases 1 and 2? Because there are N / 2 elements in each half, an exhaustive search applied to each half still requires quadratic time per half; speciﬁcally, all we have done is eliminate roughly half of the work, and half of quadratic is still quadratic. In cases 1 and 2 we can apply the same strategy—that of dividing into more halves. We can keep dividing those quarters further and further until splitting is impossible. This approach is succinctly stated as follows: Solve cases 1 and 2 recursively. As we demonstrate later, doing so lowers the running time below quadratic because the savings compound throughout the algorithm. The following is a

c h apt e r sorting algorithms Sorting is a fundamental application for computers. Much of the output eventually produced by a computation is sorted in some way, and many computations are made efﬁcient by invoking a sorting procedure internally. Thus sorting is perhaps the most intensively studied and important operation in computer science. In this chapter we discuss the problem of sorting an array of elements. We describe and analyze the various sorting algorithms. The sorts in this chapter can be done entirely in main memory, so the number of elements is relatively small (less than a few million). Sorts that cannot be performed in main memory and must be done on disk or tape are also quite important. We discuss this type of sorting, called external sorting, in Section 21.6. This discussion of sorting is a blend of theory and practice. We present several algorithms that perform differently and show how an analysis of an algorithm’s performance properties can help us make implementation decisions that are not obvious. In this chapter, we show n That simple sorts run in quadratic time n How to code Shellsort, which is a simple and efﬁcient algorithm that runs in subquadratic time

chapter 8 sorting algorithms n How to write the slightly more complicated O(N log N) mergesort and quicksort algorithms n That Ω(N log N) comparisons are required for any general-purpose sorting algorithm 8.1 why is sorting important? Recall from Section 5.6 that searching a sorted array is much easier than searching an unsorted array. This is especially true for people. That is, ﬁnding a person’s name in a phone book is easy, for example, but ﬁnding a phone number without knowing the person’s name is virtually impossible. As a result, any signiﬁcant amount of computer output is generally arranged in some sorted order so that it can be interpreted. The following are some more examples. n Words in a dictionary are sorted (and case distinctions are ignored). n Files in a directory are often listed in sorted order. n The index of a book is sorted (and case distinctions are ignored). n The card catalog in a library is sorted by both author and title. n A listing of course offerings at a university is sorted, ﬁrst by department and then by course number. n Many banks provide statements that list checks in increasing order by check number. n In a newspaper, the calendar of events in a schedule is generally sorted by date. n Musical compact disks in a record store are generally sorted by recording artist. n In the programs printed for graduation ceremonies, departments are listed in sorted order and then students in those departments are listed in sorted order. An initial sort of the  data can signiﬁcantly enhance the  performance of an  algorithm. Not surprisingly, much of the work in computing involves sorting. However, sorting also has indirect uses. For instance, suppose that we want to decide whether an array has any duplicates. Figure 8.1 shows a simple method that requires quadratic worst-case time. Sorting provides an alternative algorithm. That is, if we sort a copy of the array, then any duplicates will be adjacent to each other and can be detected in a single linear-time scan of the array. The cost of this algorithm is dominated by the time to sort, so if we can sort in

8.3 analysis of the insertion sort and other simple sorts subquadratic time, we have an improved algorithm. The performance of many algorithms is signiﬁcantly enhanced when we initially sort the data. The vast majority of signiﬁcant programming projects use a sort somewhere, and in many cases, the sorting cost determines the running time. Thus we want to be able to implement a fast and reliable sort. 8.2 preliminaries The algorithms we describe in this chapter are all interchangeable. Each is passed an array containing the elements, and only objects that implement the Comparable interface can be sorted. A comparisonbased sorting algorithm makes ordering decisions only  on the basis of  comparisons. The comparisons are the only operations allowed on the input data. An algorithm that makes ordering decisions only on the basis of comparisons is called a comparison-based sorting algorithm.1 In this chapter, N is the number of elements being sorted. 8.3 analysis of the insertion sort and other simple sorts An insertion sort is  quadratic in the  worst and average  cases. It is fast if  the input has  already been  sorted. Insertion sort is a simple sorting algorithm that is appropriate for small inputs. It is generally considered to be a good solution if only a few elements need sorting because it is such a short algorithm and the time required to sort is not likely to be an issue. However, if we are dealing with a large amount of data, insertion sort is a poor choice because it is too time consuming. The code is shown in Figure 8.2. figure 8.1 A simple quadratic  algorithm for  detecting duplicates // Return true if array a has duplicates; false otherwise     public static boolean duplicates( Object [ ] a )     {         for( int i = 0; i < a.length; i++ )             for( int j = i + 1; j < a.length; j++ )                if( a[ i ].equals( a[ j ] ) )                     return true;   // Duplicate found         return false;              // No duplicates found     } 1. As shown in Section 4.8, changing the sorting interface by requiring a Comparator function object is straightforward.

chapter 8 sorting algorithms Insertion sort works as follows. In the initial state the ﬁrst element, considering by itself, is sorted. In the ﬁnal state all elements (assume that there are N), considered as a group, are to have been sorted. Figure 8.3 shows that the basic action of insertion sort is to sort the elements in positions 0 through p (where p ranges from 1 through N – 1). In each stage p increases by 1. That is what the outer loop at line 7 in Figure 8.2 is controlling. When the body of the for loop is entered at line 12, we are guaranteed that the elements in array positions 0 through p–1 have already been sorted and that we need to extend this to positions 0 to p. Figure 8.4 gives us a closer look at what has to be done, detailing only the relevant part of the array. At each step the element in boldface type needs to be added to the previously sorted part of the array. We can easily do that by placing it in a figure 8.2 Insertion sort  implementation /**  * Simple insertion sort  */     public static <AnyType extends Comparable<? super AnyType>>     void insertionSort( AnyType [ ] a ) {         for( int p = 1; p < a.length; p++ )         {             AnyType tmp = a[ p ];             int j = p;             for( ; j > 0 && tmp.compareTo( a[ j - 1 ] ) < 0; j-- )                 a[ j ] = a[ j - 1 ];             a[ j ] = tmp;         } } Array Position Initial State After a[0..1] is sorted After a[0..2] is sorted After a[0..3] is sorted After a[0..4] is sorted After a[0..5] is sorted figure 8.3 Basic action of  insertion sort (the  shaded part is sorted)

8.3 analysis of the insertion sort and other simple sorts temporary variable and sliding all the elements that are larger than it one position to the right. Then we can copy the temporary variable into the former position of the leftmost relocated element (indicated by lighter shading on the following line). We keep a counter j, which is the position to which the temporary variable should be written back. Every time an element is slid, j decreases by 1. Lines 9–14 implement this process. The insertion sort is  quadratic in the  worst and average  cases. It is fast if  the input has  already been  sorted. Because of the nested loops, each of which can take N iterations, the insertion sort algorithm is O(N2). Furthermore, this bound is achievable because input in reverse order really does take quadratic time. A precise calculation shows that the tests at line 12 in Figure 8.2 can be executed at most P + 1 times for each value of P. Summing over all P gives a total time of However, if the input is presorted, the running time is O(N) because the test at the top of the inner for loop always fails immediately. Indeed, if the input is almost sorted (we deﬁne almost sorted more rigorously shortly), the insertion sort will run quickly. Thus the running time depends not only on the amount of input, but also on the speciﬁc ordering of the input. Because of this wide variation, analyzing the average-case behavior of this algorithm is worthwhile. The average case turns out to be θ(N 2) for the insertion sort as well as a variety of other simple sorting algorithms. An inversion measures unsortedness. An inversion is a pair of elements that are out of order in an array. In other words, it is any ordered pair (i, j) having the property that i < j but  . For example, the sequence {8, 5, 9, 2, 6, 3} has 10 inversions that correspond to the pairs (8, 5), (8, 2), (8, 6), (8, 3), (5, 2), (5, 3), (9, 2), (9, 6), (9, 3), and (6, 3). Note that the number of inversions equals the total number of times that line 13 in Figure 8.2 is executed. This condition is always true because Array Position Initial State After a[0..1] is sorted After a[0..2] is sorted After a[0..3] is sorted After a[0..4] is sorted After a[0..5] is sorted figure 8.4 A closer look at the  action of insertion sort  (the dark shading  indicates the sorted  area; the light shading  is where the new  element was placed) P + ( ) P = N – ∑ i i = N ∑ … N + + + + Θ N2 ( ) = = = Ai Aj >

chapter 8 sorting algorithms the effect of the assignment statement is to swap the two items a[j] and a[j-1]. (We avoid the actual excessive swapping by using the temporary variable, but nonetheless it is an abstract swap.) Swapping two elements that are out of place removes exactly one inversion, and a sorted array has no inversions. Thus, if there are I inversions at the start of the algorithm, we must have I implicit swaps. As O(N) other work is involved in the algorithm, the running time of the insertion sort is O(I + N), where I is the number of inversions in the original array. Thus the insertion sort runs in linear time if the number of inversions is O(N). We can compute precise bounds on the average running time of the insertion sort by computing the average number of inversions in an array. However, deﬁning average is difﬁcult. We can assume that there are no duplicate elements (if we allow duplicates, it is not even clear what the average number of duplicates is). We can also assume that the input is some arrangement of the ﬁrst N integers (as only relative ordering is important); these arrangements are called permutations. We can further assume that all these permutations are equally likely. Under these assumptions we can establish Theorem 8.1. Theorem 8.1 implies that insertion sort is quadratic on average. It also can be used to provide a very strong lower bound about any algorithm that exchanges adjacent elements only. This lower bound is expressed as Theorem 8.2. Theorem 8.1 The average number of inversions in an array of  distinct numbers is  . Proof  For any array   of numbers, consider  , which is the array in reverse order. For  example, the reverse of array 1, 5, 4, 2, 6, 3 is 3, 6, 2, 4, 5, 1. Consider any two numbers   in the array, with  . In exactly one of   and  , this ordered pair represents an inversion. The total number of these pairs in an array A and its reverse  is  . Thus an average array has half this amount, or   inversions. N N N – ( ) 4 ⁄ A Ar x y , ( ) y x > A Ar Ar N N – ( ) 2 ⁄ N N – ( ) 4 ⁄ Theorem 8.2 Any algorithm that sorts by exchanging adjacent elements requires   time on  average. Proof  The average number of inversions is initially  . Each swap removes only  one inversion, so   swaps are required. Ω N 2 ( ) N N – ( ) 4 ⁄ Ω N 2 ( )

8.4 shellsort The lower-bound proof shows that  quadratic performance is inherent  in any algorithm  that sorts by performing adjacent  comparisons. This proof is an example of a lower-bound proof. It is valid not only for the insertion sort, which performs adjacent exchanges implicitly, but also for other simple algorithms such as the bubble sort and the selection sort, which we do not describe here. In fact, it is valid over an entire class of algorithms, including undiscovered ones, that perform only adjacent exchanges. Unfortunately, any computational conﬁrmation of a proof applying to a class of algorithms would require running all algorithms in the class. That is impossible because there are inﬁnitely many possible algorithms. Hence any attempt at conﬁrmation would apply only to the algorithms that are run. This restriction makes the conﬁrmation of the validity of lower-bound proofs more difﬁcult than the usual single-algorithm upper bounds that we are accustomed to. A computation could only disprove a lower-bound conjecture; it could never prove it in general. Although this lower-bound proof is rather simple, proving lower bounds is in general much more complicated than proving upper bounds. Lower-bound arguments are much more abstract than their upper-bound counterparts. This lower bound shows us that, for a sorting algorithm to run in subquadratic or o(N2) time, it must make comparisons and, in particular, exchanges between elements that are far apart. A sorting algorithm progresses by eliminating inversions. To run efﬁciently, it must eliminate more than just one inversion per exchange. 8.4 shellsort Shellsort is a subquadratic algorithm  that works well in  practice and is simple to code. The  performance of  Shellsort is highly  dependent on the  increment sequence and  requires a challenging (and not completely resolved)  analysis. The ﬁrst algorithm to improve on the insertion sort substantially was Shellsort, which was discovered in 1959 by Donald Shell. Though it is not the fastest algorithm known, Shellsort is a subquadratic algorithm whose code is only slightly longer than the insertion sort, making it the simplest of the faster algorithms. Shell’s idea was to avoid the large amount of data movement, ﬁrst by comparing elements that were far apart and then by comparing elements that were less far apart, and so on, gradually shrinking toward the basic insertion sort. Shellsort uses a sequence   called the increment sequence. Any increment sequence will do as long as h1 = 1, but some choices are better than others. After a phase, using some increment hk, we have  for every i where i + hk is a valid index; all elements spaced hk apart are sorted. The array is then said to be hk-sorted. For example, Figure 8.5 shows an array after several phases of Shellsort. After a 5-sort, elements spaced ﬁve apart are guaranteed to be in correct sorted order. In the ﬁgure, elements spaced ﬁve apart are identically shaded h1 h2 … ht , , , a i[ ] a i hk + [ ] ≤

chapter 8 sorting algorithms and are sorted relative to each other. Similarly, after a 3-sort, elements spaced three apart are guaranteed to be in sorted order, relative to each other. An important property of Shellsort (which we state without proof) is that an hksorted array that is then hk –1-sorted remains hk-sorted. If this were not the case, the algorithm would likely be of little value because work done by early phases would be undone by later phases. A diminishing gap  sort is another  name for Shellsort. In general, an hk-sort requires that, for each position i in  , , , we place the element in the correct spot among  , and so on. Although this order does not affect the implementation, careful examination shows that an hk-sort performs an insertion sort on hk independent subarrays (shown in different shades in Figure 8.5). Therefore, not surprisingly, in Figure 8.7, which we come to shortly, lines 9 to 17 represent a gap insertion sort. In a gap insertion sort, after the loop has been executed, elements separated by a distance of gap in the array are sorted. For instance, when gap is 1, the loop is identical, statement by statement, to an insertion sort.Thus Shellsort is also known as diminishing gap sort. Shell’s increment  sequence is an  improvement over  the insertion sort  (although better  sequences are  known). As we have shown, when gap is 1 the inner loop is guaranteed to sort the array a. If gap is never 1, there is always some input for which the array cannot be sorted. Thus Shellsort sorts so long as gap eventually equals 1. The only issue remaining, then, is to choose the increment sequence. Shell suggested starting gap at   and halving it until it reaches 1, after which the program can terminate. Using these increments, Shellsort represents a substantial improvement over the insertion sort, despite the fact that it nests three for loops instead of two, which is usually inefﬁcient. By altering the sequence of gaps, we can further improve the algorithm’s performance. A

chapt er randomization Many situations in computing require the use of random numbers. For example, modern cryptography, simulation systems, and, surprisingly, even searching and sorting algorithms rely on random number generators. Yet good random number generators are difﬁcult to implement. In this chapter we discuss the generation and use of random numbers. In this chapter, we show n How random numbers are generated n How random permutations are generated n How random numbers allow the design of efﬁcient algorithms, using a general technique known as the randomized algorithm 9.1 why do we need  random numbers? Random numbers are used in many applications. In this section we discuss a few of the most common ones.

chapter 9 randomization Random numbers  have many important uses, including  cryptography, simulation, and program  testing. One important application of random numbers is in program testing. Suppose, for example, that we want to test whether a sorting algorithm written in Chapter 8 actually works. Of course, we could provide some small amount of input, but if we want to test the algorithms for the large data sets they were designed for, we need lots of input. Providing sorted data as input tests one case, but more convincing tests would be preferable. For instance, we would want to test the program by perhaps running 5,000 sorts for inputs of size 1,000. To do so requires writing a routine to generate the test data, which in turn requires the use of random numbers. A permutation of  1, 2, ..., N is a  sequence of N integers that includes  each of 1, 2,  ..., N exactly once. Once we have the random number inputs, how do we know whether the sorting algorithm works? One test is to determine whether the sort arranged the array in nondecreasing order. Clearly, we can run this test in a linear-time sequential scan. But how do we know that the items present after the sort are the same as those prior to the sort? One method is to ﬁx the items in an arrangement of 1, 2, ..., N. In other words, we start with a random permutation of the ﬁrst N integers. A permutation of 1, 2, ..., N is a sequence of N integers that includes each of 1, 2, ..., N exactly once. Then, no matter what permutation we start with, the result of the sort will be the sequence 1, 2, ..., N, which is also easily tested. In addition to helping us generate test data to verify program correctness, random numbers are useful in comparing the performance of various algorithms. The reason is that, once again, they can be used to provide a large number of inputs. Another use of random numbers is in simulations. If we want to know the average time required for a service system (for example, teller service in a bank) to process a sequence of requests, we can model the system on a computer. In this computer simulation we generate the request sequence with random numbers. Still another use of random numbers is in the general technique called the randomized algorithm, wherein a random number is used to determine the next step performed in the algorithm. The most common type of randomized algorithm involves selecting from among several possible alternatives that are more or less indistinguishable. For instance, in a commercial computer chess program, the computer generally chooses its ﬁrst move randomly rather than playing deterministically (i.e., rather than always playing the same move). In this chapter we look at several problems that can be solved more efﬁciently by using a randomized algorithm. 9.2 random number generators How are random numbers generated? True randomness is impossible to achieve on a computer, because any numbers obtained depend on the algorithm

9.2 random number generators used to generate them and thus cannot possibly be random. Generally, it is sufﬁcient to produce pseudorandom numbers, or numbers that appear to be random because they satisfy many of the properties of random numbers. Producing them is much easier said than done. Suppose that we need to simulate a coin ﬂip. One way to do so is to examine the system clock. Presumably, the system clock maintains the number of seconds as part of the current time. If this number is even, we can return 0 (for heads); if it is odd, we can return 1 (for tails). The problem is that this strategy does not work well if we need a sequence of random numbers. One second is a long time, and the clock might not change at all while the program is running, generating all 0s or all 1s, which is hardly a random sequence. Even if the time were recorded in units of microseconds (or smaller) and the program were running by itself, the sequence of numbers generated would be far from random because the time between calls to the generator would be essentially identical on every program invocation. In a uniform distribution, all numbers  in the speciﬁed  range are equally  likely to occur. What we really need is a sequence of pseudorandom numbers, that is, a sequence with the same properties as a random sequence. Suppose that we want random numbers between 0 and 999, uniformly distributed. In a uniform distribution, all numbers in the speciﬁed range are equally likely to occur. Other distributions are also widely used. The class skeleton shown in Figure 9.1 supports several distributions, and some of the basic methods are identical to the java.util.Random class. Most distributions can be derived from the uniform distribution, so that is the one we consider ﬁrst. The following properties hold if the sequence 0, ..., 999 is a true uniform distribution. n The ﬁrst number is equally likely to be 0, 1, 2, ..., 999. n The ith number is equally likely to be 0, 1, 2, ..., 999. n The expected average of all the generated numbers is 499.5. Typically a random  sequence, rather  than one random  number, is required. These properties are not particularly restrictive. For instance, we could generate the ﬁrst number by examining a system clock that was accurate to 1 ms and then using the number of milliseconds. We could generate subsequent numbers by adding 1 to the preceding number, and so on. Clearly, after 1,000 numbers are generated, all the previous properties hold. However, stronger properties do not.  Two stronger properties that would hold for uniformly distributed random numbers are the following. n The sum of two consecutive random numbers is equally likely to be even or odd. n If 1,000 numbers are randomly generated, some will be duplicated. (Roughly 368 numbers will never appear.) Pseudorandom numbers have  many properties of  random numbers.  Good random number generators are  hard to ﬁnd.

chapter 9 randomization figure 9.1 Skeleton for the Random class that generates random numbers package weiss.util; // Random class // // CONSTRUCTION: with (a) no initializer or (b) an integer //     that specifies the initial state of the generator. //     This random number generator is really only 31 bits, //     so it is weaker than the one in java.util. // // ******************PUBLIC OPERATIONS********************* //     Return a random number according to some distribution: // int nextInt( )                          --> Uniform, [1 to 2^31-1) // double nextDouble( )                    --> Uniform, (0 to 1) // int nextInt( int high )                 --> Uniform [0..high) // int nextInt( int low, int high )        --> Uniform [low..high] // int nextPoisson( double expectedVal )   --> Poisson // double nextNegExp( double expectedVal ) --> Negative exponential // void permute( Object [ ] a )            --> Randomly permutate /**  * Random number class, using a 31-bit  * linear congruential generator.  */ public class Random {     public Random( )       { /* Figure 9.2 */ }     public Random( int initialValue )       { /* Figure 9.2 */ }     public int nextInt( )       { /* Figure 9.2 */ }     public int nextInt( int high )       { /* Implementation in online code. */ }     public double nextDouble( )       { /* Implementation in online code. */ }     public int nextInt( int low, int high )       { /* Implementation in online code. */ }     public int nextPoisson( double expectedValue )       { /* Figure 9.4 */ }     public double nextNegExp( double expectedValue )       { /* Figure 9.5 */ }     public static final void permute( Object [ ] a )       { /* Figure 9.6 */ }     private void swapReferences( Object [ ] a, int i, int j )       { /* Implementation in online code. */ }     private int state; }

9.2 random number generators Our numbers do not satisfy these properties. Consecutive numbers always sum to an odd number, and our sequence is duplicate-free. We say then that our simple pseudorandom number generator has failed two statistical tests. All pseudorandom number generators fail some statistical tests, but the good generators fail fewer tests than the bad ones. (See Exercise 9.16 for a common statistical test.) The linear congruential generator is a  good algorithm for  generating uniform  distributions. In this section we describe the simplest uniform generator that passes a reasonable number of statistical tests. By no means is it the best generator. However, it is suitable for use in applications wherein a good approximation to a random sequence is acceptable. The method used is the linear congruential generator, which was ﬁrst described in 1951. The linear congruential generator is a good algorithm for generating uniform distributions. It is a random number generator in which numbers  ,  ,   are generated that satisfy (9.1) Equation 9.1 states that we can get the  th number by multiplying the ith number by some constant A and computing the remainder when the result is divided by M. In Java we would have x[ i + 1 ] = A * x[ i ] % M The seed is the initial value of the  random number  generator. We specify the constants A and M shortly. Note that all generated numbers will be smaller than M. Some value   must be given to start the sequence. This initial value of the random number generator is the seed. If  , the sequence is not random because it generates all zeros. But if A and M are carefully chosen, any other seed satisfying   is equally valid. If M is prime,   is never 0. For example, if  ,  , and the seed  , the numbers generated are 7, 5, 2, 3, 10, 4, 6, 9, 8, 1, 7, 5, 2, ... (9.2) Generating a number a second time results in a repeating sequence. In our case the sequence repeats after   numbers. The length of a sequence until a number is repeated is called the period of the sequence. The period obtained with this choice of A is clearly as good as possible because all nonzero numbers smaller than M are generated. (We must have a repeated number generated on the 11th iteration.) If M is prime, several choices of A give a full period of  , and this type of random number generator is called a full-period linear congruential generator. Some choices of A do not give a full period. For instance, if  and  , the sequence has a short period of 5: 5, 3, 4, 9, 1, 5, 3, 4, ... (9.3) X1 X2 … Xi + AXi modM ( ) = i + ( ) X0 X0 = X0 ≤ M < Xi M = A = X0 = M – = M – A = X0 = The length of a  sequence until a  number is repeated  is called its period. A random number  generator with  period P generates the same sequence  of numbers after P iterations. A full-period linear  congruential generator has period  M – 1.

chapter 9 randomization If we choose M to be a 31-bit prime, the period should be signiﬁcantly large for many applications. The 31-bit prime   = 2,147,483,647 is a common choice. For this prime, A = 48,271 is one of the many values that gives a full-period linear congruential generator. Its use has been well studied and is recommended by experts in the ﬁeld. As we show later in the chapter, tinkering with random number generators usually means breaking, so you are well advised to stick with this formula until told otherwise. Implementing this routine seems simple enough. If state represents the last value computed by the nextInt routine, the new value of state should be given by state = ( A * state ) % M;     // Incorrect Because of overﬂow, we must rearrange calculations. Unfortunately, if this computation is done on 32-bit integers, the multiplication is certain to overﬂow. Although Java provides a 64-bit long, using it is more computationally expensive than working with ints, not all languages support 64-bit math, and even so, at that point, a larger value of  would be warranted. Later in this section, we use a 48-bit M, but for now we use 32-bit arithmetic. If we stick with the 32-bit int, we could argue that the result is part of the randomness. However, overﬂow is unacceptable because we would no longer have the guarantee of a full period. A slight reordering allows the computation to proceed without overﬂow. Speciﬁcally, if Q and R are the quotient and remainder of  , then we can rewrite Equation 9.1 as (9.4) and the following conditions hold (see Exercise 9.5). n The ﬁrst term can always be evaluated without overﬂow. n The second term can be evaluated without overﬂow if  . n δ(Xi) evaluates to 0 if the result of the subtraction of the ﬁrst two terms is positive; it evaluates to 1 if the result of the subtraction is negative. Stick with these  numbers until you  are told otherwise. For the values of M and A, we have Q = 44,488 and R = 3,399. Consequently, and a direct application now gives an implementation of the Random class for generating random numbers. The resulting code is shown in Figure 9.2. The class works as long as int is capable of holding M. The routine nextInt returns the value of the state. Several additional methods are provided in the skeleton given in Figure 9.1. One generates a random real number in the open interval from 0 to 1, and another generates a random integer in a speciﬁed closed interval (see the online code). M – = M M A ⁄ Xi + A Xi mod Q ( ) ( ) R Xi/Q – Mδ Xi ( ) + = R Q < R Q <

9.2 random number generators Finally, the Random class provides a generator for nonuniform random numbers when they are required. In Section 9.3 we provide the implementation for the methods nextPoisson and nextNegExp. It may seem that we can get a better random number generator by adding a constant to the equation. For instance, we might conclude that would somehow be more random. However, when we use this equation, we see that (48,271 ⋅ 179,424,105 + 1) mod (231 – 1) = 179,424,105 Hence, if the seed is 179,424,105, the generator gets stuck in a cycle of period 1, illustrating how fragile these generators are. You might be tempted to assume that all machines have a random number generator at least as good as the one shown in Figure 9.2. Sadly, that is not the case. Many libraries have generators based on the function where B is chosen to match the number of bits in the machine’s integer, and C is odd. These libraries, like the nextInt routine in Figure 9.2, also return the newly computed state directly, instead of (for example) a value between 0 and 1. Unfortunately, these generators always produce values of   that alternate between even and odd—obviously an undesirable property. Indeed, the lower k bits cycle with a period of   (at best). Many other random number generators have much smaller cycles than the one we provided. These generators are not suitable for any application requiring long sequences of random numbers. The Java library has a generator of this form. However, it uses a 48-bit linear congruential generator and returns only the high 32 bits, thus avoiding the cycling problem in lower-order bits. The constants are A = 25,214,903,917, B = 48, and C = 11. This generator is also the basis for drand48 used in the C and C++ libraries. Because Java provides 64-bit longs, implementing a basic 48-bit random number generator in standard Java can be illustrated in only a page of code. It is somewhat slower than the 31-bit random number generator, but not much so and yields a signiﬁcantly longer period. Figure 9.3 shows a respectable implementation of this random number generator. Lines 10-13 show the basic constants of the random number generator. Because M is a power of 2, we can use bitwise operators (see Appendix C for more information on the bitwise operators). M =   can be computed Xi + 48,271Xi + ( ) mod 231 – ( ) = Xi + AXi C + ( ) mod 2B = Xi 2k 2B

chapter 9 randomization figure 9.2 Random number generator that works if INT_MAX is at least 231–1  private static final int A = 48271;     private static final int M = 2147483647;     private static final int Q = M / A;     private static final int R = M % A;     /**      * Construct this Random object with      * initial state obtained from system clock.      */     public Random( )     {         this( (int) ( System.nanoTime( ) % Integer.MAX_VALUE ) );     }     /**      * Construct this Random object with      * specified initial state      * @param initialValue the initial state.      */     public Random( int initialValue )     {         if( initialValue < 0 )         {             initialValue += M;             initialValue++;         }         state = initialValue;         if( state <= 0 )             state = 1;     }     /**      * Return a pseudorandom int, and change the      * internal state.      * @return the pseudorandom int.      */     public int nextInt( )     {         int tmpState = A * ( state % Q ) - R * ( state / Q );         if( tmpState >= 0 )             state = tmpState;         else             state = tmpState + M;         return state;     }

9.2 random number generators figure 9.3 48-Bit Random Number Generator package weiss.util; /**  * Random number class, using a 48-bit  * linear congruential generator.  * @author Mark Allen Weiss  */ public class Random48 {     private static final long A = 25214903917L;     private static final long B = 48;     private static final long C = 11;     private static final long M = (1L<<B);     private static final long MASK = M-1;     public Random48( )       { this( System.nanoTime( ) ); }     public Random48( long initialValue )       { state = initialValue & MASK; }     public int nextInt( )       { return next( 32 ); }     public int nextInt( int N )       { return (int) ( Math.abs( nextLong( ) ) % N ); }     public double nextDouble( )       { return ( ( (long) ( next( 26 ) ) << 27 ) + next( 27 ) ) / (double)( 1L << 53 ); }     public long nextLong( ) { return   ( (long) ( next( 32 ) ) << 32 ) + next( 32 ); }     /**      * Return specified number of random bits      * @param bits number of bits to return      * @return specified random bits      * @throws IllegalArgumentException if bits is more than 32      */     private int next( int bits )     {         if( bits <= 0 || bits > 32 )             throw new IllegalArgumentException( );         state = ( A * state + C ) & MASK;         return (int) ( state >>> ( B - bits ) );     }     private long state; }

chapter 9 randomization by a bit shift, and instead of using the modulus operator %, we can use a bitwise and operator. This is because MASK=M-1 consists of the low 48 bits all set to 1, and a bitwise and operator with MASK thus has the effect of yielding a 48-bit result. The next routine returns a speciﬁed number (at most 32) of random bits from the computed state, using the high order bits which are more random than the lower bits. Line 45 is a direct application of the previously stated linear congruential formula, and line 47 is a bitwise shift (zero-ﬁlled in the high bits to avoid negative numbers). Zero-parameter nextInt obtains 32 bits; nextLong obtains 64 bits in two separate calls; nextDouble obtains 53 bits (representing the mantissa; the other 11 bits of a double represent the exponent) also in two separate calls; and one-parameter nextInt uses a mod operator to obtain a pseudorandom number in the speciﬁed range. The exercises suggest some improvements that are possible for the one-parameter nextInt, when the parameter N is a power of 2. The 48-bit random number generator (and even the 31-bit generator) is quite adequate for many applications, simple to implement in 64-bit arithmetic, and uses little space. However linear congruential generators are unsuitable for some applications, such as cryptography or in simulations that require large numbers of highly independent and uncorrelated random numbers. 9.3 nonuniform random numbers Not all applications require uniformly distributed random numbers. For example, grades in a large course are generally not uniformly distributed. Instead, they satisfy the classic bell curve distribution, more formally known as the normal or Gaussian distribution. A uniform random number generator can be used to generate random numbers that satisfy other distributions. The Poisson distribution models the  number of occurrences of a rare  event and is used in  simulations. An important nonuniform distribution that occurs in simulations is the Poisson distribution, which models the number of occurrences of a rare event. Occurrences that happen under the following circumstances satisfy the Poisson distribution. 1. The probability of one occurrence in a small region is proportional to the size of the region. 2. The probability of two occurrences in a small region is proportional to the square of the size of the region and is usually small enough to be ignored.

9.3 nonuniform random numbers 3. The event of getting k occurrences in one region and the event of getting j occurrences in another region disjoint from the ﬁrst region are independent. (Technically this statement means that you can get the probability of both events simultaneously occurring by multiplying the probability of individual events.) 4. The mean number of occurrences in a region of some size is known. If the mean number of occurrences is the constant a, the probability of exactly k occurrences is  . The Poisson distribution generally applies to events that have a low probability of a single occurrence. For example, consider the event of purchasing a winning lottery ticket, where the odds of winning the jackpot are 14,000,000 to 1. Presumably the picked numbers are more or less random and independent. If a person buys 100 tickets, the odds of winning become 140,000 to 1 (the odds improve by a factor of 100), so condition 1 holds. The odds of the person holding two winning tickets are negligible, so condition 2 holds. If someone else buys 10 tickets, that person’s odds of winning are 1,400,000 to 1, and these odds are independent of the ﬁrst person’s, so condition 3 holds. Suppose that 28,000,000 tickets are sold. The mean number of winning tickets in this situation is 2 (the number we need for condition 4). The actual number of winning tickets is a random variable with an expected value of 2, and it satisﬁes the Poisson distribution. Thus the probability that exactly k winning tickets have been sold is  , which gives the distribution shown in Figure 9.4. If the expected number of winners is the constant a, the probability of k winning tickets is  . To generate a random unsigned integer according to a Poisson distribution that has an expected value of a, we can adopt the following strategy (whose mathematical justiﬁcation is beyond the scope of this book): Repeatedly generate uniformly distributed random numbers in the interval (0, 1) until their product is smaller than (or equal to)  . The code shown in Figure 9.5 does just that, using a mathematically equivalent technique that is less sensitive to overﬂow. The code adds the logarithm of the uniform random numbers until their sum is smaller than (or equal to) –a. ake a – k! ⁄ 2ke 2 – k! ⁄ figure 9.4 Distribution of lottery  winners if the  expected number of  winners is 2 Winning Tickets Frequency 0.135 0.271 0.271 0.180 0.090 0.036 ake a – k! ⁄ e a –

chapter 9 randomization The negative exponential distribution has the same mean  and variance. It is  used to model the  time between  occurrences of random events. Another important nonuniform distribution is the negative exponential distribution, shown in Figure 9.6, which has the same mean and variance and is used to model the time between occurrences of random events. We use it in the simulation application shown in Section 13.2. Many other distributions are commonly used. Our main purpose here is to show that most can be generated from the uniform distribution. Consult any book on probability and statistics to ﬁnd out more about these functions.  9.4 generating a random  permutation Consider the problem of simulating a card game. The deck consists of 52 distinct cards, and in the course of a deal, we must generate cards from the deck, without duplicates. In effect, we need to shufﬂe the cards and then iterate figure 9.5 Generation of a  random number  according to the  Poisson distribution  /**  * Return an int using a Poisson distribution, and  * change the internal state.  * @param expectedValue the mean of the distribution.  * @return the pseudorandom int.  */     public int nextPoisson( double expectedValue )     {         double limit = -expectedValue;         double product = Math.log( nextDouble( ) );         int count;         for( count = 0; product > limit; count++ )             product += Math.log( nextDouble( ) );         return count;     } figure 9.6 Generation of a  random number  according to the  negative exponential  distribution /**  * Return a double using a negative exponential  * distribution, and change the internal state.  * @param expectedValue the mean of the distribution.  * @return the pseudorandom double.  */     public double nextNegExp( double expectedValue )     {         return - expectedValue * Math.log( nextDouble( ) );     }

9.4 generating a random permutation through the deck. We want the shufﬂe to be fair. That is, each of the 52! possible orderings of the deck should be equally likely as a result of the shufﬂe. A random permutation can be generated in linear time,  using one random  number per item. This type of problem involves the use of a random permutation. In general, the problem is to generate a random permutation of 1, 2, ..., N, with all permutations being equally likely. The randomness of the random permutation is, of course, limited by the randomness of the pseudorandom number generator. Thus all permutations being equally likely is contingent on all the random numbers being uniformly distributed and independent. We demonstrate that random permutations can be generated in linear time, using one random number per item. A routine, permute, to generate a random permutation is shown in Figure 9.7. The loop performs a random shufﬂing. In each iteration of the loop, we swap a[j] with some array element in positions 0 to j (it is possible to perform no swap). The correctness of  permute is subtle. Clearly, permute generates shufﬂed permutations. But are all permutations equally likely? The answer is both yes and no. The answer, based on the algorithm, is yes. There are N! possible permutations, and the number of different possible outcomes of the N – 1 calls to nextInt at line 11 is also N! The reason is that the ﬁrst call produces 0 or 1, so it has two outcomes. The second call produces 0, 1, or 2, so it has three outcomes. The last call has N outcomes. The total number of outcomes is the product of all these possibilities because each random number is independent of the previous random numbers. All we have to show is that each sequence of random numbers corresponds to only one permutation. We can do so by working backward (see Exercise 9.6). However, the answer is actually no—all permutations are not equally likely. There are only 231 – 2 initial states for the random number generator, so there can be only 231 – 2 different permutations. This condition could be a problem in some situations. For instance, a program that generates 1,000,000 permutations (perhaps by splitting the work among many computers) to measure the performance of a sorting algorithm almost certainly generates some permutations twice—unfortunately. Better random number generators are needed to help the practice meet the theory. figure 9.7 A permutation routine /**  * Randomly rearrange an array.  * The random numbers used depend on the time and day.  * @param a the array.  */     public static final void permute( Object [ ] a )     {         Random r = new Random( );         for( int j = 1; j < a.length; j++ )             swapReferences( a, j, r.nextInt( 0, j ) );     }

chapter 9 randomization Note that rewriting the call to swap with the call to r.nextInt(0,n-1) does not work, even for three elements. There are 3! = 6 possible permutations, and the number of different sequences that could be computed by the three calls to nextInt is 33 = 27. Because 6 does not divide 27 exactly, some permutations must be more likely than others. 9.5 randomized algorithms Suppose that you are a professor who is giving weekly programming assignments. You want to ensure that the students are doing their own programs or, at the very least, that they understand the code that they are submitting. One solution is to give a quiz on the day each program is due. However, these quizzes take time from class and doing so might be practical for only roughly half the programs. Your problem is to decide when to give the quizzes. Of course, if you announce the quizzes in advance, that could be interpreted as an implicit license to cheat for the 50 percent of the programs that will not get a quiz. You could adopt the unannounced strategy of giving quizzes on alternate programs, but students would quickly ﬁgure out that strategy. Another possibility is to give quizzes on what seem like the important programs, but that would likely lead to similar quiz patterns from semester to semester. Student grapevines being what they are, this strategy would probably be worthless after one semester. One method that seems to eliminate these problems is to ﬂip a coin. You make a quiz for every program (making quizzes is not nearly as time consuming as grading them), and at the start of class, you ﬂip a coin to decide whether the quiz is to be given. This way neither you nor your students can know before class whether a quiz will be given. Also, the patterns do not repeat from semester to semester. The students can expect a quiz to occur with 50 percent probability, regardless of previous quiz patterns. The disadvantage of this strategy is that you could end up giving no quizzes during an entire semester. Assuming a large number of programming assignments, however, this is not likely to happen unless the coin is suspect. Each semester the expected number of quizzes is half the number of programs, and with high probability, the number of quizzes will not deviate much from this. A randomized algorithm uses  random numbers  rather than  deterministic decisions to control  branching. This example illustrates the randomized algorithm, which uses random numbers, rather than deterministic decisions, to control branching. The running time of the algorithm depends not only on the particular input, but also on the random numbers that occur. The worst-case running time of a randomized algorithm is almost always the same as the worst-case running time of the nonrandomized algorithm. The

9.5 randomized algorithms important difference is that a good randomized algorithm has no bad inputs— only bad random numbers (relative to the particular input). This difference may seem only philosophical, but actually it is quite important, as we show in the following example. The running time of  a randomized algorithm depends on  the random numbers that occur, as  well as the particular input. Let us say that your boss asks you to write a program to determine the median of a group of 1,000,000 numbers. You are to submit the program and then run it on an input that the boss will choose. If the correct answer is given within a few seconds of computing time (which would be expected for a linear algorithm), your boss will be very happy, and you will get a bonus. But if your program does not work or takes too much time, your boss will ﬁre you for incompetence. Your boss already thinks that you are overpaid and is hoping to be able to take the second option. What should you do? The quickselect algorithm described in Section 8.7 might seem like the way to go. Although the algorithm (see Figure 8.23) is very fast on average, recall that it has quadratic worst-case time if the pivot is continually poor. By using median-of-three partitioning, we have guaranteed that this worst case will not occur for common inputs, such as those that have been sorted or that contain a lot of duplicates. However, there is still a quadratic worst case, and as Exercise 8.8 showed, the boss will read your program, realize how you are choosing the pivot, and be able to construct the worst case. Consequently, you will be ﬁred. Randomized quickselect is statistically  guaranteed to work  in linear time. By using random numbers, you can statistically guarantee the safety of your job. You begin the quickselect algorithm by randomly shufﬂing the input by using lines 10 and 11 in Figure 9.7.1 As a result, your boss essentially loses control of specifying the input sequence. When you run the quickselect algorithm, it will now be working on random input, so you expect it to take linear time. Can it still take quadratic time? The answer is yes. For any original input, the shufﬂing may get you to the worst case for quickselect, and thus the result would be a quadratic-time sort. If you are unfortunate enough to have this happen, you lose your job. However, this event is statistically impossible. For a million items, the chance of using even twice as much time as the average would indicate is so small that you can essentially ignore it. The computer is much more likely to break. Your job is secure. Instead of using a shufﬂing technique, you can achieve the same result by choosing the pivot randomly instead of deterministically. Take a random item in the array and swap it with the item in position low. Take another 1. You need to be sure that the random number generator is sufﬁciently random and that its output cannot be predicted by the boss.

chapter 9 randomization random item and swap it with the item in position high. Take a third random item and swap it with the item in the middle position. Then continue as usual. As before, degenerate partitions are always possible, but they now happen as a result of bad random numbers, not bad inputs. Let us look at the differences between randomized and nonrandomized algorithms. So far we have concentrated on nonrandomized algorithms. When calculating their average running times, we assume that all inputs are equally likely. This assumption does not hold, however, because nearly sorted input, for instance, occurs much more often than is statistically expected. This situation can cause problems for some algorithms, such as quicksort. But when we use a randomized algorithm, the particular input is no longer important. The random numbers are important, and we get an expected running time, in which we average over all possible random numbers for any particular input. Using quickselect with random pivots (or a shufﬂe preprocessing step) gives an O(N) expected time algorithm. That is, for any input, including already sorted input, the running time is expected to be O(N), based on the statistics of random numbers. On the one hand an expected time bound is somewhat stronger than an averagecase time bound because the assumptions used to generate it are weaker (random numbers versus random input) but it is weaker than the corresponding worst-case time bound. On the other hand, in many instances solutions that have good worst-case bounds frequently have extra overhead built in to assure that the worst case does not occur. The O(N) worst-case algorithm for selection, for example, is a marvelous theoretical result but is not practical. Some randomized  algorithms work in  a ﬁxed amount of  time but randomly  make mistakes  (presumably with  low probability).  These mistakes are  false positives or  false negatives. Randomized algorithms come in two basic forms. The ﬁrst, as already shown, always gives a correct answer but it could take a long time, depending on the luck of the random numbers. The second type is what we examine in the remainder of this chapter. Some randomized algorithms work in a ﬁxed amount of time but randomly make mistakes (presumably with low probability), called false positives or false negatives. This technique is commonly accepted in medicine. False positives and false negatives for most tests are actually fairly common, and some tests have surprisingly high error rates. Furthermore, for some tests the errors depend on the individual, not random numbers, so repeating the test is certain to produce another false result. In randomized algorithms we can rerun the test on the same input using different random numbers. If we run a randomized algorithm 10 times and get 10 positives—and if a single false positive is an unlikely occurrence (say, 1 chance in 100)—the probability of 10 consecutive false positives (1 chance in 10010 or one hundred billion billion) is essentially zero.

9.6 randomized primality testing 9.6 randomized primality testing Recall that in Section 7.4 we described some numerical algorithms and showed how they could be used to implement the RSA encryption scheme. An important step in the RSA algorithm is to produce two prime numbers p and q. We can ﬁnd a prime number by repeatedly trying successive odd numbers until we ﬁnd one that is prime. Thus the issue boils down to determining whether a given number is prime. Trial division is the  simplest algorithm  for primality testing. It is fast for  small (32-bit) numbers but cannot be  used for larger  numbers. The simplest algorithm for primality testing is trial division. In this algorithm, an odd number greater than 3 is prime if it is not divisible by any other odd number smaller than or equal to  . A direct implementation of this strategy is shown in Figure 9.8. Trial division is reasonably fast for small (32-bit) numbers, but it is unusable for larger numbers because it could require the testing of roughly  divisors, thus using   time.2 What we need is a test whose running time is of the same order of magnitude as the power routine in Section 7.4.2. A well-known theorem, called Fermat’s Little Theorem, looks promising. We state and provide a proof of it in Theorem 9.1 for completeness, but the proof is not needed for an understanding of the primality-testing algorithm. Fermat’s Little Theorem is necessary  but not sufﬁcient to  establish primality. If the converse of Fermat’s Little Theorem were true, then we would have a primality-testing algorithm that would be computationally equivalent to modular exponentiation (i.e., O(log N)). Unfortunately, the converse is not true. For example, 2340 ≡ 1(mod 341), but 341 is composite (11 × 31). 2. Though   may seem small, if N is a 100-digit number, then   is still a 50-digit number; tests that take   time are thus out of the question for the BigInteger type. N N 2 ⁄ O N ( ) N N O N ( ) Fermat’s Little Theorem: If P is prime and  , then AP – 1 ≡ 1(mod P). Theorem 9.1 Consider any  . Clearly, Ak ≡0(mod P) is impossible because P is prime and  is greater than A and k. Now consider any  .   would imply  , which is impossible by the previous argument because  . Thus the sequence  , when considered (mod P), is a  permutation of  . The product of both sequences (mod P) must be  equivalent (and non-zero), yielding the equivalence AP – 1(P – 1)! ≡(P – 1)! (mod P) from which the theorem follows. Proof A P < < k P < ≤ i j P < < ≤ Ai Aj mod P ( ) ≡ A j i – ( ) 0 mod P ( ) ≡ j i – P < ≤ A 2A … P – ( )A , , , 1 2 … P – , , ,

chapter 9 randomization To do the primality test, we need an additional theorem, Theorem 9.2. A combination of Theorems 9.1 and 9.2 is useful. Let A be any integer between 2 and N – 2. If we compute AN – 1(mod N) and the result is not 1, we know that N cannot be prime; otherwise, we would contradict Fermat’s Little Theorem. As a result, A is a value that proves that N is not prime. We say then that A is a witness to N’s compositeness. Every composite number N has some witnesses A, but for some numbers, called the Carmichael numbers, these witnesses are hard to ﬁnd. We need to be sure that we have a high probability of ﬁnding a witness no matter what the choice of N is. To improve our chances, we use Theorem 9.1. In the course of computing  , we compute  . So we let and  . Note that X and Y are computed automatically as part of the power routine. If Y is 1 and if X is not ±1(mod N), then by Theorem 9.1, N cannot be prime. We can return 0 for the value of   when that condition is detected, and N will appear to have failed the test of primality implied by Fermat’s Little Theorem. The routine witness, shown in Figure 9.9, computes Ai(mod P), augmented to return 0 if a violation of Theorem 9.1 is detected. If witness does not return 1, then A is a witness to the fact that N cannot be prime. Lines 12 through 14 make a recursive call and produce X. We then compute  , as is figure 9.8 Primality testing by  trial division /**  * Returns true if odd integer n is prime.  */     public static boolean isPrime( long n )     {         for( int i = 3; i * i <= n; i += 2 )             if( n % i == 0 )                 return false; // not prime         return true;          // prime     } Theorem 9.2 If P is prime and X2 ≡ 1(mod P), then X ≡±1(mod P). Proof Because X2 – 1 ≡ 0(mod P) implies (X – 1)(X + 1) ≡ 0(mod P) and P is prime, then  X – 1 or X + 1 ≡ 0(mod P). Ai A i 2 ⁄ ( )2 X A i 2 ⁄ = Y X 2 = Ai X 2

9.6 randomized primality testing figure 9.9 A randomized test for primality /**  * Private method that implements the basic primality test.  * If witness does not return 1, n is definitely composite.  * Do this by computing a^i (mod n) and looking for  * nontrivial square roots of 1 along the way.  */     private static long witness( long a, long i, long n )     {         if( i == 0 )             return 1;         long x = witness( a, i / 2, n );         if( x == 0 )    // If n is recursively composite, stop             return 0;         // n is not prime if we find a nontrivial square root of 1         long y = ( x * x ) % n;         if( y == 1 && x != 1 && x != n - 1 )             return 0;         if( i % 2 != 0 )             y = ( a * y ) % n;         return y;     }     /**      * The number of witnesses queried in randomized primality test.      */     public static final int TRIALS = 5;     /**      * Randomized primality test.      * Adjust TRIALS to increase confidence level.      * @param n the number to test.      * @return if false, n is definitely not prime.      *     If true, n is probably prime.      */     public static boolean isPrime( long n )     {         Random r = new Random( );         for( int counter = 0; counter < TRIALS; counter++ )             if( witness( r.nextInt( (int) n - 3 ) + 2, n - 1, n ) != 1 )                 return false;         return true;     }

chapter 9 randomization normal for the power computation. We check whether Theorem 9.1 is violated, returning 0 if it is. Otherwise, we complete the power computation. If the algorithm  declares a number  not to be prime, it is  not prime with 100  percent certainty.  Each random  attempt has at  most a 25 percent  false positive rate. The only remaining issue is correctness. If our algorithm declares that N is composite, then N must be composite. If N is composite, are all witnesses? The answer, unfortunately, is no. That is, some choices of A will trick our algorithm into declaring that N is prime. In fact, if we choose A randomly, we have at most a 1/4 chance of failing to detect a composite number and thus making an error. Note that this outcome is true for any N. If it were obtained only by averaging over all N, we would not have a good enough routine. Analogous to medical tests, our algorithm generates false positives at most 25 percent of the time for any N. Some composites  will pass the test  and be declared  prime. A composite  is very unlikely to  pass 20 consecutive independent  random tests. These odds do not seem very good because a 25 percent error rate generally is considered very high. However, if we independently use 20 values of A, the chances that none of them will witness a composite number is 1/420, which is about one in a million million. Those odds are much more reasonable and can be made even better by using more trials. The routine isPrime, which is also shown in Figure 9.9, uses ﬁve trials.3

c hapt e r fun and games In this chapter we introduce three important algorithmic techniques and show how to use them by implementing programs to solve two recreational problems. The ﬁrst problem is the word search puzzle and involves ﬁnding words in a two-dimensional grid of characters. The second is optimal play in the game of Tic-Tac-Toe. In this chapter, we show n How to use the binary search algorithm to incorporate information from unsuccessful searches and to solve large instances of a word search problem in under 1 sec n How to use the alpha–beta pruning algorithm to speed up the recursive algorithm presented in Section 7.7 n How to use maps to increase the speed of the Tic-Tac-Toe algorithm 10.1 word search puzzles The input to the word search puzzle problem is a two-dimensional array of characters and a list of words, and the object is to ﬁnd the words in the grid. These words may be horizontal, vertical, or diagonal in any direction (for a total of eight directions). As an example, the grid shown in Figure 10.1

chapter 10 fun and games The word search  puzzle requires  searching for  words in a twodimensional grid of  letters. Words may  be oriented in one  of eight directions. contains the words this, two, fat, and that. The word this begins at row 0, column 0—the point (0, 0)—and extends to (0, 3); two goes from (0, 0) to (2, 0); fat goes from (3, 0) to (1, 2); and that goes from (3, 3) to (0, 0). (Additional, mostly shorter, words are not listed here.) 10.1.1   theory The brute-force  algorithm searches  each word in the  word list. We can use any of several naive algorithms to solve the word search puzzle problem. The most direct is the following brute-force approach: for each word W in the word list     for each row R         for each column C             for each direction D                 check if W exists at row R, column C in direction D An alternative algorithm searches  from each point in  the grid in each  direction for each  word length and  looks for the word  in the word list. Because there are eight directions, this algorithm requires eight word/row/ column (8WRC) checks. Typical puzzles published in magazines feature 40 or so words and a 16 × 16 grid, which involves roughly 80,000 checks. That number is certainly easy to compute on any modern machine. Suppose, however, that we consider the variation in which only the puzzle board is given and the word list is essentially an English dictionary. In this case, the number of words might be 40,000 instead of 40, resulting in 80,000,000 checks. Doubling the grid would require 320,000,000 checks, which is no longer a trivial calculation. We want an algorithm that can solve a puzzle of this size in a fraction of a second (not counting disk I/O time), so we must consider an alternative algorithm: for each row R     for each column C         for each direction D             for each word length L                 check if L chars starting at row R column C                             in direction D form a word figure 10.1 A sample word  search grid t h i s w a t s o a h g f g d t

10.1 word search puzzles Our implementation follows the  algorithm description. The lookups can be  done by a binary  search. This algorithm rearranges the loop to avoid searching for every word in the word list. If we assume that words are limited to 20 characters, the number of checks used by the algorithm is 160RC. For a 32 × 32 puzzle, this number is roughly 160,000 checks. The problem, of course, is that we must now decide whether a word is in the word list. If we use a linear search, we lose. If we use a good data structure, we can expect an efﬁcient search. If the word list is sorted, which is to be expected for an online dictionary, we can use a binary search (shown in Figure 5.12) and perform each check in roughly   string comparisons. For 40,000 words, doing so involves perhaps 16 comparisons per check, for a total of less than 3,000,000 string comparisons. This number of comparisons can certainly be done in a few seconds and is a factor of 100 better than the previous algorithm. If a character  sequence is not a  preﬁx of any word  in the dictionary, we  can terminate  searching in that  direction. We can further improve the algorithm based on the following observation. Suppose that we are searching in some direction and see the character sequence qx. An English dictionary will not contain any words beginning with qx. So is it worth continuing the innermost loop (over all word lengths)? The answer obviously is no: If we detect a character sequence that is not a preﬁx of any word in the dictionary, we can immediately look in another direction. This algorithm is given by the following pseudocode: for each row R     for each column C         for each direction D             for each word length L                 check if L chars starting at row R column                             C in direction D form a word                 if they do not form a prefix,                     break;   // the innermost loop Preﬁx testing can  also be done by  binary search. The only remaining algorithmic detail is the implementation of the preﬁx test: Assuming that the current character sequence is not in the word list, how can we decide whether it is a preﬁx of some word in the word list? The answer turns out to be simple. Recall from Section 6.4.3 that the binarySearch method in the Collections API returns either the index of a match or the position of the smallest element that is at least as large as the target (as a negative number). The caller can easily check on whether a match is found. If a match is not found, verifying that the character sequence is a preﬁx of some word in the list also is easy, because, if it is, it must be a preﬁx of the word in the position implied in the return value (in Exercise 10.3 you are asked to prove this outcome). 10.1.2   java implementation Our Java implementation follows the algorithm description almost verbatim. We design a WordSearch class to store the grid and word list, as well as the corresponding input streams. The class skeleton is shown in Figure 10.2. The public W log

chapter 10 fun and games part of the class consists of a constructor and a single method, solvePuzzle. The private part includes the data members and supporting routines. figure 10.2 The WordSearch class skeleton 1 import java.io.BufferedReader; 2 import java.io.FileReader; 3 import java.io.InputStreamReader; 4 import java.io.IOException; 6 import java.util.Arrays; 7 import java.util.ArrayList; 8 import java.util.Iterator; 9 import java.util.List; 12 // WordSearch class interface: solve word search puzzle 13 // 14 // CONSTRUCTION: with no initializer 15 // ******************PUBLIC OPERATIONS****************** 16 // int solvePuzzle( )   --> Print all words found in the 17 //                          puzzle; return number of matches 19 public class WordSearch 20 { 21     public WordSearch( ) throws IOException 22       { /* Figure 10.3 */ } 23     public int solvePuzzle( ) 24       { /* Figure 10.7 */ } 26     private int rows; 27     private int columns; 28     private char theBoard[ ][ ]; 29     private String [ ] theWords; 30     private BufferedReader puzzleStream; 31     private BufferedReader wordStream; 32     private BufferedReader in = new 33                 BufferedReader( new InputStreamReader( System.in ) ); 35     private static int prefixSearch( String [ ] a, String x ) 36       { /* Figure 10.8 */ } 37     private BufferedReader openFile( String message ) 38       { /* Figure 10.4 */ } 39     private void readWords( ) throws IOException 40       { /* Figure 10.5 */ } 41     private void readPuzzle( ) throws IOException 42       { /* Figure 10.6 */ } 43     private int solveDirection( int baseRow, int baseCol, 44                                 int rowDelta, int colDelta ) 45       { /* Figure 10.8 */ } 46 }

10.1 word search puzzles The constructor  opens and reads  the data ﬁles. We  skimp on error  checks for brevity.  Figure 10.3 gives the code for the constructor. It merely opens and reads the two ﬁles corresponding to the grid and the word list. The supporting routine openFile, shown in Figure 10.4, repeatedly prompts for a ﬁle until an open is successful. The readWords routine, shown in Figure 10.5, reads the word list. figure 10.3 The WordSearch class constructor /**  * Constructor for WordSearch class.  * Prompts for and reads puzzle and dictionary files.  */     public WordSearch( ) throws IOException     {         puzzleStream = openFile( "Enter puzzle file" );         wordStream   = openFile( "Enter dictionary name" );         System.out.println( "Reading files..." );         readPuzzle( );         readWords( );     } /**  * Print a prompt and open a file.  * Retry until open is successful.  * Program exits if end of file is hit.  */     private BufferedReader openFile( String message )     {         String fileName = "";         FileReader theFile;         BufferedReader fileIn = null;         do         {             System.out.println( message + ": " );             try             {                 fileName = in.readLine( );                 if( fileName == null )                     System.exit( 0 );                 theFile = new FileReader( fileName );                 fileIn  = new BufferedReader( theFile );             }             catch( IOException e )               { System.err.println( "Cannot open " + fileName ); }         } while( fileIn == null );         System.out.println( "Opened " + fileName );         return fileIn;     } figure 10.4 The openFile routine  for opening either the  grid or word list file

chapter 10 fun and games The code includes an error check to ensure that the word list has been sorted. Similarly, readPuzzle, shown in Figure 10.6, reads the grid and is also concerned with error handling. We need to be sure that we can handle missing puzzles, and we want to warn the user if the grid is not rectangular.  We use two loops  to iterate over the  eight directions. The solvePuzzle routine shown in Figure 10.7 nests the row, column, and direction loops and then calls the private routine solveDirection for each possibility. The return value is the number of matches found. We give a direction by indicating a column direction and then a row direction. For instance, south is indicated by cd=0 and rd=1 and northeast by cd=1 and rd=-1; cd can range from -1 to 1 and rd from -1 to 1, except that both cannot be 0 simultaneously. All that remains to be done is to provide solveDirection, which is coded in Figure 10.8. The solveDirection routine constructs a string by starting at the base row and column and extending in the appropriate direction. We also assume that one-letter matches are not allowed (because any oneletter match would be reported eight times). At lines 14 through 16, we iterate and extend the string while ensuring that we do not go past the grid’s boundary. At line 18 we tack on the next character, using +=, and perform a binary figure 10.5 The readWords routine for reading the word list /**  * Routine to read the dictionary.  * Error message is printed if dictionary is not sorted.  */     private void readWords( ) throws IOException     {         List<String> words = new ArrayList<String>( );         String lastWord = null;         String thisWord;         while( ( thisWord = wordStream.readLine( ) ) != null )         {             if( lastWord != null && thisWord.compareTo( lastWord ) < 0 )             {                 System.err.println( "Dictionary is not sorted... skipping" );                 continue;             }             words.add( thisWord );             lastWord = thisWord;         }         theWords = new String[ words.size( ) ]; theWords = words.toArray( theWords );     }

10.2 the game of tic-tac-toe The minimax strategy examines lots  of positions. We can  get by with less  without losing any  information. search at line 19. If we do not have a preﬁx, we can stop looking and return. Otherwise, we know that we have to continue after checking at line 26 for a possible exact match. Line 35 returns the number of matches found when the call to solveDirection can ﬁnd no more words. A simple main program is shown in Figure 10.9. 10.2 the game of tic-tac-toe Recall from Section 7.7 a simple algorithm, known as the minimax strategy, allows the computer to select an optimal move in a game of Tic-Tac-Toe. This recursive strategy involves the following decisions.  figure 10.6 The readPuzzle routine for reading the grid /**  * Routine to read the grid.  * Checks to ensure that the grid is rectangular.  * Checks to make sure that capacity is not exceeded is omitted.  */     private void readPuzzle( ) throws IOException     {         String oneLine;         List<String> puzzleLines = new ArrayList<String>( );         if( ( oneLine = puzzleStream.readLine( ) ) == null )             throw new IOException( "No lines in puzzle file" );         columns = oneLine.length( );         puzzleLines.add( oneLine );         while( ( oneLine = puzzleStream.readLine( ) ) != null )         {             if( oneLine.length( ) != columns )                 System.err.println( "Puzzle is not rectangular; skipping row" );             else                 puzzleLines.add( oneLine );         }         rows = puzzleLines.size( );         theBoard = new char[ rows ][ columns ];         int r = 0;         for( String theLine : puzzleLines )             theBoard[ r++ ] = theLine.toCharArray( );     }

chapter 10 fun and games 1. A terminal position can immediately be evaluated, so if the position is terminal, return its value. 2. Otherwise, if it is the computer’s turn to move, return the maximum value of all positions reachable by making one move. The reachable values are calculated recursively. A refutation is a  countermove that  proves that a proposed move is not  an improvement  over moves previously considered. If  we ﬁnd a refutation,  we do not have to  examine any more  moves and the  recursive call can  return. 3. Otherwise, it is the human player’s turn to move. Return the minimum value of all positions reachable by making one move. The reachable values are calculated recursively. 10.2.1   alpha–beta pruning Although the minimax strategy gives an optimal Tic-Tac-Toe move, it performs a lot of searching. Speciﬁcally, to choose the ﬁrst move, it makes roughly a half-million recursive calls. One reason for this large number of calls is that the algorithm does more searching than necessary. Suppose that the computer is considering ﬁve moves: C1, C2, C3, C4, and C5. Suppose also that the recursive evaluation of C1 reveals that C1 forces a draw. Now C2 is evaluated. At this stage, we have a position from which it would be the human player’s turn to move. Suppose that in response to C2, the human player can consider H2a, H2b, H2c, and H2d. Further, suppose that an evaluation of H2a shows a forced draw. Automatically, C2 is at best a draw and possibly even a loss for the computer (because the human player is assumed to play optimally). Because we need to improve on C1, we do not have to evaluate any of H2b, H2c, and H2d. We say that H2a is a refutation, meaning that it proves that figure 10.7 The solvePuzzle routine for searching  in all directions from  all starting points /**  * Routine to solve the word search puzzle.  * Performs checks in all eight directions.  * @return number of matches  */     public int solvePuzzle( )     {         int matches = 0;         for( int r = 0; r < rows; r++ )             for( int c = 0; c < columns; c++ )                 for( int rd = -1; rd <= 1; rd++ )                     for( int cd = -1; cd <= 1; cd++ )                         if( rd != 0 || cd != 0 )                             matches += solveDirection( r, c, rd, cd );         return matches;     }

10.2 the game of tic-tac-toe figure 10.8 Implementation of a  single search /**  * Search the grid from a starting point and direction.  * @return number of matches  */     private int solveDirection( int baseRow, int baseCol,                                 int rowDelta, int colDelta )     {         String charSequence = "";         int numMatches = 0;         int searchResult;         charSequence += theBoard[ baseRow ][ baseCol ];         for( int i = baseRow + rowDelta, j = baseCol + colDelta;                  i >= 0 && j >= 0 && i < rows && j < columns;                  i += rowDelta, j += colDelta )         {             charSequence += theBoard[ i ][ j ];             searchResult = prefixSearch( theWords, charSequence );             if( searchResult == theWords.length )                 break;             if( !theWords[ searchResult ].startsWith( charSequence ) )                 break;             if( theWords[ searchResult ].equals( charSequence ) )             {                 numMatches++;                 System.out.println( "Found " + charSequence + " at " +                                     baseRow + " " + baseCol + " to " +                                     i + " " + j );             }         }         return numMatches;     }     /**      * Performs the binary search for word search.      * Returns the last position examined this position      * either matches x, or x is a prefix of the mismatch, or there is      * no word for which x is a prefix.      */     private static int prefixSearch( String [ ] a, String x )     {         int idx = Arrays.binarySearch( a, x );         if( idx < 0 )             return -idx - 1;         else             return idx; }

chapter 10 fun and games C2 is not a better move than what has already been seen. Thus we return that C2 is a draw and keep C1 as the best move seen so far, as shown in Figure 10.10. In general, then, a refutation is a countermove that proves that a proposed move is not an improvement over moves previously considered. figure 10.9 A simple main routine  for the word search  puzzle problem   // Cheap main     public static void main( String [ ] args )     {         WordSearch p = null;         try         {             p = new WordSearch( );         }         catch( IOException e )         {             System.out.println( "IO Error: " );             e.printStackTrace( );             return;         }         System.out.println( "Solving..." );         p.solvePuzzle( );     } H2a C1 H2b H2c C2 C3 H2d DRAW DRAW ? ? ? Use best result Use worst result figure 10.10 Alpha–beta pruning:  After H2a is evaluated,  C2, which is the  minimum of the H2’s, is at best a draw.  Consequently, it  cannot be an  improvement over C2. We therefore do not  need to evaluate H2b, H2c, and H2d and can  proceed directly to C3.

10.2 the game of tic-tac-toe Alpha–beta pruning is used to  reduce the number  of positions evaluated in a minimax  search. Alpha is the  value that the  human player has  to refute, and beta  is the value that the  computer has to  refute. We do not need to evaluate each node completely; for some nodes, a refutation sufﬁces and some loops can terminate early. Speciﬁcally, when the human player evaluates a position, such as C2, a refutation, if found, is just as good as the absolute best move. The same logic applies to the computer. At any point in the search, alpha is the value that the human player has to refute, and beta is the value that the computer has to refute. When a search is done on the human player’s side, any move less than alpha is equivalent to alpha; when a search is done on the computer side, any move greater than beta is equivalent to beta. This strategy of reducing the number of positions evaluated in a minimax search is commonly called alpha–beta pruning. As Figure 10.11 shows, alpha–beta pruning requires only a few changes to chooseMove. Both alpha and beta are passed as additional parameters. Initially, chooseMove is started with alpha and beta representing HUMAN_WIN and COMPUTER_WIN, respectively. Lines 17 and 21 reﬂect a change in the initialization of value. The move evaluation is only slightly more complex than originally shown in Figure 7.29. The recursive call at line 30 includes the parameters alpha and beta, which are adjusted at line 37 or 39 if needed. The only other change is at line 42, which provides for an immediate return when a refutation is found. Alpha–beta pruning  works best when it  ﬁnds refutations  early. To take full advantage of alpha–beta pruning, game programs usually try to apply heuristics to place the best moves early in the search. This approach results in even more pruning than we would expect from a random search of positions. In practice, alpha–beta pruning limits the searching to  nodes, where N is the number of nodes that would be examined without alpha–beta pruning, resulting in a huge savings. The Tic-Tac-Toe example is not ideal because there are so many identical values. Even so, the initial search is reduced to roughly 18,000 positions. 10.2.2   transposition tables A transposition table stores previously evaluated  positions. Another commonly employed practice is to use a table to keep track of all positions that have been evaluated. For instance, in the course of searching for the ﬁrst move, the program will examine the positions shown in Figure 10.12. If the values of the positions are saved, the second occurrence of a position need not be recomputed; it essentially becomes a terminal position. The data structure that records and stores previously evaluated positions is called a transposition table; it is implemented as a map of positions to values.1 O N ( ) 1. We discussed this generic technique, which avoids repeated recursive calls by storing values in a table, in a different context in Section 7.6. This technique is also known as memoizing. The term transposition table is slightly misleading because fancier implementations of this technique recognize and avoid searching not only exactly identical positions, but also symmetrically identical positions.

chapter 10 fun and games figure 10.11 The chooseMove routine for computing an optimal Tic-Tac-Toe move, using alpha–beta pruning    // Find optimal move     private Best chooseMove( int side, int alpha, int beta, int depth )     {         int opp;              // The other side         Best reply;           // Opponent's best reply         int dc;               // Placeholder         int simpleEval;       // Result of an immediate evaluation         int bestRow = 0;         int bestColumn = 0;         int value;         if( ( simpleEval = positionValue( ) ) != UNCLEAR )             return new Best( simpleEval );         if( side == COMPUTER )         {             opp = HUMAN; value = alpha;         }         else         {             opp = COMPUTER; value = beta;         }     Outer:         for( int row = 0; row < 3; row++ )             for( int column = 0; column < 3; column++ )                 if( squareIsEmpty( row, column ) )                 {                     place( row, column, side );                     reply = chooseMove( opp, alpha, beta, depth + 1 );                     place( row, column, EMPTY );                     if( side == COMPUTER && reply.val > value ||                         side == HUMAN && reply.val < value )                     {                         if( side == COMPUTER )                             alpha = value = reply.val;                         else                             beta = value = reply.val;                         bestRow = row; bestColumn = column;                         if( alpha >= beta )                             break Outer;  // Refutation                     }                 }         return new Best( value, bestRow, bestColumn );     }

10.2 the game of tic-tac-toe A map is used to  implement the  transposition table.  Often the underlying implementation  is a hash table. We do not need an ordered map—so the HashMap—an unordered map, with a data structure called a hash table as the underlying implementation is used to implement the transposition table. We discuss hash tables in Chapter 20. To implement the transposition table we ﬁrst deﬁne a Position class, as shown in Figure 10.13, which we use to store each position. Values in the board will be HUMAN, COMPUTER, or EMPTY (deﬁned shortly in the TicTacToe class, as shown in Figure 10.14). The HashMap requires that we deﬁne equals and hashCode. Recall that if equals declares two Position objects as equal, hashCode must yield identical values for those objects. We also provide a constructor that can be initialized with a matrix representing the board. We do not store  positions that are at  the bottom of the  recursion in the  transposition table. An important issue concerns whether including all positions in the transposition table is worthwhile. The overhead of maintaining the table suggests that positions near the bottom of the recursion ought not be saved because n There are so many. n The point of alpha–beta pruning and transposition tables is to reduce search times by avoiding recursive calls early in the game; saving a recursive call very deep in the search does not greatly reduce the number of positions examined because that recursive call would examine only a few positions anyway. The chooseMove method has additional parameters,  all of which have  defaults. We show how this technique applies to the game of Tic-Tac-Toe when we implement the transposition table. The changes needed in the TicTacToe class are shown in Figure 10.14. The additions are the new data member at line 7 and the new declaration for chooseMove at line 14. We now pass alpha and beta (as in alpha–beta pruning) and also the depth of the recursion, which is zero by default. The initial call to chooseMove is shown at line 11. X X O X X O X X O X O X figure 10.12 Two searches that  arrive at identical  positions

chapter 10 fun and games Figures 10.15 and 10.16 show the new chooseMove. At line 8, we declare a Position object, thisPosition. When the time comes it will be placed in the transposition table. tableDepth tells us how deep in the search to allow positions to be placed in the transposition table. By experimenting we found that depth 5 was optimal. Allowing positions at depth 6 to be saved hurt because figure 10.13 The Position class 1 final class Position 2 { 3     private int [ ][ ] board; 5     public Position( int [ ][ ] theBoard ) 6     { 7         board = new int[ 3 ][ 3 ]; 8         for( int i = 0; i < 3; i++ ) 9             for( int j = 0; j < 3; j++ ) 10                 board[ i ][ j ] = theBoard[ i ][ j ]; 11     } 13     public boolean equals( Object rhs ) 14     { 15         if( ! (rhs instanceof Position ) ) 16             return false; 18         Position other = (Position) rhs; 20         for( int i = 0; i < 3; i++ ) 21             for( int j = 0; j < 3; j++ ) 22                 if( board[ i ][ j ] != ( (Position) rhs ).board[ i ][ j ] ) 23                     return false; 24         return true; 25     } 27     public int hashCode( ) 28     { 29         int hashVal = 0; 31         for( int i = 0; i < 3; i++ ) 32             for( int j = 0; j < 3; j++ ) 33                 hashVal = hashVal * 4 + board[ i ][ j ]; 35         return hashVal; 36     } 37 }

10.2 the game of tic-tac-toe the extra cost of maintaining the larger transposition table was not offset by the fewer examined positions. The code has a few  little tricks but  nothing major. Lines 17 to 24 are new. If we are in the ﬁrst call to chooseMove, we initialize the transposition table. Otherwise, if we are at an appropriate depth, we determine whether the current position has been evaluated; if it has, we return its value. The code has two tricks. First, we can transpose only at depth 3 or higher, as Figure 10.12 suggests. The only other difference is the addition of lines 57 and 58. Immediately before the return, we store the value of the position in the transposition table. The use of the transposition table in this Tic-Tac-Toe algorithm removes about half the positions from consideration, with only a slight cost for the transposition table operations. The program’s speed is almost doubled. 10.2.3   computer chess Terminal positions  cannot be searched  in computer chess.  In the best programs, considerable knowledge is  built into the evaluation function. In a complex game such as Chess or Go, it is infeasible to search all the way to the terminal nodes: Some estimates claim that there are roughly 10100 legal chess positions, and all the tricks in the world will not bring it down to a manageable level. In this case, we have to stop the search after a certain depth of recursion is reached. The nodes at which the recursion is stopped become terminal nodes. These terminal nodes are evaluated with a function that estimates the value of the position. For instance, in a chess program, the figure 10.14 Changes to the TicTacToe class to incorporate transposition table and alpha–beta pruning // Original import directives plus: 2 import java.util.Map; 3 import java.util.HashMap; 5 class TicTacToe 6 { 7     private Map<Position,Integer> transpositions 8                                     = new HashMap<Position,Integer>( ); 10     public Best chooseMove( int side ) 11       { return chooseMove( side, HUMAN_WIN, COMPUTER_WIN, 0 ); } 13         // Find optimal move 14     private Best chooseMove( int side, int alpha, int beta, int depth ) 15       { /* Figures 10.15 and 10.16 */ } 17       ... 18 }

chapter 10 fun and games evaluation function measures such variables as the relative amount and strength of pieces and other positional factors. The best computer  chess programs  play at grandmaster  level. Computers are especially adept at playing moves involving deep combinations that result in exchanges of material. The reason is that the strength of pieces is easily evaluated. However, extending the search depth merely one level requires an increase in processing speed by a factor of about 6 (because the number of positions increases by about a factor of 36). Each extra level of search greatly enhances the ability of the program, up to a certain limit (which appears to have been reached by the best programs). On the other figure 10.15 The Tic-Tac-Toe algorithm with alpha–beta pruning and transposition table (part 1)    // Find optimal move     private Best chooseMove( int side, int alpha, int beta, int depth )     {         int opp;              // The other side         Best reply;           // Opponent's best reply         int dc;               // Placeholder         int simpleEval;       // Result of an immediate evaluation         Position thisPosition = new Position( board );         int tableDepth = 5;   // Max depth placed in Trans. table         int bestRow = 0;         int bestColumn = 0;         int value;         if( ( simpleEval = positionValue( ) ) != UNCLEAR )             return new Best( simpleEval );         if( depth == 0 )             transpositions.clear( );         else if( depth >= 3 && depth <= tableDepth )         {             Integer lookupVal = transpositions.get( thisPosition );             if( lookupVal != null )                 return new Best( lookupVal );         }         if( side == COMPUTER )         {             opp = HUMAN; value = alpha;         }         else         {             opp = COMPUTER; value = beta;         }

10.2 the game of tic-tac-toe hand, computers generally are not as good at playing quiet positional games in which more subtle evaluations and knowledge of the game is required. However, this shortcoming is apparent only when the computer is playing very strong opposition. The mass-marketed computer chess programs are better than all but a small fraction of today’s players. In 1997, the computer program Deep Blue, using an enormous amount of computational power (evaluating as many as 200 million moves per second), was able to defeat the reigning world chess champion in a six-game match. Its evaluation function, although top secret, is known to contain a large number of factors, was aided by several chess grandmasters, and was the result of years of experimentation. Writing the top computer chess program is certainly not a trivial task. figure 10.16 The Tic-Tac-Toe algorithm with alpha–beta pruning and transposition table (part 2) Outer:         for( int row = 0; row < 3; row++ )             for( int column = 0; column < 3; column++ )                 if( squareIsEmpty( row, column ) )                 {                     place( row, column, side );                     reply = chooseMove( opp, alpha, beta, depth + 1 );                     place( row, column, EMPTY );                     if( side == COMPUTER && reply.val > value ||                         side == HUMAN && reply.val < value )                     {                         if( side == COMPUTER )                             alpha = value = reply.val;                         else                             beta = value = reply.val;                         bestRow = row; bestColumn = column;                         if( alpha >= beta )                             break Outer;  // Refutation                     }                 }         if( depth <= tableDepth )             transpositions.put( thisPosition, value );         return new Best( value, bestRow, bestColumn );     }

chapt er stacks and  compilers S tacks are used extensively in compilers. In this chapter we present two simple components of a compiler: a balanced symbol checker and a simple calculator. We do so to show simple algorithms that use stacks and to show how the Collections API classes described in Chapter 6 are used. In this chapter, we show n How to use a stack to check for balanced symbols n How to use a state machine to parse symbols in a balanced symbol program n How to use operator precedence parsing to evaluate inﬁx expressions in a simple calculator program 11.1 balanced-symbol checker As discussed in Section 6.6, compilers check your programs for syntax errors. Frequently, however, a lack of one symbol (such as a missing */ comment ender or }) can cause the compiler to produce numerous lines of diagnostics without identifying the real error. A useful tool to help debug compiler error

chapter 11 stacks and compilers messages is a program that checks whether symbols are balanced. In other words, every { must correspond to a }, every [ to a ], and so on. However, simply counting the numbers of each symbol is insufﬁcient. For example, the sequence [()] is legal, but the sequence [(]) is wrong. 11.1.1   basic algorithm A stack can be  used to detect mismatched symbols. A stack is useful here because we know that when a closing symbol such as ) is seen, it matches the most recently seen unclosed (. Therefore, by placing an opening symbol on a stack, we can easily determine whether a closing symbol makes sense. Speciﬁcally, we have the following algorithm. 1. Make an empty stack. 2. Read symbols until the end of the ﬁle. a. If the symbol is an opening symbol, push it onto the stack. b. If it is a closing symbol, do the following. i. If the stack is empty, report an error. ii. Otherwise, pop the stack. If the symbol popped is not the corresponding opening symbol, report an error. 3. At the end of the ﬁle, if the stack is not empty, report an error. In this algorithm, illustrated in Figure 11.1, the fourth, ﬁfth, and sixth symbols all generate errors. The } is an error because the symbol popped from the top of the stack is a (, so a mismatch is detected. The ) is an error because the stack is empty, so there is no corresponding (. The [ is an error detected when the end of input is encountered and the stack is not empty. figure 11.1 Stack operations  in a balancedsymbol algorithm Symbols: (  [  ]  }  )  [ ( ( ( ( [ [ ] eof* }* )* [ [ Errors (indicated by *): { when expecting) ( with no matching opening symbol [ unmatched at end of input

11.1 balanced-symbol checker Symbols in comments, string  constants, and  character constants need not be  balanced. To make this technique work for Java programs, we need to consider all the contexts in which parentheses, braces, and brackets need not match. For example, we should not consider a parenthesis as a symbol if it occurs inside a comment, string constant, or character constant. We thus need routines to skip comments, string constants, and character constants. A character constant in Java can be difﬁcult to recognize because of the many escape sequences possible, so we need to simplify things. We want to design a program that works for the bulk of inputs likely to occur. Line numbers are  needed for meaningful error messages. For the program to be useful, we must not only report mismatches but also attempt to identify where the mismatches occur. Consequently, we keep track of the line numbers where the symbols are seen. When an error is encountered, obtaining an accurate message is always difﬁcult. If there is an extra }, does that mean that the } is extraneous? Or was a { missing earlier? We keep the error handling as simple as possible, but once one error has been reported, the program may get confused and start ﬂagging many errors. Thus only the ﬁrst error can be considered meaningful. Even so, the program developed here is very useful. 11.1.2   implementation Tokenization is the  process of generating the sequence of  symbols (tokens)  that need to be  recognized. The program has two basic components. One part, called tokenization, is the process of scanning an input stream for opening and closing symbols (the tokens) and generating the sequence of tokens that need to be recognized. The second part is running the balanced symbol algorithm, based on the tokens. The two basic components are represented as separate classes. Figure 11.2 shows the Tokenizer class skeleton, and Figure 11.3 shows the Balance class skeleton. The Tokenizer class provides a constructor that requires a Reader and then provides a set of accessors that can be used to get n The next token (either an opening/closing symbol for the code in this chapter or an identiﬁer for the code in Chapter 12) n The current line number n The number of errors (mismatched quotes and comments) The Tokenizer class maintains most of this information in private data members. The Balance class also provides a similar constructor, but its only publicly visible routine is checkBalance, shown at line 24. Everything else is a supporting routine or a class data member. We begin by describing the Tokenizer class. It is a reference to a PushbackReader object and is initialized at construction. Because of the I/O hierarchy (see Section 4.5.3), it may be constructed with any Reader object. The current character being scanned is stored in ch, and

chapter 11 stacks and compilers figure 11.2 The Tokenizer class  skeleton, used to  retrieve tokens from  an input stream 1 import java.io.Reader; 2 import java.io.PushbackReader; 3 import java.io.IOException; 5 // Tokenizer class. 6 // 7 // CONSTRUCTION: with a Reader object 8 // ******************PUBLIC OPERATIONS*********************** 9 // char getNextOpenClose( ) --> Get next opening/closing symbol 10 // int getLineNumber( )     --> Return current line number 11 // int getErrorCount( )     --> Return number of parsing errors 12 // String getNextID( )      --> Get next Java identifier 13 //                               (see Section 12.2) 14 // ******************ERRORS********************************** 15 // Error checking on comments and quotes is performed 17 public class Tokenizer 18 { 19     public Tokenizer( Reader inStream ) 20       { errors = 0; ch = '\0'; currentLine = 1; 21         in = new PushbackReader( inStream ); } 23     public static final int SLASH_SLASH = 0; 24     public static final int SLASH_STAR  = 1; 26     public int getLineNumber( ) 27       { return currentLine; } 28     public int getErrorCount( ) 29       { return errors; } 30     public char getNextOpenClose( ) 31       { /* Figure 11.7 */ } 32     public char getNextID( ) 33       { /* Figure 12.29 */ } 35     private boolean nextChar( ) 36       { /* Figure 11.4 */ } 37     private void putBackChar( ) 38       { /* Figure 11.4 */ } 39     private void skipComment( int start ) 40       { /* Figure 11.5 */ } 41     private void skipQuote( char quoteType ) 42       { /* Figure 11.6 */ } 43     private void processSlash( ) 44       { /* Figure 11.7 */ } 45     private static final boolean isIdChar( char ch ) 46       { /* Figure 12.27 */ } 47     private String getRemainingString( ) 48       { /* Figure 12.28 */ } 50     private PushbackReader in;    // The input stream 51     private char ch;              // Current character 52     private int currentLine;      // Current line 53     private int errors;           // Number of errors seen 54 }

11.1 balanced-symbol checker figure 11.3 Class skeleton  for a balancedsymbol program 1 import java.io.Reader; 2 import java.io.FileReader; 3 import java.io.IOException; 4 import java.io.InputStreamReader; 6 import java.util.Stack; 9 // Balance class: check for balanced symbols 10 // 11 // CONSTRUCTION: with a Reader object 12 // ******************PUBLIC OPERATIONS*********************** 13 // int checkBalance( )   --> Print mismatches 14 //                           return number of errors 15 // ******************ERRORS********************************** 16 // Error checking on comments and quotes is performed 17 // main checks for balanced symbols. 19 public class Balance 20 { 21     public Balance( Reader inStream ) 22       { errors = 0; tok = new Tokenizer( inStream ); } 24     public int checkBalance( ) 25       { /* Figure 11.8 */ } 27     private Tokenizer tok; 28     private int errors; 30     /** 31      * Symbol nested class; 32      * represents what will be placed on the stack. 33      */ 34     private static class Symbol 35     { 36         public char token; 37         public int  theLine; 39         public Symbol( char tok, int  line ) 40         { 41             token   = tok; 42             theLine = line; 43         } 44     } 46     private void checkMatch( Symbol opSym, Symbol clSym ) 47       { /* Figure 11.9 */ }  48 }

chapter 11 stacks and compilers the current line number is stored in currentLine. Finally, an integer that counts the number of errors is declared at line 53. The constructor, shown at lines 19 to 21, initializes the error count to 0 and the current line number to 1 and sets the PushbackReader reference. Lexical analysis is  used to ignore  comments and recognize symbols. We can now implement the class methods, which as we mentioned, are concerned with keeping track of the current line and attempting to differentiate symbols that represent opening and closing tokens from those that are inside comments, character constants, and string constants. This general process of recognizing tokens in a stream of symbols is called lexical analysis. Figure 11.4 shows a pair of routines, nextChar and putBackChar. The nextChar method reads the next character from in, assigns it to ch, and updates currentLine if a newline is encountered. It returns false only if the end of the ﬁle has been reached. The complementary procedure putBackChar figure 11.4 The nextChar routine for reading the next character, updating currentLine if necessary, and returning true if not at  the end of file; and the putBackChar routine for putting back ch and updating currentLine if necessary /**  * nextChar sets ch based on the next character in the input stream.  * putBackChar puts the character back onto the stream.  * It should be used only once after a call to nextChar.  * Both routines adjust currentLine if necessary.  */     private boolean nextChar( )     {         try         {             int readVal = in.read( );             if( readVal == -1 )                 return false;             ch = (char) readVal;             if( ch == '\n' )                 currentLine++;             return true;         }         catch( IOException e )           { return false; }     }     private void putBackChar( )     {         if( ch == '\n' )             currentLine--;         try            { in.unread( (int) ch ); }         catch( IOException e ) { }     }

11.1 balanced-symbol checker puts the current character, ch, back onto the input stream, and decrements currentLine if the character is a newline. Clearly, putBackChar should be called at most once between calls to nextChar; as it is a private routine, we do not worry about abuse on the part of the class user. Putting characters back onto the input stream is a commonly used technique in parsing. In many instances we have read one too many characters, and undoing the read is useful. In our case this occurs after processing a /. We must determine whether the next character begins the comment start token; if it does not, we cannot simply disregard it because it could be an opening or closing symbol or a quote. Thus we pretend that it is never read. Next is the routine skipComment, shown in Figure 11.5. Its purpose is to skip over the characters in the comment and position the input stream so that the next read is the ﬁrst character after the comment ends. This technique is complicated by the fact that comments can either begin with //, in which case the line ends the comment, or /*, in which case */ ends the comment.1 In the // case, we continually get the next character until either the end of ﬁle is reached (in which 1. We do not consider deviant cases involving \, nor /**/. figure 11.5 The skipComment routine for moving  past an already  started comment /**  * Precondition: We are about to process a comment;  *                 have already seen comment-start token  * Postcondition: Stream will be set immediately after  *                 comment-ending token  */     private void skipComment( int start )     {         if( start == SLASH_SLASH )         {             while( nextChar( ) && ( ch != '\n' ) )                 ;             return;         }             // Look for a */ sequence         boolean state = false;   // True if we have seen *         while( nextChar( ) )         {             if( state && ch == '/' )                 return;             state = ( ch == '*' );         }         errors++;         System.out.println( "Unterminated comment!" );     }

chapter 11 stacks and compilers case, the ﬁrst half of the && operator fails) or we get a newline. At that point we return. Note that the line number is updated automatically by nextChar. Otherwise, we have the /* case, which is processed starting at line 17. The state machine is a common technique used to parse  symbols; at any  point, it is in some  state, and each  input character  takes it to a new  state. Eventually,  the state machine  reaches a state in  which a symbol has  been recognized. The skipComment routine uses a simpliﬁed state machine. The state machine is a common technique used to parse symbols; at any point, it is in some state, and each input character takes it to a new state. Eventually, it reaches a state at which a symbol has been recognized. In skipComment, at any point, it has matched 0, 1, or 2 characters of the */ terminator, corresponding to states 0, 1, and 2. If it matches two characters, it can return. Thus, inside the loop, it can be in only state 0 or 1 because, if it is in state 1 and sees a /, it returns immediately. Thus the state can be represented by a Boolean variable that is true if the state machine is in state 1. If it does not return, it either goes back to state 1 if it encounters a * or goes back to state 0 if it does not. This procedure is stated succinctly at line 23. If we never ﬁnd the comment-ending token, eventually nextChar returns false and the while loop terminates, resulting in an error message. The skipQuote method, shown in Figure 11.6, is similar. Here, the parameter is the opening quote character, which is either " or '. In either case, we need to see that character as the closing quote. However, we must be prepared to handle the \ character; otherwise, our program will report errors when it is run on its figure 11.6 The skipQuote routine for moving past an  already started  character or string  constant /**  * Precondition: We are about to process a quote;   *                   have already seen beginning quote.  * Postcondition: Stream will be set immediately after  *                   matching quote  */     private void skipQuote( char quoteType )     {         while( nextChar( ) )         {             if( ch == quoteType )                 return;             if( ch == '\n' )             {                 errors++;                 System.out.println( "Missing closed quote at line " +                                     currentLine );                 return;             }             else if( ch == '\\' )                 nextChar( );         }     }

11.1 balanced-symbol checker own source. Thus we repeatedly digest characters. If the current character is a closing quote, we are done. If it is a newline, we have an unterminated character or string constant. And if it is a backslash, we digest an extra character without examining it. Once we’ve written the skipping routine, writing getNextOpenClose is easier. The bulk of the logic is deferred to processSlash. If the current character is a /, we read a second character to see whether we have a comment. If so, we call skipComment; if not, we undo the second read. If we have a quote, we call skipQuote. If we have an opening or closing symbol, we can return. Otherwise, we keep reading until we eventually run out of input or ﬁnd an opening or closing symbol. Both getNextOpenClose and processSlash are shown in Figure 11.7. The getLineNumber and getErrorCount methods are one-liners that return the values of the corresponding data members and are shown in Figure 11.2. We discuss the getNextID routine in Section 12.2.2 when it is needed. In the Balance class, the balanced symbol algorithm requires that we place opening symbols on a stack. In order to print diagnostics, we store a line number with each symbol, as shown previously in the Symbol nested class at lines 34 to 44 in Figure 11.3. The checkBalance routine is implemented as shown in Figure 11.8. It follows the algorithm description almost verbatim. A stack that stores pending opening symbols is declared at line 9. Opening symbols are pushed onto the stack with the current line number. When a closing symbol is encountered and the stack is empty, the closing symbol is extraneous; otherwise, we remove the top item from the stack and verify that the opening symbol that was on the stack matches the closing symbol just read. To do so we use the checkMatch routine, which is shown in Figure 11.9. Once the end of input is encountered, any symbols on the stack are unmatched; they are repeatedly output in the while loop that begins at line 40. The total number of errors detected is then returned. The checkBalance routine does all the  algorithmic work. The current implementation allows multiple calls to checkBalance. However, if the input stream is not reset externally, all that happens is that the end of the ﬁle is immediately detected and we return immediately. We can add functionality to the Tokenizer class, allowing it to change the stream source, and then add functionality to the Balance class to change the input stream (passing on the change to the Tokenizer class). We leave this task for you to do as Exercise 11.9. Figure 11.10 shows that we expect a Balance object to be created and then checkBalance to be invoked. In our example, if there are no command-line arguments, the associated Reader is attached to System.in (via an InputStreamReader bridge); otherwise, we repeatedly use Readers associated with the ﬁles given in the command-line argument list.

chapter 11 stacks and compilers figure 11.7 The getNextOpenClose routine for skipping  comments and quotes  and returning the next  opening or closing  character, along with  the processSlash routine /**  * Get the next opening or closing symbol.  * Return false if end of file.  * Skip past comments and character and string constants  */    public char getNextOpenClose( )    {        while( nextChar( ) )        {            if( ch == '/' )                processSlash( );            else if( ch == '\'' || ch == '”' )                skipQuote( ch );            else if( ch == '(' || ch == '[' || ch == '{' ||                     ch == ')' || ch == ']' || ch == '}' )                return ch;        }        return '\0';            // End of file    }    /**     * After the opening slash is seen deal with next character.     * If it is a comment starter, process it; otherwise putback     * the next character if it is not a newline.     */    private void processSlash( )    {        if( nextChar( ) )        {            if( ch == '*' )            {                // Javadoc comment                if( nextChar( ) && ch != '*' )                    putBackChar( );                skipComment( SLASH_STAR );            }            else if( ch == '/' )                skipComment( SLASH_SLASH );            else if( ch != '\n' )                putBackChar( );        }    }

11.1 balanced-symbol checker figure 11.8 The checkBalance algorithm /**  * Print an error message for unbalanced symbols.  * @return number of errors detected.  */     public int checkBalance( )     {         char ch;         Symbol match = null;         Stack<Symbol> pendingTokens = new Stack<Symbol>( );         while( ( ch = tok.getNextOpenClose( ) ) != '\0' )         {             Symbol lastSymbol = new Symbol( ch, tok.getLineNumber( ) );             switch( ch )             {               case '(': case '[': case '{':                 pendingTokens.push( lastSymbol );                 break;               case ')': case ']': case '}':                 if( pendingTokens.isEmpty( ) )                 {                     errors++;                     System.out.println( "Extraneous " + ch +                                         " at line " + tok.getLineNumber( ) );                 }                 else                 {                     match = pendingTokens.pop( );                     checkMatch( match, lastSymbol );                 }                 break;               default: // Cannot happen                 break;             }         }         while( !pendingTokens.isEmpty( ) )         {             match = pendingTokens.pop( );             System.out.println( "Unmatched " + match.token +                                 " at line "  + match.theLine );             errors++;         }         return errors + tok.getErrorCount( ); }

chapter 11 stacks and compilers 11.2 a simple calculator Some of the techniques used to implement compilers can be used on a smaller scale in the implementation of a typical pocket calculator. Typically, calculators evaluate inﬁx expressions, such as 1+2, which consist of a binary operator with arguments to its left and right. This format, although often fairly easy to evaluate, can be more complex. Consider the expression 1 + 2 * 3 In an inﬁx expression a binary operator has arguments  to its left and right.  Mathematically, this expression evaluates to 7 because the multiplication operator has higher precedence than addition. Some calculators give the answer 9, illustrating that a simple left-to-right evaluation is not sufﬁcient; we cannot begin by evaluating 1+2. Now consider the expressions 10 - 4 - 3 2 ^ 3 ^ 3 When there are  several operators,  precedence and  associativity determine how the operators are  processed. in which ^ is the exponentiation operator. Which subtraction and which exponentiation get evaluated ﬁrst? On the one hand, subtractions are processed left-toright, giving the result 3. On the other hand, exponentiation is generally processed right-to-left, thereby reﬂecting the mathematical   rather than  . Thus subtraction associates left-to-right, whereas exponentiation associates from right-toleft. All of these possibilities suggest that evaluating an expression such as figure 11.9 The checkMatch routine for checking that the closing symbol matches the opening symbol /**  * Print an error message if clSym does not match opSym.  * Update errors.  */     private void checkMatch( Symbol opSym, Symbol clSym )     {         if( opSym.token == '(' && clSym.token != ')' ||             opSym.token == '[' && clSym.token != ']' ||             opSym.token == '{' && clSym.token != '}' )         {             System.out.println( "Found " + clSym.token + " on line " +                   tok.getLineNumber( ) + "; does not match " + opSym.token                   + " at line " + opSym.theLine );             errors++;         }     } ( )3

11.2 a simple calculator 1 - 2 - 4 ^ 5 * 3 * 6 / 7 ^ 2 ^ 2 would be quite challenging. If the calculations are performed in integer math (i.e., rounding down on division), the answer is -8. To show this result, we insert parentheses to clarify ordering of the calculations: ( 1 - 2 ) - ( ( ( ( 4 ^ 5 ) * 3 ) * 6 ) / ( 7 ^ ( 2 ^ 2 ) ) ) figure 11.10 The main routine with  command-line arguments // main routine for balanced symbol checker. // If no command line parameters, standard output is used. // Otherwise, files in command line are used.     public static void main( String [ ] args )     {         Balance p;         if( args.length == 0 )         {             p = new Balance( new InputStreamReader( System.in ) );             if( p.checkBalance( ) == 0 )                 System.out.println( "No errors!" );             return;         }         for( int i = 0; i < args.length; i++ )         {             FileReader f = null;             try              {                 f = new FileReader( args[ i ] );                 System.out.println( args[ i ] + ": " );                 p = new Balance( f );                 if( p.checkBalance( ) == 0 )                      System.out.println( "   ...no errors!" );             }             catch( IOException e )               { System.err.println( e + args[ i ] ); }             finally             {                 try                    { if( f != null ) f.close( ); }                 catch( IOException e )                   { }             }         }     }

chapter 11 stacks and compilers Although the parentheses make the order of evaluations unambiguous, they do not necessarily make the mechanism for evaluation any clearer. A different expression form, called a postﬁx expression, which can be evaluated by a postﬁx machine without using any precedence rules, provides a direct mechanism for evaluation. In the next several sections we explain how it works. First, we examine the postﬁx expression form and show how expressions can be evaluated in a simple left-toright scan. Next, we show algorithmically how the previous expressions, which are presented as inﬁx expressions, can be converted to postﬁx. Finally, we give a Java program that evaluates inﬁx expressions containing additive, multiplicative, and exponentiation operators—as well as overriding parentheses. We use an algorithm called operator precedence parsing to convert an inﬁx expression to a postﬁx expression in order to evaluate the inﬁx expression. 11.2.1   postfix machines A postﬁx expression is a series of operators and operands. A postﬁx machine is used to evaluate a postﬁx expression as follows. When an operand is seen, it is pushed onto a stack. When an operator is seen, the appropriate number of operands are popped from the stack, the operator is evaluated, and the result is pushed back onto the stack. For binary operators, which are the most common, two operands are popped. When the complete postﬁx expression is evaluated, the result should be a single item on the stack that represents the answer. The postﬁx form represents a natural way to evaluate expressions because precedence rules are not required. A simple example is the postﬁx expression 1 2 3 * + The evaluation proceeds as follows: 1, then 2, and then 3 are each pushed onto the stack. To process *, we pop the top two items on the stack: 3 and then 2. Note that the ﬁrst item popped becomes the rhs parameter to the binary operator and that the second item popped is the lhs parameter; thus parameters are popped in reverse order. For multiplication, the order does not matter, but for subtraction and division, it does. The result of the multiplication is 6, and that is pushed back onto the stack. At this point, the top of the stack is 6; below it is 1. To process the +, the 6 and 1 are popped, and their sum, 7, is pushed. At this point, the expression has been read and the stack has only one item. Thus the ﬁnal answer is 7. Every valid inﬁx expression can be converted to postﬁx form. For example, the earlier long inﬁx expression can be written in postﬁx notation as 1 2 - 4 5 ^ 3 * 6 * 7 2 2 ^ ^ / - Evaluation of a  postﬁx expression  takes linear time. Figure 11.11 shows the steps used by the postﬁx machine to evaluate this expression. Each step involves a single push. Consequently, as there are 9 operands and A postﬁx expression  can be evaluated as  follows. Operands  are pushed onto a  single stack. An  operator pops its  operands and then  pushes the result.  At the end of the  evaluation, the stack  should contain only  one element, which  represents the  result.

11.2 a simple calculator 8 operators, there are 17 steps and 17 pushes. Clearly, the time required to evaluate a postﬁx expression is linear. The remaining task is to write an algorithm to convert from inﬁx notation to postﬁx notation. Once we have it, we also have an algorithm that evaluates an inﬁx expression. 11.2.2   infix to postfix conversion The operator precedence parsing  algorithm converts  an inﬁx expression  to a postﬁx expression, so we can  evaluate the inﬁx  expression. The basic principle involved in the operator precedence parsing algorithm, which converts an inﬁx expression to a postﬁx expression, is the following. When an operand is seen, we can immediately output it. However, when we see an operator, we can never output it because we must wait to see the second operand, so we must save it. In an expression such as 1 + 2 * 3 ^ 4 which in postﬁx form is 1 2 3 4 ^ * + a postﬁx expression in some cases has operators in the reverse order than they appear in an inﬁx expression. Of course, this order can occur only if the Postfix Expression: 1 2 - 4 5 ^ 3 * 6 * 7 2 2 ^ ^ / - -1 -1 -1 ^ -1 -1 - -1 * -1 -1 * -1 -1 -1 -1 ^ -1 ^ -1 / -8 - figure 11.11 Steps in the  evaluation of a postfix  expression

chapter 11 stacks and compilers precedence of the involved operators is increasing as we go from left to right. Even so, this condition suggests that a stack is appropriate for storing operators. Following this logic, then, when we read an operator it must somehow be placed on a stack. Consequently, at some point the operator must get off the stack. The rest of the algorithm involves deciding when operators go on and come off the stack. In another simple inﬁx expression 2 ^ 5 - 1 When an operator  is seen on the  input, operators of  higher priority (or  left-associative operators of equal  priority) are  removed from the  stack, signaling that  they should be  applied. The input  operator is then  placed on the  stack. when we reach the - operator, 2 and 5 have been output and ^ is on the stack. Because - has lower precedence than ^, the ^ needs to be applied to 2 and 5. Thus we must pop the ^ and any other operands of higher precedence than - from the stack. After doing so, we push the -. The resulting postﬁx expression is 2 5 ^ 1 - In general, when we are processing an operator from input, we output those operators from the stack that the precedence (and associativity) rules tell us need to be processed. A second example is the inﬁx expression 3 * 2 ^ 5 - 1 When we reach the ^ operator, 3 and 2 have been output and * is on the stack. As ^ has higher precedence than *, nothing is popped and ^ goes on the stack. The 5 is output immediately. Then we encounter a - operator. Precedence rules tell us that ^ is popped, followed by the *. At this point, nothing is left to pop, we are done popping, and - goes onto the stack. We then output 1. When we reach the end of the inﬁx expression, we can pop the remaining operators from the stack. The resulting postﬁx expression is 3 2 5 ^ * 1 - Before the summarizing algorithm, we need to answer a few questions. First, if the current symbol is a + and the top of the stack is a +, should the + on the stack be popped or should it stay? The answer is determined by deciding whether the input + implies that the stack + has been completed. Because + associates from left to right, the answer is yes. However, if we are talking about the ^ operator, which associates from right to left, the answer is no. Therefore, when examining two operators of equal precedence, we look at the associativity to decide, as shown in Figure 11.12. What about parentheses? A left parenthesis can be considered a highprecedence operator when it is an input symbol, a low-precedence operator A left parenthesis is  treated as a high-  precedence operator when it is an  input symbol but as  a low-precedence  operator when it is  on the stack. A left  parenthesis is  removed only by a  right parenthesis. An operator stack is  used to store operators that have  been seen but not  yet output.

11.2 a simple calculator when it is on the stack. Consequently, the input left parenthesis is simply placed on the stack. When a right parenthesis appears on the input, we pop the operator stack until we come to a left parenthesis. The operators are written, but the parentheses are not. The following is a

chapt er utilities In this chapter we discuss two utility applications of data structures: data compression and cross-referencing. Data compression is an important technique in computer science. It can be used to reduce the size of ﬁles stored on disk (in effect increasing the capacity of the disk) and also to increase the effective rate of transmission by modems (by transmitting less data). Virtually all newer modems perform some type of compression. Cross-referencing is a scanning and sorting technique that is done, for example, to make an index for a book. In this chapter, we show n An implementation of a ﬁle-compression algorithm called Huffman’s algorithm n An implementation of a cross-referencing program that lists, in sorted order, all identiﬁers in a program and gives the line numbers on which they occur

chapter 12 utilities 12.1  file compression A standard encoding of C characters uses ⎡log C⎤ bits. The ASCII character set consists of roughly 100 printable characters. To distinguish these characters,   bits are required. Seven bits allow the representation of 128 characters, so the ASCII character set adds some other “unprintable” characters. An eighth bit is added to allow parity checks. The important point, however, is that if the size of the character set is C, then  bits are needed in a standard ﬁxed-length encoding. Suppose that you have a ﬁle that contains only the characters a, e, i, s, and t, blank spaces (sp), and newlines (nl). Suppose further that the ﬁle has 10 a’s, 15 e’s, 12 i’s, 3 s’s, 4 t’s, 13 blanks, and 1 newline. As Figure 12.1 shows, representing this ﬁle requires 174 bits because there are 58 characters and each character requires 3 bits. Reducing the number of bits required  for data representation is called  compression, which  actually consists of  two phases: the  encoding phase  (compressing) and  the decoding phase  (uncompressing). In real life, ﬁles can be quite large. Many very large ﬁles are the output of some program, and there is usually a big disparity between the most frequently and least frequently used characters. For instance, many large data ﬁles have an inordinately large number of digits, blanks, and newlines but few q’s and x’s. In many situations reducing the size of a ﬁle is desirable. For instance, disk space is precious on virtually every machine, so decreasing the amount of space required for ﬁles increases the effective capacity of the disk. When data are being transmitted across phone lines by a modem, the effective rate of transmission is increased if the amount of data transmitted can be reduced. Reducing the number of bits required for data representation is called compression, which actually consists of two phases: the encoding phase (compression) and the decoding phase (uncompression). A simple strategy discussed in this chapter achieves 25 percent savings on some large ﬁles and as much as 50 or 60 percent savings on some large data ﬁles. Extensions provide somewhat better compression. log = C log figure 12.1 A standard coding  scheme Character Code Frequency Total Bits a e i s t sp nl Total

12.1 file compression In a variable-length  code, the mostfrequent characters  have the shortest  representation. The general strategy is to allow the code length to vary from character to character and to ensure that frequently occurring characters have short codes. If all characters occur with the same or very similar frequency, you cannot expect any savings. 12.1.1   prefix codes In a binary trie, a left  branch represents  0 and a right  branch represents 1.  The path to a node  indicates its  representation. The binary code presented in Figure 12.1 can be represented by the binary tree shown in Figure 12.2. In this data structure, called a binary trie (pronounced “try”), characters are stored only in leaf nodes; the representation of each character is found by starting at the root and recording the path, using a 0 to indicate the left branch and a 1 to indicate the right branch. For instance, s is reached by going left, then right, and ﬁnally right. This is encoded as 011. If character   is at depth   and occurs fi times, the cost of the code is  di fi. We can obtain a better code than the one given in Figure 12.2 by recognizing that nl is an only child. By placing it one level higher (replacing its parent), we obtain the new tree shown in Figure 12.3. This new tree has a cost of 173 but is still far from optimal. Note that the tree in Figure 12.3 is a full tree, in which all nodes either are leaves or have two children. An optimal code always has this property; otherwise, as already shown, nodes with only one child could move up a level. If the characters are placed only at the leaves, any sequence of bits can always be decoded unambiguously. a e i s t sp nl figure 12.2 Representation of the  original code by a tree ci di ∑ a e i s t sp nl figure 12.3 A slightly better tree In a full tree, all  nodes either are  leaves or have two  children.

chapter 12 utilities For instance, suppose that the encoded string is 010011110001011000 1000111. Figure 12.3 shows that 0 and 01 are not character codes but that 010 represents i, so the ﬁrst character is i. Then 011 follows, which is an s. Then 11 follows, which is a newline (nl). The remainder of the code is a, sp, t, i, e, and nl. In a preﬁx code, no  character code is a  preﬁx of another  character code.  This is guaranteed  if the characters  are only in leaves. A  preﬁx code can  be decoded  unambiguously. The character codes can be different lengths, as long as no character code is a preﬁx of another character code, an encoding called a preﬁx code. Conversely, if a character is contained in a nonleaf node, guaranteeing unambiguous decoding is no longer possible. Thus our basic problem is to ﬁnd the full binary tree of minimum cost (as deﬁned previously) in which all characters are contained in the leaves. The tree shown in Figure 12.4 is optimal for our sample alphabet. As shown in Figure 12.5, this code requires only 146 bits. There are many optimal codes, which can be obtained by swapping children in the encoding tree. a e i s sp nl t figure 12.4 An optimal prefix  code tree figure 12.5 Optimal prefix code Character Code Frequency Total Bits a e i s t sp nl Total

12.1 file compression 12.1.2   huffman’s algorithm Huffman’s algorithm constructs an optimal preﬁx code. It  works by repeatedly merging the  two minimumweight trees. How is the coding tree constructed? The coding system algorithm was given by Huffman in 1952. Commonly called Huffman’s algorithm, it constructs an optimal preﬁx code by repeatedly merging trees until the ﬁnal tree is obtained. Throughout this section, the number of characters is C. In Huffman’s algorithm we maintain a forest of trees. The weight of a tree is the sum of the frequencies of its leaves.   times, two trees,   and  , of smallest weight are selected, breaking ties arbitrarily, and a new tree is formed with subtrees   and  . At the beginning of the algorithm, there are C singlenode trees (one for each character). At the end of the algorithm, there is one tree, giving an  optimal Huffman tree. In Exercise 12.4 you are asked to prove Huffman’s algorithm gives an optimal tree. Ties are broken  arbitrarily.  An example helps make operation of the algorithm clear. Figure 12.6 shows the initial forest; the weight of each tree is shown in small type at the root. The two trees of lowest weight are merged, creating the forest shown in Figure 12.7. The new root is T1. We made s the left child arbitrarily; any tiebreaking procedure can be used. The total weight of the new tree is just the sum of the weights of the old trees and can thus be easily computed.  Now there are six trees, and we again select the two trees of smallest weight, T1 and t. They are merged into a new tree with root T2 and weight 8, as shown in Figure 12.8. The third step merges T2 and a, creating T3, with weight  . Figure 12.9 shows the result of this operation.  After completion of the third merge, the two trees of lowest weight are the single-node trees representing i and sp. Figure 12.10 shows how these trees are merged into the new tree with root T4. The ﬁfth step is to merge the trees with roots e and T3 because these trees have the two smallest weights, giving the result shown in Figure 12.11.  Finally, an optimal tree, shown previously in Figure 12.4, is obtained by merging the two remaining trees. Figure 12.12 shows the optimal tree, with root T6. C – T 1 T 2 T 1 T 2 s nl a t e i sp figure 12.6 Initial stage of  Huffman’s algorithm t nl a sp e i s T1 figure 12.7 Huffman’s algorithm  after the first merge + =

chapter 12 utilities nl a sp e i s T1 t T2 figure 12.8 Huffman’s algorithm  after the second  merge nl a sp e i s T1 t T2 T3 figure 12.9 Huffman’s algorithm  after the third merge nl a sp e i s T1 t T2 T3 T4 figure 12.10 Huffman’s algorithm  after the fourth merge nl a sp i s T1 t T2 T3 T4 e T5 figure 12.11 Huffman’s algorithm  after the fifth merge

12.1 file compression 12.1.3   implementation We now provide an implementation of the Huffman coding algorithm, without attempting to perform any signiﬁcant optimizations; we simply want a working program that illustrates the basic algorithmic issues. After discussing the implementation we comment on possible enhancements. Although signiﬁcant error checking needs to be added to the program, we have not done so because we did not want to obscure the basic ideas. Figure 12.13 illustrates some of the I/O classes and constants to be used. We maintain a priority queue of tree nodes (recall that we are to select two trees of lowest weight). a e i s sp nl t T1 T2 T3 T5 T6 T4 figure 12.12 Huffman’s algorithm  after the final merge figure 12.13 The import directives  and some constants  used in the main  compression program  algorithms 1 import java.io.IOException; 2 import java.io.InputStream; 3 import java.io.OutputStream; 4 import java.io.FileInputStream; 5 import java.io.FileOutputStream; 6 import java.io.DataInputStream; 7 import java.io.DataOutputStream; 8 import java.io.BufferedInputStream; 9 import java.io.BufferedOutputStream; 10 import java.util.PriorityQueue; 12 interface BitUtils 13 { 14     public static final int BITS_PER_BYTES = 8; 15     public static final int DIFF_BYTES = 256; 16     public static final int EOF = 256; 17 }

chapter 12 utilities In addition to the standard I/O classes, our program consists of several additional classes. Because we need to perform bit-at-a-time I/O, we write wrapper classes representing bit-input and bit-output streams. We write other classes to maintain character counts and create and return information about a Huffman coding tree. Finally, we write compression and uncompression stream wrappers. To summarize, the classes that we write are BitInputStream Wraps an Inputstream and provides bit-at-a-time input. BitOutputStream Wraps an Outputstream and provides bit-at-a-time  output. CharCounter Maintains character counts. HuffmanTree Manipulates Huffman coding trees. HZIPInputStream Contains an uncompression wrapper. HZIPOutputStream Contains a compression wrapper. bit-input and bit-output stream classes The BitInputStream and BitOutputStream classes are similar and are shown in Figures 12.14 and 12.15, respectively. Both work by wrapping a stream. A reference to the stream is stored as a private data member. Every eighth readBit of the BitInputStream (or writeBit of the BitOutputStream classes) causes a byte to be read (or written) on the underlying stream. The byte is stored in a buffer, appropriately named buffer, and bufferPos provides an indication of how much of the buffer is unused. The getBit and setBit methods are used to access an individual bit in an 8-bit byte; they work by using bit operations. (Appendix C describes the bit operators in more detail.) In readBit, we check at line 19 to ﬁnd out whether the bits in the buffer have already been used. If so, we get 8 more bits at line 21 and reset the position indicator at line 24. Then we can call getBit at line 27. The BitOutputStream class is similar to BitInputStream. One difference is that we provide a flush method because there may be bits left in the buffer at the end of a sequence of writeBit calls. The flush method is called when a call to writeBit ﬁlls the buffer and also is called by close. Neither class performs error checking; instead they propagate any IOExceptions. Thus full error checking is available.

12.1 file compression figure 12.14 The BitInputStream class 1 // BitInputStream class: Bit-input stream wrapper class. 2 // 3 // CONSTRUCTION: with an open InputStream. 4 // 5 // ******************PUBLIC OPERATIONS*********************** 6 // int readBit( )              --> Read one bit as a 0 or 1 7 // void close( )               --> Close underlying stream 9 public class BitInputStream 10 { 11     public BitInputStream( InputStream is ) 12     { 13         in = is; 14         bufferPos = BitUtils.BITS_PER_BYTES; 15     } 17     public int readBit( ) throws IOException 18     { 19         if( bufferPos == BitUtils.BITS_PER_BYTES ) 20         { 21             buffer = in.read( ); 22             if( buffer == -1 ) 23                 return -1; 24             bufferPos = 0; 25         } 27         return getBit( buffer, bufferPos++ ); 28     } 30     public void close( ) throws IOException 31     { 32         in.close( ); 33     } 35     private static int getBit( int pack, int pos ) 36     { 37         return ( pack & ( 1 << pos ) ) != 0 ? 1 : 0; 38     } 40     private InputStream in; 41     private int buffer; 42     private int bufferPos; 43 }

chapter 12 utilities figure 12.15 The BitOutputStream class 1 // BitOutputStream class: Bit-output stream wrapper class. 2 // 3 // CONSTRUCTION: with an open OutputStream. 4 // 5 // ******************PUBLIC OPERATIONS*********************** 6 // void writeBit( val )        --> Write one bit (0 or 1) 7 // void writeBits( vals )      --> Write array of bits 8 // void flush( )               --> Flush buffered bits 9 // void close( )               --> Close underlying stream 11 public class BitOutputStream 12 { 13     public BitOutputStream( OutputStream os ) 14       { bufferPos = 0; buffer = 0; out = os; } 16     public void writeBit( int val ) throws IOException 17     { 18         buffer = setBit( buffer, bufferPos++, val ); 19         if( bufferPos == BitUtils.BITS_PER_BYTES ) 20             flush( ); 21     } 23     public void writeBits( int [ ] val ) throws IOException 24     { 25         for( int i = 0; i < val.length; i++ ) 26             writeBit( val[ i ] ); 27     } 29     public void flush( ) throws IOException 30     { 31         if( bufferPos == 0 ) 32             return; 33         out.write( buffer ); 34         bufferPos = 0; 35         buffer = 0; 36     } 38     public void close( ) throws IOException 39       { flush( ); out.close( ); } 41     private int setBit( int pack, int pos, int val ) 42     { 43         if( val == 1 ) 44             pack |= ( val << pos ); 45         return pack; 46     } 48     private OutputStream out; 49     private int buffer; 50     private int bufferPos; 51 }

12.1 file compression the character-counting class Figure 12.16 provides the CharCounter class, which is used to obtain the character counts in an input stream (typically a ﬁle). Alternatively, the character counts can be set manually and then obtained later. (Implicitly, we are treating eight-bit bytes as ASCII characters for this program.) the huffman tree class The tree is maintained as a collection of nodes. Each node has links to its left  child, right child, and parent (in Chapter 18 we discuss the implementation of  trees in detail). The node declaration is shown in Figure 12.17. The HuffmanTree class skeleton is provided in Figure 12.18. We can create a HuffmanTree object by providing a CharCounter object, in which case the tree is built immediately. Alternatively, it can be created without a CharCounter object. In that case, the character counts are read by a subsequent call to readEncodingTable, and at that point the tree is built. figure 12.16 The CharCounter class 1 // CharCounter class: A character counting class. 2 // 3 // CONSTRUCTION: with no parameters or an open InputStream. 4 // 5 // ******************PUBLIC OPERATIONS*********************** 6 // int getCount( ch )           --> Return # occurrences of ch 7 // void setCount( ch, count )   --> Set # occurrences of ch 8 // ******************ERRORS********************************** 9 // No error checks. 11 class CharCounter 12 { 13     public CharCounter( ) 14       { } 16     public CharCounter( InputStream input ) throws IOException 17     { 18         int ch; 19         while( ( ch = input.read( ) ) != -1 ) 20             theCounts[ ch ]++; 21     } 23     public int getCount( int ch ) 24       { return theCounts[ ch & 0xff ]; } 26     public void setCount( int ch, int count ) 27       { theCounts[ ch & 0xff ] = count; } 29     private int [ ] theCounts = new int[ BitUtils.DIFF_BYTES ]; 30 }

chapter 12 utilities The HuffmanTree class provides the writeEncodingTable method to write the tree out to an output stream (in a form suitable for a call to readEncodingTable). It also provides public methods to convert from a character to a code, and vice versa.1 Codes are represented by an int[] or String, as appropriate, in which each element is either a 0 or 1. Internally, root is a reference to the root node of the tree, and theCounts is a CharCounter object that can be used to initialize the tree nodes. We also maintain an array, theNodes, which maps each character to the tree node that contains it. Figure 12.19 shows the constructors and the routine (public method and private helper) to return the code for a given character. The constructors start with empty trees, and the one-parameter constructor initializes the CharCounter object, and immediately calls the private routine createTree. The CharCounter object is initialized to be empty in the zero-parameter constructor. For getCode, by consulting theNodes, we obtain the tree node that stores the character whose code we are looking for. If the character is not represented, we signal an error by returning a null reference. Otherwise we use a straightforward loop up the tree, following parent links, until we reach the root (which has no parent). Each step prepends a 0 or 1 to a string, which is converted to an array of int prior to returning (of course, this creates many temporary strings; we leave it to the reader to optimize this step). figure 12.17 Node declaration for  the Huffman coding  tree // Basic node in a Huffman coding tree. 2 class HuffNode implements Comparable<HuffNode> 3 { 4     public int value; 5     public int weight; 7     public int compareTo( HuffNode rhs ) 8     { 9         return weight - rhs.weight; 10     } 12     HuffNode left; 13     HuffNode right; 14     HuffNode parent; 16     HuffNode( int v, int w, HuffNode lt, HuffNode rt, HuffNode pt ) 17       { value = v; weight = w; left = lt; right = rt; parent = pt; } 18 } 1. Technical alert: An int is used instead of byte to allow all characters and the EOF symbol.

12.1 file compression figure 12.18 The HuffmanTree class skeleton 1 // Huffman tree class interface: manipulate Huffman coding tree. 2 // 3 // CONSTRUCTION: with no parameters or a CharCounter object. 4 // 5 // ******************PUBLIC OPERATIONS*********************** 6 // int [ ] getCode( ch )        --> Return code given character 7 // int getChar( code )          --> Return character given code 8 // void writeEncodingTable( out ) --> Write coding table to out 9 // void readEncodingTable( in ) --> Read encoding table from in 10 // ******************ERRORS********************************** 11 // Error check for illegal code. 13 class HuffmanTree 14 { 15     public HuffmanTree( ) 16       { /* Figure 12.19 */ } 17     public HuffmanTree( CharCounter cc ) 18       { /* Figure 12.19 */ } 20     public static final int ERROR = -3; 21     public static final int INCOMPLETE_CODE = -2; 22     public static final int END = BitUtils.DIFF_BYTES; 24     public int [ ] getCode( int ch ) 25       { /* Figure 12.19 */ }  26     public int getChar( String code ) 27       { /* Figure 12.20 */ } 29       // Write the encoding table using character counts 30     public void writeEncodingTable( DataOutputStream out ) throws IOException 31       { /* Figure 12.21 */ } 32     public void readEncodingTable( DataInputStream in ) throws IOException 33       { /* Figure 12.21 */ } 35     private CharCounter theCounts; 36     private HuffNode [ ] theNodes = new HuffNode[ BitUtils.DIFF_BYTES + 1 ]; 37     private HuffNode root; 39     private void createTree( ) 40       { /* Figure 12.22 */ } 41 }

chapter 12 utilities The getChar method shown in Figure 12.20 is simpler: We start at the root and branch left or right, as directed by the code. Reaching null prematurely figure 12.19 Some of the Huffman  tree methods,  including constructors  and the routine for  returning a code for a  given character public HuffmanTree( )     {         theCounts = new CharCounter( );         root = null;     }     public HuffmanTree( CharCounter cc )     {         theCounts = cc;         root = null;         createTree( );     }     /**      * Return the code corresponding to character ch.      * (The parameter is an int to accommodate EOF).      * If code is not found, return an array of length 0.      */     public int [ ] getCode( int ch )     {         HuffNode current = theNodes[ ch ];         if( current == null )             return null;         String v = "";         HuffNode par = current.parent;         while ( par != null )         {             if( par.left == current )                 v = "0" + v;             else                 v = "1" + v;             current = current.parent;             par = current.parent;         }         int [ ] result = new int[ v.length( ) ];         for( int i = 0; i < result.length; i++ )             result[ i ] = v.charAt( i ) == '0' ? 0 : 1;         return result;     }

12.1 file compression generates an error. Otherwise, we return the value stored in the node  (which for nonleaf nodes turns out to be the symbol INCOMPLETE). In Figure 12.21 we have routines to read and write the encoding table. The format that we use is simple and is not necessarily the most space-efficient. For each character that has a code, we write it out (using one byte) and then write out its character count (using four bytes). We signal the end of the table by writing out an extra entry containing a null terminator character '\0' with a count of zero. The count of zero is the special signal. The readEncodingTable method initializes all the character counts to zero and then reads the table, and updates the counts as they are read. It calls createTree, shown in Figure 12.22, to build the Huffman tree. In that routine, we maintain a priority queue of tree nodes. To do so we must provide a comparison function for tree nodes. Recall from Figure 12.17 that HuffNode implements Comparable<HuffNode>, ordering HuffNode objects on the basis of node weight. We then search for characters that have appeared at least once. When the test at line 9 succeeds, we have such a character. We create a new tree node at lines 11 and 12, add it to theNodes at line 13, and then add it to the priority queue at line 14. At lines 17 and 18 we add the end-of-ﬁle symbol. The loop that extends from lines 20 to 28 is a line-for-line translation of the tree construction algorithm. While we have two or more trees, we extract two trees from the priority queue, merge the result, and put it back in the priority queue. At the end of the loop, only one tree is left in the priority queue, and we can extract it and set root. figure 12.20 A routine for decoding  (generating a  character, given the  code) /**  * Get the character corresponding to code.  */ public int getChar( String code )     {         HuffNode p = root;         for( int i = 0; p != null && i < code.length( ); i++ )             if( code.charAt( i ) == '0' )                 p = p.left;             else                 p = p.right;         if( p == null )             return ERROR;         return p.value;     }

chapter 12 utilities figure 12.21 Routines for reading and writing encoding tables  /**   * Writes an encoding table to an output stream.   * Format is character, count (as bytes).   * A zero count terminates the encoding table.   */    public void writeEncodingTable( DataOutputStream out ) throws IOException    {        for( int i = 0; i < BitUtils.DIFF_BYTES; i++ )        {            if( theCounts.getCount( i ) > 0 )            {                out.writeByte( i );                out.writeInt( theCounts.getCount( i ) );            }        }        out.writeByte( 0 );        out.writeInt( 0 );    }    /**     * Read the encoding table from an input stream in format     * given and then construct the Huffman tree.     * Stream will then be positioned to read compressed data.     */    public void readEncodingTable( DataInputStream in ) throws IOException    {        for( int i = 0; i < BitUtils.DIFF_BYTES; i++ )            theCounts.setCount( i, 0 );        int ch;        int num;        for( ; ; )        {            ch = in.readByte( );            num = in.readInt( );            if( num == 0 )                break;            theCounts.setCount( ch, num );        }        createTree( );    }

12.1 file compression The tree produced by createTree is dependent on how the priority queue breaks ties. Unfortunately, this means that if the program is compiled on two different machines, with two different priority queue implementations, it is possible to compress a ﬁle on the ﬁrst machine, and then be unable to obtain the original when attempting to uncompress on the second machine. Avoiding this problem requires some additional work. figure 12.22 A routine for constructing the Huffman coding tree /**  * Construct the Huffman coding tree.  */     private void createTree( )     {         PriorityQueue<HuffNode> pq = new PriorityQueue<HuffNode>( );         for( int i = 0; i < BitUtils.DIFF_BYTES; i++ )             if( theCounts.getCount( i ) > 0 )             {                 HuffNode newNode = new HuffNode( i,                             theCounts.getCount( i ), null, null, null );                 theNodes[ i ] =  newNode;                 pq.add( newNode );             }         theNodes[ END ] = new HuffNode( END, 1, null, null, null );         pq.add( theNodes[ END ] );         while( pq.size( ) > 1 )         {             HuffNode n1 = pq.remove( );             HuffNode n2 = pq.remove( );             HuffNode result = new HuffNode( INCOMPLETE_CODE,                                   n1.weight + n2.weight, n1, n2, null );             n1.parent = n2.parent = result;             pq.add( result );         }         root = pq.element( );     }

chapter 12 utilities compression stream classes All that is left to do is to write a compression and uncompression stream wrapper and then a main that calls them. We repeat our earlier disclaimer about skimping on error checking so that we can illustrate the basic algorithmic ideas. The HZIPOutputStream class is shown in Figure 12.23. The constructor initiates a DataOutputStream, on which we can write the compressed stream. We also maintain a ByteArrayOutputStream. Each call to write appends onto the ByteArrayOutputStream. When close is called, the actual compressed stream is written. The close routine extracts all the bytes that have been stored in the ByteArrayOutputStream for reading at line 26. It then constructs a CharCounter object at line 29 and a HuffmanTree object at line 32. Since CharCounter needs an InputStream, we construct a ByteArrayInputStream from the array of bytes that were just extracted. At line 33 we write out the encoding table. At this point we are ready to do the main encoding. We create a bit-output stream object at line 35. The rest of the algorithm repeatedly gets a character and writes its code (line 38). There is a tricky piece of code at line 38: The int passed to getCode may be confused with EOF if we simply use the byte because the high bit can be interpreted as a sign bit. Thus we use a bit mask. When we exit the loop, we have reached the end of ﬁle, so we write out the end-of-ﬁle code at line 39. The BitOutputStream close ﬂushes any remaining bits to the output ﬁle, so an explicit call to flush is not needed. The HZIPInputStream class is next, in Figure 12.24. The constructor creates a DataInputStream and constructs a HuffmanTree object by reading the encoding table (lines 15 and 16) from the compressed stream. We then create a bit-input stream at line 18. The dirty work is done in the read method. The bits object, declared at line 23, represents the (Huffman) code that we are currently examining. Each time we read a bit at line 29, we add the bit to the end of the Huffman code (at line 33). We then look up the Huffman code at line 34. If it is incomplete, we continue the loop (lines 35 and 36). If there is an illegal Huffman code, we throw an IOException (lines 37 to 38). If we reach the end-of-ﬁle code, we return –1, as is standard for read (lines 39 and 40); otherwise, we have a match, so we return the character that matches the Huffman code (line 42).

12.1 file compression figure 12.23 The HZIPOutputStream class 1 import java.io.IOException; 2 import java.io.OutputStream; 3 import java.io.DataOutputStream; 4 import java.io.ByteArrayInputStream; 5 import java.io.ByteArrayOutputStream; 7 /** 8  * Writes to HZIPOutputStream are compressed and 9  * sent to the output stream being wrapped. 10  * No writing is actually done until close. 11  */ 12 public class HZIPOutputStream extends OutputStream 13 { 14     public HZIPOutputStream( OutputStream out ) throws IOException 15     { 16         dout = new DataOutputStream( out ); 17     } 19     public void write( int ch ) throws IOException 20     {  21         byteOut.write( ch ); 22     } 24     public void close( ) throws IOException 25     { 26         byte [ ] theInput = byteOut.toByteArray( ); 27         ByteArrayInputStream byteIn = new ByteArrayInputStream( theInput ); 29         CharCounter countObj = new CharCounter( byteIn ); 30         byteIn.close( ); 32         HuffmanTree codeTree = new HuffmanTree( countObj ); 33         codeTree.writeEncodingTable( dout ); 35         BitOutputStream bout = new BitOutputStream( dout ); 37         for( int i = 0; i < theInput.length; i++ ) 38             bout.writeBits( codeTree.getCode( theInput[ i ] & 0xff ) ); 39         bout.writeBits( codeTree.getCode( BitUtils.EOF ) ); 41         bout.close( ); 42         byteOut.close( ); 43     } 45     private ByteArrayOutputStream byteOut = new ByteArrayOutputStream( ); 46     private DataOutputStream dout; 47 }

chapter 12 utilities figure 12.24 The HZIPInputStream class 1 import java.io.IOException; 2 import java.io.InputStream; 3 import java.io.DataInputStream; 5 /** 6  * HZIPInputStream wraps an input stream. read returns an 7  * uncompressed byte from the wrapped input stream. 8  */ 9 public class HZIPInputStream extends InputStream 10 { 11     public HZIPInputStream( InputStream in ) throws IOException 12     { 13         DataInputStream din = new DataInputStream( in ); 15         codeTree = new HuffmanTree( ); 16         codeTree.readEncodingTable( din ); 18         bin = new BitInputStream( in ); 19     } 21     public int read( ) throws IOException 22     {  23         String bits = ""; 24         int bit; 25         int decode; 27         while( true ) 28         { 29             bit = bin.readBit( ); 30             if( bit == -1 ) 31                 throw new IOException( "Unexpected EOF" ); 33             bits += bit; 34             decode = codeTree.getChar( bits ); 35             if( decode == HuffmanTree.INCOMPLETE_CODE ) 36                 continue; 37             else if( decode == HuffmanTree.ERROR ) 38                 throw new IOException( "Decoding error" ); 39             else if( decode == HuffmanTree.END ) 40                 return -1; 41             else 42                 return decode; 43         } 44     } 46     public void close( ) throws IOException 47       { bin.close( ); } 49     private BitInputStream bin; 50     private HuffmanTree codeTree; 51 }

12.1 file compression the main routine The main routine is shown in the online code. If invoked with the -c argument, it compresses; with the -u argument it uncompresses. Figure 12.25 illustrates the wrapping of streams for compression and uncompression. Compression adds a “.huf” to the ﬁlename; uncompression adds a “.uc” to the ﬁlename, to avoid clobbering original ﬁles. improving the program The program, as written, serves its main purpose of illustrating the basics of the Huffman coding algorithm. It achieves some compression, even on moderately sized ﬁles. For instance, it obtains roughly 40 percent compression when run on its own source ﬁle, Hzip.java. However, the program could be improved in several ways. 1. The error checking is limited. A production program should rigorously ensure that the ﬁle being decompressed is actually a compressed ﬁle. (One way to have it do so is to write extra information in the encoding table.) The internal routines should have more checks. 2. Little effort has been made to minimize the size of the encoding table. For large ﬁles this lack is of little consequence, but for smaller ﬁles a large encoding table can be unacceptable because the encoding table takes up space itself. 3. A robust program checks the size of the resulting compressed ﬁle and aborts if the size is larger than the original. 4. In many places we made little attempt to optimize for speed. Memoization could be used to avoid repeated searching of the tree for codes. Further improvements to the program are left for you to do as Exercises 12.14–12.16.

chapter 12 utilities figure 12.25 A simple main for file compression and uncompression 1 class Hzip 2 { 3     public static void compress( String inFile ) throws IOException 4     { 5         String compressedFile = inFile + ".huf"; 6         InputStream in = new BufferedInputStream( 7                          new FileInputStream( inFile ) ); 8         OutputStream fout = new BufferedOutputStream( 9                             new FileOutputStream( compressedFile ) ); 10         HZIPOutputStream hzout = new HZIPOutputStream( fout ); 11         int ch; 12         while( ( ch = in.read( ) ) != -1 ) 13             hzout.write( ch ); 14         in.close( ); 15         hzout.close( ); 16     } 18     public static void uncompress( String compressedFile ) throws IOException 19     { 20         String inFile; 21         String extension; 23         inFile = compressedFile.substring( 0, compressedFile.length( ) - 4 ); 24         extension = compressedFile.substring( compressedFile.length( ) - 4 ); 26         if( !extension.equals( ".huf" ) ) 27         { 28             System.out.println( "Not a compressed file!" ); 29             return; 30         } 32         inFile += ".uc";    // for debugging, to not clobber original 33         InputStream fin = new BufferedInputStream( 34                           new FileInputStream( compressedFile ) ); 35         DataInputStream in = new DataInputStream( fin ); 36         HZIPInputStream hzin = new HZIPInputStream( in ); 38         OutputStream fout = new BufferedOutputStream( 39                             new FileOutputStream( inFile ) ); 40         int ch; 41         while( ( ch = hzin.read( ) ) != -1 ) 42             fout.write( ch ); 44         hzin.close( ); 45         fout.close( ); 46     } 47 }

12.2 a cross-reference generator 12.2 a cross-reference generator A cross-reference generator lists identiﬁers and their line  numbers. It is a  common application because it is  similar to creating  an index. In this section, we design a program called a cross-reference generator that scans a Java source ﬁle, sorts the identiﬁers, and outputs all the identiﬁers, along with the line numbers on which they occur. One compiler application is to list, for each method, the names of all other methods that it directly calls. However, this is a general problem that occurs in many other contexts. For instance, it can be used to generalize the creation of an index for a book. Another use, spell checking, is described in Exercise 12.23. As a spelling checker detects misspelled words in a document, those words are gathered, along with the lines on which they occur. This process avoids repeatedly printing out the same misspelled word and indicates where the errors are. 12.2.1   basic ideas We use a map to  store identiﬁers and  their line numbers.  We store the line  numbers for each  identiﬁer in a list. Our main algorithmic idea is to use a map to store each identiﬁer and the line numbers on which it occurs. In the map, the identiﬁer is the key, and the list of line numbers is the value. After the source ﬁle has been read and the map built, we can iterate over the collection, outputting identiﬁers and their corresponding line numbers. 12.2.2   java implementation The Xref class skeleton is shown in Figure 12.26. It is similar to (but simpler than) the Balance class shown in Figure 11.3, which was part of a balanced symbol program. Like that class, it makes use of the Tokenizer class deﬁned in Figure 11.2. We can now discuss the implementation of the two remaining routines in the Tokenizer class: getNextID and getRemainingString. These new parsing routines deal with recognizing an identiﬁer. The parsing routines are straightforward, though as  usual they require  some effort. The routine shown in Figure 12.27 tests whether a character is part of an identiﬁer. In the getRemainingString routine shown in Figure 12.28 we assume that the ﬁrst character of an identiﬁer has already been read and is stored in the Tokenizer class data member ch. It repeatedly reads characters until one that is not part of an identiﬁer appears. At that point we put the character back (at line 12) and then return a String. The StringBuilder is used to avoid repeated, expensive, String concatenations. Section 15.4 describes the issues involved.

chapter 12 utilities The getNextID routine shown in Figure 12.29 is similar to the routine shown in Figure 11.7. The difference is that here at line 17, if the ﬁrst character of an identiﬁer is encountered, we call getRemainingString to return the figure 12.26 The Xref class  skeleton 1 import java.io.InputStreamReader; 2 import java.io.IOException; 3 import java.io.FileReader; 4 import java.io.Reader; 5 import java.util.Set 6 import java.util.TreeMap; 7 import java.util.List; 8 import java.util.ArrayList; 9 import java.util.Iterator; 10 import java.util.Map; 12 // Xref class interface: generate cross-reference 13 // 14 // CONSTRUCTION: with a Reader object 15 // 16 // ******************PUBLIC OPERATIONS*********************** 17 // void generateCrossReference( ) --> Name says it all ... 18 // ******************ERRORS********************************** 19 // Error checking on comments and quotes is performed 21 public class Xref 22 { 23     public Xref( Reader inStream ) 24       { tok = new Tokenizer( inStream ); } 26     public void generateCrossReference( ) 27       { /* Figure 12.30 */ } 29     private Tokenizer tok;   // tokenizer object 30 } figure 12.27 A routine for testing  whether a character  could be part of an  identifier /**  * Return true if ch can be part of a Java identifier  */     private static final boolean isIdChar( char ch )     {         return Character.isJavaIdentifierPart( ch );     }

12.2 a cross-reference generator token. The fact that getNextID and getNextOpenClose are so similar suggests that it would have been worthwhile to write a private member function that performs their common tasks. figure 12.28 A routine for returning  a String from input  /**      * Return an identifier read from input stream      * First character is already read into ch      */     private String getRemainingString( )     {         StringBuilder result = new StringBuilder( ch );         for( ; nextChar( ); result.append( ch ) )             if( !isIdChar( ch ) )             {                 putBackChar( );                 break;             }         return new String( result );     } figure 12.29 A routine for returning  the next identifier /**  * Return next identifier, skipping comments  * string constants, and character constants.  * Place identifier in currentIdNode.word and return false  * only if end of stream is reached.  */     public String getNextID( )     {         while( nextChar( ) )         {             if( ch == '/' )                 processSlash( );             else if( ch == '\\' )                 nextChar( );             else if( ch == '\'' || ch == '"' )                 skipQuote( ch );             else if( !Character.isDigit( ch ) && isIdChar( ch ) )                 return getRemainingString( );         }         return null;       // End of file     }

chapter 12 utilities With all the supporting routines written, let us consider the only method, generateCrossReference, shown in Figure 12.30. Lines 6 and 7 create an empty map. We read the input and build the map at lines 11–20. At each iteration, we figure 12.30 The main cross-reference algorithm /**  * Output the cross reference  */   public void generateCrossReference( )     {         Map<String,List<Integer>> theIdentifiers =                                   new TreeMap<String,List<Integer>>( );         String current;             // Insert identifiers into the search tree         while( ( current = tok.getNextID( ) ) != null )         {             List<Integer> lines = theIdentifiers.get( current );             if( lines == null )             {                 lines = new ArrayList<Integer>( );                 theIdentifiers.put( current, lines );             }             lines.add( tok.getLineNumber( ) );         }             // Iterate through search tree and output             // identifiers and their line number         Set entries = theIdentifiers.entrySet( );         for( Map.Entry<String,List<Integer>> thisNode : entries )         {             Iterator<Integer> lineItr = thisNode.getValue( ).iterator( );                 // Print identifier and first line where it occurs             System.out.print( thisNode.getKey( ) + ": " );             System.out.print( lineItr.next( ) );                 // Print all other lines on which it occurs             while( lineItr.hasNext( ) )                 System.out.print( ", " + lineItr.next( ) );             System.out.println( );         }     }

chapt er simulation An important use of  computers is simulation, in which the  computer is used to  emulate the operation of a real system and gather  statistics. An important use of computers is for simulation, in which the computer is used to emulate the operation of a real system and gather statistics. For example, we might want to simulate the operation of a bank with k tellers to determine the minimum value of k that gives reasonable service time. Using a computer for this task has many advantages. First, the information would be gathered without involving real customers. Second, a simulation by computer can be faster than the actual implementation because of the speed of the computer. Third, the simulation could be easily replicated. In many cases, the proper choice of data structures can help us improve the efﬁciency of the simulation. In this chapter, we show n How to simulate a game modeled on the Josephus problem n How to simulate the operation of a computer modem bank 13.1 the josephus problem The Josephus problem is the following game: N people, numbered 1 to N, are sitting in a circle; starting at person 1, a hot potato is passed; after M passes,

chapter 13 simulation the person holding the hot potato is eliminated, the circle closes ranks, and the game continues with the person who was sitting after the eliminated person picking up the hot potato; the last remaining person wins. A common assumption is that M is a constant, although a random number generator can be used to change M after each elimination. In the Josephus problem, a hot  potato is repeatedly  passed; when passing terminates, the  player holding the  potato is eliminated; the game  continues, and the  last remaining  player wins. The Josephus problem arose in the ﬁrst century A.D. in a cave on a mountain in Israel where Jewish zealots were being besieged by Roman soldiers. The historian Josephus was among them. To Josephus’s consternation, the zealots voted to enter into a suicide pact rather than surrender to the Romans. He suggested the game that now bears his name. The hot potato was the sentence of death to the person next to the one who got the potato. Josephus rigged the game to get the last lot and convinced the remaining intended victim that the two of them should surrender. That is how we know about this game; in effect, Josephus cheated.1 If M = 0, the players are eliminated in order, and the last player always wins. For other values of M, things are not so obvious. Figure 13.1 shows that if N = 5 and M = 1, the players are eliminated in the order 2, 4, 1, 5. In this case, player 3 wins. The steps are as follows. 1. At the start, the potato is at player 1. After one pass, it is at player 2. 2. Player 2 is eliminated. Player 3 picks up the potato, and after one pass, it is at player 4. 3. Player 4 is eliminated. Player 5 picks up the potato and passes it to player 1. 4. Player 1 is eliminated. Player 3 picks up the potato and passes it to player 5. 5. Player 5 is eliminated, so player 3 wins. 1. Thanks to David Teague for relaying this story. The version that we solve differs from the historical description. In Exercise 13.11 you are asked to solve the historical version. (a)                             (b)                       (c)                (d)         (e) figure 13.1 The Josephus  problem: At each step,  the darkest circle  represents the initial  holder and the lightly  shaded circle  represents the player  who receives the hot  potato (and is  eliminated). Passes  are made clockwise.

13.1 the josephus problem First, we write a program that simulates, pass for pass, a game for any values of N and M. The running time of the simulation is O(MN), which is acceptable if the number of passes is small. Each step takes O(M) time because it performs M passes. We then show how to implement each step in O(log N) time, regardless of the number of passes performed. The running time of the simulation becomes O(N log N). 13.1.1   the simple solution We can represent  the players by a  linked list and use  the iterator to simulate the passing.  The passing stage in the Josephus problem suggests that we represent the players in a linked list. We create a linked list in which the elements 1, 2,  , N are inserted in order. We then set an iterator to the front element. Each pass of the potato corresponds to a next operation on the iterator. At the last player (currently remaining) in the list we implement the pass by creating a new iterator positioned prior to the ﬁrst element. This action mimics the circle. When we have ﬁnished passing, we remove the element on which the iterator has landed. An implementation is shown in Figure 13.2. The linked list and iterator are declared at lines 8 and 15, respectively. We construct the initial list by using the loop at lines 11 and 12. In Figure 13.2, the code at lines 18 to 25 plays one step of the algorithm by passing the potato (lines 18 to 24) and then eliminating a player (line 25). This procedure is repeated until the test at line 16 tells us that only one player remains. At that point we return the player’s number at line 30. The running time of this routine is O(MN) because that is exactly the number of passes that occur during the algorithm. For small M, this running time is acceptable, although we should mention that the case M = 0 does not yield a running time of O(0); obviously the running time is O(N). We do not merely multiply by zero when trying to interpret a Big-Oh expression. Note that we can replace LinkedList with ArrayList, without affecting the running time. We can also use a TreeSet, but the cost of construction will not be O(N). 13.1.2   a more efficient algorithm If we implement  each round of  passing in a single  logarithmic operation, the simulation  will be faster. A more efﬁcient algorithm can be obtained if we use a data structure that supports accessing the kth smallest item (in logarithmic time). Doing so allows us to implement each round of passing in a single operation. Figure 13.1 shows why. Suppose that we have N players remaining and are currently at player P from the front. Initially N is the total number of players and P is 1. After M passes, a calculation tells us that we are at player ((P + M) mod N) from the front, except if that would give us player 0, in which case, we go to player N. The calculation is fairly tricky, but the concept is not. …

chapter 13 simulation The calculation is  tricky because of  the circle. Applying this calculation to Figure 13.1, we observe that M is 1, N is initially 5, and P is initially 1. So the new value of P is 2. After the deletion, N drops to 4, but we are still at position 2, as part (b) of the ﬁgure suggests. The next value of P is 3, also shown in part (b), so the third element in the list is deleted and N falls to 3. The next value of P is 4 mod 3, or 1, so we are back at the ﬁrst player in the remaining list, as shown in part (c). This player is removed and N becomes 2. At this point, we add M to P, obtaining 2. Because 2 mod 2 is 0, we set P to player N, and thus the last player in the list is the one that is removed. This action agrees with part (d). After the removal, N is 1 and we are done. All we need then is a data structure that efﬁciently supports the findKth operation. The findKth operation returns the kth (smallest) item, for any figure 13.2 Linked list  implementation of the  Josephus problem /**  * Return the winner in the Josephus problem.  * Linked list implementation.  * (Can replace with ArrayList or TreeSet).  */     public static int josephus( int people, int passes )     {         Collection<Integer> theList = new LinkedList<Integer>( );             // Construct the list         for( int i = 1; i <= people; i++ )             theList.add( i );             // Play the game;         Iterator<Integer> itr = theList.iterator( );         while( people-- != 1 )         {             for( int i = 0; i <= passes; i++ )             {                 if( !itr.hasNext( ) )                     itr = theList.iterator( );                 itr.next( );             }             itr.remove( );         }         itr = theList.iterator( );         return itr.next( );     }

13.1 the josephus problem parameter k.2 Unfortunately, no Collections API data structures support the findKth operation. However, we can use one of the generic data structures that we implement in Part Four. Recall from the discussion in Section 6.7 that the data structures we implement in Chapter 19 follow a basic protocol that uses insert, remove, and find. We can then add findKth to the implementation. There are several similar alternatives. All of them use the fact that, as discussed in Section 6.7, TreeSet could have supported the ranking operation in logarithmic time on average or logarithmic time in the worst case if we had used a sophisticated binary search tree. Consequently, we can expect an O(N log N) algorithm if we exercise care. The simplest method is to insert the items sequentially into a worst-case efﬁcient binary search tree such as a red-black tree, an AA-tree, or a splay tree (we discuss these trees in later chapters). We can then call findKth and remove, as appropriate. It turns out that a splay tree is an excellent choice for this application because the findKth and insert operations are unusually efﬁcient and remove is not terribly difﬁcult to code. We use an alternative here, however, because the implementations of these data structures that we provide in the later chapters leave implementing findKth for you to do as an exercise. A balanced search  tree will work, but it  is not needed if we  are careful and  construct a simple  binary search tree  that is not unbalanced at the start.  A class method can  be used to construct a perfectly  balanced tree in linear time. We use the BinarySearchTreeWithRank class that supports the findKth operation and is completely implemented in Section 19.2. It is based on the simple binary search tree and thus does not have logarithmic worst-case performance but merely average-case performance. Consequently, we cannot merely insert the items sequentially; that would cause the search tree to exhibit its worstcase performance. There are several options. One is to insert a random permutation of 1, ..., N into the search tree. The other is to build a perfectly balanced binary search tree with a class method. Because a class method would have access to the inner workings of the search tree, it could be done in linear time. This routine is left for you to do as Exercise 19.18 when search trees are discussed. We construct the  same tree by recursive insertions but  use O(N log N) time. The method we use is to write a recursive routine that inserts items in a balanced order. By inserting the middle item at the root and recursively building the two subtrees in the same manner, we obtain a balanced tree. The cost of our routine is an acceptable O(N log N). Although not as efﬁcient as the linear-time class routine, it does not adversely affect the asymptotic running time of the overall algorithm. The remove operations are then guaranteed to be logarithmic. This routine is called buildTree; it and the josephus method are then coded as shown in Figure 13.3. 2. The parameter k for findKth ranges from 1 to N, inclusive, where N is the number of items in the data structure. findKth can be supported by a search  tree.

chapter 13 simulation figure 13.3 An O(N log N) solution of the Josephus problem /**  * Recursively construct a perfectly balanced BinarySearchTreeWithRank  * by repeated insertions in O( N log N ) time.  * t should be empty on the initial call.  */     public static void buildTree( BinarySearchTreeWithRank<Integer> t,                                   int low, int high )     {         int center = ( low + high ) / 2;         if( low <= high )         {             t.insert( center );             buildTree( t, low, center - 1 );             buildTree( t, center + 1, high );         }     }     /**      * Return the winner in the Josephus problem.      * Search tree implementation.      */     public static int josephus( int people, int passes )     {         BinarySearchTreeWithRank<Integer> t =                  new BinarySearchTreeWithRank<Integer>( );         buildTree( t, 1, people );         int rank = 1;         while( people > 1 )         {             rank = ( rank + passes ) % people;             if( rank == 0 )                 rank = people;             t.remove( t.findKth( rank ) );             people--;         }         return t.findKth( 1 );     }

13.2 event-driven simulation 13.2 event-driven simulation Let us return to the bank simulation problem described in the introduction. Here, we have a system in which customers arrive and wait in line until one of k tellers is available. Customer arrival is governed by a probability distribution function, as is the service time (the amount of time to be served once a teller becomes available). We are interested in statistics such as how long on average a customer has to wait and what percentage of the time tellers are actually servicing requests. (If there are too many tellers, some will not do anything for long periods.) With certain probability distributions and values of k, we can compute these answers exactly. However, as k gets larger the analysis becomes considerably more difﬁcult and the use of a computer to simulate the operation of the bank is extremely helpful. In this way, bank ofﬁcers can determine how many tellers are needed to ensure reasonably smooth service. Most simulations require a thorough knowledge of probability, statistics, and queueing theory. 13.2.1   basic ideas A discrete event simulation consists of processing events. Here, the two events are (1) a customer arriving and (2) a customer departing, thus freeing up a teller. The tick is the  quantum unit of  time in a simulation. We can use a probability function to generate an input stream consisting of ordered pairs of arrival and service time for each customer, sorted by arrival time.3 We do not need to use the exact time of day. Rather, we can use a quantum unit, referred to as a tick. A discrete timedriven simulation processes each  unit of time consecutively. It is inappropriate if the  interval between  successive events  is large. In a discrete time-driven simulation we might start a simulation clock at zero ticks and advance the clock one tick at a time, checking to see whether an event occurs. If so, we process the event(s) and compile statistics. When no customers are left in the input stream and all the tellers are free, the simulation is over.  The problem with this simulation strategy is that its running time does not depend on the number of customers or events (there are two events per customer in this case). Rather, it depends on the number of ticks, which is not really part of the input. To show why this condition is important, let us change the clock units to microticks and multiply all the times in the input by 1,000,000. The simulation would then take 1,000,000 times longer. 3. The probability function generates interarrival times (times between arrivals), thus guaranteeing that arrivals are generated chronologically.

chapter 13 simulation An event-driven simulation advances the current time to the  next event. The key to avoiding this problem is to advance the clock to the next event time at each stage, called an event-driven simulation, which is conceptually easy to do. At any point, the next event that can occur is either the arrival of the next customer in the input stream or the departure of one of the customers from a teller’s station. All the times at which the events will happen are available, so we just need to ﬁnd the event that happens soonest and process that event (setting the current time to the time that the event occurs). If the event is a departure, processing includes gathering statistics for the departing customer and checking the line (queue) to determine whether another customer is waiting. If so, we add that customer, process whatever statistics are required, compute the time when the customer will leave, and add that departure to the set of events waiting to happen. If the event is an arrival, we check for an available teller. If there is none, we place the arrival in the line (queue). Otherwise, we give the customer a teller, compute the customer’s departure time, and add the departure to the set of events waiting to happen. The event set (i.e.,  events waiting to  happen) is organized as a priority  queue. The waiting line for customers can be implemented as a queue. Because we need to ﬁnd the next soonest event, the set of events should be organized in a priority queue. The next event is thus an arrival or departure (whichever is sooner); both are easily available. An event-driven simulation is appropriate if the number of ticks between events is expected to be large. 13.2.2   example: a call bank simulation The main algorithmic item in a simulation is the organization of the events in a priority queue. To focus on this requirement, we write a simple simulation. The system we simulate is a call bank at a large company. The call bank  removes the waiting line from the  simulation. Thus  there is only one  data structure.  A call bank consists of a large number of operators who handle phone calls. An operator is reached by dialing one telephone number. If any of the operators are available, the user is connected to one of them. If all the operators are already taking a phone call, the phone will give a busy signal. This can be viewed as the mechanism that is used by an automated customer service facility. Our simulation models the service provided by the pool of operators. The variables are n The number of operators in the bank n The probability distribution that governs dial-in attempts n The probability distribution that governs connect time n The length of time the simulation is to be run The call bank simulation is a simpliﬁed version of the bank teller simulation because there is no waiting line. Each dial-in is an arrival, and the total

13.2 event-driven simulation time spent once a connection has been established is the service time. By removing the waiting line, we remove the need to maintain a queue. Thus we have only one data structure, the priority queue. In Exercise 13.17 you are asked to incorporate a queue; as many as L calls will be queued if all the operators are busy. We list each event  as it happens; gathering statistics is a  simple extension. To simplify matters, we do not compute statistics. Instead, we list each event as it is processed. We also assume that attempts to connect occur at constant intervals; in an accurate simulation, we would model this interarrival time by a random process. Figure 13.4 shows the output of a simulation. The Event class represents events. In a  complex simulation,  it would derive all  possible types of  events as subclasses. Using inheritance for the Event class would complicate the code. The simulation class requires another class to represent events. The Event class is shown in Figure 13.5. The data members consist of the customer number, the time that the event will occur, and an indication of what type of event (DIAL_IN or HANG_UP) it is. If this simulation were more complex, with several types of events, we would make Event an abstract base class and derive subclasses from it. We do not do that here because that would complicate things and obscure the basic workings of the simulation algorithm. The Event class contains a constructor and a comparison function used by the priority queue. The Event class grants package visible status to the call bank simulation class so figure 13.4 Sample output for the  call bank simulation  involving three phone  lines: A dial-in is  attempted every  minute; the average  connect time is 5  minutes; and the  simulation is run for  18 minutes User 0 dials in at time 0 and connects for 1 minute User 0 hangs up at time 1 User 1 dials in at time 1 and connects for 5 minutes User 2 dials in at time 2 and connects for 4 minutes User 3 dials in at time 3 and connects for 11 minutes User 4 dials in at time 4 but gets busy signal User 5 dials in at time 5 but gets busy signal User 6 dials in at time 6 but gets busy signal User 1 hangs up at time 6 User 2 hangs up at time 6 User 7 dials in at time 7 and connects for 8 minutes User 8 dials in at time 8 and connects for 6 minutes User 9 dials in at time 9 but gets busy signal User 10 dials in at time 10 but gets busy signal User 11 dials in at time 11 but gets busy signal User 12 dials in at time 12 but gets busy signal User 13 dials in at time 13 but gets busy signal User 3 hangs up at time 14 User 14 dials in at time 14 and connects for 6 minutes User 8 hangs up at time 14 User 15 dials in at time 15 and connects for 3 minutes User 7 hangs up at time 15 User 16 dials in at time 16 and connects for 5 minutes User 17 dials in at time 17 but gets busy signal User 15 hangs up at time 18 User 18 dials in at time 18 and connects for 7 minutes

chapter 13 simulation that Event’s internal members can be accessed by CallSim methods. The Event class is nested inside the CallSim class. The call bank simulation class skeleton, CallSim, is shown in Figure 13.6. It consists of a lot of data members, a constructor, and two methods. The data members include a random number object r shown at line 27. At line 28 the eventSet is maintained as a priority queue of Event objects. The remaining data members are availableOperators, which is initially the number of operators in the simulation but changes as users connect and hang up, and avgCallLen and freqOfCalls, which are parameters of the simulation. Recall that a dial-in attempt will be made every freqOfCalls ticks. The constructor, declared at line 15 and implemented in Figure 13.7, initializes these members and places the ﬁrst arrival in the eventSet priority queue. figure 13.5 The Event class used  for call bank  simulation  /**      * The event class.      * Implements the Comparable interface      * to arrange events by time of occurrence.      * (nested in CallSim)      */ private static class Event implements Comparable<Event>     {         static final int DIAL_IN = 1;         static final int HANG_UP = 2;         public Event( )         {             this( 0, 0, DIAL_IN );         }         public Event( int name, int tm, int type )         {             who  = name;             time = tm;             what = type;         }         public int compareTo( Event rhs )         {             return time - rhs.time;         }         int who;        // the number of the user         int time;       // when the event will occur         int what;       // DIAL_IN or HANG_UP     }

13.2 event-driven simulation The nextCall method adds a dialin request to the  event set. The simulation class consists of only two methods. First, nextCall, shown in Figure 13.8, adds a dial-in request to the event set. It maintains two private variables: the number of the next user who will attempt to dial in and when that event will occur. Again, we have made the simplifying assumption that calls are made at regular intervals. In practice, we would use a random number generator to model the arrival stream. figure 13.6 The CallSim class  skeleton import weiss.util.Random; import java.util.PriorityQueue; // CallSim clas interface: run a simulation // // CONSTRUCTION: with three parameters: the number of //     operators, the average connect time, and the //     interarrival time // // ******************PUBLIC OPERATIONS********************* // void runSim( )       --> Run a simulation public class CallSim {     public CallSim( int operators, double avgLen, int callIntrvl )       { /* Figure 13.7 */ }       // Run the simulation.     public void runSim( long stoppingTime )       { /* Figure 13.9 */ }       // Add a call to eventSet at the current time,       // and schedule one for delta in the future.     private void nextCall( int delta )       { /* Figure 13.8 */ }     private Random r;                       // A random source     private PriorityQueue<Event> eventSet;  // Pending events         // Basic parameters of the simulation     private int availableOperators;     // Number of available operators     private double avgCallLen;        // Length of a call     private int freqOfCalls;          // Interval between calls     private static class Event implements Comparable<Event>       { /* Figure 13.5 */ } 37 }

chapter 13 simulation The runSim method  runs the simulation. The other method is runSim, which is called to run the entire simulation. The runSim method does most of the work and is shown in Figure 13.9. It is called with a single parameter that indicates when the simulation should end. As long as the event set is not empty, we process events. Note that it should never be empty because at the time we arrive at line 12 there is exactly one dial-in request in the priority queue and one hang-up request for every currently connected caller. Whenever we remove an event at line 12 and it is conﬁrmed to be a dial-in, we generate a replacement dial-in event at line 40. A hang-up event is also generated at line 35 if the dial-in succeeds. Thus the only way to ﬁnish the routine is if nextCall is set up not to generate an event eventually or (more likely) by executing the break statement at line 15. figure 13.7 The CallSim constructor  /**      * Constructor.      * @param operator number of operators.      * @param avgLen averge length of a call.      * @param callIntrvl the average time between calls.      */     public CallSim( int operators, double avgLen, int callIntrvl )     {         eventSet           = new PriorityQueue<Event>( );         availableOperators = operators;         avgCallLen         = avgLen;         freqOfCalls        = callIntrvl;         r                  = new Random( );         nextCall( freqOfCalls );  // Schedule first call     } figure 13.8 The nextCall method  places a new DIAL_IN event in the event  queue and advances  the time when the  next DIAL_IN event will  occur private int userNum = 0; 2     private int nextCallTime = 0;     /**      * Place a new DIAL_IN event into the event queue.      * Then advance the time when next DIAL_IN event will occur.      * In practice, we would use a random number to set the time.      */     private void nextCall( int delta )     {         Event ev = new Event( userNum++, nextCallTime, Event.DIAL_IN );         eventSet.add( ev );         nextCallTime += delta;     }

13.2 event-driven simulation figure 13.9 The basic simulation routine /**      * Run the simulation until stoppingTime occurs.      * Print output as in Figure 13.4.      */     public void runSim( long stoppingTime )     {         Event e = null;         int howLong;         while( !eventSet.isEmpty( ) )         {             e = eventSet.remove( );             if( e.time > stoppingTime )                 break;             if( e.what == Event.HANG_UP )    // HANG_UP             {                 availableOperators++;                 System.out.println( "User " + e.who +    " hangs up at time " + e.time );             }             else                      // DIAL_IN             {                 System.out.print( "User " + e.who +       " dials in at time " + e.time + " " );                 if( availableOperators > 0 )                 {                     availableOperators--;                     howLong = r.nextPoisson( avgCallLen );                     System.out.println( "and connects for "                                          + howLong + " minutes" );                     e.time += howLong;                     e.what = Event.HANG_UP;                     eventSet.add( e );                 }                 else                     System.out.println( "but gets busy signal" );                 nextCall( freqOfCalls );             }         }     }

chapter 13 simulation A hang-up increases  avaliableOperators. A dial-in  checks on whether  an operator is available and if so  decreases availableOperators. Let us summarize how the various events are processed. If the event is a hang-up, we increment availableOperators at line 19 and print a message at lines 20 and 21. If the event is a dial-in, we generate a partial line of output that records the attempt, and then, if any operators are available, we connect the user. To do so, we decrement availableOperators at line 29, generate a connection time (using a Poisson distribution rather than a uniform distribution) at line 30, print the rest of the output at lines 31–32, and add a hang-up to the event set (lines 33–35). Otherwise, no operators are available, and we give the busy signal message. Either way, an additional dial-in event is generated. Figure 13.10 shows the state of the priority queue after each deleteMin for the early stages of the sample output shown in Figure 13.4. The time at which each event occurs is shown in boldface, and the number of free operators (if any) are shown to the right of the priority queue. (Note that the call length is not actually stored in an Event object; we include it, when appropriate, to make the ﬁgure more selfcontained. A ‘?’ for the call length signiﬁes a dial-in event that eventually will result in a busy signal; however, that outcome is not known at the time the event is added to the priority queue.) The sequence of priority queue steps is as follows. 1. The ﬁrst DIAL_IN request is inserted. 2. After DIAL_IN is removed, the request is connected, thereby resulting in a HANG_UP and a replacement DIAL_IN request. 3. A HANG_UP request is processed. 4. A DIAL_IN request is processed resulting in a connect. Thus both a HANG_UP event and a DIAL_IN event are added (three times). 5. A DIAL_IN request fails; a replacement DIAL_IN is generated (three times). 6. A HANG_UP request is processed (twice). 7. A DIAL_IN request succeeds, and HANG_UP and DIAL_IN are added. The simulation uses  a poor model. Negative exponential  distributions would  more accurately  model the time  between dial-in  attempts and total  connect time. Again, if Event were an abstract base class, we would expect a procedure doEvent to be deﬁned through the Event hierarchy; then we would not need long chains of if/else statements. However to access the priority queue, which is in the simulation class, we would need Event to store a reference to the simulation CallSim class as a data member. We would insert it at construction time. A minimal (in the truest sense) main routine is shown for completeness in Figure 13.11. Note that using a Poisson distribution to model connect time is not appropriate. A better choice would be to use a negative exponential distribution

13.2 event-driven simulation figure 13.10 The priority queue for  the call bank  simulation after each  step DIAL_IN User 0, Len 1 HANG_UP User 0, Len 1 DIAL_IN User 1, Len 5 HANG_UP User 1, Len 5 DIAL_IN User 2, Len 4 HANG_UP User 1, Len 5 HANG_UP User 2, Len 4 DIAL_IN User 3, Len 11 HANG_UP User 1, Len 5 HANG_UP User 1, Len 5 HANG_UP User 1, Len 5 HANG_UP User 1, Len 5 HANG_UP User 2, Len 4 14 HANG_UP User 3, Len 11 14 HANG_UP User 3, Len 11 14 HANG_UP User 3, Len 11 DIAL_IN User 7, Len 8 15 HANG_UP User 7, Len 8 DIAL_IN User 7, Len 8 DIAL_IN User 8, Len 6 DIAL_IN User 1, Len 5 HANG_UP User 2, Len 4 14 HANG_UP User 3, Len 11 DIAL_IN User 4, Len ? HANG_UP User 2, Len 4 14 HANG_UP User 3, Len 11 DIAL_IN User 5, Len ? HANG_UP User 2, Len 4 14 HANG_UP User 3, Len 11 DIAL_IN User 6, Len ? HANG_UP User 2, Len 4 14 HANG_UP User 3, Len 11 DIAL_IN User 7, Len 8

chapter 13 simulation (but the reasons for doing so are beyond the scope of this text). Additionally, assuming a ﬁxed time between dial-in attempts is also inaccurate. Again, a negative exponential distribution would be a better model. If we change the simulation to use these distributions, the clock would be represented as a double. In Exercise 13.13 you are asked to implement these changes.

chapt er graphs and paths In this chapter we examine the graph and show how to solve a particular kind of problem—namely, calculation of shortest paths. The computation of shortest paths is a fundamental application in computer science because many interesting situations can be modeled by a graph. Finding the fastest routes for a mass transportation system, and routing electronic mail through a network of computers are but a few examples. We examine variations of the shortest path problems that depend on an interpretation of shortest and the graph’s properties. Shortest-path problems are interesting because, although the algorithms are fairly simple, they are slow for large graphs unless careful attention is paid to the choice of data structures. In this chapter, we show n Formal deﬁnitions of a graph and its components n The data structures used to represent a graph n Algorithms for solving several variations of the shortest-path problem, with complete Java implementations

chapter 14 graphs and paths 14.1 definitions A graph consists of  a set of vertices  and a set of edges  that connect the  vertices. If the edge  pair is ordered, the  graph is a directed graph. A graph consists of a set of vertices and a set of edges that connect the vertices. That is, G = (V, E), where V is the set of vertices and E is the set of edges. Each edge is a pair (v, w), where v, w ∈V. Vertices are sometimes called nodes, and edges are sometimes called arcs. If the edge pair is ordered, the graph is called a directed graph. Directed graphs are sometimes called digraphs. In a digraph, vertex w is adjacent to vertex v if and only if (v, w) ∈E. Sometimes an edge has a third component, called the edge cost (or weight) that measures the cost of traversing the edge. In this chapter, all graphs are directed. Vertex w is adjacent to vertex v if there  is an edge from v to w. The graph shown in Figure 14.1 has seven vertices, and 12 edges, A path is a  sequence of vertices connected by  edges. The following vertices are adjacent to V3: V2, V4, V5, and V6. Note that V0 and V1 are not adjacent to V3. For this graph,   and  ; here,  represents the size of set S. A path in a graph is a sequence of vertices connected by edges. In other words,   the sequence of vertices is such that (wi , wi + 1) ∈E for . The path length is the number of edges on the path—namely, —also called the unweighted path length. The weighted path length is the sum of the costs of the edges on the path. For example,   is a V2 V0 V3 V4 V1 V5 V6 figure 14.1 A directed graph V V0 V1 V2 V3 V4 V5 V6 , , , , , , { } = E V0 V1 2 , , ( ) V0 V3 1 , , ( ) V1 V3 3 , , ( ) V1 V4 10 , , ( ) , , , V3 V4 2 , , ( ) V3 V6 4 , , ( ) V3 V5 8 , , ( ) V3 V2 2 , , ( ) , , , V2 V0 4 , , ( ) V2 V5 5 , , ( ) V4 V6 6 , , ( ) V6 V5 1 , , ( ) , , , ⎩ ⎭ ⎪ ⎪ ⎨ ⎬ ⎪ ⎪ ⎧ ⎫ = V = E = S w1 w2 … wN , , , i N < ≤ N – V0 V3 V5 , , The unweighted path length measures the number of  edges on a path.

14.1 definitions The weighted path  length is the sum of  the edge costs on a  path. A cycle in a directed  graph is a path that  begins and ends at  the same vertex and  contains at least  one edge. A directed acyclic  graph has no cycles.  Such graphs are an  important class of  graphs. path from vertex   to  . The path length is two edges—the shortest path between   and  , and the weighted path length is 9. However, if cost is important, the weighted shortest path between these vertices has cost 6 and is . A path may exist from a vertex to itself. If this path contains no edges, the path length is 0, which is a convenient way to deﬁne an otherwise special case. A simple path is a path in which all vertices are distinct, except that the ﬁrst and last vertices can be the same. A cycle in a directed graph is a path that begins and ends at the same vertex and contains at least one edge. That is, it has a length of at least 1 such that ; this cycle is simple if the path is simple. A directed acyclic graph (DAG) is a type of directed graph having no cycles. An example of a real-life situation that can be modeled by a graph is the airport system. Each airport is a vertex. If there is a nonstop ﬂight between two airports, two vertices are connected by an edge. The edge could have a weight representing time, distance, or the cost of the ﬂight. In an undirected graph, an edge (v, w) would imply an edge (w, v). However, the costs of the edges might be different because ﬂying in different directions might take longer (depending on prevailing winds) or cost more (depending on local taxes). Thus we use a directed graph with both edges listed, possibly with different weights. Naturally, we want to determine quickly the best ﬂight between any two airports; best could mean the path with the fewest edges or one, or all, of the weight measures (distance, cost, and so on). A second example of a real-life situation that can be modeled by a graph is the routing of electronic mail through computer networks. Vertices represent computers, the edges represent links between pairs of computers, and the edge costs represent communication costs (phone bill per megabyte), delay costs (seconds per megabyte), or combinations of these and other factors. A graph is dense if  the number of  edges is large  (generally quadratic). Typical  graphs are not  dense. Instead, they  are sparse. For most graphs, there is likely at most one edge from any vertex v to any other vertex w (allowing one edge in each direction between v and w). Consequently,  . When most edges are present, we have  . Such a graph is considered to be a dense graph—that is, it has a large number of edges, generally quadratic. In most applications, however, a sparse graph is the norm. For instance, in the airport model, we do not expect direct ﬂights between every pair of airports. Instead, a few airports are very well connected and most others have relatively few ﬂights. In a complex mass transportation system involving buses and trains, for any one station we have only a few other stations that are directly reachable and thus represented by an edge. Moreover, in a computer network most computers are attached to a few other local computers. So, in most cases, the graph is relatively sparse, where   or perhaps slightly more (there is no standard deﬁnition of sparse). The algorithms that we develop, then, must be efﬁcient for sparse graphs. V0 V5 V0 V5 V0 V3 V6 V5 , , , w1 wN = E V 2 ≤ E Θ V 2 ( ) = E Θ V ( ) =

chapter 14 graphs and paths 14.1.1   representation The ﬁrst thing to consider is how to represent a graph internally. Assume that the vertices are sequentially numbered starting from 0, as the graph shown in Figure 14.1 suggests. One simple way to represent a graph is to use a two-dimensional array called an adjacency matrix. For each edge (v, w), we set a[v][w] equal to the edge cost; nonexistent edges can be initialized with a logical INFINITY. The initialization of the graph seems to require that the entire adjacency matrix be initialized to INFINITY. Then, as an edge is encountered, an appropriate entry is set. In this scenario, the initialization takes   time. Although the quadratic initialization cost can be avoided (see Exercise 14.6), the space cost is still  , which is ﬁne for dense graphs but completely unacceptable for sparse graphs. An adjacency list represents a graph,  using linear space. For sparse graphs, a better solution is an adjacency list, which represents a graph by using linear space. For each vertex, we keep a list of all adjacent vertices. An adjacency list representation of the graph in Figure 14.1 using a linked list is shown in Figure 14.2. Because each edge appears in a list node, the number of list nodes equals the number of edges. Consequently,  space is used to store the list nodes. We have   lists, so   additional space is also required. If we assume that every vertex is in some edge, the number of edges is at least  . Hence we may disregard any  terms when an   term is present. Consequently, we say that the space requirement is  , or linear in the size of the graph. Adjacency lists can  be constructed in  linear time from a  list of edges. The adjacency list can be constructed in linear time from a list of edges. We begin by making all the lists empty. When we encounter an edge , we add an entry consisting of w and the cost   to v’s adjacency list. The insertion can be anywhere; inserting it at the front can be done in constant time. Each edge can be inserted in constant time, so the entire adjacency list structure can be constructed in linear time. Note that O V 2 ( ) O V 2 ( ) 1 (2) 4 (10) 0 (4) 4 (2) 3 (1) 3 (3) 5 (5) 6 (4) 5 (8) 2 (2) 6 (6) 5 (1) figure 14.2 Adjacency list  representation of the  graph shown in  Figure 14.1; the  nodes in list i represent vertices  adjacent to i and the  cost of the connecting  edge. O E ( ) V O V ( ) V ⁄ O V ( ) O E ( ) O E ( ) v w cv w , , , ( ) cv w , An adjacency matrix represents a graph  and uses quadratic  space.

14.1 definitions when inserting an edge, we do not check whether it is already present. That cannot be done in constant time (using a simple linked list), and doing the check would destroy the linear-time bound for construction. In most cases, ignoring this check is unimportant. If there are two or more edges of different cost connecting a pair of vertices, any shortest-path algorithm will choose the lower cost edge without resorting to any special processing. Note also that ArrayLists can be used instead of linked lists, with the constanttime add operation replacing insertions at the front. A map can be used  to map vertex  names to internal  numbers. In most real-life applications the vertices have names, which are unknown at compile time, instead of numbers. Consequently, we must provide a way to transform names to numbers. The easiest way to do so is to provide a map by which we map a vertex name to an internal number ranging from 0 to  (the number of vertices is determined as the program runs). The internal numbers are assigned as the graph is read. The ﬁrst number assigned is 0. As each edge is input, we check whether each of the two vertices has been assigned a number, by looking in the map. If it has been assigned an internal number, we use it. Otherwise, we assign to the vertex the next available number and insert the vertex name and number in the map. With this transformation, all the graph algorithms use only the internal numbers. Eventually, we have to output the real vertex names, not the internal numbers, so for each internal number we must also record the corresponding vertex name. One way to do so is to keep a string for each vertex. We use this technique to implement a Graph class. The class and the shortest-path algorithms require several data structures—namely, a list, a queue, a map, and a priority queue. The import directives are shown in Figure 14.3. The queue (implemented with a linked list) and priority queue are used in various shortest-path calculations. The adjacency list is represented with LinkedList. A HashMap is also used to represent the graph. V – figure 14.3 The import directives for the Graph class import java.io.FileReader; import java.io.InputStreamReader; import java.io.BufferedReader; import java.io.IOException; import java.util.StringTokenizer; import java.util.Collection; import java.util.List; import java.util.LinkedList; import java.util.Map; import java.util.HashMap; import java.util.Iterator; import java.util.Queue; import java.util.PriorityQueue; import java.util.NoSuchElementException;

chapter 14 graphs and paths When we write an actual Java implementation, we do not need internal vertex numbers. Instead, each vertex is stored in a Vertex object, and instead of using a number, we can use a reference to the Vertex object as its (uniquely identifying) number. However, when describing the algorithms, assuming that vertices are numbered is often convenient, and we occasionally do so. Before we show the Graph class skeleton, let us examine Figures 14.4 and 14.5, which show how our graph is to be represented. Figure 14.4 shows the representation in which we use internal numbers. Figure 14.5 replaces the internal numbers with Vertex variables, as we do in our code. Although this simpliﬁes the code, it greatly complicates the picture. Because the two ﬁgures represent identical inputs, Figure 14.4 can be used to follow the complications in Figure 14.5. As indicated in the part labeled Input, we can expect the user to provide a list of edges, one per line. At the start of the algorithm, we do not know the names of any of the vertices, how many vertices there are, or how many edges there are. We use two basic data structures to represent the graph. As we mentioned in the preceding paragraph, for each vertex we maintain a Vertex object that stores some information. We describe the details of Vertex (in particular, how different Vertex objects interact with each other) last. As mentioned earlier, the ﬁrst major data structure is a map that allows us to ﬁnd, for any vertex name, the Vertex object that represents it. This map is shown in Figure 14.5 as vertexMap (Figure 14.4 maps the name to an int in the component labeled Dictionary). figure 14.4 An abstract scenario  of the data structures  used in a shortestpath calculation, with  an input graph taken  from a file. The  shortest weighted  path from A to C is A to B to E to D to C (cost is  76). C D E A B D  C  A  B  D  B  A  D  E  D  B  E  C  A dist D C A B E Graph table Input Visual representation of graph Dictionary prev name adj 3 (23),1 (10) 2 (19) 0 (87),3 (12) 4 (11) 0 (43) D (0) B (3) E (4) C (1) A (2) -1

14.1 definitions The second major data structure is the Vertex object that stores information about all the vertices. Of particular interest is how it interacts with other Vertex objects. Figures 14.4 and 14.5 show that a Vertex object maintains four pieces of information for each vertex. n name: The name corresponding to this vertex is established when the vertex is placed in the map and never changes. None of the shortestpath algorithms examines this member. It is used only to print a ﬁnal path. figure 14.5 Data structures used  in a shortest-path  calculation, with an  input graph taken  from a file; the  shortest weighted  path from A to C is  A to B to E to D to C (cost is 76). D C A B E D  C  A  B  D  B  A  D  E  D  B  E  C  A Input C D E A B Visual representation of graph D C A B E vertexMap Legend: Dark-bordered boxes are Vertex objects. The unshaded portion in each box  contains the name and adjacency list and does not change when shortest-path computation  is performed. Each adjacency list entry contains an Edge that stores a reference to another  Vertex object and the edge cost. Shaded portion is dist and prev, filled in after shortest path  computation runs. Dark arrows emanate from vertexMap. Light arrows are adjacency list entries. Dashed arrows  are the prev data member that results from a shortest-path computation.

chapter 14 graphs and paths n adj: This list of adjacent vertices is established when the graph is read. None of the shortest-path algorithms changes the list. In the abstract, Figure 14.4 shows that it is a list of Edge objects that each contain an internal vertex number and edge cost. In reality, Figure 14.5 shows that each Edge object contains a reference to a Vertex and an edge cost and that the list is actually stored by using an ArrayList or LinkedList. n dist: The length of the shortest path (either weighted or unweighted, depending on the algorithm) from the starting vertex to this vertex as computed by the shortest-path algorithm. n prev: The previous vertex on the shortest path to this vertex, which in the abstract (Figure 14.4) is an int but in reality (the code and Figure 14.5) is a reference to a Vertex. To be more speciﬁc, in Figures 14.4 and 14.5 the unshaded items are not altered by any of the shortest-path calculations. They represent the input graph and do not change unless the graph itself changes (perhaps by addition or deletion of edges at some later point). The shaded items are computed by the shortest-path algorithms. Prior to the calculation we can assume that they are uninitialized.1 The shortest-path  algorithms are single source algorithms that compute  the shortest paths  from some starting  point to all vertices. The shortest-path algorithms are all single-source algorithms, which begin at some starting point and compute the shortest paths from it to all vertices. In this example the starting point is A, and by consulting the map we can ﬁnd its Vertex object. Note that the shortest-path algorithm declares that the shortest path to A is 0. The prev data member allows us to print out the shortest path, not just its length. For instance, by consulting the Vertex object for C, we see that the shortest path from the starting vertex to C has a total cost of 76. Obviously, the last vertex on this path is C. The vertex before C on this path is D, before D is E, before E is B, and before B is A— the starting vertex. Thus, by tracing back through the prev data member, we can construct the shortest path. Although this trace gives the path in reverse order, unreversing it is a simple matter. In the remainder of this section we describe how the unshaded parts of all the Vertex objects are constructed and give the method that prints out a shortest path, assuming that the dist and prev data members have been computed. We discuss individually the algorithms used to ﬁll in the shortest path. Figure 14.6 shows the Edge class that represents the basic item placed in the adjacency list. The Edge consists of a reference to a Vertex and the edge cost. The Vertex class is shown in Figure 14.7. An additional member named scratch is provided and has different uses in the various algorithms. 1. The computed information (shaded) could be separated into a separate class, with Vertex maintaining a reference to it, making the code more reusable but more complex. The prev member  can be used to  extract the actual  path. The item in an adjacency list is a reference to the Vertex object of the adjacent vertex and the  edge cost.

14.1 definitions Everything else follows from our preceding description. The reset method is used to initialize the (shaded) data members that are computed by the shortest-path algorithms; it is called when a shortest-path computation is restarted. We are now ready to examine the Graph class skeleton, which is shown in Figure 14.8. The vertexMap ﬁeld stores the map. The rest of the class provides methods that perform initialization, add vertices and edges, print the shortest path, and perform various shortest-path calculations. We discuss each routine when we examine its implementation. First, we consider the constructor. The default creates an empty map via ﬁeld initialization; that works, so we accept it.  figure 14.6 The basic item stored  in an adjacency list // Represents an edge in the graph. class Edge {     public Vertex dest;        // Second vertex in Edge     public double cost;        // Edge cost     public Edge( Vertex d, double c )     {         dest = d;         cost = c;     } } figure 14.7 The Vertex class  stores information for  each vertex // Represents a vertex in the graph. class Vertex {     public String     name;   // Vertex name     public List<Edge> adj;    // Adjacent vertices     public double     dist;   // Cost     public Vertex     prev;   // Previous vertex on shortest path     public int        scratch;// Extra variable used in algorithm     public Vertex( String nm )       { name = nm; adj = new LinkedList<Edge>( ); reset( ); }     public void reset( ) { dist = Graph.INFINITY; prev = null; pos = null; scratch = 0; }     }

chapter 14 graphs and paths figure 14.8 The Graph class skeleton 1 // Graph class: evaluate shortest paths. 2 // 3 // CONSTRUCTION: with no parameters. 4 // 5 // ******************PUBLIC OPERATIONS********************** 6 // void addEdge( String v, String w, double cvw ) 7 //                              --> Add additional edge 8 // void printPath( String w )   --> Print path after alg is run 9 // void unweighted( String s )  --> Single-source unweighted 10 // void dijkstra( String s )    --> Single-source weighted 11 // void negative( String s )    --> Single-source negative weighted 12 // void acyclic( String s )     --> Single-source acyclic 13 // ******************ERRORS********************************* 14 // Some error checking is performed to make sure that graph is ok 15 // and that graph satisfies properties needed by each 16 // algorithm.  Exceptions are thrown if errors are detected. 18 public class Graph 19 { 20     public static final double INFINITY = Double.MAX_VALUE; 22     public void addEdge( String sourceName, String destName, double cost ) 23       { /* Figure 14.10 */ } 24     public void printPath( String destName ) 25       { /* Figure 14.13 */ } 26     public void unweighted( String startName ) 27       { /* Figure 14.22 */ } 28     public void dijkstra( String startName ) 29       { /* Figure 14.27 */ } 30     public void negative( String startName ) 31       { /* Figure 14.29 */ } 32     public void acyclic( String startName ) 33       { /* Figure 14.32 */ } 35     private Vertex getVertex( String vertexName ) 36       { /* Figure 14.9 */ } 37     private void printPath( Vertex dest ) 38       { /* Figure 14.12 */ } 39     private void clearAll( ) 40       { /* Figure 14.11 */ } private Map<String,Vertex> vertexMap = new HashMap<String,Vertex>( ); 43 } 45 // Used to signal violations of preconditions for 46 // various shortest path algorithms. 47 class GraphException extends RuntimeException 48 { 49     public GraphException( String name ) 50       { super( name ); } }

14.1 definitions Edges are added  by insertions in the  appropriate adjacency list. We can now look at the main methods. The getVertex method is shown in Figure 14.9. We consult the map to get the Vertex entry. If the Vertex does not exist, we create a new Vertex and update the map. The addEdge method, shown in Figure 14.10 is short. We get the corresponding Vertex entries and then update an adjacency list. The clearAll routine clears out the  data members so  that the shortest  path algorithms  can begin. The members that are eventually computed by the shortest-path algorithm are initialized by the routine clearAll, shown in Figure 14.11. The next routine, printPath, prints a shortest path after the computation has been performed. As we mentioned earlier, we can use the prev member to trace back the path, but doing so gives the path in reverse order. This order is not a problem if we use recursion: The vertices on the path to dest are the same as those on the path to dest’s previous vertex (on the path), followed by dest. This strategy translates directly into the short recursive routine shown in Figure 14.12, assuming of course that a path actually exists. The printPath routine, shown in Figure 14.13, performs this check ﬁrst and then prints a message if the path does not exist. Otherwise, it calls the recursive routine and outputs the cost of the path. figure 14.9 The getVertex routine returns the Vertex object that represents  vertexName, creating  the object if it needs  to do so /**  * If vertexName is not present, add it to vertexMap.  * In either case, return the Vertex.  */     private Vertex getVertex( String vertexName )     {         Vertex v = vertexMap.get( vertexName );         if( v == null )         {             v = new Vertex( vertexName );             vertexMap.put( vertexName, v );         }         return v;     } figure 14.10 Add an edge to the graph /**  * Add a new edge to the graph.  */     public void addEdge( String sourceName, String destName, double cost )     {         Vertex v = getVertex( sourceName );         Vertex w = getVertex( destName );         v.adj.add( new Edge( w, cost ) );     } The printPath routine prints the  shortest path after  the algorithm has  run.

chapter 14 graphs and paths figure 14.11 Private routine for  initializing the output  members for use by  the shortest-path  algorithms /**  * Initializes the vertex output info prior to running  * any shortest path algorithm.  */     private void clearAll( )     {         for( Vertex v : vertexMap.values( ) )             v.reset( );     } figure 14.12 A recursive routine for  printing the shortest  path /**  * Recursive routine to print shortest path to dest  * after running shortest path algorithm. The path  * is known to exist.  */     private void printPath( Vertex dest )     {         if( dest.prev != null )         {             printPath( dest.prev );             System.out.print( " to " );         }         System.out.print( dest.name );     } figure 14.13 A routine for printing  the shortest path by  consulting the graph  table (see Figure  14.5) /**  * Driver routine to handle unreachables and print total cost.  * It calls recursive routine to print shortest path to  * destNode after a shortest path algorithm has run.  */     public void printPath( String destName )     {         Vertex w = vertexMap.get( destName );         if( w == null )             throw new NoSuchElementException( );         else if( w.dist == INFINITY )             System.out.println( destName + " is unreachable" );         else         {             System.out.print( "(Cost is: " + w.dist + ") " );             printPath( w );             System.out.println( );         }     }

14.2 unweighted shortest-path problem The Graph class is  easy to use. We provide a simple test program that reads a graph from an input ﬁle, prompts for a start vertex and a destination vertex, and then runs one of the shortest-path algorithms. Figure 14.14 illustrates that to construct the Graph object, we repeatedly read one line of input, assign the line to a StringTokenizer object, parse that line, and call addEdge. Using a StringTokenizer allows us to verify that every line has the three pieces corresponding to an edge. Once the graph has been read, we repeatedly call processRequest, shown in Figure 14.15. This version prompts for a starting and ending vertex and then calls one of the shortest-path algorithms. This algorithm throws a GraphException if, for instance, it is asked for a path between vertices that are not in the graph. Thus processRequest catches any GraphException that might be generated and prints an appropriate error message. 14.2 unweighted shortest-path  problem The unweighted path length measures the number  of edges on a path. Recall that the unweighted path length measures the number of edges. In this section we consider the problem of ﬁnding the shortest unweighted path length between speciﬁed vertices. unweighted single-source, shortest-path problem Find the shortest path (measured by number of edges) from a designated vertex S to every vertex. All variations of the  shortest-path problem have similar  solutions. The unweighted shortest-path problem is a special case of the weighted shortest-path problem (in which all weights are 1). Hence it should have a more efﬁcient solution than the weighted shortest-path problem. That turns out to be true, although the algorithms for all the path problems are similar. 14.2.1   theory To solve the unweighted shortest-path problem, we use the graph previously shown in Figure 14.1, with   as the starting vertex S. For now, we are concerned with ﬁnding the length of all shortest paths. Later, we maintain the corresponding paths. We can see immediately that the shortest path from S to  is a path of length 0. This information yields the graph shown in Figure 14.16. Now we can start looking for all vertices that are distance 1 from S. We can ﬁnd them by looking at the vertices adjacent to S. If we do so, we see that and  are one edge away from S, as shown in Figure 14.17. V2 V2 V0 V5

chapter 14 graphs and paths figure 14.14 A simple main     /**      * A main routine that:      * 1. Reads a file (supplied as a command-line parameter)      *    containing edges.       * 2. Forms the graph;      * 3. Repeatedly prompts for two vertices and      *    runs the shortest path algorithm.      * The data file is a sequence of lines of the format      *    source destination cost      */     public static void main( String [ ] args )     {         Graph g = new Graph( );         try         {             FileReader fin = new FileReader( args[0] );             Scanner graphFile = new Scanner( fin );             // Read the edges and insert             String line;             while( graphFile.hasNextLine( ) )             {                 line = graphFile.nextLine( );                 StringTokenizer st = new StringTokenizer( line );                 try                 {                     if( st.countTokens( ) != 3 )                     {                         System.err.println( "Skipping bad line " + line );                         continue;                     }                     String source  = st.nextToken( );                     String dest    = st.nextToken( );                     int    cost    = Integer.parseInt( st.nextToken( ) );                     g.addEdge( source, dest, cost );                 }                 catch( NumberFormatException e )                   { System.err.println( "Skipping bad line " + line ); }              }          }          catch( IOException e )            { System.err.println( e ); }          System.out.println( "File read..." );          Scanner in = new Scanner( System.in );          while( processRequest( in, g ) )              ; 50     }

14.2 unweighted shortest-path problem figure 14.15 For testing purposes,  processRequest calls  one of the shortestpath algorithms     /**      * Process a request; return false if end of file.      */     public static boolean processRequest( Scanner in, Graph g )     {         try         {             System.out.print( "Enter start node:" );             String startName = in.nextLine( );             System.out.print( "Enter destination node:" );             String destName = in.nextLine( );             System.out.print( "Enter algorithm (u, d, n, a ): " );             String alg = in.nextLine( );             if( alg.equals( "u" ) )                 g.unweighted( startName );             else if( alg.equals( "d" ) )                 g.dijkstra( startName );             else if( alg.equals( "n" ) )                 g.negative( startName );             else if( alg.equals( "a" ) )                 g.acyclic( startName );             g.printPath( destName );         }         catch( NoSuchElementException e )           { return false; }         catch( GraphException e )           { System.err.println( e ); }         return true; 33     } V2 V0 V3 V4 V1 V5 V6 figure 14.16 The graph after the  starting vertex has  been marked as  reachable in zero  edges

chapter 14 graphs and paths Next, we ﬁnd each vertex whose shortest path from S is exactly 2. We do so by ﬁnding all the vertices adjacent to or (the vertices at distance 1) whose shortest paths are not already known. This search tells us that the shortest path to and is 2. Figure 14.18 shows our progress so far. Finally, by examining the vertices adjacent to the recently evaluated and ,we ﬁnd that and have a shortest path of 3 edges. All vertices have now been calculated. Figure 14.19 shows the ﬁnal result of the algorithm. This strategy for searching a graph is called breadth-ﬁrst search, which operates by processing vertices in layers: Those closest to the start are evaluated ﬁrst, and those most distant are evaluated last. V2 V0 V3 V4 V1 V5 V6 figure 14.17 The graph after all the  vertices whose path  length from the  starting vertex is 1  have been found V0 V5 V1 V3 V2 V0 V3 V4 V1 V5 V6 figure 14.18 The graph after all the  vertices whose  shortest path from the  starting vertex is 2  have been found V1 V3 V4 V6 V2 V0 V3 V4 V1 V5 V6 figure 14.19 The final shortest  paths Breadth-ﬁrst search processes vertices  in layers: Those  closest to the start  are evaluated ﬁrst.

14.2 unweighted shortest-path problem Figure 14.20 illustrates a fundamental principle: If a path to vertex v has cost Dv and w is adjacent to v, then there exists a path to w of cost Dw = Dv + 1. All the shortest-path algorithms work by starting with Dw = ∞and reducing its value when an appropriate v is scanned. To do this task efﬁciently, we must scan vertices v systematically. When a given v is scanned, we update the vertices w adjacent to v by scanning through v’s adjacency list. The roving eyeball moves from vertex  to vertex and  updates distances  for adjacent vertices. From the preceding discussion, we conclude that an algorithm for solving the unweighted shortest-path problem is as follows. Let Di be the length of the shortest path from S to i. We know that DS = 0 and that Di = ∞ initially for all i ≠S. We maintain a roving eyeball that hops from vertex to vertex and is initially at S. If v is the vertex that the eyeball is currently on, then, for all w that are adjacent to v, we set Dw = Dv + 1 if Dw = ∞. This reﬂects the fact that we can get to w by following a path to v and extending the path by the edge (v, w)—again, illustrated in Figure 14.20. So we update vertices w as they are seen from the vantage point of the eyeball. Because the eyeball processes each vertex in order of its distance from the starting vertex and the edge adds exactly 1 to the length of the path to w, we are guaranteed that the ﬁrst time Dw is lowered from ∞, it is lowered to the value of the length of the shortest path to w. These actions also tell us that the next-to-last vertex on the path to w is v, so one extra line of code allows us to store the actual path.  After we have processed all of v’s adjacent vertices, we move the eyeball to another vertex u (that has not been visited by the eyeball) such that . If that is not possible, we move to a u that satisﬁes  . If that is not possible, we are done. Figure 14.21 shows how the eyeball visits vertices and updates distances. The lightly shaded node at each stage represents the position of the eyeball. In this picture and those that follow, the stages are shown top to bottom, left to right. All vertices adjacent to v are found  by scanning v’s adjacency list. The remaining detail is the data structure, and there are two basic actions to take. First, we repeatedly have to ﬁnd the vertex at which to place the eyeball. Second, we need to check all w’s adjacent to v (the current vertex) throughout the algorithm. The second action is easily implemented by iterating through v’s adjacency list. Indeed, as each edge is processed only once, figure 14.20 If w is adjacent to v and there is a path to  v, there also is a path  to w. v w Dv Dv +1 S Du Dv ≡ Du Dv + =

chapter 14 graphs and paths the total cost of all the iterations is  . The ﬁrst action is more challenging: We cannot simply scan through the graph table (see Figure 14.4) looking for an appropriate vertex because each scan could take   time and we need to perform it   times. Thus the total cost would be  , which is unacceptable for sparse graphs. Fortunately, this technique is not needed. When a vertex w has its Dw lowered from ∞, it becomes a candidate for an eyeball visitation at some point in the future. That is, after the eyeball visits vertices in the current distance group  , it visits the next distance group Dv + 1, which is the group containing w. Thus w just needs to wait in V2 V0 V3 V4 V1 V5 V6 V0 V3 V4 V1 V5 V6 V3 V4 V1 V6 V4 V6 V2 V0 V3 V4 V1 V5 V6 V3 V4 V1 V5 V6 V3 V4 V6 V6 V2 V0 V2 V0 V2 V0 V2 V0 V2 V0 V2 V3 V4 V1 V5 V1 V5 V5 V3 V1 V5 figure 14.21 Searching the graph  in the unweighted  shortest-path computation. The  darkest-shaded vertices have already  been completely  processed, the  lightest vertices have  not yet been used as  v, and the mediumshaded vertex is the  current vertex, v.  The  stages proceed left to  right, top to bottom, as  numbered. O E ( ) O V ( ) V O V 2 ( ) Dv

14.3 positive-weighted, shortest-path problem line for its turn. Also, as it clearly does not need to go before any other vertices that have already had their distances lowered, w needs to be placed at the end of a queue of vertices waiting for an eyeball visitation. To select a vertex v for the eyeball, we merely choose the front vertex from the queue. We start with an empty queue and then we enqueue the starting vertex S. A vertex is enqueued and dequeued at most once per shortest-path calculation, and queue operations are constant time, so the cost of choosing the vertex to select is only   for the entire algorithm. Thus the cost of the breadth-ﬁrst search is dominated by the scans of the adjacency list and is , or linear, in the size of the graph. 14.2.2   java implementation Implementation is  much simpler than  it sounds. It follows  the algorithm  description verbatim. The unweighted shortest-path algorithm is implemented by the method unweighted, as shown in Figure 14.22. The code is a line-for-line translation of the algorithm described previously. The initialization at lines 6–13 makes all the distances inﬁnity, sets DS to 0, and then enqueues the start vertex. The queue is declared at line 12. While the queue is not empty, there are vertices to visit. Thus at line 17 we move to the vertex v that is at the front of the queue. Line 19 iterates over the adjacency list and produces all the w’s that are adjacent to v. The test Dw = ∞is performed at line 23. If it returns true, the update Dw = Dv + 1 is performed at line 25 along with the update of w’s prev data member and enqueueing of w at lines 26 and 27, respectively. 14.3 positive-weighted, shortest-path problem The weighted path  length is the sum of  the edge costs on a  path. Recall that the weighted path length of a path is the sum of the edge costs on the path. In this section we consider the problem of ﬁnding the weighted shortest path, in a graph whose edges have nonnegative cost. We want to ﬁnd the shortest weighted path from some starting vertex to all vertices. As we show shortly, the assumption that edge costs are nonnegative is important because it allows a relatively efﬁcient algorithm. The method used to solve the positive-weighted, shortest-path problem is known as Dijkstra’s algorithm. In the next section we examine a slower algorithm that works even if there are negative edge costs. positive-weighted, single-source, shortest-path problem Find the shortest path (measured by total cost) from a designated vertex S to every vertex. All edge costs are nonnegative. O V ( ) O E ( ) When a vertex has  its distance lowered (which can  happen only once),  it is placed on the  queue so that the  eyeball can visit it in  the future. The  starting vertex is  placed on the  queue when its distance is initialized to  zero.

chapter 14 graphs and paths 14.3.1   theory: dijkstra’s algorithm Dijkstra’s algorithm is used to solve the  positive-weighted shortest-path problem. The positive-weighted, shortest-path problem is solved in much the same way as the unweighted problem. However, because of the edge costs, a few things change. The following issues must be examined: 1. How do we adjust Dw? 2. How do we ﬁnd the vertex v for the eyeball to visit? figure 14.22 The unweighted shortest-path algorithm, using breadth-first search /**  * Single-source unweighted shortest-path algorithm.  */     public void unweighted( String startName )     {         clearAll( );          Vertex start = vertexMap.get( startName );         if( start == null )             throw new NoSuchElementException( "Start vertex not found" );         Queue<Vertex> q = new LinkedList<Vertex>( );         q.add( start ); start.dist = 0;         while( !q.isEmpty( ) )         {             Vertex v = q.remove( );             for( Edge e : v.adj )             {                 Vertex w = e.dest;                 if( w.dist == INFINITY )                 {                     w.dist = v.dist + 1;                     w.prev = v;                     q.add( w );                 }             }         }     }

14.3 positive-weighted, shortest-path problem We use Dv + cv, w as the new distance and to decide  whether the distance should be  updated. We begin by examining how to alter Dw. In solving the unweighted shortest-path problem, if Dw = ∞, we set Dw = Dv + 1 because we lower the value of Dw if vertex v offers a shorter path to w. The dynamics of the algorithm ensure that we need alter Dw only once. We add 1 to Dv because the length of the path to w is 1 more than the length of the path to v. If we apply this logic to the weighted case, we should set Dw = Dv + cv, w if this new value of Dw is better than the original value. However, we are no longer guaranteed that Dw is altered only once. Consequently, Dw should be altered if its current value is larger than Dv + cv, w (rather than merely testing against ∞). Put simply, the algorithm decides whether v should be used on the path to w. The original cost Dw is the cost without using v; the cost Dv + cv, w is the cheapest path using v (so far). A queue is no  longer appropriate  for storing vertices  awaiting an eyeball  visit. Figure 14.23 shows a typical situation. Earlier in the algorithm, w had its distance lowered to 8 when the eyeball visited vertex u. However, when the eyeball visits vertex v, vertex w needs to have its distance lowered to 6 because we have a new shortest path. This result never occurs in the unweighted algorithm because all edges add 1 to the path length, so   implies  and thus  . Here, even though  , we can still improve the path to w by considering v. The distance for  unvisited vertices  represents a path  with only visited  vertices as intermediate  nodes. Figure 14.23 illustrates another important point. When w has its distance lowered, it does so only because it is adjacent to some vertex that has been visited by the eyeball. For instance, after the eyeball visits v and processing has been completed, the value of Dw is 6 and the last vertex on the path is a vertex that has been visited by the eyeball. Similarly, the vertex prior to v must also have been visited by the eyeball, and so on. Thus at any point the value of Dw represents a path from S to w using only vertices that have been visited by the eyeball as intermediate nodes. This crucial fact gives us Theorem 14.1. v w S u figure 14.23 The eyeball is at v and w is adjacent, so Dw should be lowered  to 6. Du Dv ≤ Du + Dv + ≤ Dw Dv + ≤ Du Dv ≤

chapter 14 graphs and paths Figure 14.25 shows the stages of Dijkstra’s algorithm. The remaining issue is the selection of an appropriate data structure. For dense graphs, we can scan down the graph table looking for the appropriate vertex. As with the unweighted shortest-path algorithm, this scan will take   time, which is optimal for a dense graph. For a sparse graph, we want to do better. Certainly, a queue does not work. The fact that we need to ﬁnd the vertex v with minimum   suggests that a priority queue is the method of choice. There are two ways to use the priority queue. One is to store each vertex in the Theorem 14.1 If we move the eyeball to the unseen vertex with minimum  , the algorithm correctly  produces the shortest paths if there are no negative edge costs. Proof Call each eyeball visit a “stage.” We prove by induction that, after any stage, the values of   for vertices visited by the eyeball form the shortest path and that the values of   for the other vertices form the shortest path using only vertices visited by  the eyeball as intermediates. Because the ﬁrst vertex visited is the starting vertex,  this statement is correct through the ﬁrst stage. Assume that it is correct for the ﬁrst  k stages. Let v be the vertex chosen by the eyeball in stage  . Suppose, for the  purpose of showing a contradiction, that there is a path from S to v of length less  than  . This path must go through an intermediate vertex that has not yet been visited by  the eyeball. Call the ﬁrst intermediate vertex on the path not visited by the eyeball u. This situation is shown in Figure 14.24. The path to u uses only vertices visited by the  eyeball as intermediates, so by induction,   represents the optimal distance to u. Moreover,  , because u is on the supposed shorter path to v. This inequality is  a contradiction because then we would have moved the eyeball to u instead of v. The  proof is completed by showing that all the   values remain correct for nonvisited  nodes, which is clear by the update rule. Di Di Di k + Dv Du Du Dv < Di figure 14.24 If Dv is minimal  among all unseen  vertices and if all edge  costs are nonnegative,  Dv represents the  shortest path. v Dv Du d >– 0 u S O V 2 ( ) Dv

14.3 positive-weighted, shortest-path problem priority queue and use the distance (obtained by consulting the graph table) as the ordering function. When we alter any  , we must update the priority queue by reestablishing the ordering property. This action amounts to a decreaseKey operation. To take it we need to be able to ﬁnd the location of w in the priority queue. Many implementations of the priority queue do not support decreaseKey. One that does is the pairing heap; we discuss use of the pairing heap for this application in Chapter 23. figure 14.25 Stages of Dijkstra’s  algorithm. The  conventions are the  same as those in  Figure 14.21. V2 V0 V3 V4 V1 V5 V6 V2 V3 V4 V1 V5 V6 V2 V4 V5 V6 V5 V6 V2 V0 V3 V4 V1 V5 V6 V2 V4 V1 V5 V6 V2 V5 V6 V5 V2 V0 V3 V4 V1 V6 V0 V3 V4 V1 V2 V0 V3 V4 V1 V0 V3 V1 V0 V0 V3 Dw The priority queue is  an appropriate data  structure. The easiest method is to  add a new entry,  consisting of a vertex and a distance,  to the priority queue  every time a vertex  has its distance  lowered. We can  ﬁnd the new vertex  to move to by  repeatedly removing the minimum  distance vertex  from the priority  queue until an  unvisited vertex  emerges.

chapter 14 graphs and paths Rather than use a fancy priority queue, we use a method that works with a simple priority queue, such as the binary heap, to be discussed in Chapter 21. Our method involves inserting an object consisting of w and Dw in the priority queue whenever we lower Dw. To select a new vertex v for visitation, we repeatedly remove the minimum item (based on distance) from the priority queue until an unvisited vertex emerges. Because the size of the priority queue could be as large as   and there are at most   priority queue insertions and deletions, the running time is  . Because   implies  , we have the same   algorithm that we would have if we used the ﬁrst method (in which the priority queue size is at most  ). 14.3.2   java implementation Again, the implementation follows the  description fairly  closely. The object placed on the priority queue is shown in Figure 14.26. It consists of w and Dw and a comparison function deﬁned on the basis of Dw. Figure 14.27 shows the routine dijkstra that calculates the shortest paths. Line 6 declares the priority queue pq. We declare vrec at line 18 to store the result of each deleteMin. As with the unweighted shortest-path algorithm, we begin by setting all distances to inﬁnity, setting DS = 0, and placing the starting vertex in our data structure. E E O E E log ( ) E V 2 ≤ E log V log ≤ O E V log ( ) V figure 14.26 Basic item stored in the priority queue // Represents an entry in the priority queue for Dijkstra's algorithm. class Path implements Comparable<Path> {     public Vertex     dest;   // w     public double     cost;   // d(w)     public Path( Vertex d, double c )     {         dest = d;         cost = c;     }     public int compareTo( Path rhs )     {         double otherCost = rhs.cost;         return cost < otherCost ? -1 : cost > otherCost ? 1 : 0;     } }

14.3 positive-weighted, shortest-path problem Each iteration of the while loop that begins at line 16 puts the eyeball at a vertex v and processes it by examining adjacent vertices w. v is chosen by repeatedly removing entries from the priority queue (at line 18) until we figure 14.27 A positive-weighted, shortest-path algorithm: Dijkstra’s algorithm /**  * Single-source weighted shortest-path algorithm.  */     public void dijkstra( String startName )     {         PriorityQueue<Path> pq = new PriorityQueue<Path>( );         Vertex start = vertexMap.get( startName );         if( start == null )             throw new NoSuchElementException( "Start vertex not found" );         clearAll( );         pq.add( new Path( start, 0 ) ); start.dist = 0;         int nodesSeen = 0;         while( !pq.isEmpty( ) && nodesSeen < vertexMap.size( ) )         {             Path vrec = pq.remove( );             Vertex v = vrec.dest;             if( v.scratch != 0 )  // already processed v                 continue;             v.scratch = 1;             nodesSeen++;             for( Edge e : v.adj )             {                 Vertex w = e.dest;                 double cvw = e.cost;                 if( cvw < 0 )                     throw new GraphException( "Graph has negative edges" );                 if( w.dist > v.dist + cvw )                 {                     w.dist = v.dist + cvw;                     w.prev = v;                     pq.add( new Path( w, w.dist ) );                 }             }         }     }

chapter 14 graphs and paths encounter a vertex that has not been processed. We use the scratch variable to record it. Initially, scratch is 0. Thus, if the vertex is unprocessed, the test fails at line 20, and we reach line 23. Then, when the vertex is processed, scratch is set to 1 (at line 23). The priority queue might be empty if, for instance, some of the vertices are unreachable. In that case, we can return immediately. The loop at lines 26–40 is much like the loop in the unweighted algorithm. The difference is that at line 29, we must extract cvw from the adjacency list entry, ensure that the edge is nonnegative (otherwise, our algorithm could produce incorrect answers), add cvw instead of 1 at lines 34 and 36, and add to the priority queue at line 38. 14.4 negative-weighted, shortest-path problem Negative edges  cause Dijkstra’s  algorithm not to  work. An alternative algorithm is  needed. Dijkstra’s algorithm requires that edge costs be nonnegative. This requirement is reasonable for most graph applications, but sometimes it is too restrictive. In this section we brieﬂy discuss the most general case: the negativeweighted, shortest-path algorithm. negative-weighted, single-source, shortest-path problem Find the shortest path (measured by total cost) from a designated vertex S to every vertex. Edge costs may be negative. 14.4.1   theory The proof of Dijkstra’s algorithm required the condition that edge costs, and thus paths, be nonnegative. Indeed, if the graph has negative edge costs, Dijkstra’s algorithm does not work. The problem is that, once a vertex v has been processed, there may be, from some other unprocessed vertex u, a negative path back to v. In such a case, taking a path from S to u to v is better than going from S to v without using u. If the latter were to happen, we would be in trouble. Not only would the path to v be wrong, but we also would have to revisit v because the distances of vertices reachable from v may be affected. (In Exercise 14.10 you are asked to construct an explicit example; four vertices sufﬁce.) We have an additional problem to worry about. Consider the graph shown in Figure 14.28. The path from V3 to V4 has a cost of 2. However, a shorter path exists by following the loop V3, V4, V1, V3, V4, which has a cost of –3. This path is still not the shortest because we could stay in the loop arbitrarily long. Thus the shortest path between these two points is undeﬁned. A negative-cost cycle makes most,  if not all, paths  undeﬁned because  we can stay in the  cycle arbitrarily long  and obtain an arbitrarily small  weighted path  length.

14.4 negative-weighted, shortest-path problem This problem is not restricted to nodes in the cycle. The shortest path from V2 to V5 is also undeﬁned because there is a way to get into and out of the loop. This loop is called a negative-cost cycle, which when present in a graph makes most, if not all, the shortest paths undeﬁned. Negative-cost edges by themselves are not necessarily bad; it is the cycles that are. Our algorithm either ﬁnds the shortest paths or reports the existence of a negative-cost cycle. Whenever a vertex  has its distance  lowered, it must be  placed on a queue.  This may happen  repeatedly for each  vertex. A combination of the weighted and unweighted algorithms will solve the problem, but at the cost of a potentially drastic increase in running time. As suggested previously, when Dw is altered, we must revisit it at some point in the future. Consequently, we use the queue as in the unweighted algorithm, but we use   as the distance measure (as in Dijkstra’s algorithm). The algorithm that is used to solve the negative-weighted, shortest-path problem is known as the Bellman–Ford algorithm. When the eyeball visits vertex v for the ith time, the value of Dv is the length of the shortest weighted path consisting of i or fewer edges. We leave the proof for you to do as Exercise 14.13. Consequently, if there are no negative-cost cycles, a vertex can dequeue at most  times and the algorithm takes at most   time. Further, if a vertex dequeues more than  times, we have detected a negative-cost cycle. 14.4.2   java implementation The tricky part of  the implementation  is the manipulation  of the scratch variable. We attempt to  avoid having any  vertex appear on  the queue twice at  any instant. Implementation of the negative-weighted, shortest-path algorithm is given in Figure 14.29. We make one small change to the algorithm description—namely, we do not enqueue a vertex if it is already on the queue. This change involves use of the scratch data member. When a vertex is enqueued, we increment scratch (at line 31). When it is dequeued, we increment it again (at line 18). Thus scratch is odd if the vertex is on the queue, and scratch/2 tells us how many times it has left the queue (which explains the test at line 18). When some w has its distance changed, but it is already on the queue (because scratch is odd), we do not –10 V0 V2 V5 V3 V6 V4 V1 figure 14.28 A graph with a  negative-cost cycle Dv cv w , + V O E V ( ) V The running time  can be large, especially if there is a  negative-cost cycle.

chapter 14 graphs and paths enqueue it. However, we do not add 2 to it to indicate that it has gone on (and off) the queue; this is done by offsetting of lines 31 and 34. The rest of the algorithm uses code that has already been introduced in both the unweighted shortest-path algorithm (Figure 14.22) and Dijkstra’s algorithm (Figure 14.27). figure 14.29 A negative-weighted, shortest-path algorithm: Negative edges are allowed. /**      * Single-source negative-weighted shortest-path algorithm.      */     public void negative( String startName )     {         clearAll( );          Vertex start = vertexMap.get( startName );         if( start == null )             throw new NoSuchElementException( "Start vertex not found" );         Queue<Vertex> q = new LinkedList<Vertex>( );         q.add( start ); start.dist = 0; start.scratch++;         while( !q.isEmpty( ) )         {             Vertex v = q.removeFirst( );             if( v.scratch++ > 2 * vertexMap.size( ) )                 throw new GraphException( "Negative cycle detected" );             for( Edge e : v.adj )             {                 Vertex w = e.dest;                 double cvw = e.cost;                 if( w.dist > v.dist + cvw )                 {                     w.dist = v.dist + cvw;                     w.prev = v;                       // Enqueue only if not already on the queue                     if( w.scratch++ % 2 == 0 )                         q.add( w );                     else                         w.scratch--;  // undo the enqueue increment                 }             }         }     }

14.5 path problems in acyclic graphs 14.5 path problems in acyclic graphs Recall that a directed acyclic graph has no cycles. This important class of graphs simpliﬁes the solution to the shortest-path problem. For instance, we do not have to worry about negative-cost cycles because there are no cycles. Thus we consider the following problem. weighted single-source, shortest-path problem for acyclic graphs Find the shortest path (measured by total cost) from a designated vertex S to every vertex in an acyclic graph. Edge costs are unrestricted. 14.5.1   topological sorting A topological sort orders vertices in a  directed acyclic  graph such that if  there is a path from u to v, then v appears  after u in the ordering. A graph that has  a cycle cannot have  a topological order. Before considering the shortest-path problem, let us examine a related problem: a topological sort. A topological sort orders vertices in a directed acyclic graph such that if there is a path from u to v, then v appears after u in the ordering. For instance, a graph is typically used to represent the prerequisite requirement for courses at universities. An edge (v, w) indicates that course v must be completed before course w may be attempted. A topological order of the courses is any sequence that does not violate the prerequisite requirements.  Clearly, a topological sort is not possible if a graph has a cycle because, for two vertices v and w on the cycle, there is a path from v to w and w to v. Thus any ordering of v and w would contradict one of the two paths. A graph may have several topological orders, and in most cases, any legal ordering will do. In a simple algorithm for performing a topological sort we ﬁrst ﬁnd any vertex v that has no incoming edges. Then we print the vertex and logically remove it, along with its edges, from the graph. Finally, we apply the same strategy to the rest of the graph. More formally, we say that the indegree of a vertex v is the number of incoming edges (u, v). The indegree of a  vertex is the number of incoming  edges. A topological sort can be performed in linear  time by repeatedly  and logically  removing vertices  that have no incoming edges. We compute the indegrees of all vertices in the graph. In practice, logically remove means that we lower the count of incoming edges for each vertex adjacent to v. Figure 14.30 shows the algorithm applied to an acyclic graph. The indegree is computed for each vertex. Vertex V2 has indegree 0, so it is ﬁrst in the topological order. If there were several vertices of indegree 0, we could choose any one of them. When V2 and its edges are removed from the graph, the indegrees of V0, V3, and V5 are all decremented by 1. Now V0 has indegree 0, so it is next in the topological order, and V1 and V3 have their indegrees lowered. The algorithm continues, and the remaining vertices are examined in the order V1, V3, V4, V6, and V5. To reiterate, we do not physically delete edges from the graph; removing edges just makes it easier to see how the indegree count is lowered.

chapter 14 graphs and paths The algorithm produces the correct  answer and detects  cycles if the graph  is not acyclic. Two important issues to consider are correctness and efﬁciency. Clearly, any ordering produced by the algorithm is a topological order. The question is whether every acyclic graph has a topological order, and if so, whether our algorithm is guaranteed to ﬁnd one. The answer is yes to both questions. If at any point there are unseen vertices but none of them have an indegree of 0, we are guaranteed that a cycle exists. To illustrate we can pick any vertex A0. Because A0 has an incoming edge, let A1 be the vertex connected to it. And as A1 has an incoming edge, let A2 be the vertex connected to it. We V2 V0 V3 V4 V1 V5 V6 V2 V0 V3 V4 V1 V5 V6 V0 V3 V4 V1 V5 V6 V3 V4 V5 V6 V5 V6 V5 V4 V5 V6 V3 V4 V1 V5 V6 V2 V0 V3 V4 V1 V6 V2 V0 V3 V4 V1 V2 V0 V3 V1 V2 V0 V1 V2 V0 V2 figure 14.30 A topological sort. The  conventions are the  same as those in  Figure 14.21.

14.5 path problems in acyclic graphs repeat this process N times, where N is the number of unprocessed vertices left in the graph. Among  , there must be two identical vertices (because there are N vertices but N + 1 Ai’s). Tracing backward between those identical Ai and Aj exhibits a cycle. The running time is  linear if a queue is  used. We can implement the algorithm in linear time by placing all unprocessed indegree 0 vertices on a queue. Initially, all vertices of indegree 0 are placed on the queue. To ﬁnd the next vertex in the topological order, we merely get and remove the front item from the queue. When a vertex has its indegree lowered to 0, it is placed on the queue. If the queue empties before all the vertices have been topologically sorted, the graph has a cycle. The running time is clearly linear, by the same reasoning used in the unweighted shortest-path algorithm. 14.5.2   theory of the acyclic  shortest-path algorithm In an acyclic graph,  the eyeball merely  visits vertices in  topological order. An important application of topological sorting is its use in solving the shortestpath problem for acyclic graphs. The idea is to have the eyeball visit vertices in topological order. This idea works because, when the eyeball visits vertex v, we are guaranteed that Dv can no longer be lowered; by the topological ordering rule, it has no incoming edges emanating from unvisited nodes. Figure 14.31 shows the stages of the shortest-path algorithm, using topological ordering to guide the vertex visitations. Note that the sequence of vertices visited is not the same as in Dijkstra’s algorithm. Also note that vertices visited by the eyeball prior to its reaching the starting vertex are unreachable from the starting vertex and have no inﬂuence on the distances of any vertex. The result is a lineartime algorithm even  with negative edge  weights. We do not need a priority queue. Instead, we need only to incorporate the topological sort into the shortest-path computation. Thus we ﬁnd that the algorithm runs in linear time and works even with negative edge weights. 14.5.3   java implementation The implementation of the shortest-path algorithm for acyclic graphs is shown in Figure 14.32. We use a queue to perform the topological sort and maintain the indegree information in the scratch data member. Lines 15–18 compute the indegrees, and at lines 21–23 we place any indegree 0 vertices on the queue. We then repeatedly remove a vertex from the queue at line 28. Note that, if the queue is empty, the for loop is terminated by the test at line 26. If the loop terminates because of a cycle, this fact is reported at line 50. Otherwise, the loop at line 30 steps through the adjacency list and a value of w is obtained A0 A1 … AN , , , The implementation combines a  topological sort calculation and a  shortest-path calculation. The indegree information  is stored in the  scratch data  member.

chapter 14 graphs and paths at line 32. Immediately we lower w’s indegree at line 35 and, if it has fallen to 0, we place it on the queue at line 36. Vertices that  appear before S in the topological  order are unreachable. Recall that if the current vertex v appears prior to S in topological order, v must be unreachable from S. Consequently, it still has Dv ≡∞and thus cannot hope to provide a path to any adjacent vertex w. We perform a test at line 38, and if a path cannot be provided, we do not attempt any distance calculations. Otherwise, at lines 41 to 45, we use the same calculations as in Dijkstra’s algorithm to update Dw if necessary. V2 V0 V3 V4 V1 V5 V6 V0 V3 V4 V1 V5 V6 V3 V4 V5 V6 V5 V6 V2 V0 V3 V4 V1 V5 V6 V3 V4 V1 V5 V6 V4 V5 V6 V5 V2 V0 V1 V2 V3 V0 V1 V2 V4 V3 V0 V1 V2 V6 V4 V3 V0 V1 V2 V0 V2 figure 14.31 The stages of acyclic  graph algorithm. The  conventions are the  same as those in  Figure 14.21.

14.5 path problems in acyclic graphs figure 14.32 A shortest-path algorithm for acyclic graphs /**  * Single-source negative-weighted acyclic-graph shortest-path algorithm.  */     public void acyclic( String startName )     {         Vertex start = vertexMap.get( startName );         if( start == null )             throw new NoSuchElementException( "Start vertex not found" );         clearAll( );          Queue<Vertex> q = new LinkedList<Vertex>( );         start.dist = 0;           // Compute the indegrees Collection<Vertex> vertexSet = vertexMap.values( );         for( Vertex v : vertexSet )             for( Edge e : v.adj )                 e.dest.scratch++;           // Enqueue vertices of indegree zero         for( Vertex v : vertexSet )             if( v.scratch == 0 )                 q.add( v );         int iterations;         for( iterations = 0; !q.isEmpty( ); iterations++ )         {             Vertex v = q.remove( );             for( Edge e : v.adj )             {                 Vertex w = e.dest;                 double cvw = e.cost;                 if( --w.scratch == 0 )                     q.add( w );                 if( v.dist == INFINITY )                     continue;                 if( w.dist > v.dist + cvw )                 {                     w.dist = v.dist + cvw;                     w.prev = v;                 }             }         }         if( iterations != vertexMap.size( ) )             throw new GraphException( "Graph has a cycle!" );     }

chapter 14 graphs and paths 14.5.4   an application: critical-path analysis Critical-path analysis is used to  schedule tasks  associated with a  project. An important use of acyclic graphs is critical-path analysis, a form of analysis used to schedule tasks associated with a project. The graph shown in Figure 14.33 provides an example. Each vertex represents an activity that must be completed, along with the time needed to complete it. The graph is thus called an activity-node graph, in which vertices represent activities and edges represent precedence relationships. An edge (v, w) indicates that activity v must be completed before activity w may begin, which implies that the graph must be acyclic. We assume that any activities that do not depend (either directly or indirectly) on each other can be performed in parallel by different servers. An activity-node graph represents  activities as vertices  and precedence  relationships as  edges. This type of graph could be (and frequently is) used to model construction projects. Two important questions must be answered. First, what is the earliest completion time for the project? The answer, as the graph shows, is 10 time units—required along path A, C, F, H. Second, which activities can be delayed, and by how long, without affecting the minimum completion time? For instance, delaying any of A, C, F, or H would push the completion time past 10 time units. However, activity B is less critical and can be delayed up to 2 time units without affecting the ﬁnal completion time. The event-node graph consists of  event vertices that  correspond to the  completion of an  activity and all its  dependent activities. To perform these calculations, we convert the activity-node graph to an event-node graph, in which each event corresponds to the completion of an activity and all its dependent activities. Events reachable from a node v in the event-node graph may not commence until after the event v is completed. This graph can be constructed automatically or by hand (from the activity-node graph). Dummy edges and vertices may need to be inserted to avoid introducing false dependencies (or false lack of dependencies). The event-node graph corresponding to the activity-node graph in Figure 14.33 is shown in Figure 14.34. To ﬁnd the earliest completion time of the project, we merely need to ﬁnd the length of the longest path from the ﬁrst event to the last event. For general graphs, the longest-path problem generally does not make sense because of B 2 A 3 D 2 C 3 H 1 Start Finish E 1 G 2 F 3 K 4 figure 14.33 An activity-node  graph

14.5 path problems in acyclic graphs the possibility of a positive-cost cycle, which is equivalent to a negative-cost cycle in shortest-path problems. If any positive-cost cycles are present, we could ask for the longest simple path. However, no satisfactory solution is known for this problem. Fortunately, the event-node graph is acyclic; thus we need not worry about cycles. We can easily adapt the shortest-path algorithm to compute the earliest completion time for all nodes in the graph. If ECi is the earliest completion time for node i, the applicable rules are EC1 = 0 and ECw = Max(v, w) ∈E(ECv + cv, w) The latest time an  event can ﬁnish  without delaying  the project is also  easily computable. Figure 14.35 shows the earliest completion time for each event in our example event-node graph. We can also compute the latest time, LCi , that each event can ﬁnish without affecting ﬁnal completion time. The formulas to do this are LCN = ECN and LCv = Min(v, w) ∈E(LCw – cv, w) These values can be computed in linear time by maintaining for each vertex a list of all adjacent and preceding vertices. The earliest completion times are computed for vertices by their topological order, and the latest completion times are computed by reverse topological order. The latest completion times are shown in Figure 14.36. 6d 8d 7d 10d D 2 C 3 B 2 A 3 H 1 G 2 F 3 E 1 K 4 figure 14.34 An event-node graph 6d 8d 7d 10d D 2 C 3 B 2 A 3 H 1 G 2 F 3 E 1 K 4 figure 14.35 Earliest completion  times Edges show which  activity must be  completed to  advance from one  vertex to the next.  The earliest completion time is the  longest path.

chapter 14 graphs and paths Slack time is the  amount of time that  an activity can be  delayed without  delaying overall  completion. The slack time for each edge in the event-node graph is the amount of time that the completion of the corresponding activity can be delayed without delaying the overall completion, or Slack(v, w) = LCw – ECv – cv, w Figure 14.37 shows the slack (as the third entry) for each activity in the eventnode graph. For each node, the top number is the earliest completion time and the bottom number is the latest completion time. Zero-slack activities are critical and  cannot be delayed.  A path of zeroslack edges is a  critical path. Some activities have zero slack. These are critical activities that must be ﬁnished on schedule. A path consisting entirely of zero-slack edges is a critical path.

chapt er inner classes  and implementation  of ArrayList This chapter begins our discussion of the implementation of standard data structures. One of the simplest data structures is the ArrayList that is part of the Collections API. In Part One (speciﬁcally Figure 3.17 and Figure 4.24) we have already seen skeletons of the implementation, so in this chapter we concentrate on the details of implementing the complete class, with the associated iterators. In doing so, we make use of an interesting Java syntactic creation, the inner class. We discuss the inner class in this chapter, rather than in Part One (where other syntactic elements are introduced) because we view the inner class as a Java implementation technique, rather than a core language feature. In this chapter, we will see n The uses and syntax of the inner class n An implementation of a new class called the  AbstractCollection n An implementation of the ArrayList class

chapter 15 inner classes and implementation of ArrayList 15.1 iterators and nested classes We begin by reviewing the simple iterator implementation ﬁrst described in Section 6.2. Recall that we deﬁned a simple iterator interface, which mimics the standard (nongeneric) Collections API Iterator, and this interface is shown in Figure 15.1. We then deﬁned two classes: the container and its iterator. Each container class is responsible for providing an implementation of the iterator interface. In our case, the implementation of the iterator interface is provided by the MyContainerIterator class, shown in Figure 15.2. The MyContainer class shown in Figure 15.3 provides a factory method that creates an instance of MyContainerIterator and returns this instance using the interface type Iterator. Figure 15.4 provides a main that illustrates the use of the container/iterator combination. Figures 15.1 to 15.4 simply replicate Figures 6.5 to 6.8 in the original iterator discussion from Section 6.2. figure 15.1 The Iterator interface from  Section 6.2 package weiss.ds; public interface Iterator {     boolean hasNext( );     Object next( ); } figure 15.2 Implementation of the  MyContainerIterator from Section 6.2 // An iterator class that steps through a MyContainer. package weiss.ds; class MyContainerIterator implements Iterator {     private int current = 0;     private MyContainer container;     MyContainerIterator( MyContainer c )       { container = c; }     public boolean hasNext( )       { return current < container.size; }     public Object next( )       { return container.items[ current++ ]; } }

15.1 iterators and nested classes This design hides the iterator class implementation because MyContainerIterator is not a public class. Thus the user is forced to program to the Iterator interface and does not have access to the details of how the iterator was implemented— the user cannot even declare objects of type weiss.ds.MyContainerIterator. However, it still exposes more details than we usually like. In the MyContainer class, the data are not private, and the corresponding iterator class, while not public, is still package visible. We can solve both problems by using nested classes: We simply move the iterator class inside of the container class. At that point the iterator class is a member of the container class, and thus it can be declared as a private class and its methods can access private data from MyContainer. The revised code is illustrated in Figure 15.5, with only a stylistic change of renaming MyContainerIterator as LocalIterator. No other changes are required; however the LocalIterator constructor can be made private and still be callable from MyContainer, since LocalIterator is part of MyContainer. figure 15.3 The MyContainer class from Section 6.2 1 package weiss.ds; 3 public class MyContainer 4 { 5     Object [ ] items; 6     int size; 8     public Iterator iterator( ) 9       { return new MyContainerIterator( this ); } 11     // Other methods not shown. } figure 15.4 main method to  illustrate iterator  design from  Section 6.2 public static void main( String [ ] args ) { MyContainer v = new MyContainer( ); v.add( "3" ); v.add( "2" ); System.out.println( "Container contents: " ); Iterator itr = v.iterator( ); while( itr.hasNext( ) ) System.out.println( itr.next( ) ); }

chapter 15 inner classes and implementation of ArrayList 15.2 iterators and inner classes In Section 15.1, we used a nested class to further hide details. In addition to nested classes, Java provides inner classes. An inner class is similar to a nested class in that it is a class inside another class and is treated as a member of the outer class for visibility purposes. An inner class is declared using the same syntax as a nested class, except that it is not a static class. In other words, the static qualiﬁer is missing in the inner class declaration. Before getting into the inner class speciﬁcs, let us look at the problem that they are designed to solve. Figure 15.6 illustrates the relationship between the iterator and container classes that were written in the previous section. Each instance of the LocalIterator maintains a reference to the container over which it is iterating and a notion of the iterator’s current position. The relationship that we have is that each LocalIterator must be associated with exactly one instance of MyContainer. It is impossible for the container reference in any iterator to be null, and the iterator’s existence makes no sense without knowing which MyContainer object caused its creation. figure 15.5 Iterator design using  nested class package weiss.ds; public class MyContainer {     private Object [ ] items;     private int size = 0;     // Other methods for MyContainer not shown     public Iterator iterator( )       { return new LocalIterator( this ); }     // The iterator class as a nested class     private static class LocalIterator implements Iterator     {         private int current = 0;         private MyContainer container;         private LocalIterator( MyContainer c )           { container = c; }         public boolean hasNext( )           { return current < container.size; }         public Object next( )           { return container.items[ current++ ]; }     } } An inner class is  similar to a nested  class in that it is  a class inside  another class and is  declared using the  same syntax as a  nested class,  except that it is not  a static class. An  inner class always  contains an implicit  reference to the  outer object that  created it.

15.2 iterators and inner classes Since we know that itr1 must be tied to one and only one iterator, it seems that the expression container.items is redundant: If the iterator could only remember the container that constructed it, we wouldn’t have to keep track of it ourselves. And if it remembered it, we might expect that if inside of the LocalIterator we referred to items, then since the LocalIterator does not have an items ﬁeld, the compiler (and run-time system) would be smart enough to deduce that we are talking about the items ﬁeld of the MyContainer object that caused the construction of this particular LocalIterator. This is exactly what an inner class does, and what distinguishes it from a nested class. The big difference between an inner class and a nested class is that when an instance of an inner class object is constructed, there is an implicit reference to the outer class object that caused its construction. This implies that an inner class object cannot exist without an outer class object for it to be attached to, with an exception being if it is declared in a static method (because local and anonymous classes are technically inner classes), a detail we will discuss later. If the name of the outer class is Outer, then the implicit reference is Outer.this. Thus, if LocalIterator was declared as an instance inner class (i.e., the static keyword was removed), then the MyContainer.this reference could be used to replace the container reference that the iterator is storing. The picture in Figure 15.7 illustrates that the structure would be identical. A revised class is shown in Figure 15.8. In the revised implementation, observe that LocalIterator no longer has an explicit reference to a MyContainer, and also observe that its constructor is no longer necessary, since it only initialized the MyContainer reference. Finally, Figure 15.9 illustrates that just as using this is optional in an instance method, the Outer.this reference is also optional if there is no name clash. Thus, MyContainer.this.size can be shortened to size, as long as there is no other variable named size that is in a closer scope. v itr1 itr2 container  current=3 container  current=0 items: 3,5,2 figure 15.6 Iterator/container relationship The big difference  between an inner  class and a nested  class is that when  an instance of an  inner class object  is constructed,  there is an implicit  reference to the  outer class object  that caused its  construction. If the name of the  outer class is Outer, then the implicit  reference is  Outer.this.

chapter 15 inner classes and implementation of ArrayList Local classes and anonymous classes do not specify whether they are static, and they are always technically considered inner classes. However, if such a class is declared in a static method, it has no implicit outer reference (and thus behaves like a nested class), whereas if it is declared inside an instance method, its implicit outer reference is the invoker of the method. The addition of inner classes requires a signiﬁcant set of rules, many of which attempt to deal with language corner cases and dubious coding practices. v itr1 itr2 current=3 current=0 items: 3,5,2 MyContainer.this MyContainer.this figure 15.7 Iterator/container with  inner classes figure 15.8 Iterator design using  inner class package weiss.ds; public class MyContainer {     private Object [ ] items;     private int size = 0;     // Other methods for MyContainer not shown     public Iterator iterator( )       { return new LocalIterator( ); }     // The iterator class as an inner class     private class LocalIterator implements Iterator     {         private int current = 0;         public boolean hasNext( )           { return current < MyContainer.this.size; }         public Object next( )           { return MyContainer.this.items[ current++ ]; }     } }

15.2 iterators and inner classes For instance, suppose we suspend belief for a minute and imagine that LocalIterator is public. We do so only to illustrate the complications that the language designers face when adding a new language feature. Under this assumption the iterator’s type is MyContainer.LocalIterator, and since it is visible, one might expect that MyContainer.LocalIterator itr = new MyContainer.LocalIterator( ); is legal, since like all classes, it has a public default zero-parameter constructor. However, this cannot possibly work, since there is no way to initialize the implicit reference. Which MyContainer is itr referring to? We need some syntax that won’t conﬂict with any other language rules. Here’s the rule: If there is a container c, then itr could be constructed using a bizarre syntax invented for just this case, in which the outer object in effect invokes new: MyContainer.LocalIterator itr = c.new LocalIterator( ); Notice that this implies that in an instance factory method, this.new is legal, and shorthands to the more conventional new seen in a factory method. If you ﬁnd yourself using the bizarre syntax, you probably have a bad design. In our example, once LocalIterator is private, this entire issue goes away, and if LocalIterator is not private, there is little reason to use an inner class in the ﬁrst place. There are also other rules, some of which are arbitrary. Private members of the inner or nested class are public to the outer class. To access any member of an inner class, the outer class only needs to provide a reference to an inner class instance and use the dot operator, as is normal for other classes. Thus inner and nested classes are considered part of the outer class. Both inner and nested classes can be ﬁnal, or they can be abstract, or they can be interfaces (but interfaces are always static, because they cannot have any data, including an implicit reference), or they can be none of these. Inner figure 15.9 Inner class;  Outer.this may be  optional. // The iterator class as an inner class     private class LocalIterator implements Iterator     {         private int current = 0;         public boolean hasNext( )           { return current < size; }         public Object next( )           { return items[ current++ ]; }     }

chapter 15 inner classes and implementation of ArrayList classes may not have static ﬁelds or methods, except for static ﬁnal ﬁelds. Inner classes may have nested classes or interfaces. Finally, when you compile the above example, you will see that the compiler generates a class ﬁle named MyContainer$LocalIterator.class, which would have to be included in any distribution to clients. In other words, each inner and nested class is a class and has a corresponding class ﬁle. Anonymous classes use numbers instead of names. 15.3 the AbstractCollection class The AbstractCollection implements some of the  methods in the  Collection interface. Before we implement the ArrayList class, observe that some of the methods in the Collection interface can be easily implemented in terms of others. For instance, isEmpty is easily implemented by checking if the size is 0. Rather than doing so in ArrayList, LinkedList, and all the other concrete implementations, it would be preferable to do this once and use inheritance to obtain isEmpty. We could even override isEmpty if it turns out that for some collections there is a faster way of performing isEmpty than computing the current size. However, we cannot implement isEmpty in the Collection interface; this can only be done in an abstract class. This will be the AbstractCollection class. To simplify implementations, programmers designing new Collections classes can extend the AbstractCollection class rather than implementing the Collection interface. A sample implementation of AbstractCollection is shown in Figures 15.10 to 15.12. The Collections API also deﬁnes additional classes such as AbstractList, AbstractSequentialList, and AbstractSet. We have chosen not to implement those, in keeping with our intention of providing a simpliﬁed subset of the Collections API. If, for some reason, you are implementing your own collections and extending the Java Collections API, you should extend the most speciﬁc abstract class. In Figure 15.10, we see implementations of isEmpty, clear, and add. The ﬁrst two methods have straightforward implementations. Certainly the implementation of clear is usable, since it removes all items in the collection, but there might be more efﬁcient ways of performing the clear, depending on the type of collection being manipulated. Thus this implementation of clear serves as a default, but it is likely to be overridden. There is no sensible way of providing a usable implementation for add. So the two alternatives are to make add abstract (which is clearly doable, since AbstractCollection is abstract) or to provide an implementation that throws a runtime exception. We have done the latter, which matches the behavior in java.util. (Further down the road, this decision also makes it easier to create the class needed to express the values of a map). Figure 15.11 provides default implementations

15.3 the AbstractCollection class of contains and remove. Both implementations use a sequential search, so they are not efﬁcient, and need to be overridden by respectable implementations of the Set interface. figure 15.10 Sample implementation of AbstractCollection (part 1) package weiss.util; /**  * AbstractCollection provides default implementations for  * some of the easy methods in the Collection interface.  */ public abstract class AbstractCollection<AnyType> implements Collection<AnyType> {     /**      * Tests if this collection is empty.      * @return true if the size of this collection is zero.      */     public boolean isEmpty( )     {         return size( ) == 0;     }     /**      * Change the size of this collection to zero.      */     public void clear( )     {         Iterator<AnyType> itr = iterator( );         while( itr.hasNext( ) )         {             itr.next( );             itr.remove( );         }     }     /**  * Adds x to this collections.  * This default implementation always throws an exception.  * @param x the item to add.  * @throws UnsupportedOperationException always.  */     public boolean add( AnyType x )     {         throw new UnsupportedOperationException( );     }

chapter 15 inner classes and implementation of ArrayList Figure 15.12 contains the implementations of the two toArray methods. The zero-parameter toArray is fairly simple to implement. The one-parameter toArray makes use of a feature of Java known as reﬂection to create an array object that matches the parameter type in the case that the parameter is not large enough to store the underlying collection. figure 15.11 Sample implementation of  AbstractCollection (part 2) /**  * Returns true if this collection contains x.  * If x is null, returns false.  * (This behavior may not always be appropriate.)  * @param x the item to search for.  * @return true if x is not null and is found in  * this collection.  */     public boolean contains( Object x )     {         if( x == null )             return false; for( AnyType val : this )             if( x.equals( val ) )                 return true;         return false;     }     /**      * Removes non-null x from this collection.      * (This behavior may not always be appropriate.)      * @param x the item to remove.      * @return true if remove succeeds.      */     public boolean remove( Object x )     {         if( x == null )             return false;         Iterator<AnyType> itr = iterator( );         while( itr.hasNext( ) )             if( x.equals( itr.next( ) ) )             {                 itr.remove( );                 return true;             }         return false;     }

15.3 the AbstractCollection class figure 15.12 Sample implementation of AbstractCollection (part 3) /**  * Obtains a primitive array view of the collection.  * @return the primitive array view.  */     public Object [ ] toArray( )     {         Object [ ] copy = new Object[ size( ) ];         int i = 0;         for( AnyType val : this )             copy[ i++ ] = val;         return copy;     } public <OtherType> OtherType [ ] toArray( OtherType [ ] arr ) {         int theSize = size( ); 101         if( arr.length < theSize ) 102             arr = ( OtherType [ ] ) java.lang.reflect.Array.newInstance( 103                             arr.getClass( ).getComponentType( ), theSize ); else if( theSize < arr.length )     arr[ theSize ] = null; Object [ ] copy = arr; int i = 0; 110         for( AnyType val : this ) 111             copy[ i++ ] = val; 113         return copy; 114     } 116     /** 117      * Return a string representation of this collection. 118      */ 119     public String toString( ) 120     { 121         StringBuilder result = new StringBuilder( "[ " ); 123         for( AnyType obj : this ) 124             result.append( obj + " " ); 126         result.append( "]" ); 128         return result.toString( ); 129     }  130 }

chapter 15 inner classes and implementation of ArrayList 15.4 StringBuilder Figure 15.12 also shows a respectable linear-time implementation of toString, using a StringBuilder to avoid quadratic running time. (StringBuilder was added in Java 5 and is slightly faster than StringBuffer; it is preferable for singlethreaded applications). To see why StringBuilder is needed, consider the following code fragment that builds a String with N A’s: String result = ""; for( int i = 0; i < N; i++ )     result += 'A'; While there is no doubt that this fragment works correctly because String objects are immutable, each call to result += 'A' is rewritten as result = result + 'A', and once we see that, it is apparent that each String concatenation creates a new String object. As we get further into the loop, these String objects become more expensive to create. We can estimate the cost of the ith String concatenation to be i, so the total cost is 1 + 2 + 3 + ... + N, or O(N2). If N is 100,000, it is simple to write the code and see that the running time is signiﬁcant. Yet a simple rewrite char [ ] theChars = new char[ N ]; for( int i = 0; i < N; i++ )     theChars[ i ] = 'A'; String result = new String( theChars ); results in a linear-time algorithm that executes in the blink of the eye. The use of an array of characters works only if we know the ﬁnal size of the String. Otherwise, we have to use something like ArrayList<char>. A StringBuilder is similar in concept to an ArrayList<char>, with array doubling but with method names that are speciﬁc for String operations. Using a StringBuilder, the code looks like StringBuilder sb = new StringBuilder( ); for( int i = 0; i < N; i++ )     sb.append( 'A' ); String result = new String( sb ); This code is linear-time and runs quickly. Some String concatenations, such as those in a single expression, are optimized by the compiler to avoid repeated creations of Strings. But if your concatenations are intermingled with other statements, as is the case here, then you often can use a StringBuilder for more efﬁcient code.

15.5 implementation of ArrayList with an iterator 15.5 implementation of  ArrayList with an iterator The various ArrayList classes shown in Part One were not iterator-aware. This section provides an implementation of ArrayList that we will place in weiss.util and includes support for bidirectional iterators. In order to keep the amount of code somewhat manageable, we have stripped out the bulk of the javadoc comments. They can be found in the online code. The implementation is found in Figures 15.13 to 15.16. At line 3 we see that ArrayList extends the AbstractCollection abstract class, and at line 4 ArrayList declares that it implements the List interface. The internal array, theItems, and collection size, theSize, are declared at lines 9 and 10, respectively. More interesting is modCount, which is declared at line 11. modCount represents the number of structural modiﬁcations (adds, removes) made to the ArrayList. The idea is that when an iterator is constructed, the iterator saves this value in its data member expectedModCount. When any iterator operation is performed, the iterator’s expectedModCount member is compared with the ArrayList’s modCount, and if they disagree, a ConcurrentModificationException can be thrown. Line 16 illustrates the typical constructor that performs a shallow copy of the members in another collection, simply by stepping through the collection and calling add. The clear method, started at line 26, initializes the ArrayList and can be called from the constructor. It also resets theItems, which allows the garbage collector to reclaim all the otherwise unreferenced objects that were in the ArrayList. The remaining routines in Figure 15.13 are relatively straightforward. Figure 15.14 implements the remaining methods that do not depend on iterators. findPos is a private helper that returns the position of an object that is either being removed or subjected to a contains call. Extra code is present because it is legal to add null to the ArrayList, and if we were not careful, the call to equals at line 60 could have generated a NullPointerException. Observe that both add and remove will result in a change to modCount. In Figure 15.15 we see the two factory methods that return iterators, and we see the beginning of the implementation of the ListIterator interface. Observe that ArrayListIterator IS-A ListIterator and ListIterator IS-A Iterator. So ArrayListIterator can be returned at lines 103 and 106. In the implementation of ArrayListIterator, done as a private inner class, we maintain the current position at line 111. The current position represents the index of the element that would be returned by calling next. At line 112 we declare the expectedModCount member. Like all class members, it is initialized

chapter 15 inner classes and implementation of ArrayList figure 15.13 ArrayList implementation (part 1) package weiss.util; public class ArrayList<AnyType> extends AbstractCollection<AnyType>                                 implements List<AnyType> {     private static final int DEFAULT_CAPACITY = 10;     private static final int NOT_FOUND = -1;      private AnyType [ ] theItems;     private int theSize;     private int modCount = 0;     public ArrayList( )       { clear( ); }     public ArrayList( Collection<? extends AnyType> other )     {         clear( );         for( AnyType obj : other )             add( obj );     }     public int size( )       { return theSize; }     public void clear( )     {         theSize = 0;         theItems = (AnyType []) new Object[ DEFAULT_CAPACITY ];         modCount++;     }     public AnyType get( int idx )     {         if( idx < 0 || idx >= size( ) )             throw new ArrayIndexOutOfBoundsException( );         return theItems[ idx ];     }     public AnyType set( int idx, AnyType newVal )     {         if( idx < 0 || idx >= size( ) )             throw new ArrayIndexOutOfBoundsException( );         AnyType old = theItems[ idx ];         theItems[ idx ] = newVal;         return old;     }     public boolean contains( Object x )       { return findPos( x ) != NOT_FOUND; }

15.5 implementation of ArrayList with an iterator figure 15.14 ArrayList implementation (part 2) private int findPos( Object x )     {         for( int i = 0; i < size( ); i++ )             if( x == null )             {                 if( theItems[ i ] == null )                     return i;             }             else if( x.equals( theItems[ i ] ) )                 return i;         return NOT_FOUND;     }     public boolean add( AnyType x )     {         if( theItems.length == size( ) )         {             AnyType [ ] old = theItems;             theItems = (AnyType []) new Object[ theItems.length * 2 + 1 ];             for( int i = 0; i < size( ); i++ )                 theItems[ i ] = old[ i ];         }         theItems[ theSize++ ] = x;         modCount++;         return true;     }     public boolean remove( Object x )     {         int pos = findPos( x );         if( pos == NOT_FOUND )             return false;         else         {             remove( pos );             return true;         }     }     public AnyType remove( int idx )     {         AnyType removedItem = theItems[ idx ];         for( int i = idx; i < size( ) - 1; i++ )             theItems[ i ] = theItems[ i + 1 ];         theSize--;         modCount++; 100         return removedItem; 101     }

chapter 15 inner classes and implementation of ArrayList when an instance of the iterator is created (immediately prior to calling the constructor); modCount is a shorthand for ArrayList.this.modCount. The two Boolean instance members that follow are ﬂags used to verify that a call to remove is legal. The ArrayListIterator constructor is declared package visible; thus it is usable by the ArrayList. Of course it could be declared public, but there is no reason to do so and even if it were private, it would still be usable by ArrayList. Package visible, however, seems most natural in this situation. Both hasNext and hasPrevious verify that there have been no external structural modiﬁcations since the iterator was created, throwing an exception if the ArrayList modCount does not match the ArrayListIterator expectedModCount. figure 15.15 ArrayList implementation (part 3) public Iterator<AnyType> iterator( ) 103       { return new ArrayListIterator( 0 ); } public ListIterator<AnyType> listIterator( int idx ) 106       { return new ArrayListIterator( idx ); } // This is the implementation of the ArrayListIterator private class ArrayListIterator implements ListIterator<AnyType> { 111         private int current; 112         private int expectedModCount = modCount; 113         private boolean nextCompleted = false; 114         private boolean prevCompleted = false; 116         ArrayListIterator( int pos ) 117         { 118             if( pos < 0 || pos > size( ) ) 119                 throw new IndexOutOfBoundsException( ); 120             current = pos; 121         } 123         public boolean hasNext( ) 124         { 125             if( expectedModCount != modCount ) 126                 throw new ConcurrentModificationException( ); 127             return current < size( ); 128         } 130         public boolean hasPrevious( ) 131         { 132             if( expectedModCount != modCount ) 133                 throw new ConcurrentModificationException( ); 134             return current > 0; 135         }

15.5 implementation of ArrayList with an iterator The ArrayListIterator class is completed in Figure 15.16. next and previous are mirror image symmetries. Examining next, we see ﬁrst a test at line 138 to make sure we have not exhausted the iteration (implicitly this tests for structural modiﬁcations also). We then set nextCompleted to true to allow remove to succeed, and then we return the array item that current is examining, advancing current after its value has been used. The previous method is similar, except that we must lower current’s value ﬁrst. This is because when traversing in reverse, if current equals the container size, we have not yet started the iteration, and when current equals zero, we have completed the iteration (but can remove the item in this position figure 15.16 ArrayList implementation (part 4) public AnyType next( ) { 138             if( !hasNext( ) )  139                 throw new NoSuchElementException( ); 140             nextCompleted = true;  141             prevCompleted = false; 142             return theItems[ current++ ]; 143         } 145         public AnyType previous( ) 146         { 147             if( !hasPrevious( ) )  148                 throw new NoSuchElementException( ); 149             prevCompleted = true; 150             nextCompleted = false;  151             return theItems[ --current ]; 152         } 154         public void remove( ) 155         { 156             if( expectedModCount != modCount ) 157                 throw new ConcurrentModificationException( ); 159             if( nextCompleted ) 160                 ArrayList.this.remove( --current ); 161             else if( prevCompleted ) 162                 ArrayList.this.remove( current ); 163             else 164                 throw new IllegalStateException( ); 166             prevCompleted = nextCompleted = false; 167             expectedModCount++; 168         } 169     } 170 }

chapter 15 inner classes and implementation of ArrayList if the prior operation was previous). Observe that next followed by previous yields identical items. Finally, we come to remove, which is extremely tricky because the semantics of remove depend on which direction the traversal is proceeding. In fact, this probably suggests a bad design in the Collections API: Method semantics should not depend so strongly on which methods have been called prior to it. But remove is what it is, so we have to implement it. The implementation of remove begins with the test for structural modiﬁcation at line 156. If the prior iterator state change operation was a next, as evidenced by the test at line 159 showing that nextCompleted is true, then we call the ArrayList remove method (started at line 93 in Figure 15.14) that takes an index as a parameter. The use of ArrayList.this.remove is required because the local version of remove hides the outer class version. Because we have already advanced past the item to be removed, we must remove the item in position current-1. This slides the next item from current to current-1 (since the old current-1 position has now been removed), so we use the expression --current in line 160. When traversing the other direction, we are sitting on the last item that was returned, so we simply pass current as a parameter to the outer remove. After it returns, the elements in higher indices are slid one index lower, so current is sitting on the correct element and can be used in the expression at line 162. In either case, we cannot do another remove until we do a next or previous, so at line 166 we clear both ﬂags. Finally, at line 167, we increase the value of expectedModCount to match the container’s. Observe that this is increased only for this iterator, so any other iterators are now invalidated. This class, which is perhaps the simplest of the Collections API classes that contains iterators, illustrates why in Part Four we elect to begin with a simple protocol and then provide more complete implementations at the end of the chapter.

chapt er stacks and queues In this chapter we discuss implementation of the stack and queue data structures. Recall from Chapter 6 that the basic operations are expected to take constant time. For both the stack and queue, there are two basic ways to arrange for constant-time operations. The ﬁrst is to store the items contiguously in an array, and the second is to store items noncontiguously in a linked list. We present implementations for both data structures, using both methods, in this chapter. In this chapter, we show n An array-based implementation of the stack and queue n A linked list–based implementation of the stack and queue n A brief comparison of the two methods n An illustration of Collections API stack implementation 16.1 dynamic array implementations In this section we use a simple array to implement the stack and queue. The resulting algorithms are extremely efﬁcient and also are simple to code. Recall that we have been using ArrayList instead of arrays. The add method of ArrayList

chapter 16 stacks and queues Recall that array  doubling does not  affect performance  in the long run. is, in effect, the same as push. However, because we are interested in a general discussion of the algorithms, we implement the array-based stack using basic arrays, duplicating some of the code seen earlier in the ArrayList implementations. 16.1.1   stacks A stack can be  implemented with  an array and an  integer that indicates the index of  the top element. As Figure 16.1 shows, a stack can be implemented with an array and an integer. The integer tos (top of stack) provides the array index of the top element of the stack. Thus when tos is –1, the stack is empty. To push, we increment tos and place the new element in the array position tos.  Accessing the top element is thus trivial, and we can perform the pop by decrementing tos. In Figure 16.1, we begin with an empty stack. Then we show the stack after three operations: push(a), push(b), and pop. Figure 16.2 shows the skeleton for the array-based Stack class. It speciﬁes two data members: theArray, which is expanded as needed, stores the items in the stack; and topOfStack gives the index of the current top of the stack. For an empty stack, this index is –1. The constructor is shown in Figure 16.3. Most of the stack  routines are applications of previously discussed  ideas. The public methods are listed in lines 22–33 of the skeleton. Most of these routines have simple implementations. The isEmpty and makeEmpty routines are one-liners, as shown in Figure 16.4. The push method is shown in Figure 16.5. If it were not for the array doubling, the push routine would be only the single line of code shown at line 9. Recall that the use of the preﬁx ++ operator means that topOfStack is incremented and that its new value is used to index theArray. The remaining routines are equally short, as shown in Figures 16.6 and 16.7. The postﬁx -- operator used in Figure 16.7 indicates that, although topOfStack is decremented, its prior value is used to index theArray. If there is no array doubling, every operation takes constant time. A push that involves array doubling will take O(N) time. If this were a frequent occurrence, we would need to worry. However, it is infrequent because an (a) tos (–1) (b) tos (0) (c) tos (1) (d) tos (0) a b a a figure 16.1 How the stack  routines work:  (a) empty stack;  (b) push(a); (c) push(b); (d) pop( )

16.1 dynamic array implementations array doubling that involves N elements must be preceded by at least N/2 pushes that do not involve an array doubling. Consequently, we can charge the O(N) cost of the doubling over these N/2 easy pushes, thereby effectively raising the cost of each push by only a small constant. This technique is known as amortization. figure 16.2 Skeleton for the  array-based stack  class package weiss.nonstandard; // ArrayStack class // // CONSTRUCTION: with no initializer // // ******************PUBLIC OPERATIONS********************* // void push( x )         --> Insert x // void pop( )            --> Remove most recently inserted item // AnyType top( )         --> Return most recently inserted item // AnyType topAndPop( )   --> Return and remove most recent item // boolean isEmpty( )     --> Return true if empty; else false // void makeEmpty( )      --> Remove all items // ******************ERRORS******************************** // top, pop, or topAndPop on empty stack public class ArrayStack<AnyType> implements Stack<AnyType> {     public ArrayStack( )       { /* Figure 16.3 */ }     public boolean isEmpty( )       { /* Figure 16.4 */ }     public void makeEmpty( )       { /* Figure 16.4 */ }     public AnyType top( )       { /* Figure 16.6 */ }     public void pop( )       { /* Figure 16.6 */ }     public AnyType topAndPop( )       { /* Figure 16.7 */ }     public void push( AnyType x )       { /* Figure 16.5 */ }     private void doubleArray( )       { /* Implementation in online code */ }     private AnyType [ ] theArray;     private int         topOfStack;     private static final int DEFAULT_CAPACITY = 10; }

chapter 16 stacks and queues A real-life example of amortization is payment of income taxes. Rather than pay your entire bill on April 15, the government requires that you pay most of your taxes through withholding. The total tax bill is always the same; figure 16.3 The zero-parameter  constructor for the  ArrayStack class /**  * Construct the stack.  */     public ArrayStack( )     {         theArray = (AnyType []) new Object[ DEFAULT_CAPACITY ];         topOfStack = -1;     } figure 16.4 The isEmpty and  makeEmpty routines for  the ArrayStack class /**  * Test if the stack is logically empty.  * @return true if empty, false otherwise.  */     public boolean isEmpty( )     {         return topOfStack == -1;     } /**  * Make the stack logically empty.  */     public void makeEmpty( )     {         topOfStack = -1;     } figure 16.5 The push method for  the ArrayStack class /**  * Insert a new item into the stack.  * @param x the item to insert.  */     public void push( AnyType x )     {         if( topOfStack + 1 == theArray.length )             doubleArray( );         theArray[ ++topOfStack ] = x;     }

16.1 dynamic array implementations it is when the tax is paid that varies. The same is true for the time spent in the push operations. We can charge for the array doubling at the time it occurs, or we can bill each push operation equally. An amortized bound requires that we bill each operation in a sequence for its fair share of the total cost. In our example, the cost of array doubling therefore is not excessive. figure 16.6 The top and pop methods for the  ArrayStack class /**  * Get the most recently inserted item in the stack.  * Does not alter the stack.  * @return the most recently inserted item in the stack.  * @throws UnderflowException if the stack is empty.  */     public AnyType top( )     {         if( isEmpty( ) )             throw new UnderflowException( "ArrayStack top" );         return theArray[ topOfStack ];     }     /**      * Remove the most recently inserted item from the stack.      * @throws UnderflowException if the stack is empty.      */     public void pop( )     {         if( isEmpty( ) )             throw new UnderflowException( "ArrayStack pop" );         topOfStack--;     } figure 16.7 The topAndPop method for the ArrayStack class /**  * Return and remove the most recently inserted item  * from the stack.  * @return the most recently inserted item in the stack.  * @throws Underflow if the stack is empty.  */     public AnyType topAndPop( )     {         if( isEmpty( ) )             throw new UnderflowException( "ArrayStack topAndPop" );         return theArray[ topOfStack-- ];     }

chapter 16 stacks and queues 16.1.2   queues Storing the queue  items beginning at  the start of any  array makes  dequeueing expensive. The easiest way to implement the queue is to store the items in an array with the front item in the front position (i.e., array index 0). If back represents the position of the last item in the queue, then to enqueue we merely increment back and place the item there. The problem is that the dequeue operation is very expensive. The reason is that, by requiring that the items be placed at the start of the array, we force the dequeue to shift all the items one position after we remove the front item. A dequeue is  implemented by  incrementing the  front position. Figure 16.8 shows that we can overcome this problem when performing a dequeue by incrementing front rather than shifting all the elements. When the queue has one element, both front and back represent the array index of that element. Thus, for an empty queue, back must be initialized to front-1. This implementation ensures that both enqueue and dequeue can be performed in constant time. The fundamental problem with this approach is shown in the ﬁrst line of Figure 16.9. After three more enqueue operations, we cannot add any more items, even though the queue is not really full. Array a a b b front back makeEmpty(  ) size = 0  front back enqueue(a) size = 1 front back enqueue(b) size = 2  front back dequeue(  ) size = 1 front back dequeue(  ) size = 0 figure 16.8 Basic array  implementation of  the queue

16.1 dynamic array implementations doubling does not solve the problem because, even if the size of the array is 1,000, after 1,000 enqueue operations there is no room in the queue, regardless of its actual size. Even if 1,000 dequeue operations have been performed, thus abstractly making the queue empty, we cannot add to it. Wraparound returns front or  back to the beginning of the array  when either  reaches the end.  Using wraparound  to implement the  queue is called a  circular array  implementation. As Figure 16.9 shows, however, there is plenty of extra space: All the positions before front are unused and can thus be recycled. Hence we use wraparound; that is, when either back or front reaches the end of the array, we reset it to the beginning. This operation implementing a queue is called a circular array implementation. We need to double the array only when the number of elements in the queue equals the number of array positions. To enqueue(f), we therefore reset back to the start of the array and place f there. After three dequeue operations, front is also reset to the start of the array. The skeleton for the ArrayQueue class is shown in Figure 16.10. The ArrayQueue class has four data members: a dynamically expanding array, the number of items currently in the queue, the array index of the front item, and the array index of the back item. f f front back After 3 enqueues size = 3 front  back enqueue(f) size = 4 front back dequeue( ) size = 3 front back dequeue( ) size = 2 front back dequeue( ) size = 1 f f c d e c d e d e e figure 16.9 Array implementation  of the queue with  wraparound

chapter 16 stacks and queues If the queue is full,  we must implement array doubling  carefully. We declare two methods in the private section. These methods are used internally by the ArrayQueue methods but are not made available to the user of the class. One of these methods is the increment routine, which adds 1 to its parameter and returns the new value. Because this method implements wraparound, if the result would equal the array size it is wrapped around to zero. This routine is shown in Figure 16.11. The other routine is doubleQueue, which figure 16.10 Skeleton for the  array-based queue  class package weiss.nonstandard; // ArrayQueue class // // CONSTRUCTION: with no initializer // // ******************PUBLIC OPERATIONS********************* // void enqueue( x )      --> Insert x // AnyType getFront( )    --> Return least recently inserted item // AnyType dequeue( )     --> Return and remove least recent item // boolean isEmpty( )     --> Return true if empty; else false // void makeEmpty( )      --> Remove all items // ******************ERRORS******************************** // getFront or dequeue on empty queue public class ArrayQueue<AnyType> {     public ArrayQueue( )       { /* Figure 16.12 */ }     public boolean isEmpty( )       { /* Figure 16.13 */ }     public void makeEmpty( )       { /* Figure 16.17 */ }     public AnyType dequeue( )       { /* Figure 16.16 */ }     public AnyType getFront( )       { /* Figure 16.16 */ }     public void enqueue( AnyType x )       { /* Figure 16.14 */ }     private int increment( int x )       { /* Figure 16.11 */ }     private void doubleQueue( )       { /* Figure 16.15 */ }     private AnyType [ ] theArray;     private int         currentSize;     private int         front;     private int         back;     private static final int DEFAULT_CAPACITY = 10; }

16.1 dynamic array implementations is called if an enqueue requires a doubling of the array. It is slightly more complex than the usual expansion because the queue items are not necessarily stored in an array starting at location 0. Thus items must be copied carefully. We discuss doubleQueue along with enqueue. Many of the public methods resemble their stack counterparts, including the constructor shown in Figure 16.12 and isEmpty, shown in Figure 16.13. This constructor is not particularly special, except that we must be sure that we have the correct initial values for both front and back. This is done by calling makeEmpty. When we double  the queue array, we  cannot simply copy  the entire array  directly. The enqueue routine is shown in Figure 16.14. The basic strategy is simple enough, as illustrated by lines 9–11 in the enqueue routine. The doubleQueue routine, shown in Figure 16.15, begins by resizing the array. We must move items starting at position front, rather than 0. figure 16.11 The wraparound routine /**  * Internal method to increment with wraparound.  * @param x any index in theArray's range.  * @return x+1, or 0 if x is at the end of theArray.  */     private int increment( int x )     {         if( ++x == theArray.length )             x = 0;         return x;     } figure 16.12 The constructor for  the ArrayQueue class /**  * Construct the queue.  */     public ArrayQueue( )     {         theArray = (AnyType []) new Object[ DEFAULT_CAPACITY ];         makeEmpty( );     } figure 16.13 The isEmpty routine  for the ArrayQueue class /**  * Test if the queue is logically empty.  * @return true if empty, false otherwise.  */     public boolean isEmpty( )     {         return currentSize == 0;     }

chapter 16 stacks and queues Thus doubleQueue steps through the old array and copies each item to the new part of the array at lines 11–12. Then we reset back at line 16. The dequeue and getFront routines are shown in Figure 16.16; both are short. Finally, the makeEmpty routine is shown in Figure 16.17. The queue routines clearly are constant-time operations, so the cost of array doubling can be amortized over the sequence of enqueue operations, as for the stack. The circular array implementation of the queue can easily be done incorrectly when attempts to shorten the code are made. For instance, if you attempt to avoid using the size member by using front and back to infer the size, the array must be resized when the number of items in the queue is 1 less than the array’s size. figure 16.14 The enqueue routine  for the ArrayQueue class /**  * Insert a new item into the queue.  * @param x the item to insert.  */     public void enqueue( AnyType x )     {         if( currentSize == theArray.length )             doubleQueue( );         back = increment( back );         theArray[ back ] = x;         currentSize++;     } figure 16.15 Dynamic expansion for the ArrayQueue class /**  * Internal method to expand theArray.  */     private void doubleQueue( )     {         AnyType [ ] newArray;         newArray = (AnyType []) new Object[ theArray.length * 2 ];             // Copy elements that are logically in the queue         for( int i = 0; i < currentSize; i++, front = increment( front ) )             newArray[ i ] = theArray[ front ];         theArray = newArray;         front = 0;         back = currentSize - 1;     }

16.2 linked list implementations 16.2 linked list implementations An alternative to the contiguous array implementation is a linked list. Recall from Section 6.5 that in a linked list, we store each item in a separate object that also contains a reference to the next object in the list. figure 16.16 The dequeue and  getFront routines for  the ArrayQueue class /**  * Return and remove the least recently inserted item  * from the queue.  * @return the least recently inserted item in the queue.  * @throws UnderflowException if the queue is empty.  */     public AnyType dequeue( )     {         if( isEmpty( ) )             throw new UnderflowException( "ArrayQueue dequeue" );         currentSize--;         AnyType returnValue = theArray[ front ];         front = increment( front );         return returnValue;     }     /**      * Get the least recently inserted item in the queue.      * Does not alter the queue.      * @return the least recently inserted item in the queue.      * @throws UnderflowException if the queue is empty.      */     public AnyType getFront( )     {         if( isEmpty( ) )             throw new UnderflowException( "ArrayQueue getFront" );         return theArray[ front ];     } figure 16.17 The makeEmpty routine for the ArrayQueue class /**  * Make the queue logically empty.  */     public void makeEmpty( )     {         currentSize = 0;         front = 0;         back = -1;     }

chapter 16 stacks and queues The advantage of a  linked list implementation is that  the excess memory  is only one reference per item. The  disadvantage is that  the memory management could be  time consuming. The advantage of the linked list is that the excess memory is only one reference per item. In contrast, a contiguous array implementation uses excess space equal to the number of vacant array items (plus some additional memory during the doubling phase). The linked list advantage can be signiﬁcant in other languages if the vacant array items store uninitialized instances of objects that consume signiﬁcant space. In Java this advantage is minimal. Even so, we discuss the linked list implementations for three reasons. 1. An understanding of implementations that might be useful in other languages is important. 2. Implementations that use linked lists can be shorter for the queue than the comparable array versions. 3. These implementations illustrate the principles behind the more general linked list operations given in Chapter 17. For the implementation to be competitive with contiguous array implementations, we must be able to perform the basic linked list operations in constant time. Doing so is easy because the changes in the linked list are restricted to the elements at the two ends (front and back) of the list. 16.2.1   stacks In implementing the  stack class, the top  of the stack is represented by the  ﬁrst item in a linked  list. The stack class can be implemented as a linked list in which the top of the stack is represented by the ﬁrst item in the list, as shown in Figure 16.18. To implement a push, we create a new node in the list and attach it as the new ﬁrst element. To implement a pop, we merely advance the top of the stack to the second item in the list (if there is one). An empty stack is represented by an empty linked list. Clearly, each operation is performed in constant time because, by restricting operations to the ﬁrst node, we have made all calculations independent of the size of the list. All that remains is the Java implementation. Figure 16.19 provides the class skeleton. Lines 39 to 49 give the type declaration for the nodes in the list. A ListNode consists of two data members: figure 16.18 Linked list  implementation of the  Stack class D C B A topOfStack

16.2 linked list implementations figure 16.19 Skeleton for  linked list-based  stack class package weiss.nonstandard; // ListStack class // // CONSTRUCTION: with no initializer // // ******************PUBLIC OPERATIONS********************* // void push( x )         --> Insert x // void pop( )            --> Remove most recently inserted item // AnyType top( )         --> Return most recently inserted item // AnyType topAndPop( )   --> Return and remove most recent item // boolean isEmpty( )     --> Return true if empty; else false // void makeEmpty( )      --> Remove all items // ******************ERRORS******************************** // top, pop, or topAndPop on empty stack public class ListStack<AnyType> implements Stack<AnyType> {     public boolean isEmpty( )       { return topOfStack == null; }     public void makeEmpty( )       { topOfStack = null; }     public void push( AnyType x )       { /* Figure 16.20 */ }     public void pop( )       { /* Figure 16.20 */ }     public AnyType top( )       { /* Figure 16.21 */ }     public AnyType topAndPop( )       { /* Figure 16.21 */ }     private ListNode<AnyType> topOfStack = null; } // Basic node stored in a linked list. // Note that this class is not accessible outside // of package weiss.nonstandard class ListNode<AnyType> {     public ListNode( AnyType theElement )       { this( theElement, null ); }     public ListNode( AnyType theElement, ListNode<AnyType> n )       { element = theElement; next = n; }     public AnyType   element;     public ListNode next; }

chapter 16 stacks and queues element stores the item and next stores a reference to the next ListNode in the linked list. We provide constructors for ListNode that can be used to execute both ListNode<AnyType> p1 = new ListNode<AnyType>( x ); and ListNode<AnyType> p2 = new ListNode<AnyType>( x, ptr2 ); The ListNode declaration is packagevisible but can be  used by the queue  implementation in  the same package. One option is to nest ListNode in the Stack class. We use the slightly inferior alternative of making it a top-level class that is only package-visible, thus enabling reuse of the class for the queue implementation. The stack itself is represented by a single data member, topOfStack, which is a reference to the ﬁrst ListNode in the linked list. The constructor is not explicitly written, since by default we obtain an empty stack by setting topOfStack to NULL. makeEmpty and isEmpty are thus trivial and are shown at lines 19–22. The stack routines  are essentially oneliners. Two routines are shown in Figure 16.20. The push operation is essentially one line of code, in which we allocate a new ListNode whose data member contains the item x to be pushed. The next reference for this new node is the original topOfStack. This node then becomes the new topOfStack. We do all this at line 7. The pop operation also is simple. After the obligatory test for emptiness, we reset topOfStack to the second node in the list. figure 16.20 The push and pop routines for the  ListStack class /**  * Insert a new item into the stack.  * @param x the item to insert.  */     public void push( AnyType x )     {         topOfStack = new ListNode<AnyType>( x, topOfStack );     } /**  * Remove the most recently inserted item from the stack.      * @throws UnderflowException if the stack is empty.      */     public void pop( )     {         if( isEmpty( ) )             throw new UnderflowException( "ListStack pop" );         topOfStack = topOfStack.next;     }

16.2 linked list implementations Finally, top and topAndPop are straightforward routines and are implemented as shown in Figure 16.21. 16.2.2   queues The queue can be implemented by a linked list, provided we keep references to both the front and back of the list. Figure 16.22 shows the general idea. figure 16.21 The top and topAndPop routines for the  ListStack class /**  * Get the most recently inserted item in the stack.  * Does not alter the stack.  * @return the most recently inserted item in the stack.  * @throws UnderflowException if the stack is empty.  */     public AnyType top( )     {         if( isEmpty( ) )             throw new UnderflowException( "ListStack top" );         return topOfStack.element;     } /**  * Return and remove the most recently inserted item  * from the stack.  * @return the most recently inserted item in the stack.  * @throws UnderflowException if the stack is empty.  */     public AnyType topAndPop( )     {         if( isEmpty( ) )             throw new UnderflowException( "ListStack topAndPop" );         AnyType topItem = topOfStack.element;         topOfStack = topOfStack.next;         return topItem;     } A B C D front back figure 16.22 Linked list  implementation of the  queue class A linked list in which  we maintain a reference to the ﬁrst and  last item can be  used to implement  the queue in constant time per  operation.

chapter 16 stacks and queues The ListQueue class is similar to the ListStack class. The ListQueue class skeleton is given in Figure 16.23. The only new thing here is that we maintain two references instead of one. Figure 16.24 shows the constructors for the ListQueue class. figure 16.23 Skeleton for the  linked list-based  queue class package weiss.nonstandard; // ListQueue class // // CONSTRUCTION: with no initializer // // ******************PUBLIC OPERATIONS********************* // void enqueue( x )      --> Insert x // AnyType getFront( )    --> Return least recently inserted item // AnyType dequeue( )     --> Return and remove least recent item // boolean isEmpty( )     --> Return true if empty; else false // void makeEmpty( )      --> Remove all items // ******************ERRORS******************************** // getFront or dequeue on empty queue public class ListQueue<AnyType> {     public ListQueue( )       { /* Figure 16.24 */ }     public boolean isEmpty( )       { /* Figure 16.27 */ }     public void enqueue( AnyType x )       { /* Figure 16.25 */ }     public AnyType dequeue( )       { /* Figure 16.25 */ }     public AnyType getFront( )        { /* Figure 16.27 */ }     public void makeEmpty( )       { /* Figure 16.27 */ }     private ListNode<AnyType> front;     private ListNode<AnyType> back; } figure 16.24 Constructor for the  linked list-based  ListQueue class /**  * Construct the queue.  */     public ListQueue( )     {         front = back = null;     }

16.2 linked list implementations Enqueueing the  ﬁrst element is a  special case  because there is no  next reference to  which a new node  can be attached. Figure 16.25 implements both enqueue and dequeue. The dequeue routine is logically identical to a stack pop (actually popAndTop). The enqueue routine has two cases. If the queue is empty, we create a one-element queue by calling new and having both front and back reference the single node. Otherwise, we create a new node with data value x, attach it at the end of the list, and then reset the end of the list to this new node, as illustrated in Figure 16.26. Note that enqueueing the ﬁrst element is a special case because there is no next reference to which a new node can be attached. We do all this at line 10 in Figure 16.25. The remaining methods for the ListQueue class are identical to the corresponding ListStack routines. They are shown in Figure 16.27. figure 16.25 The enqueue and  dequeue routines for  the ListQueue class /**  * Insert a new item into the queue.  * @param x the item to insert.  */     public void enqueue( AnyType x )     {         if( isEmpty( ) )    // Make a queue of one element             back = front = new ListNode<AnyType>( x );         else                // Regular case             back = back.next = new ListNode<AnyType>( x );     }     /**      * Return and remove the least recently inserted item      * from the queue.      * @return the least recently inserted item in the queue.      * @throws UnderflowException if the queue is empty.      */     public AnyType dequeue( )     {         if( isEmpty( ) )             throw new UnderflowException( "ListQueue dequeue" );         AnyType returnValue = front.element;         front = front.next;         return returnValue;     }

chapter 16 stacks and queues back x back ... ... (a) Before (b) After figure 16.26 The enqueue operation for the linked listbased implementation figure 16.27 Supporting routines  for the ListQueue class /**  * Get the least recently inserted item in the queue.  * Does not alter the queue.  * @return the least recently inserted item in the queue.  * @throws UnderflowException if the queue is empty.  */     public AnyType getFront( )      {         if( isEmpty( ) )             throw new UnderflowException( "ListQueue getFront" );         return front.element;     }     /**      * Make the queue logically empty.      */     public void makeEmpty( )     {         front = null;         back = null;     }     /**      * Test if the queue is logically empty.      */     public boolean isEmpty( )     {         return front == null;     }

16.4 the java.util.Stack class 16.3 comparison of the two methods The array versus  linked list implementations represent a classic  time–space trade-off. Both the array and linked list versions run in constant time per operation. Thus they are so fast that they are unlikely to be the bottleneck of any algorithm and, in that regard, which version is used rarely matters. The array versions of these data structures are likely to be faster than their linked list counterparts, especially if an accurate estimation of capacity is available. If an additional constructor is provided to specify the initial capacity (see Exercise 16.2) and the estimate is correct, no doubling is performed. Also, the sequential access provided by an array is typically faster than the potential nonsequential access offered by dynamic memory allocation. The array implementation does have two drawbacks, however. First, for queues, the array implementation is arguably more complex than the linked list implementation, owing to the combined code for wraparound and array doubling. Our implementation of array doubling was not as efﬁcient as possible (see Exercise 16.8), thus a faster implementation of the queue would require a few additional lines of code. Even the array implementation of the stack uses a few more lines of code than its linked list counterpart. The second drawback affects other languages, but not Java. When doubling, we temporarily require three times as much space as the number of data items suggests. The reason is that, when the array is doubled, we need to have memory to store both the old and the new (double-sized) array. Further, at the queue’s peak size, the array is between 50 percent and 100 percent full; on average it is 75 percent full, so for every three items in the array, one spot is empty. The wasted space is thus 33 percent on average and 100 percent when the table is only half full. As discussed earlier, in Java, each element in the array is simply a reference. In other languages, such as C++, objects are stored directly, rather than referenced. In these languages, the wasted space could be signiﬁcant when compared to the linked list–based version that uses only an extra reference per item. 16.4 the java.util.Stack class The Collections API provides a Stack class. The Stack class in java.util is considered a legacy class and is not widely used. Figure 16.28 provides an implementation.

chapter 16 stacks and queues figure 16.28 A simplified  Collections-style Stack class, based on  the ArrayList class package weiss.util; /**  * Stack class. Unlike java.util.Stack, this is not extended from  * Vector. This is the minimum respectable set of operations.  */ public class Stack<AnyType> implements java.io.Serializable {     public Stack( )     {         items = new ArrayList<AnyType>( );     }     public AnyType push( AnyType x )     {         items.add( x );         return x;     }     public AnyType pop( )     {         if( isEmpty( ) )             throw new EmptyStackException( );         return items.remove( items.size( ) - 1 );     }     public AnyType peek( )     {         if( isEmpty( ) )             throw new EmptyStackException( );         return items.get( items.size( ) - 1 );     }     public boolean isEmpty( )     {         return size( ) == 0;     }     public int size( )     {         return items.size( );     }     public void clear( )     {         items.clear( );     }     private ArrayList<AnyType> items; }

chapt er linked lists In Chapter 16 we demonstrated that linked lists can be used to store items noncontiguously. The linked lists used in that chapter were simpliﬁed, with all the accesses performed at one of the list’s two ends. In this chapter, we show n How to allow access to any item by using a general linked list n The general algorithms for the linked list operations n How the iterator class provides a safe mechanism for traversing and accessing linked lists n List variations, such as doubly linked lists and circularly linked lists n How to use inheritance to derive a sorted linked list class n How to implement the Collections API LinkedList class 17.1 basic ideas In this chapter we implement the linked list and allow general access (arbitrary insertion, deletion, and ﬁnd operations) through the list. The basic linked list consists of a collection of connected, dynamically allocated nodes. In a

chapter 17 linked lists singly linked list, each node consists of the data element and a link to the next node in the list. The last node in the list has a null next link. In this section we assume that the node is given by the following ListNode declaration, which does not use generics: class ListNode {     Object element;     ListNode   next; } The ﬁrst node in the linked list is accessible by a reference, as shown in Figure 17.1. We can print or search in the linked list by starting at the ﬁrst item and following the chain of next links. The two basic operations that must be performed are insertion and deletion of an arbitrary item x. Insertion consists  of splicing a node  into the list and can  be accomplished  with one statement. For insertion we must deﬁne where the insertion is to take place. If we have a reference to some node in the list, the easiest place to insert is immediately after that item. As an example, Figure 17.2 shows how we insert x after item a in a linked list. We must perform the following steps: tmp = new ListNode( );        // Create a new node tmp.element = x;              // Place x in the element member tmp.next = current.next;      // x's next node is b current.next = tmp;           // a's next node is x As a result of these statements, the old list . . . a, b, . . . now appears as . . . a, x, b, . . . . We can simplify the code if the ListNode has a constructor that initializes the data members directly. In that case, we obtain b c d frontOfList a figure 17.1 Basic linked list a b x current tmp ... ... figure 17.2 Insertion in a linked  list: Create new node  (tmp), copy in x, set  tmp’s next link, and set  current’s next link.

17.1 basic ideas tmp = new ListNode( x, current.next ); // Create new node current.next = tmp;                    // a's next node is x We now see that tmp is no longer necessary. Thus we have the one-liner current.next = new ListNode( x, current.next ); Removal can be  accomplished by  bypassing the  node. We need a  reference to the  node prior to the  one we want to  remove. The remove command can be executed in one link change. Figure 17.3 shows that to remove item x from the linked list, we set current to be the node prior to x and then have current’s next link bypass x. This operation is expressed by the statement current.next = current.next.next; The list . . . a, x, b, . . . now appears as . . . a, b, . . . . Linked list operations use only a  constant number of  data movements. The preceding discussion summarizes the basics of inserting and removing items at arbitrary places in a linked list. The fundamental property of a linked list is that changes to it can be made by using only a constant number of data movements, which is a great improvement over an array implementation. Maintaining contiguousness in an array means that whenever an item is added or deleted, all items that follow it in the list must move. 17.1.1   header nodes There is one problem with the basic description: It assumes that whenever an item x is removed, some previous item is always present to allow a bypass. Consequently, removal of the ﬁrst item in the linked list becomes a special case. Similarly, the insert routine does not allow us to insert an item to be the new ﬁrst element in the list. The reason is that insertions must follow some existing item. So, although the basic algorithm works ﬁne, some annoying special cases must be dealt with. Special cases are always problematic in algorithm design and frequently lead to bugs in the code. Consequently, writing code that avoids special cases is generally preferable. One way to do that is to introduce a header node. A header node is an extra node in a linked list that holds no data but serves to satisfy the requirement that every node containing an item have a a b x current ... ... figure 17.3 Deletion from a  linked list

chapter 17 linked lists previous node in the list. The header node for the list a, b, c is shown in Figure 17.4. Note that a is no longer a special case. It can be deleted just like any other node by having current reference the node before it. We can also add a new ﬁrst element to the list by setting current equal to the header node and calling the insertion routine. By using the header node, we greatly simplify the code—with a negligible space penalty. In more complex applications, header nodes not only simplify the code but also improve speed because, after all, fewer tests mean less time. The use of a header node is somewhat controversial. Some argue that avoiding special cases is not sufﬁcient justiﬁcation for adding ﬁctitious cells; they view the use of header nodes as little more than old-style hacking. Even so, we use them here precisely because they allow us to demonstrate the basic link manipulations without obscuring the code with special cases. Whether a header should be used is a matter of personal preference. Furthermore, in a class implementation, its use would be completely transparent to the user. However, we must be careful: The printing routine must skip over the header node, as must all searching routines. Moving to the front now means setting the current position to header.next, and so on. Furthermore, as Figure 17.5 shows, with a dummy header node, a list is empty if header.next is null. 17.1.2   iterator classes The typical primitive strategy identiﬁes a linked list by a reference to the header node. Each individual item in the list can then be accessed by providing a reference to the node that stores it. The problem with that strategy is that a b c header figure 17.4 Using a header node  for the linked list header figure 17.5 Empty list when a  header node is used A header node holds no data but  serves to satisfy the  requirement that  every node have a  previous node. A header node allows  us to avoid special  cases such as  insertion of a new  ﬁrst element and  removal of the ﬁrst  element.

17.1 basic ideas checking for errors is difﬁcult. For example, a user could pass a reference to something that is a node in a different list. One way to guarantee that this cannot happen is to store a current position as part of a list class. To do so, we add a second data member, current. Then, as all access to the list goes through the class methods, we can be certain that current always represents a node in the list, the header node, or null. An iterator class maintains a current  position and typically is packagevisible or an inner  class of a list (or  other container)  class. This scheme has a problem: With only one position, the case of two iterators needing to access the list independently is left unsupported. One way to avoid this problem is to deﬁne a separate iterator class, which maintains a notion of its current position. A list class would then not maintain any notion of a current position and would only have methods that treat the list as a unit, such as isEmpty and makeEmpty, or that accept an iterator as a parameter, such as insert. Routines that depend only on an iterator itself, such as the advance routine that advances the iterator to the next position, would reside in the iterator class. Access to the list is granted by making the iterator class either packagevisible or an inner class. We can view each instance of an iterator class as one in which only legal list operations, such as advancing in the list, are allowed. In Section 17.2 we deﬁne a generic list class LinkedList and an iterator class LinkedListIterator. The LinkedList class does not have the same semantics as java.util.LinkedList. However, later in the chapter we deﬁne a version that does. To show how the nonstandard version works, let us look at a static method that returns the size of a linked list, as shown in Figure 17.6. We declare itr as an iterator that can access the linked list theList. We initialize itr to the ﬁrst element in theList (skipping over the header, of course) by referencing the iterator given by theList.first(). The test itr.isValid() attempts to mimic the test p!=null that would be conducted if p were a visible reference to a node. Finally, the expression itr.advance() mimics the conventional idiom p=p.next. figure 17.6 A static method that returns the size of a list // In this routine, LinkedList and LinkedListIterator are the     // classes written in Section 17.2.     public static <AnyType> int listSize( LinkedList<AnyType> theList )     {         LinkedListIterator<AnyType> itr;         int size = 0;         for( itr = theList.first(); itr.isValid(); itr.advance() )             size++;         return size;     } By storing a current position in a list  class, we ensure  that access is controlled.

chapter 17 linked lists Thus, as long as the iterator class deﬁnes a few simple operations, we can iterate over the list naturally. In Section 17.2 we provide its implementation in Java. The routines are surprisingly simple. There is a natural parallel between the methods deﬁned in the LinkedList and LinkedListIterator classes and those in the Collections API LinkedList class. For instance, the LinkedListIterator advance method is roughly equivalent to hasNext in the Collections API iterators. The list class in Section 17.2 is simpler than the Collections API LinkedList class; as such it illustrates many basic points and is worth examining. In Section 17.5 we implement most of the Collections API LinkedList class. 17.2 java implementation As suggested in the preceding description, a list is implemented as three separate generic classes: one class is the list itself (LinkedList), another represents the node (ListNode), and the third represents the position (LinkedListIterator). ListNode was shown in Chapter 16. Next, Figure 17.7 presents the class that implements the concept of position—namely, LinkedListIterator. The class stores a reference to a ListNode, representing the current position of the iterator. The isValid method returns true if the position is not past the end of the list, retrieve returns the element stored in the current position, and advance advances the current position to the next position. The constructor for LinkedListIterator requires a reference to a node that is to be the current node. Note that this constructor is package-visible and thus cannot be used by client methods. Instead, the general idea is that the LinkedList class returns preconstructed LinkedListIterator objects, as appropriate; LinkedList is in the same package as LinkedListIterator, so it can invoke the LinkedListIterator constructor. The LinkedList class skeleton is shown in Figure 17.8. The single data member is a reference to the header node allocated by the constructor. isEmpty is an easily implemented short one-liner. The methods zeroth and first return iterators corresponding to the header and ﬁrst element, respectively, as shown in Figure 17.9. Other routines either search the list for some item or change the list via insertion or deletion, and are shown later. Figure 17.10 illustrates how the LinkedList and LinkedListIterator classes interact. The printList method outputs the contents of a list. printList uses only public methods and a typical iteration sequence of obtaining a starting point (via first), testing that it has not gone past the ending point (via isValid), and advancing in each iteration (via advance).

17.2 java implementation figure 17.7 The LinkedListIterator class package weiss.nonstandard; // LinkedListIterator class; maintains "current position" // // CONSTRUCTION: Package visible only, with a ListNode // // ******************PUBLIC OPERATIONS********************* // void advance( )        --> Advance // boolean isValid( )     --> True if at valid position in list // AnyType retrieve       --> Return item in current position public class LinkedListIterator<AnyType> {     /**      * Construct the list iterator      * @param theNode any node in the linked list.      */     LinkedListIterator( ListNode<AnyType> theNode )       { current = theNode; }     /**      * Test if the current position is a valid position in the list.      * @return true if the current position is valid.      */     public boolean isValid( )       { return current != null; }     /**      * Return the item stored in the current position.      * @return the stored item or null if the current position      * is not in the list.      */     public AnyType retrieve( )       { return isValid( ) ? current.element : null; }     /**      * Advance the current position to the next node in the list.      * If the current position is null, then do nothing.      */     public void advance( )     {         if( isValid( ) )             current = current.next;     }     ListNode<AnyType> current;    // Current position }

chapter 17 linked lists figure 17.8 The LinkedList class skeleton package weiss.nonstandard; // LinkedList class // // CONSTRUCTION: with no initializer // Access is via LinkedListIterator class // // ******************PUBLIC OPERATIONS********************* // boolean isEmpty( )     --> Return true if empty; else false // void makeEmpty( )      --> Remove all items // LinkedListIterator zeroth( ) //                        --> Return position to prior to first // LinkedListIterator first( ) //                        --> Return first position // void insert( x, p )    --> Insert x after current iterator position p // void remove( x )       --> Remove x // LinkedListIterator find( x ) //                        --> Return position that views x // LinkedListIterator findPrevious( x ) //                        --> Return position prior to x // ******************ERRORS******************************** // No special errors public class LinkedList<AnyType> {     public LinkedList( )       { /* Figure 17.9 */ }     public boolean isEmpty( )       { /* Figure 17.9 */ }     public void makeEmpty( )       { /* Figure 17.9 */ }     public LinkedListIterator<AnyType> zeroth( )       { /* Figure 17.9 */ }     public LinkedListIterator<AnyType> first( )       { /* Figure 17.9 */ }     public void insert( AnyType x, LinkedListIterator<AnyType> p )       { /* Figure 17.14 */ }     public LinkedListIterator<AnyType> find( AnyType x )       { /* Figure 17.11 */ }     public LinkedListIterator<AnyType> findPrevious( AnyType x )       { /* Figure 17.13 */ }     public void remove( Object x )       { /* Figure 17.12 */ }     private ListNode<AnyType> header; }

17.2 java implementation Let us revisit the issue of whether all three classes are necessary. For instance, couldn’t we just have the LinkedList class maintain a notion of a current position? Although this option is feasible and works for many applications, using a separate iterator class expresses the abstraction that the figure 17.9 Some LinkedList class one-liners /**  * Construct the list  */     public LinkedList( )     {         header = new ListNode<AnyType>( null );     } /**  * Test if the list is logically empty.  * @return true if empty, false otherwise.  */     public boolean isEmpty( )     {         return header.next == null;     }     /**      * Make the list logically empty.      */     public void makeEmpty( )     {         header.next = null;     }     /**      * Return an iterator representing the header node.      */     public LinkedListIterator<AnyType> zeroth( )     {         return new LinkedListIterator<AnyType>( header );     }     /**      * Return an iterator representing the first node in the list.      * This operation is valid for empty lists.      */     public LinkedListIterator<AnyType> first( )     {         return new LinkedListIterator<AnyType>( header.next );     }

chapter 17 linked lists position and list actually are separate objects. Moreover, it allows for a list to be accessed in several places simultaneously. For instance, to remove a sublist from a list, we can easily add a remove operation to the list class that uses two iterators to specify the starting and ending points of the sublist to be removed. Without the iterator class, this action would be more difﬁcult to express. We can now implement the remaining LinkedList methods. First is find, shown in Figure 17.11, which returns the position in the list of some element. Line 10 takes advantage of the fact that the and (&&) operation is short-circuited: figure 17.10 A method for printing the contents of a LinkedList // Simple print method     public static <AnyType> void printList( LinkedList<AnyType> theList )     {         if( theList.isEmpty( ) )             System.out.print( "Empty list" );         else         {             LinkedListIterator<AnyType> itr = theList.first( );             for( ; itr.isValid( ); itr.advance( ) )                 System.out.print( itr.retrieve( ) + " " );         }         System.out.println( );     } figure 17.11 The find routine for the LinkedList class /**  * Return iterator corresponding to the first node containing x.  * @param x the item to search for.  * @return an iterator; iterator isPastEnd if item is not found.  */     public LinkedListIterator<AnyType> find( AnyType x )     {         ListNode<AnyType> itr = header.next;         while( itr != null && !itr.element.equals( x ) )             itr = itr.next;         return new LinkedListIterator<AnyType>( itr );     } Short-circuiting is  used in the find routine at line 10  and in the similar  part of the remove routine.

17.2 java implementation If the ﬁrst half of the and is false, the result is automatically false and the second half is not evaluated. This code is not  foolproof: There  may be two iterators, and one can  be left dangling if  the other removes  a node. Our next routine removes some element x from the list. We need to decide what to do if x occurs more than once or not at all. Our routine removes the ﬁrst occurrence of x and does nothing if x is not in the list. To make that happen, we ﬁnd p, which is the cell prior to the one containing x, via a call to findPrevious. The code for implementing the remove routine is shown in Figure 17.12. This code is not foolproof: There may be two iterators, and one can be left logically in limbo if the other removes a node. The findPrevious routine is similar to the find routine and is shown in Figure 17.13.  /**  * Remove the first occurrence of an item.  * @param x the item to remove.  */     public void remove( AnyType x )     {         LinkedListIterator<AnyType> p = findPrevious( x );         if( p.current.next != null )             p.current.next = p.current.next.next;  // Bypass deleted node     } figure 17.12 The remove routine for the LinkedList class figure 17.13 The findPrevious routine—similar to the find routine—for use with remove /**  * Return iterator prior to the first node containing an item.  * @param x the item to search for.  * @return appropriate iterator if the item is found. Otherwise, the  * iterator corresponding to the last element in the list is returned.  */     public LinkedListIterator<AnyType> findPrevious( AnyType x )     {         ListNode<AnyType> itr = header;         while( itr.next != null && !itr.next.element.equals( x ) )             itr = itr.next;         return new LinkedListIterator<AnyType>( itr );     }

chapter 17 linked lists The insert routine  takes constant  time. The last routine we write here is an insertion routine. We pass an element to be inserted and a position p. This particular insertion routine inserts an element after position p, as shown in Figure 17.14. Note that the insert routine makes no use of the list it is in; it depends only on p. The find and  findPrevious routines take O(N) time. With the exception of the find and findPrevious routines (and remove, which calls findPrevious), all the operations that we have coded so far take O(1) time. The find and findPrevious routines take O(N) time in the worst case because the entire list might need to be traversed if the element either is not found or is last in the list. On average, the running time is O(N), because on average half the list must be traversed. The retreat method is not efﬁciently supported. A  doubly linked list is  used if that is a liability. We certainly could have added more operations, but this basic set is quite powerful. Some operations, such as retreat, are not efﬁciently supported by this version of the linked list; variations on the linked list that allow constanttime implementation of that and other operators are discussed later in this chapter. 17.3 doubly linked lists  and circularly linked lists A doubly linked list allows bidirectional  traversal by storing  two links per node. As we mentioned in Section 17.2, the singly linked list does not efﬁciently support some important operations. For instance, although it is easy to go to the front of the list, it is time consuming to go to the end. Although we can easily advance via advance, implementing retreat cannot be done efﬁciently with only a next link. In some applications that might be crucial. For instance, figure 17.14 The insertion routine for the LinkedList class /**  * Insert after p.  * @param x the item to insert.  * @param p the position prior to the newly inserted item.  */     public void insert( AnyType x, LinkedListIterator<AnyType> p )     {         if( p != null && p.current != null )             p.current.next = new ListNode<AnyType>( x, p.current.next );     }

17.3 doubly linked lists and circularly linked lists when designing a text editor, we can maintain the internal image of the ﬁle as a linked list of lines. We want to be able to move up just as easily as down in the list, to insert both before and after a line rather than just after, and to be able to get to the last line quickly. A moment’s thought suggests that to implement this procedure efﬁciently we should have each node maintain two links: one to the next node in the list and one to the previous node. Then, to make everything symmetric, we should have not only a header but also a tail. A linked list that allows bidirectional traversal by storing two links per node is called a doubly linked list. Figure 17.15 shows the doubly linked list representing a and b. Each node now has two links (next and prev), and searching and moving can easily be performed in both directions. Obviously, there are some important changes from the singly linked list. Symmetry demands  that we use both a  head and a tail and that we support  roughly twice as  many operations. First, an empty list now consists of a head and tail, connected as shown in Figure 17.16. Note that head.prev and tail.next are not needed in the algorithms and are not even initialized. The test for emptiness is now head.next == tail or tail.prev == head When we advance  past the end of the  list, we now hit the  tail node instead  of null. We no longer use null to decide whether an advance has taken us past the end of the list. Instead, we have gone past the end if current is either head or tail (recall that we can go in either direction). The retreat operation can be implemented by current = current.prev; a b head tail figure 17.15 A doubly linked list head tail figure 17.16 An empty doubly  linked list

chapter 17 linked lists Insertion and  removal involve  twice as many link  changes as for a  singly linked list. Before describing some of the additional operations that are available, let us consider how the insertion and removal operations change. Naturally, we can now do both insertBefore and insertAfter. Twice as many link moves are involved for insertAfter with doubly linked lists as with singly linked lists. If we write each statement explicitly, we obtain newNode = new DoublyLinkedListNode( x ); newNode.prev = current;                    // Set x's prev link newNode.next = current.next;               // Set x's next link newNode.prev.next = newNode;               // Set a's next link newNode.next.prev = newNode;               // Set b's prev link current = newNode; As we showed earlier, the ﬁrst two link moves can be collapsed into the DoublyLinkedListNode construction that is done by new. The changes (in order 1, 2, 3, 4) are illustrated in Figure 17.17. The remove operation can proceed  from the current  node because  we can obtain the  previous node  instantly. Figure 17.17 can also be used as a guide in the removal algorithm. Unlike the singly linked list, we can remove the current node because the previous node is available to us automatically. Thus to remove x we have to change a’s next link and b’s prev link. The basic moves are current.prev.next = current.next;     // Set a's next link current.next.prev = current.prev;     // Set b's prev link current = head;                       // So current is not stale To do a complete doubly linked list implementation, we need to decide which operations to support. We can reasonably expect twice as many operations as in the singly linked list. Each individual procedure is similar to the linked list routines; only the dynamic operations involve additional link moves. Moreover, for many of the routines, the code is dominated by error checks. Although some of the checks will change (e.g., we do not test against null), they certainly do not become any more complex. In Section 17.5, we use a doubly linked list to implement the Collections API linked list class, figure 17.17 Insertion in a doubly  linked list by getting  new node and then  changing pointers in  the order indicated a b x ... ...

17.4 sorted linked lists along with its associated iterators. There are lots of routines, but most are short. In a circularly linked  list, the last cell’s  next link references  first. This action is  useful when wraparound matters. A popular convention is to create a circularly linked list, in which the last cell’s next link references first, which can be done with or without a header. Typically, it is done without a header because the header’s main purpose is to ensure that every node has a previous node, which is already true for a nonempty circularly linked list. Without a header, we have only the empty list as a special case. We maintain a reference to the ﬁrst node, but that is not the same as a header node. We can use circularly linked lists and doubly linked lists simultaneously, as shown in Figure 17.18. The circular list is useful when we want searching to allow wraparound, as is the case for some text editors. In Exercise 17.16 you are asked to implement a circularly and doubly linked list. 17.4 sorted linked lists We can maintain  items in sorted  order by deriving a  SortedLinkedList class from  LinkedList. Sometimes we want to keep the items in a linked list in sorted order, which we can do with a sorted linked list. The fundamental difference between a sorted linked list and an unsorted linked list is the insertion routine. Indeed, we can obtain a sorted list class by simply altering the insertion routine from our already written list class. Because the insert routine is part of the LinkedList class, we should be able to base a new derived class, SortedLinkedList, from LinkedList. We can, and it is shown in Figure 17.19. The new class has two versions of insert. One version takes a position and then ignores it; the insertion point is determined solely by the sorted order. The other version of insert requires more code. The one-parameter insert uses two LinkedListIterator objects to traverse down the corresponding list until the correct insertion point is found. At that point we can apply the base class insert routine. figure 17.18 A circularly and doubly  linked list b c d first a

chapter 17 linked lists figure 17.19 The SortedLinkedList class, in which insertions are restricted to sorted order package weiss.nonstandard; // SortedLinkedList class // // CONSTRUCTION: with no initializer // Access is via LinkedListIterator class // // ******************PUBLIC OPERATIONS********************* // void insert( x )       --> Insert x // void insert( x, p )    --> Insert x (ignore p) // All other LinkedList operations // ******************ERRORS******************************** // No special errors public class SortedLinkedList<AnyType extends Comparable<? super AnyType>>                               extends LinkedList<AnyType> {     /**      * Insert after p.      * @param x the item to insert.      * @param p this parameter is ignored.      */     public void insert( AnyType x, LinkedListIterator<AnyType> p )     {         insert( x );     }     /**      * Insert in sorted order.      * @param x the item to insert.      */     public void insert( AnyType x )     {         LinkedListIterator<AnyType> prev = zeroth( );         LinkedListIterator<AnyType> curr = first( );         while( curr.isValid( ) && x.compareTo( curr.retrieve( ) ) > 0 )         {             prev.advance( );             curr.advance( );         }         super.insert( x, prev );     } }

17.5 implementing the collections api LinkedList class 17.5 implementing the collections  api LinkedList class In this section we implement the Collections API LinkedList class discussed in Section 6.5. Although we present lots of code, we described most of the techniques earlier in this chapter. As we indicated previously, we need a class to store the basic list node, a class for the iterator, and a class for the list itself. The skeleton for the LinkedList class is shown in Figure 17.20. LinkedList implements the List and Queue interfaces and, as usual, it extends AbstractCollection. Line 5 begins the declaration for the Node class, which is nested and private. Line 7 begins the declaration for the LinkedListIterator, which is a private inner figure 17.20a Class skeleton for standard LinkedList class (continues) package weiss.util; public class LinkedList<AnyType> extends AbstractCollection<AnyType>                                  implements List<AnyType>, Queue<AnyType> {     private static class Node<AnyType>       { /* Figure 17.21 */ }     private class LinkedListIterator<AnyType> implements ListIterator<AnyType>       { /* Figure 17.30 */ }     public LinkedList( )       { /* Figure 17.22 */ }     public LinkedList( Collection<? extends AnyType> other )       { /* Figure 17.22 */ }     public int size( )       { /* Figure 17.23 */ }     public boolean contains( Object x )       { /* Figure 17.23 */ }     public boolean add( AnyType x )       { /* Figure 17.24 */ }     public void add( int idx, AnyType x )       { /* Figure 17.24 */ }     public void addFirst( AnyType x )       { /* Figure 17.24 */ }     public void addLast( AnyType x )       { /* Figure 17.24 */ }     public AnyType element( )       { /* Added in Java 5; same as getFirst */ }     public AnyType getFirst( )       { /* Figure 17.25 */ }     public AnyType getLast( )   { /* Figure 17.25 */ }

chapter 17 linked lists class. The iterator pattern was described in Chapter 6. The same pattern was used in the ArrayList implementation with inner classes in Chapter 15. The list class keeps track of its size in a data member declared at line 54. We use this approach so that the size method can be performed in constant time. modCount is used by the iterators to determine if the list has changed while an iteration is in progress; the same idea was used in ArrayList. beginMarker and endMarker correspond to head and tail in Section 17.3. All the methods use signatures that we have shown before. Figure 17.21 shows the Node class, which is similar to the ListNode class. The main difference is that, because we use a doubly linked list, we have both prev and next links. Note that inner and nested classes are considered part of the outer class. Thus, regardless of whether the Node’s data ﬁelds are public or private, they will be visible to the LinkedList class. Because the Node class itself is private, only the LinkedList class will be able to see that Node is a valid type. Consequently, in this instance, it does not matter whether the node’s data ﬁelds are public or figure 17.20b Class skeleton for  standard LinkedList class (continued) public AnyType remove( )   { /* Added in Java 5; same as removeFirst */ } public AnyType removeFirst( )   { /* Figure 17.27 */ } public AnyType removeLast( )   { /* Figure 17.27 */ } public boolean remove( Object x )   { /* Figure 17.28 */ } public AnyType get( int idx )   { /* Figure 17.25 */ } public AnyType set( int idx, AnyType newVal )   { /* Figure 17.25 */ } public AnyType remove( int idx )   { /* Figure 17.27 */ } public void clear( )   { /* Figure 17.22 */ } public Iterator<AnyType> iterator( )   { /* Figure 17.29 */ } public ListIterator<AnyType> listIterator( int idx )   { /* Figure 17.29 */ }     private int theSize;     private Node<AnyType> beginMarker;     private Node<AnyType> endMarker;     private int modCount = 0;     private static final Node<AnyType> NOT_FOUND = null;     private Node<AnyType> findPos( Object x )       { /* Figure 17.23 */ }     private AnyType remove( Node<AnyType> p )       { /* Figure 17.27 */ } private Node<AnyType> getNode( int idx )   { /* Figure 17.26 */} }

17.5 implementing the collections api LinkedList class private. It could matter if Node was to be extended inside of LinkedList, and this would argue toward making the data private. On the other hand, since in our implementation, LinkedList is accessing the Node’s data ﬁelds directly, rather than invoking methods, it seems more appropriate to mark the data as public. The implementation of LinkedList begins in Figure 17.22, where we have the constructors and clear. All in all, little is new here; we combined figure 17.21 Node nested class for  standard LinkedList class /**  * This is the doubly linked list node.  */     private static class Node<AnyType>     {         public Node( AnyType d, Node<AnyType> p, Node<AnyType> n )         {             data = d; prev = p; next = n;         }         public AnyType       data;         public Node<AnyType> prev;         public Node<AnyType> next;     } figure 17.22 Constructors and  clear method for  standard LinkedList class /**  * Construct an empty LinkedList.  */     public LinkedList( )     {         clear( );     }     /**      * Construct a LinkedList with same items as another Collection.      */     public LinkedList( Collection<? extends AnyType> other )     {         clear( );         for( AnyType val : other )     add( val );     }     /**      * Change the size of this collection to zero.      */     public void clear( )     {         beginMarker = new Node<AnyType>( null, null, null );         endMarker = new Node<AnyType>( null, beginMarker, null );         beginMarker.next = endMarker;         theSize = 0;         modCount++;     }

chapter 17 linked lists a lot of the nonstandard LinkedList code with the concepts presented in Section 17.3. Figure 17.23 shows size, which is trivial, and contains, which is also trivial because it calls the private findPos routine that does all the work. findPos deals with null values at lines 30–34; otherwise, it would be four lines of code. Figure 17.24 shows the various add methods. All of these eventually funnel into the last add method at lines 39–47, which splices into the doubly linked list as was done in Section 17.3. It requires a private routine, getNode, whose implementation we will discuss shortly. getNode returns a reference to /**  * Returns the number of items in this collection.  * @return the number of items in this collection.  */     public int size( )     {         return theSize;     }     /**      * Tests if some item is in this collection.      * @param x any object.      * @return true if this collection contains an item equal to x.      */     public boolean contains( Object x )     {         return findPos( x ) != NOT_FOUND;     }      /**      * Returns the position of first item matching x      * in this collection, or NOT_FOUND if not found.      * @param x any object.      * @return the position of first item matching x      * in this collection, or NOT_FOUND if not found.      */     private Node<AnyType> findPos( Object x )     {         for( Node<AnyType> p = beginMarker.next; p != endMarker; p = p.next )             if( x == null )             {                 if( p.data == null )                     return p;             }             else if( x.equals( p.data ) )                 return p;         return NOT_FOUND;     } figure 17.23 size and contains for standard LinkedList class

17.5 implementing the collections api LinkedList class the node at index idx. In order for this to be suitable for addLast, getNode will start its search from the end closest to the target node. figure 17.24 add methods for  standard LinkedList class /**  * Adds an item to this collection, at the end.  * @param x any object.  * @return true.  */     public boolean add( AnyType x )     {         addLast( x );         return true;     }     /**      * Adds an item to this collection, at the front.      * Other items are slid one position higher.      * @param x any object.      */     public void addFirst( AnyType x )     {         add( 0, x );     }     /**      * Adds an item to this collection, at the end.      * @param x any object.      */     public void addLast( AnyType x )     {         add( size( ), x );     }      /**      * Adds an item to this collection, at a specified position.      * Items at or after that position are slid one position higher.      * @param x any object.      * @param idx position to add at.      * @throws IndexOutOfBoundsException if idx is not      *         between 0 and size(), inclusive.      */     public void add( int idx, AnyType x )     {         Node<AnyType> p = getNode( idx, 0, size);         Node<AnyType> newNode = new Node<AnyType>( x, p.prev, p );         newNode.prev.next = newNode;         p.prev = newNode;         theSize++;         modCount++;     }

chapter 17 linked lists Figure 17.25 details the various get methods, plus a set method. There is little special in any of those routines. The element method from the Queue interface is not shown. Figure 17.26 has the previously mentioned private figure 17.25 get and set methods  for standard  LinkedList class /**  * Returns the first item in the list.  * @throws NoSuchElementException if the list is empty.  */     public AnyType getFirst( )     {         if( isEmpty( ) )             throw new NoSuchElementException( );         return getNode( 0 ).data;     } /**      * Returns the last item in the list.      * @throws NoSuchElementException if the list is empty.      */     public AnyType getLast( )     {         if( isEmpty( ) )             throw new NoSuchElementException( );         return getNode( size( ) - 1 ).data;     }     /**      * Returns the item at position idx.      * @param idx the index to search in.      * @throws IndexOutOfBoundsException if index is out of range.      */     public AnyType get( int idx )     {         return getNode( idx ).data;     }     /**      * Changes the item at position idx.      * @param idx the index to change.      * @param newVal the new value.      * @return the old value.      * @throws IndexOutOfBoundsException if index is out of range.      */     public AnyType set( int idx, AnyType newVal )     {         Node<AnyType> p = getNode( idx );         AnyType oldVal = p.data;         p.data = newVal;         return oldVal;     }

17.5 implementing the collections api LinkedList class getNode method. The three-parameter version is needed speciﬁcallly for add and the LinkedListIterator constructor; the more common one-parameter version is used for all other calls to getNode. If the index represents a node in the ﬁrst half of the list, then at lines 19–21 we step through the linked list, in the forward direction. Otherwise, we go backwards, starting at the end, as shown on lines 25–27. figure 17.26 Private getNode for standard LinkedList class     /**      * Gets the Node at position idx, which must range from lower to upper.      * @param idx index to search at.      * @param lower lowest valid index.      * @param upper highest valid index.      * @return internal node corrsponding to idx.      * @throws IndexOutOfBoundsException if idx is not  *         between lower and upper, inclusive.      */     private Node<AnyType> getNode( int idx, int lower, int upper )     {         Node<AnyType> p;         if( idx < lower || idx > upper )             throw new IndexOutOfBoundsException( );         if( idx < size( ) / 2 )         {             p = beginMarker.next;             for( int i = 0; i < idx; i++ )                 p = p.next;         }         else         {             p = endMarker;             for( int i = size( ); i > idx; i-- )                 p = p.prev;         }          return p;     }     /**      * Gets the Node at position idx, which must range from 0 to size( ) - 1.      * @param idx index to search at.      * @return internal node corrsponding to idx.      * @throws IndexOutOfBoundsException if idx is not  *         between 0 and size()-1, inclusive.      */     private Node<AnyType> getNode( int idx )     {         return getNode( idx, 0, size( ) - 1 );     }

chapter 17 linked lists The remove methods are shown in Figures 17.27 and 17.28, and those funnel through a private remove method, shown at lines 40–48 (in Figure 17.27), that mimics the algorithm in Section 17.3. figure 17.27 remove methods for  standard LinkedList class /**  * Removes the first item in the list.  * @return the item was removed from the collection.  * @throws NoSuchElementException if the list is empty.  */     public AnyType removeFirst( )     {         if( isEmpty( ) )             throw new NoSuchElementException( );         return remove( getNode( 0 ) );     }     /**      * Removes the last item in the list.      * @return the item was removed from the collection.      * @throws NoSuchElementException if the list is empty.      */     public AnyType removeLast( )     {         if( isEmpty( ) )             throw new NoSuchElementException( );         return remove( getNode( size( ) - 1 ) );     }     /**      * Removes an item from this collection.      * @param idx the index of the object.      * @return the item that was removed from the collection.      */     public AnyType remove( int idx )     {         return remove( getNode( idx ) );     }     /**      * Removes the object contained in Node p.      * @param p the Node containing the object.      * @return the item that was removed from the collection.      */     private AnyType remove( Node<AnyType> p )     {         p.next.prev = p.prev;         p.prev.next = p.next;         theSize--;         modCount++;         return p.data;     }

17.5 implementing the collections api LinkedList class The iterator factories are shown in Figure 17.29. Both return a freshly constructed LinkedListIterator object. Finally, the LinkedListIterator, which is perhaps the trickiest part of the whole implementation, is shown in Figure 17.30. figure 17.28 Additional remove method for standard  LinkedList class /**  * Removes an item from this collection.  * @param x any object.  * @return true if this item was removed from the collection.  */     public boolean remove( Object x )     {         Node<AnyType> pos = findPos( x );         if( pos == NOT_FOUND )             return false;         else         {             remove( pos );             return true;         }     }  figure 17.29 Iterator factory methods for standard LinkedList class /**  * Obtains an Iterator object used to traverse the collection.  * @return an iterator positioned prior to the first element.  */     public Iterator<AnyType> iterator( )     {         return new LinkedListIterator( 0 );     }     /**      * Obtains a ListIterator object used to traverse the      * collection bidirectionally.      * @return an iterator positioned prior to the requested element.      * @param idx the index to start the iterator. Use size() to do      * complete reverse traversal. Use 0 to do complete forward traversal.      * @throws IndexOutOfBoundsException if idx is not      *         between 0 and size(), inclusive.      */     public ListIterator<AnyType> listIterator( int idx )     {         return new LinkedListIterator( idx );     }

chapter 17 linked lists The iterator maintains a current position, shown at line 8. current represents the node containing the item that is to be returned by a call to next. Observe that when current is positioned at the endmarker, a call to next is illegal, but the call to previous should give the ﬁrst item, going backwards. As in the ArrayList, the iterator also maintains the modCount of the list it is iterating over, initialized at the time the iterator was constructed. This variable, expectedModCount, can change only if the iterator performs a remove. lastVisited is used to represent the last node that was visited; this is used by figure 17.30a Iterator inner class implementation for standard LinkedList class (continues) /**  * This is the implementation of the LinkedListIterator.  * It maintains a notion of a current position and of  * course the implicit reference to the LinkedList.  */     private class LinkedListIterator implements ListIterator<AnyType>     {         private Node<AnyType> current;         private Node<AnyType> lastVisited = null;         private boolean lastMoveWasPrev = false;         private int expectedModCount = modCount;         public LinkedListIterator( int idx )         {             current = getNode( idx, 0, size( ) );         }         public boolean hasNext( )         {             if( expectedModCount != modCount )                 throw new ConcurrentModificationException( );             return current != endMarker;         }         public AnyType next( )         {             if( !hasNext( ) )                 throw new NoSuchElementException( );              AnyType nextItem = current.data;             lastVisited = current;             current = current.next;             lastMoveWasPrev = false;             return nextItem;         }

17.5 implementing the collections api LinkedList class remove. If lastVisited is null, the remove is illegal. Finally, lastMoveWasPrev is true if the last movement of the iterator prior to remove was via previous; it is false if the last movement was via next. The hasNext and hasPrevious methods are fairly routine. Both throw an exception if an external modiﬁcation to the list has been detected. The next method advances current (line 32) after getting the value in the node (line 30) that is to be returned (line 34). Data ﬁelds lastVisited and lastMoveWasPrev are updated at lines 31 and 33, respectively. The implementation of previous is not exactly symmetric, because for previous, we advance current prior to obtaining the value. This is evident when one considers that the initial state for backwards iteration is that current is at the endmarker. figure 17.30b Iterator inner class  implementation for  standard LinkedList class (continued) public void remove( )         {             if( expectedModCount != modCount )                 throw new ConcurrentModificationException( );             if( lastVisited == null )                 throw new IllegalStateException( );             LinkedList.this.remove( lastVisited );             lastVisited = null;             if( lastMoveWasPrev )                 current = current.next;             expectedModCount++;         }         public boolean hasPrevious( )         {             if( expectedModCount != modCount )                 throw new ConcurrentModificationException( );             return current != beginMarker.next;         }         public AnyType previous( )         { if( !hasPrevious( ) )                 throw new NoSuchElementException( );              current = current.prev;             lastVisited = current;             lastMoveWasPrev = true;             return current.data;         }     }

chapter 17 linked lists Finally, remove is shown at lines 36–48. After the obligatory error checks, we use the LinkedList remove method to remove the lastVisited node. The explicit reference to the outer class is required because the iterator remove hides the list remove. After making lastVisited null, to disallow a second remove, we check whether the last operation was a next or previous. In the latter case, we adjust current, as shown on line 46, to its state prior to the previous/ remove combination. All in all, there is a large amount of code, but it simply embellishes the basics presented in the original implementation of the nonstandard LinkedList class in Section 17.2.

chapt er trees The tree is a fundamental structure in computer science. Almost all operating systems store ﬁles in trees or treelike structures. Trees are also used in compiler design, text processing, and searching algorithms. We discuss the latter application in Chapter 19. In this chapter, we show n A deﬁnition of a general tree and discuss how it is used in a ﬁle system n An examination of the binary tree n Implementation of tree operations, using recursion n Nonrecursive traversal of a tree 18.1 general trees Trees can be deﬁned in two ways: nonrecursively and recursively. The nonrecursive deﬁnition is the more direct technique, so we begin with it. The recursive formulation allows us to write simple algorithms to manipulate trees.

chapter 18 trees A tree can be  deﬁned nonrecursively as a set of  nodes and a set of  directed edges that  connect them. Parents and children are naturally  deﬁned. A directed  edge connects the  parent to the child. 18.1.1   definitions Nonrecursively, a tree consists of a set of nodes and a set of directed edges that connect pairs of nodes. Throughout this text we consider only rooted trees. A rooted tree has the following properties. n One node is distinguished as the root. n Every node c, except the root, is connected by an edge from exactly one other node p. Node p is c’s parent, and c is one of p’s children. n A unique path traverses from the root to each node. The number of edges that must be followed is the path length. Parents and children are naturally deﬁned. A directed edge connects the parent to the child. A leaf has no  children. Figure 18.1 illustrates a tree. The root node is A; A’s children are B, C, D, and E. Because A is the root, it has no parent; all other nodes have parents. For instance, B’s parent is A. A node that has no children is called a leaf. The leaves in this tree are C, F, G, H, I, and K. The length of the path from A to K is 3 (edges); the length of the path from A to A is 0 (edges). The depth of a node is the length of the  path from the root  to the node. The  height of a node is  the length of the  path from the node  to the deepest leaf. A tree with N nodes must have N – 1 edges because every node except the parent has an incoming edge. The depth of a node in a tree is the length of the path from the root to the node. Thus the depth of the root is always 0, and the depth of any node is 1 more than the depth of its parent. The height of a node in a tree is the length of the path from the node to the deepest leaf. Thus the height of E is 2. The height of any node is 1 more than the height of its maximum-height child. Thus the height of a tree is the height of the root. C D E H I J K A B F G Node A B C D E F G H I J K Height Depth figure 18.1 A tree, with height and  depth information

18.1 general trees The size of a node is the number of  descendants the  node has (including  the node itself). Nodes with the same parent are called siblings; thus B, C, D, and E are all siblings. If there is a path from node u to node v, then u is an ancestor of v and v is a descendant of u. If u ≠v, then u is a proper ancestor of v and v is a proper descendant of u. The size of a node is the number of descendants the node has (including the node itself). Thus the size of B is 3, and the size of C is 1. The size of a tree is the size of the root. Thus the size of the tree shown in Figure 18.1 is the size of its root A, or 11. An alternative deﬁnition of the tree is recursive: Either a tree is empty or it consists of a root and zero or more nonempty subtrees T1, T2, ..., Tk, each of whose roots are connected by an edge from the root, as illustrated in Figure 18.2. In certain instances (most notably, the binary trees discussed later in the chapter), we may allow some of the subtrees to be empty. 18.1.2   implementation General trees can  be implemented by  using the ﬁrst child/ next sibling method, which requires two  links per item. One way to implement a tree would be to have in each node a link to each child of the node in addition to its data. However, as the number of children per node can vary greatly and is not known in advance, making the children direct links in the data structure might not be feasible—there would be too much wasted space. The solution—called the ﬁrst child/next sibling method— is simple: Keep the children of each node in a linked list of tree nodes, with each node keeping two links, one to its leftmost child (if it is not a leaf) and one to its right sibling (if it is not the rightmost sibling). This type of implementation is illustrated in Figure 18.3. Arrows that point downward are firstChild links, and arrows that point left to right are nextSibling links. We did not draw null links because there are too many of them. In this tree, node B has both a link to a sibling (C) and a link to a leftmost child (F); some nodes have only one of these links and some have neither. Given this representation, implementing a tree class is straightforward. figure 18.2 A tree viewed  recursively Root T1 T2 T3 Tk  •  •  •

chapter 18 trees 18.1.3   an application: file systems File systems use  treelike structures. Trees have many applications. One of their popular uses is the directory structure in many operating systems, including Unix, VAX/VMS, and Windows/ DOS. Figure 18.4 shows a typical directory in the Unix ﬁle system. The root of this directory is mark. (The asterisk next to the name indicates that mark is itself a directory.) Note that mark has three children: books, courses, and .login, two of which are themselves directories. Thus mark contains two directories and one regular ﬁle. The ﬁlename mark/books/dsaa/ch1 is obtained by following the leftmost child three times. Each / after the ﬁrst name indicates an edge; the result is a pathname. If the path begins at the root of the entire ﬁle system, rather than at an arbitrary directory inside the ﬁle system, it is a full pathname; otherwise, it is a relative pathname (to the current directory). This hierarchical ﬁle system is popular because it allows users to organize their data logically. Furthermore, two ﬁles in different directories can share the same name because they have different paths from the root and thus have figure 18.3 First child/next sibling  representation of the  tree in Figure 18.1 C D E H I J K A B F G figure 18.4 A Unix directory ch1 ch2 dsaa* ch1 ch2 ipps* ch1 ch2 ecp* books* syl cop3223* courses* .login syl cop3530* mark*

18.1 general trees different full pathnames. A directory in the Unix ﬁle system is just a ﬁle with a list of all its children,1 so the directories can be traversed with an iteration scheme; that is, we can sequentially iterate over each child. Indeed, on some systems, if the normal command to print a ﬁle is applied to a directory, the ﬁlenames in the directory appear in the output (along with other non-ASCII information). The directory structure is most easily  traversed by using  recursion. Suppose that we want to list the names of all the ﬁles in a directory (including its subdirectories), and in our output format ﬁles of depth d have their names indented by d tab characters. A short algorithm to do this task is given in Figure 18.5. Output for the directory presented in Figure 18.4 is shown in Figure 18.6. 1. Each directory in the Unix ﬁle system also has one entry (.) that points to itself and another entry (..) that points to the parent of the directory, which introduces a cycle. Thus, technically, the Unix ﬁle system is not a tree but is treelike. The same is true for Windows/DOS. figure 18.5 A routine for listing a  directory and its  subdirectories in a  hierarchical file  system void listAll( int depth = 0 ) // depth is initially 0 {         printName( depth );       // Print the name of the object         if( isDirectory( ) )             for each file c in this directory (for each child)                 c.listAll( depth + 1 ); } figure 18.6 The directory listing  for the tree shown in  Figure 18.4 mark           books                 dsaa                         ch1                         ch2                 ecp                         ch1                         ch2                 ipps                         ch1                         ch2           courses                 cop3223                         syl                 cop3530                         syl           .login

chapter 18 trees We assume the existence of the class FileSystem and two methods, printName and isDirectory. printName outputs the current FileSystem object indented by depth tab stops; isDirectory tests whether the current FileSystem object is a directory, returning true if it is. Then we can write the recursive routine listAll. We need to pass it the parameter depth, indicating the current level in the directory relative to the root. The listAll routine is started with depth 0 to signify no indenting for the root. This depth is an internal bookkeeping variable and is hardly a parameter about which a calling routine should be expected to know. Thus the pseudocode speciﬁes a default value of 0 for depth (speciﬁcation of a default value is not legal Java). The logic of the algorithm is simple to follow. The current object is printed out, with appropriate indentation. If the entry is a directory, we process all the children recursively, one by one. These children are one level deeper in the tree and thus must be indented an extra tab stop. We make the recursive call with depth+1. It is hard to imagine a shorter piece of code that performs what appears to be a very difﬁcult task. In this algorithmic technique, known as a preorder tree traversal, work at a node is performed before (pre) its children are processed. In addition to being a compact algorithm, the preorder traversal is efﬁcient because it takes constant time per node. We discuss why later in this chapter. Another common method of traversing a tree is the postorder tree traversal, in which the work at a node is performed after (post) its children are evaluated. It also takes constant time per node. As an example, Figure 18.7 represents the same directory structure as that shown in Figure 18.4. The numbers in parentheses represent the number of disk blocks taken up by each ﬁle. The directories themselves are ﬁles, so they also use disk blocks (to store the names and information about their children). Suppose that we want to compute the total number of blocks used by all ﬁles in our example tree. The most natural way to do so is to ﬁnd the total figure 18.7 The Unix directory  with file sizes mark*(1) ch1(9) ch2(7) dsaa*(1) ch1(3) ch2(8) ipps*(1) ch1(4) ch2(6) ecp*(1) books*(1) syl(2) cop3223*(1) courses*(1) .login(2) syl(3) cop3530*(1) In a preorder tree  traversal, work at a  node is performed  before its children  are processed. The  traversal takes constant time per node. In a postorder tree  traversal, work at a  node is performed  after its children are  evaluated. The traversal takes constant time per node.

18.1 general trees number of blocks contained in all the children (which may be directories that must be evaluated recursively): books (41), courses (8), and .login (2). The total number of blocks is then the total in all the children plus the blocks used at the root (1), or 52. The size routine shown in Figure 18.8 implements this strategy. If the current FileSystem object is not a directory, size merely returns the number of blocks it uses. Otherwise, the number of blocks in the current directory is added to the number of blocks (recursively) found in all the children. To illustrate the difference between postorder traversal and preorder traversal, in Figure 18.9 we show how the size of each directory (or ﬁle) is produced by the algorithm. We get a classic postorder signature because the total size of an entry is not computable until the information for its children has been computed. As indicated previously, the running time is linear. We have much more to say about tree traversals in Section 18.4. figure 18.8 A routine for  calculating the total  size of all files in a  directory int size( )     {         int totalSize = sizeOfThisFile( );         if( isDirectory( ) )             for each file c in this directory (for each child)                 totalSize += c.size( );         return totalSize;     } figure 18.9 A trace of the size method                         ch1                         9                         ch2                         7                 dsaa                               17                         ch1                         4                         ch2                         6                 ecp                                11                         ch1                         3                         ch2                         8                 ipps                               12         books                                      41                         syl                         2                 cop3223                             3                         syl                         3                 cop3530                             4         courses                                     8         .login                                      2 mark                                               52

chapter 18 trees java implementation Java provides a class named File in package java.io that can be used to traverse directory hierarchies. We can use it to implement the pseudocode in Figure 18.8. The size method can also be implemented; this is done in the online code. The class File provides several useful methods. A File can be constructed by providing a ﬁlename. getName provides the name of a File object. It does not include the directory part of the path; this can be obtained by getPath. isDirectory returns true if the File is a directory, and its size in bytes can be obtained by a call to length. If the ﬁle is a directory, the listFiles method returns an array of File that represents the ﬁles in the directory (not including . and ..). To implement the logic described in the pseudocode, we simply provide printName, listAll, (both a public driver and a private recursive routine), and size as shown in Figure 18.10. We also provide a simple main that tests the logic on the current directory. 18.2  binary trees A binary tree has no  node with more  than two children. A binary tree is a tree in which no node can have more than two children. Because there are only two children, we can name them left and right. Recursively, a binary tree is either empty or consists of a root, a left tree, and a right tree. The left and right trees may themselves be empty; thus a node with one child could have either a left or right child. We use the recursive definition several times in the design of binary tree algorithms. Binary trees have many important uses, two of which are illustrated in Figure 18.11. An expression tree  is one example of  the use of binary  trees. Such trees  are central data  structures in compiler design. One use of the binary tree is in the expression tree, which is a central data structure in compiler design. The leaves of an expression tree are operands, such as constants or variable names; the other nodes contain operators. This particular tree is binary because all the operations are binary. Although this case is the simplest, nodes can have more than two children (and in the case of unary operators, only one child). We can evaluate an expression tree T by applying the operator at the root to the values obtained by recursively evaluating the left and right subtrees. Doing so yields the expression (a+((b-c)*d)). (See Section 11.2 for a discussion of the construction of expression trees and their evaluation.) A second use of the binary tree is the Huffman coding tree, which is used to implement a simple but relatively effective data compression algorithm. Each symbol in the alphabet is stored at a leaf. Its code is obtained by following the path to it from the root. A left link corresponds to a 0 and a right link

18.2 binary trees figure 18.10 Java implementation  for a directory listing import java.io.File; public class FileSystem {       // Output file name with indentation     public static void printName( String name, int depth )     {         for( int i = 0; i < depth; i++ )             System.out.print( " " );         System.out.println( name );     }       // Public driver to list all files in directory     public static void listAll( File dir )     {         listAll( dir, 0 );     }       // Recursive method to list all files in directory     private static void listAll( File dir, int depth )     {         printName( dir.getName( ), depth );         if( dir.isDirectory( ) )             for( File child : dir.listFiles( ) )                 listAll( child, depth + 1 );     }     public static long size( File dir )     {         long totalSize = dir.length( );         if( dir.isDirectory( ) )             for( File child : dir.listFiles( ) )                 totalSize += size( child );         return totalSize;     }       // Simple main to list all files in current directory     public static void main( String [ ] args )     {         File dir = new File( "." );         listAll( dir );         System.out.println( "Total bytes: " + size( dir ) );     } }

chapter 18 trees The BinaryNode class is implemented separately  from the BinaryTree class. The only data  member in the  BinaryTree class is  a reference to the  root node. to a 1. Thus b is coded as 100. (See Section 12.1 for a discussion of the construction of the optimal tree, that is, the best code.) An important use of  binary trees is in  other data structures, notably the  binary search tree  and the priority  queue. Other uses of the binary tree are in binary search trees (discussed in Chapter 19), which allow logarithmic time insertions and accessing of items, and priority queues, which support the access and deletion of the minimum in a collection of items. Several efﬁcient implementations of priority queues use trees (discussed in Chapters 21–23). Figure 18.12 gives the skeleton for the BinaryNode class. Lines 49–51 indicate that each node consists of a data item plus two links. The constructor, shown at lines 18 to 20, initializes all the data members of the BinaryNode class. Lines 22–33 provide accessors and mutators for each of the data members. Many of the  BinaryNode routines are recursive.  The BinaryTree methods use the  BinaryNode routines on the root. The duplicate method, declared at line 39, is used to replicate a copy of the tree rooted at the current node. The routines’ size and height, declared at lines 35 and 37, compute the named properties for the node referenced by parameter t. We implement these routines in Section 18.3. (Recall that static methods do not require a controlling object.) We also provide, at lines 42–47, routines that print out the contents of a tree rooted at the current node, using various recursive traversal strategies. We discuss tree traversals in Section 18.4. Why do we pass a parameter for size and height and make them static but use the current object for the traversals and duplicate? There is no particular reason; it is a matter of style, and we show both styles here. The implementations show that the difference between them occurs when the required test for an empty tree (given by a null reference) is performed. In this section we describe implementation of the BinaryTree class. The BinaryNode class is implemented separately, instead of as a nested class. The BinaryTree class skeleton is shown in Figure 18.13. For the most part, the routines are short because they call BinaryNode methods. Line 44 declares the only data member—a reference to the root node. figure 18.11 Uses of binary trees:  (a) an expression tree  and (b) a Huffman  coding tree a + * d – b c (a) a d b c (b)

18.2 binary trees figure 18.12 The BinaryNode class  skeleton // BinaryNode class; stores a node in a tree. 2 // 3 // CONSTRUCTION: with no parameters, or an Object, 4 //     left child, and right child. 5 // 6 // *******************PUBLIC OPERATIONS********************** 7 // int size( )            --> Return size of subtree at node 8 // int height( )          --> Return height of subtree at node 9 // void printPostOrder( ) --> Print a postorder tree traversal 10 // void printInOrder( )   --> Print an inorder tree traversal 11 // void printPreOrder( )  --> Print a preorder tree traversal 12 // BinaryNode duplicate( )--> Return a duplicate tree 14 class BinaryNode<AnyType> 15 { 16     public BinaryNode( ) 17       { this( null, null, null ); } 18     public BinaryNode( AnyType theElement, 19                        BinaryNode<AnyType> lt, BinaryNode<AnyType> rt ) 20       { element = theElement; left = lt; right = rt; } 22     public AnyType getElement( ) 23       { return element; } 24     public BinaryNode<AnyType> getLeft( ) 25       { return left; } 26     public BinaryNode<AnyType> getRight( ) 27       { return right; } 28     public void setElement( AnyType x ) 29       { element = x; } 30     public void setLeft( BinaryNode<AnyType> t ) 31       { left = t; } 32     public void setRight( BinaryNode<AnyType> t ) 33       { right = t; } 35     public static <AnyType> int size( BinaryNode<AnyType> t ) 36       { /* Figure 18.19 */ } 37     public static <AnyType> int height( BinaryNode<AnyType> t ) 38       { /* Figure 18.21 */ } 39     public BinaryNode<AnyType> duplicate( ) 40       { /* Figure 18.17 */ } 42     public void printPreOrder( ) 43       { /* Figure 18.22 */ } 44     public void printPostOrder( ) 45       { /* Figure 18.22 */ } 46     public void printInOrder( ) 47       { /* Figure 18.22 */ } 49     private AnyType             element; 50     private BinaryNode<AnyType> left; 51     private BinaryNode<AnyType> right; 52 }

chapter 18 trees Two basic constructors are provided. The one at lines 16 and 17 creates an empty tree, and the one at lines 18 and 19 creates a one-node tree. Routines to traverse the tree are written at lines 28–33. They apply a BinaryNode method to figure 18.13 The BinaryTree class,  except for merge // BinaryTree class; stores a binary tree. 2 // 3 // CONSTRUCTION: with (a) no parameters or (b) an object to 4 //    be placed in the root of a one-element tree. 5 // 6 // *******************PUBLIC OPERATIONS********************** 7 // Various tree traversals, size, height, isEmpty, makeEmpty. 8 // Also, the following tricky method: 9 // void merge( Object root, BinaryTree t1, BinaryTree t2 ) 10 //                        --> Construct a new tree 11 // *******************ERRORS********************************* 12 // Error message printed for illegal merges. 14 public class BinaryTree<AnyType> 15 { 16     public BinaryTree( ) 17       { root = null; } 18     public BinaryTree( AnyType rootItem ) 19       { root = new BinaryNode<AnyType>( rootItem, null, null ); } 21     public BinaryNode<AnyType> getRoot( ) 22       { return root; } 23     public int size( ) 24       { return BinaryNode.size( root ); } 25     public int height( ) 26       { return BinaryNode.height( root ); } 28     public void printPreOrder( ) 29       { if( root != null ) root.printPreOrder( ); } 30     public void printInOrder( ) 31       { if( root != null ) root.printInOrder( ); } 32     public void printPostOrder( ) 33       { if( root != null ) root.printPostOrder( ); } 35     public void makeEmpty( ) 36       { root = null; } 37     public boolean isEmpty( ) 38       { return root == null; } 40     public void merge( AnyType rootItem, 41                        BinaryTree<AnyType> t1, BinaryTree<AnyType> t2 ) 42       { /* Figure 18.16 */ } 44     private BinaryNode<AnyType> root; 45 }

18.2 binary trees the root, after verifying that the tree is not empty. An alternative traversal strategy that can be implemented is level-order traversal. We discuss these traversal routines in Section 18.4. Routines to make an empty tree and test for emptiness are given, with their inline implementations, at lines 35 to 38, as are routines to compute the tree’s size and height. Note that, as size and height are static methods in BinaryNode, we can call them by simply using BinaryNode.size and BinaryNode.height. The last method in the class is the merge routine, which uses two trees—t1 and t2—and an element to create a new tree, with the element at the root and the two existing trees as left and right subtrees. In principle, it is a one-liner: root = new BinaryNode<AnyType>( rootItem, t1.root, t2.root ); If things were always this simple, programmers would be unemployed. Fortunately for our careers, there are a host of complications. Figure 18.14 shows the result of the simple one-line merge. A problem becomes apparent: Nodes in t1 and t2’s trees are now in two trees (their original trees and the merged result). This sharing is a problem if we want to remove or otherwise alter subtrees (because multiple subtrees may be removed or altered unintentionally). If the two input  trees are aliases,  we should disallow  the operation  unless the trees  are empty. The solution is simple in principle. We can ensure that nodes do not appear in two trees by setting t1.root and t2.root to null after the merge. Complications ensue when we consider some possible calls that contain aliasing: t1.merge( x, t1, t2 ); t2.merge( x, t1, t2 ); t1.merge( x, t3, t3 ); The ﬁrst two cases are similar, so we consider only the ﬁrst one. A diagram of the situation is shown in Figure 18.15. Because t1 is an alias for the current object, t1.root and root are aliases. Thus, after the call to new, if we execute t1.root=null, we change root to the null reference, too. Consequently, we need to be very careful with the aliases for these cases. figure 18.14 Result of a naive  merge operation:  Subtrees are shared. root t1.root t2.root x We set the original  trees’ root to null so that each node  is in one tree. The merge routine is  a one-liner in principle. However, we  must also handle  aliasing, ensure  that a node is not in  two trees, and  check for errors.

chapter 18 trees If an input tree is  aliased to the output tree, we must  avoid having the  resultant root reference being set to  null. The third case must be disallowed because it would place all the nodes that are in tree t3 in two places in t1. However, if t3 represents an empty tree, the third case should be allowed. All in all, we got a lot more than we bargained for. The resulting code is shown in Figure 18.16. What used to be a one-line routine has gotten quite large. figure 18.15 Aliasing problems in  the merge operation;  t1 is also the current  object. root t1.root t2.root x root t1.root old old figure 18.16 The merge routine for the BinaryTree class /**      * Merge routine for BinaryTree class.      * Forms a new tree from rootItem, t1 and t2.      * Does not allow t1 and t2 to be the same.      * Correctly handles other aliasing conditions.      */     public void merge( AnyType rootItem,                        BinaryTree<AnyType> t1, BinaryTree<AnyType> t2 )     {         if( t1.root == t2.root && t1.root != null )     throw new IllegalArgumentException( );             // Allocate new node         root = new BinaryNode<AnyType>( rootItem, t1.root, t2.root );             // Ensure that every node is in one tree         if( this != t1 )             t1.root = null;         if( this != t2 )             t2.root = null;     }

18.3 recursion and trees 18.3 recursion and trees Recursive routines  are used for size and duplicate. Because trees can be deﬁned recursively, many tree routines, not surprisingly, are most easily implemented by using recursion. Recursive implementations for almost all the remaining BinaryNode and BinaryTree methods are provided here. The resulting routines are amazingly compact. Because duplicate is a BinaryNode method, we make  recursive calls only  after verifying that  the subtrees are  not null. We begin with the duplicate method of the BinaryNode class. Because it is a BinaryNode method, we are assured that the tree we are duplicating is not empty. The recursive algorithm is then simple. First, we create a new node with the same data ﬁeld as the current root. Then we attach a left tree by calling duplicate recursively and attach a right tree by calling duplicate recursively. In both cases, we make the recursive call after verifying that there is a tree to copy. This description is coded verbatim in Figure 18.17. The size routine is  easily implemented  recursively after a  drawing is made. The next method we write is the size routine in the BinaryNode class. It returns the size of the tree rooted at a node referenced by t, which is passed as a parameter. If we draw the tree recursively, as shown in Figure 18.18, we see that the size of a tree is the size of the left subtree plus the size of the right figure 18.17 A routine for returning  a copy of the tree  rooted at the current  node /**      * Return a reference to a node that is the root of a      * duplicate of the binary tree rooted at the current node.      */     public BinaryNode<AnyType> duplicate( )     {         BinaryNode<AnyType> root =                    new BinaryNode<AnyType>( element, null, null );         if( left != null )            // If there's a left subtree             root.left = left.duplicate( );    // Duplicate; attach         if( right != null )          // If there's a right subtree             root.right = right.duplicate( );  // Duplicate; attach         return root;                      // Return resulting tree     } figure 18.18 Recursive view used  to calculate the size of  a tree:  ST = SL + SR + 1. SL SR

chapter 18 trees subtree plus 1 (because the root counts as a node). A recursive routine requires a base case that can be solved without recursion. The smallest tree that size might have to handle is the empty tree (if t is null), and the size of an empty tree is clearly 0. We should verify that the recursion produces the correct answer for a tree of size 1. Doing so is easy, and the recursive routine is implemented as shown in Figure 18.19. The ﬁnal recursive routine presented in this section calculates the height of a node. Implementing this routine is difﬁcult to do nonrecursively but is trivial recursively, once we have made a drawing. Figure 18.20 shows a tree viewed recursively. Suppose that the left subtree has height HL and the right subtree has height HR. Any node that is d levels deep with respect to the root of the left subtree is   levels deep with respect to the root of the entire tree. The same holds for the right subtree. Thus the path length of the deepest node in the original tree is 1 more than its path length with respect to the root of its subtree. If we compute this value for both subtrees, the maximum of these two values plus 1 is the answer we want. The code for doing so is shown in Figure 18.21. figure 18.19 A routine for  computing the size of  a node /**      * Return the size of the binary tree rooted at t.      */     public static <AnyType> int size( BinaryNode<AnyType> t )     {         if( t == null )             return 0;         else             return 1 + size( t.left ) + size( t.right );     } figure 18.20 Recursive view of the node height calculation: HT = Max (HL + 1, HR + 1) HL +1 HL HR HR +1 d + The height routine  is also easily implemented recursively.  The height of an  empty tree is –1.

18.4 tree traversal: iterator classes 18.4 tree traversal: iterator classes In this chapter we have shown how recursion can be used to implement the binary tree methods. When recursion is applied, we compute information about not only a node but also about all its descendants. We say then that we are traversing the tree. Two popular traversals that we have already mentioned are the preorder and postorder traversals. In a preorder traversal, the node is processed and then its children are processed recursively. The duplicate routine is an example of a preorder traversal because the root is created ﬁrst. Then a left subtree is copied recursively, followed by copying the right subtree. In a postorder traversal, the node is processed after both children are processed recursively. Two examples are the methods size and height. In both cases, information about a node (e.g., its size or height) can be obtained only after the corresponding information is known for its children. In an inorder traversal, the current  node is processed between recursive  calls. A third common recursive traversal is the inorder traversal, in which the left child is recursively processed, the current node is processed, and the right child is recursively processed. This mechanism is used to generate an algebraic expression corresponding to an expression tree. For example, in Figure 18.11 the inorder traversal yields (a+((b-c)*d)). Simple traversal  using any of these  strategies takes linear time. Figure 18.22 illustrates routines that print the nodes in a binary tree using each of the three recursive tree traversal algorithms. Figure 18.23 shows the order in which nodes are visited for each of the three strategies. The running time of each algorithm is linear. In every case, each node is output only once. Consequently, the total cost of an output statement over any traversal is O(N). As a result, each if statement is also executed at most once per node, for a total cost of O(N). The total number of method calls made (which involves the constant work of the internal run-time stack pushes and pops) is likewise once per node, or O(N). Thus the total running time is O(N). figure 18.21 A routine for  computing the height  of a node /**      * Return the height of the binary tree rooted at t.      */     public static <AnyType> int height( BinaryNode<AnyType> t )     {         if( t == null )             return -1;         else             return 1 + Math.max( height( t.left ), height( t.right ) );     }

chapter 18 trees figure 18.22 Routines for printing  nodes in preorder,  postorder, and inorder // Print tree rooted at current node using preorder traversal.     public void printPreOrder( )     {         System.out.println( element );       // Node         if( left != null )             left.printPreOrder( );           // Left         if( right != null )             right.printPreOrder( );          // Right     }     // Print tree rooted at current node using postorder traversal.     public void printPostOrder( )     {         if( left != null )                   // Left             left.printPostOrder( );         if( right != null )                  // Right             right.printPostOrder( );          System.out.println( element );       // Node     }     // Print tree rooted at current node using inorder traversal.     public void printInOrder( )     {         if( left != null )                   // Left             left.printInOrder( );         System.out.println( element );       // Node         if( right != null )             right.printInOrder( );           // Right     } figure 18.23 (a) Preorder,  (b) postorder, and  (c) inorder visitation  routes (b) (a) (c)

18.4 tree traversal: iterator classes We can traverse  nonrecursively by  maintaining the  stack ourselves.  Must we use recursion to implement the traversals? The answer is clearly no because, as discussed in Section 7.3, recursion is implemented by using a stack. Thus we could keep our own stack.2 We might expect that a somewhat faster program could result because we can place only the essentials on the stack rather than have the compiler place an entire activation record on the stack. The difference in speed between a recursive and nonrecursive algorithm is very dependent on the platform, and on modern computers may well be negligible. It is possible for instance, that if an array-based stack is used, the bounds checks that must be performed for all array access could be signiﬁcant; the run-time stack might not be subjected to such tests if an aggressive optimizing compiler proves that a stack underﬂow is impossible. Thus in many cases, the speed improvement does not justify the effort involved in removing recursion. Even so, knowing how to do so is worthwhile, in case your platform is one that would beneﬁt from recursion removal and also because seeing how a program is implemented nonrecursively can sometimes make the recursion clearer. An iterator class  allows step-by-step  traversal. We write three iterator classes, each in the spirit of the linked list. Each allows us to go to the ﬁrst node, advance to the next node, test whether we have gone past the last node, and access the current node. The order in which nodes are accessed is determined by the type of traversal. We also implement a level-order traversal, which is inherently nonrecursive and in fact uses a queue instead of a stack and is similar to the preorder traversal. The abstract tree  iterator class has methods similar to  those of the linked  list iterator. Each  type of traversal is  represented by a  derived class. Figure 18.24 provides an abstract class for tree iteration. Each iterator stores a reference to the tree root and an indication of the current node.3 These are declared at lines 47 and 48, respectively, and initialized in the constructor. They are protected to allow the derived classes to access them. Four methods are declared at lines 22–42. The isValid and retrieve methods are invariant over the hierarchy, so an implementation is provided and they are declared final. The abstract methods first and advance must be provided by each type of iterator. This iterator is similar to the linked list iterator (LinkedListIterator, in Section 17.2), except that here the first method is part of the tree iterator, whereas in the linked list the first method was part of the list class itself. 2. We can also add parent links to each tree node to avoid both recursion and stacks. In this chapter we demonstrate the relation between recursion and stacks, so we do not use parent links. 3. In these implementations, once the iterators have been constructed, structurally modifying the tree during an iteration is unsafe because references may become stale.

chapter 18 trees figure 18.24 The tree iterator abstract base class import java.util.NoSuchElementException; 3 // TreeIterator class; maintains "current position" 4 // 5 // CONSTRUCTION: with tree to which iterator is bound 6 // 7 // ******************PUBLIC OPERATIONS********************** 8 //     first and advance are abstract; others are final 9 // boolean isValid( ) --> True if at valid position in tree 10 // AnyType retrieve( ) --> Return item in current position 11 // void first( ) --> Set current position to first 12 // void advance( ) --> Advance (prefix) 13 // ******************ERRORS********************************* 14 // Exceptions thrown for illegal access or advance 16 abstract class TreeIterator<AnyType> 17 { 18     /** 19      * Construct the iterator. The current position is set to null. 20      * @param theTree the tree to which the iterator is bound. 21      */ 22     public TreeIterator( BinaryTree<AnyType> theTree ) 23       { t = theTree; current = null; } 25     /** 26      * Test if current position references a valid tree item. 27      * @return true if the current position is not null; false otherwise. 28      */ 29     final public boolean isValid( ) 30       { return current != null; } 32     /** 33      * Return the item stored in the current position. 34      * @return the stored item. 35      * @exception NoSuchElementException if the current position is invalid. 36      */ 37     final public AnyType retrieve( ) 38     { 39         if( current == null ) 40             throw new NoSuchElementException( ); 41         return current.getElement( ); 42     } 44     abstract public void first( ); 45     abstract public void advance( ); 47     protected BinaryTree<AnyType> t;          // The tree root 48     protected BinaryNode<AnyType> current;    // The current position 49 }

18.4 tree traversal: iterator classes 18.4.1   postorder traversal Postorder traversal  maintains a stack  that stores nodes  that have been visited but whose  recursive calls are  not yet complete. The postorder traversal is implemented by using a stack to store the current state. The top of the stack will represent the node that we are visiting at some instant in the postorder traversal. However, we may be at one of three places in the algorithm: 1. About to make a recursive call to the left subtree 2. About to make a recursive call to the right subtree 3. About to process the current node Consequently, each node is placed on the stack three times during the course of the traversal. If a node is popped from the stack a third time, we can mark it as the current node to be visited. Each node is  placed on the stack  three times. The  third time off, the  node is declared  visited. The other  times, we simulate  a recursive call. Otherwise, the node is being popped for either the ﬁrst time or the second time. In this case, it is not yet ready to be visited, so we push it back onto the stack and simulate a recursive call. If the node was popped for a ﬁrst time, we need to push the left child (if it exists) onto the stack. Otherwise, the node was popped for a second time, and we push the right child (if it exists) onto the stack. In any event, we then pop the stack, applying the same test. Note that, when we pop the stack, we are simulating the recursive call to the appropriate child. If the child does not exist and thus was never pushed onto the stack, when we pop the stack we pop the original node again. When the stack is  empty, every node  has been visited. Eventually, either the process pops a node for the third time or the stack empties. In the latter case, we have iterated over the entire tree. We initialize the algorithm by pushing a reference to the root onto the stack. An example of how the stack is manipulated is shown in Figure 18.25. A quick

chapt er binary search trees For large amounts of input, the linear access time of linked lists is prohibitive. In this chapter we look at an alternative to the linked list: the binary search tree, a simple data structure that can be viewed as extending the binary search algorithm to allow insertions and deletions. The running time for most operations is O(log N) on average. Unfortunately, the worst-case time is O(N) per operation. In this chapter, we show n The basic binary search tree n A method for adding order statistics (i.e., the findKth operation) n Three different ways to eliminate the O(N) worst case (namely, the AVL tree, red-black tree, and AA-tree) n Implementation of the Collections API TreeSet and TreeMap n Use of the B-tree to search a large database quickly 19.1 basic ideas In the general case, we search for an item (or element) by using its key. For instance, a student transcript could be searched on the basis of a student ID number. In this case, the ID number is referred to as the item’s key.

chapter 19 binary search trees For any node in the  binary search tree, all smaller keyed  nodes are in the  left subtree and all  larger keyed nodes  are in the right subtree. Duplicates are  not allowed. The binary search tree satisﬁes the search order property; that is, for every node X in the tree, the values of all the keys in the left subtree are smaller than the key in X and the values of all the keys in the right subtree are larger than the key in X. The tree shown in Figure 19.1(a) is a binary search tree, but the tree shown in Figure 19.1(b) is not because key 8 does not belong in the left subtree of key 7. The binary search tree property implies that all the items in the tree can be ordered consistently (indeed, an inorder traversal yields the items in sorted order). This property also does not allow duplicate items. We could easily allow duplicate keys; storing different items having identical keys in a secondary structure is generally better. If these items are exact duplicates, having one item and keeping a count of the number of duplicates is best. binary search tree order property In a binary search tree, for every node X, all keys in X’s left subtree have smaller values than the key in X, and all keys in X’s right subtree have larger values than the key in X. 19.1.1   the operations A find operation is  performed by  repeatedly branching either left or  right, depending on  the result of a  comparison. For the most part, the operations on a binary search tree are simple to visualize. We can perform a find operation by starting at the root and then repeatedly branching either left or right, depending on the result of a comparison. For instance, to ﬁnd 5 in the binary search tree shown in Figure 19.1(a), we start at 7 and go left. This takes us to 2, so we go right, which takes us to 5. To look for 6, we follow the same path. At 5, we would go right and encounter a null link and thus not ﬁnd 6, as shown in Figure 19.2(a). Figure 19.2(b) shows that 6 can be inserted at the point at which the unsuccessful search terminated. The binary search tree efﬁciently supports the findMin and findMax operations. To perform a findMin, we start at the root and repeatedly branch left as long as there is a left child. The stopping point is the smallest element. The (a) (b) figure 19.1 Two binary trees: (a) a  search tree; (b) not a  search tree

19.1 basic ideas findMax operation is similar, except that branching is to the right. Note that the cost of all the operations is proportional to the number of nodes on the search path. The cost tends to be logarithmic, but it can be linear in the worst case. We establish this result later in the chapter. The hardest operation is remove. Once we have found the node to be removed, we need to consider several possibilities. The problem is that the removal of a node may disconnect parts of the tree. If that happens, we must carefully reattach the tree and maintain the binary search tree property. We also want to avoid making the tree unnecessarily deep because the depth of the tree affects the running time of the tree algorithms. When we are designing a complex algorithm, solving the simplest case ﬁrst is often easiest, leaving the most complicated case until last. Thus, in examining the various cases, we start with the easiest. If the node is a leaf, its removal does not disconnect the tree, so we can delete it immediately. If the node has only one child, we can remove the node after adjusting its parent’s child link to bypass the node. This is illustrated in Figure 19.3, with the removal of node 5. Note that removeMin and removeMax are not complex because the affected nodes are either leaves or have only one child. Note also that the root is a special case because it does not have a parent. However, (a) (b) figure 19.2 Binary search trees  (a) before and  (b) after the insertion  of 6 (a) (b) figure 19.3 Deletion of node 5  with one child:  (a) before and  (b) after The findMin operation is performed by  following left nodes  as long as there is a  left child. The  findMax operation is  similar. The remove operation is difﬁcult  because nonleaf  nodes hold the tree  together and we do  not want to disconnect the tree. If a node has one  child, it can be  removed by having  its parent bypass it.  The root is a special  case because it  does not have a  parent.

chapter 19 binary search trees when the remove method is implemented, the special case is handled automatically. A node with two  children is replaced  by using the smallest item in the right  subtree. Then  another node is  removed. The complicated case deals with a node having two children. The general strategy is to replace the item in this node with the smallest item in the right subtree (which is easily found, as mentioned earlier) and then remove that node (which is now logically empty). The second remove is easy to do because, as just indicated, the minimum node in a tree does not have a left child. Figure 19.4 shows an initial tree and the result of removing node 2. We replace the node with the smallest node (3) in its right subtree and then remove 3 from the right subtree. Note that in all cases removing a node does not make the tree deeper.1 Many alternatives do make the tree deeper; thus these alternatives are poor options. 19.1.2   java implementation In principle, the binary search tree is easy to implement. To keep the Java features from clogging up the code, we introduce a few simpliﬁcations. First, Figure 19.5 shows the BinaryNode class. In the new BinaryNode class, we make everything package-visible. More typically, BinaryNode would be a nested class. The BinaryNode class contains the usual list of data members (the item and two links). The BinarySearchTree class skeleton is shown in Figure 19.6. The only data member is the reference to the root of the tree, root. If the tree is empty, root is null. The public BinarySearchTree class methods have implementations that call the hidden methods. The constructor, declared at line 21, merely sets root to null. The publicly visible methods are listed at lines 24–39. 1. The deletion can, however, increase the average node depth if a shallow node is removed. (a) (b) figure 19.4 Deletion of node 2  with two children:  (a) before and  (b) after The root references at the  root of the tree,  which is null if the  tree is empty. The public class  functions call  hidden private  routines.

19.1 basic ideas figure 19.5 The BinaryNode class  for the binary search  tree 1 package weiss.nonstandard; 3 // Basic node stored in unbalanced binary search trees 4 // Note that this class is not accessible outside 5 // this package. 7 class BinaryNode<AnyType> 8 { 9         // Constructor 10     BinaryNode( AnyType theElement ) 11     { 12         element = theElement; 13         left = right = null; 14     } 16       // Data; accessible by other package routines 17     AnyType             element;  // The data in the node 18     BinaryNode<AnyType> left;     // Left child 19     BinaryNode<AnyType> right;    // Right child } figure 19.6a The BinarySearchTree class skeleton (continues) package weiss.nonstandard; // BinarySearchTree class // // CONSTRUCTION: with no initializer // // ******************PUBLIC OPERATIONS********************* // void insert( x )       --> Insert x // void remove( x )       --> Remove x // void removeMin( )      --> Remove minimum item // Comparable find( x )   --> Return item that matches x // Comparable findMin( )  --> Return smallest item // Comparable findMax( )  --> Return largest item // boolean isEmpty( )     --> Return true if empty; else false // void makeEmpty( )      --> Remove all items // ******************ERRORS******************************** // Exceptions are thrown by insert, remove, and removeMin if warranted public class BinarySearchTree<AnyType extends Comparable<? super AnyType>> {     public BinarySearchTree( )       {  root = null; }     public void insert( AnyType x )       { root = insert( x, root ); }

chapter 19 binary search trees Next, we have several methods that operate on a node passed as a parameter, a general technique that we used in Chapter 18. The idea is that the publicly visible class routines call these hidden routines and pass root as a parameter. These hidden routines do all the work. In a few places, we use protected rather than private because we derive another class from BinarySearchTree in Section 19.2. The insert method adds x to the current tree by calling the hidden insert with root as an additional parameter. This action fails if x is already in the tree; in that case, a DuplicateItemException would be thrown. The findMin, findMax, and find operations return the minimum, maximum, or named item (respectively) from the tree. If the item is not found because the tree is empty figure 19.6b The BinarySearchTree class skeleton (continued) public void remove( AnyType x )       { root = remove( x, root ); } public void removeMin( )       { root = removeMin( root ); } public AnyType findMin( )       { return elementAt( findMin( root ) ); } public AnyType findMax( )       { return elementAt( findMax( root ) ); } public AnyType find( AnyType x )       { return elementAt( find( x, root ) ); } public void makeEmpty( )       { root = null; } public boolean isEmpty( )       { return root == null; } private AnyType elementAt( BinaryNode<AnyType> t )       { /* Figure 19.7 */ } private BinaryNode<AnyType> find( AnyType x, BinaryNode<AnyType> t )       { /* Figure 19.8 */ } protected BinaryNode<AnyType> findMin( BinaryNode<AnyType> t )       { /* Figure 19.9 */ } private BinaryNode<AnyType> findMax( BinaryNode<AnyType> t )       { /* Figure 19.9 */ } protected BinaryNode<AnyType> insert( AnyType x, BinaryNode<AnyType> t )       { /* Figure 19.10 */ } protected BinaryNode<AnyType> removeMin( BinaryNode<AnyType> t )       { /* Figure 19.11 */ } protected BinaryNode<AnyType> remove( AnyType x, BinaryNode<AnyType> t )       { /* Figure 19.12 */ }     protected BinaryNode<AnyType> root; }

19.1 basic ideas or the named item is not present, then null is returned. Figure 19.7 shows the private elementAt method that implements the elementAt logic. The removeMin operation removes the minimum item from the tree; it throws an exception if the tree is empty. The remove operation removes a named item x from the tree; it throws an exception if warranted. The makeEmpty and isEmpty methods are the usual fare. As is typical of most data structures, the find operation is easier than insert, and insert is easier than remove. Figure 19.8 illustrates the find routine. So long as a null link has not been reached, we either have a match or need to branch left or right. The code implements this algorithm quite succinctly. figure 19.7 The elementAt method /**  * Internal method to get element field.  * @param t the node.  * @return the element field or null if t is null.  */     private AnyType elementAt( BinaryNode<AnyType> t )     {         return t == null ? null : t.element;     } /**  * Internal method to find an item in a subtree.  * @param x is item to search for.  * @param t the node that roots the tree.  * @return node containing the matched item.  */     private BinaryNode<AnyType> find( AnyType x, BinaryNode<AnyType> t )     {         while( t != null )         {             if( x.compareTo( t.element ) < 0 )                 t = t.left;             else if( x.compareTo( t.element ) > 0 )                 t = t.right;             else                 return t;    // Match         }         return null;         // Not found     } figure 19.8 The find operation for binary search trees

chapter 19 binary search trees Note the order of the tests. The test against null must be performed ﬁrst; otherwise, the access t.element would be illegal. The remaining tests are arranged with the least likely case last. A recursive implementaion is possible, but we use a loop instead; we use recursion in the insert and remove methods. In Exercise 19.15 you are asked to write the searching algorithms recursively. Because of call by  value, the actual  argument (root) is  not changed. At ﬁrst glance, statements such as t=t.left may seem to change the root of the tree. That is not the case, however, because t is passed by value. In the initial call, t is simply a copy of root. Although t changes, root does not. The calls to findMin and findMax are even simpler because branching is unconditionally in one direction. These routines are shown in Figure 19.9. Note how the case of an empty tree is handled. For insert, we must  return the new tree  root and reconnect  the tree. The insert routine is shown in Figure 19.10. Here we use recursion to simplify the code. A nonrecursive implementation is also possible; we apply this technique when we discuss red–black trees later in this chapter. The basic algorithm is simple. If the tree is empty, we can create a one-node tree. The test is performed at line 10, and the new node is created at line 11. Notice carefully that, as before, local changes to t are lost. Thus we return the new root, t, at line 18. figure 19.9 The findMin and  findMax methods for  binary search trees /**  * Internal method to find the smallest item in a subtree.      * @param t the node that roots the tree.      * @return node containing the smallest item.      */     protected BinaryNode<AnyType> findMin( BinaryNode<AnyType> t )     {         if( t != null )             while( t.left != null )                 t = t.left;         return t;     }     /**      * Internal method to find the largest item in a subtree.      * @param t the node that roots the tree.      * @return node containing the largest item.      */     private BinaryNode<AnyType> findMax( BinaryNode<AnyType> t )     {         if( t != null )             while( t.right != null )                 t = t.right;         return t;     }

19.1 basic ideas If the tree is not already empty, we have three possibilities. First, if the item to be inserted is smaller than the item in node t, we call insert recursively on the left subtree. Second, if the item is larger than the item in node t, we call insert recursively on the right subtree (these two cases are coded at lines 12 to 15). Third, if the item to insert matches the item in t, we throw an exception. The remaining routines concern deletion. As described earlier, the removeMin operation is simple because the minimum node has no left child. Thus the removed node merely needs to be bypassed, which appears to require us to keep track of the parent of the current node as we descend the tree. But, again, we can avoid the explicit use of a parent link by using recursion. The code is shown in Figure 19.11. The root of the new  subtree must be  returned in the  remove routines. In  effect we maintain  the parent in the  recursion stack. If the tree t is empty, removeMin fails. Otherwise, if t has a left child, we recursively remove the minimum item in the left subtree via the recursive call at line 13. If we reach line 17, we know that we are currently at the minimum node, and thus t is the root of a subtree that has no left child. If we set t to t.right, t is now the root of a subtree that is missing its former minimum element. As before, we return the root of the resulting subtree. That is what we do at line 17. But doesn’t that disconnect the tree? The answer again is no. If t was root, the new t is returned and assigned to root in the public method. If t was not root, it is p.left, where p is t’s parent at the time of the recursive call. /**  * Internal method to insert into a subtree.  * @param x the item to insert.  * @param t the node that roots the tree.  * @return the new root.  * @throws DuplicateItemException if x is already present.  */     protected BinaryNode<AnyType> insert( AnyType x, BinaryNode<AnyType> t )     {         if( t == null )             t = new BinaryNode<AnyType>( x );         else if( x.compareTo( t.element ) < 0 )             t.left = insert( x, t.left );         else if( x.compareTo( t.element ) > 0 )             t.right = insert( x, t.right );         else             throw new DuplicateItemException( x.toString( ) );  // Duplicate         return t;     } figure 19.10 The recursive insert for the BinarySearchTree class

chapter 19 binary search trees The method that has p as a parameter (in other words, the method that called the current method) changes p.left to the new t. Thus the parent’s left link references t, and the tree is connected. All in all, it is a nifty maneuver—we have maintained the parent in the recursion stack rather than explicitly kept track of it in an iterative loop. Having used this trick for the simple case, we can then adapt it for the general remove routine shown in Figure 19.12. If the tree is empty, the remove is unsuccessful and we can throw an exception at line 11. If we do not have a match, we can recursively call remove for either the left or right subtree, as appropriate. Otherwise, we reach line 16, indicating that we have found the node that needs to be removed. The remove routine  involves tricky coding but is not too  bad if recursion is  used. The case for  one child, root with  one child, and zero  children are all handled together at  line 22. Recall (as illustrated in Figure 19.4) that, if there are two children, we replace the node with the minimum element in the right subtree and then remove the right subtree’s minimum (coded at lines 18–19). Otherwise, we have either one or zero children. If there is a left child, we set t equal to its left child, as we would do in removeMax. Otherwise, we know that there is no left child and that we can set t equal to its right child. This procedure is succinctly coded in line 22, which also covers the leaf case. Two points need to be made about this implementation. First, during the basic insert, find, or remove operation, we use two three-way comparisons per node accessed to distinguish among the cases <, =, and >. Obviously we can compute x.compareTo(t.element) once per loop iteration, and reduce the cost to one three-way comparison per node. Actually, however, we can get by with figure 19.11 The removeMin method for the  BinarySearchTree class /**  * Internal method to remove minimum item from a subtree.  * @param t the node that roots the tree.  * @return the new root.  * @throws ItemNotFoundException if t is empty.  */     protected BinaryNode<AnyType> removeMin( BinaryNode<AnyType> t )     {         if( t == null )             throw new ItemNotFoundException( );         else if( t.left != null )         {             t.left = removeMin( t.left );             return t;         }         else             return t.right;     }

19.2 order statistics only one two-way comparison per node. The strategy is similar to what we did in the binary search algorithm in Section 5.6. We discuss the technique for binary search trees in Section 19.6.2 when we illustrate the deletion algorithm for AA-trees. Second, we do not have to use recursion to perform the insertion. In fact, a recursive implementation is probably slower than a nonrecursive implementation. We discuss an iterative implementation of insert in Section 19.5.3 in the context of red–black trees. 19.2 order statistics The binary search tree allows us to ﬁnd either the minimum or maximum item in time that is equivalent to an arbitrarily named find. Sometimes, we also have to be able to access the Kth smallest element, for an arbitrary K provided as a parameter. We can do so if we keep track of the size of each node in the tree. figure 19.12 The remove method for the BinarySearchTree class /**  * Internal method to remove from a subtree.  * @param x the item to remove.  * @param t the node that roots the tree.  * @return the new root.  * @throws ItemNotFoundException if x is not found.  */     protected BinaryNode<AnyType> remove( AnyType x, BinaryNode<AnyType> t )     {         if( t == null )             throw new ItemNotFoundException( x.toString( ) );         if( x.compareTo( t.element ) < 0 )             t.left = remove( x, t.left );         else if( x.compareTo( t.element ) > 0 )             t.right = remove( x, t.right );         else if( t.left != null && t.right != null ) // Two children         {             t.element = findMin( t.right ).element;             t.right = removeMin( t.right );         }         else             t = ( t.left != null ) ? t.left : t.right;         return t;     }

chapter 19 binary search trees We can implement  findKth by maintaining the size of  each node as we  update the tree. Recall from Section 18.1 that the size of a node is the number of its descendants (including itself). Suppose that we want to ﬁnd the Kth smallest element and that K is at least 1 and at most the number of nodes in the tree. Figure 19.13 shows three possible cases, depending on the relation of K and the size of the left subtree, denoted SL. If K equals SL + 1, the root is the Kth smallest element and we can stop. If K is smaller than SL + 1 (i.e., smaller than or equal to SL), the Kth smallest element must be in the left subtree and we can ﬁnd it recursively. (The recursion can be avoided; we use it to simplify the algorithm description.) Otherwise, the Kth smallest element is the (K – SL – 1)th smallest element in the right subtree and can be found recursively. The main effort is maintaining the node sizes during tree changes. These changes occur in the insert, remove, and removeMin operations. In principle, this maintenance is simple enough. During an insert, each node on the path to the insertion point gains one node in its subtree. Thus the size of each node increases by 1, and the inserted node has size 1. In removeMin, each node on the path to the minimum loses one node in its subtree; thus the size of each node decreases by 1. During a remove, all nodes on the path to the node that is physically removed also lose one node in their subtrees. Consequently, we can maintain the sizes at the cost of only a slight amount of overhead. 19.2.1   java implementation We derive a new  class that supports  the order statistic. Logically, the only changes required are the adding of findKth and the maintenance of a size data member in insert, remove, and removeMin. We derive a new class from BinarySearchTree, the skeleton for which is shown in Figure 19.14. We provide a nested class that extends BinaryNode and adds a size data member. BinarySearchTreeWithRank adds only one public method, namely findKth, shown at lines 31 and 32. All other public methods are inherited unchanged. We must override some of the protected recursive routines (lines 36–41). X SR SL K < SL + 1 (a) X SR SL K = SL + 1 (b) X SR SL K > SL + 1 (c) figure 19.13 Using the size data  member to implement  findKth

19.2 order statistics The findKth operation shown in Figure 19.15 is written recursively, although clearly it need not be. It follows the algorithmic description line for line. The test against null at line 10 is necessary because k could be invalid. package weiss.nonstandard; 3 // BinarySearchTreeWithRank class 4 // 5 // CONSTRUCTION: with no initializer 6 // 7 // ******************PUBLIC OPERATIONS********************* 8 // Comparable findKth( k )--> Return kth smallest item 9 // All other operations are inherited 10 // ******************ERRORS******************************** 11 // IllegalArgumentException thrown if k is out of bounds 13 public class BinarySearchTreeWithRank<AnyType extends Comparable<? super AnyType>> 14                     extends BinarySearchTree<AnyType> 15 { 16     private static class BinaryNodeWithSize<AnyType> extends BinaryNode<AnyType> 17     { 18         BinaryNodeWithSize( AnyType x ) 19           { super( x ); size = 0; } 21         int size; 22     } 24     /** 25      * Find the kth smallest item in the tree. 26      * @param k the desired rank (1 is the smallest item). 27      * @return the kth smallest item in the tree. 28      * @throws IllegalArgumentException if k is less 29      *     than 1 or more than the size of the subtree. 30      */ 31     public AnyType findKth( int k ) 32       { return findKth( k, root ).element; } 34     protected BinaryNode<AnyType> findKth( int k, BinaryNode<AnyType> t ) 35       { /* Figure 19.15 */ } 36     protected BinaryNode<AnyType> insert( AnyType x, BinaryNode<AnyType> tt ) 37       { /* Figure 19.16 */ } 38     protected BinaryNode<AnyType> remove( AnyType x, BinaryNode<AnyType> tt ) 39       { /* Figure 19.18 */ } 40     protected BinaryNode<AnyType> removeMin( BinaryNode<AnyType> tt ) 41       { /* Figure 19.17 */ } } figure 19.14 The BinarySearchTreeWithRank class skeleton

chapter 19 binary search trees Lines 12 and 13 compute the size of the left subtree. If the left subtree exists, accessing its size member gives the required answer. If the left subtree does not exist, its size can be taken to be 0. Note that this test is performed after we are sure that t is not null. The insert operation is shown in Figure 19.16. The potentially tricky part is that, if the insertion call succeeds, we want to increment t’s size member. If the recursive call fails, t’s size member is unchanged and an exception should be thrown. In an unsuccessful insertion, can some sizes change? The answer is no; size is updated only if the recursive call succeeds without an exception. Note that when a new node is allocated by a call to new, the size member is set to 0 by the BinaryNodeWithSize constructor, and then incremented at line 20. Figure 19.17 shows that the same trick can be used for removeMin. If the recursive call succeeds, the size member is decremented; if the recursive call fails, size is unchanged. The remove operation is similar and is shown in Figure 19.18. figure 19.15 The findKth operation for a search tree with order statistics /**  * Internal method to find kth smallest item in a subtree.  * @param k the desired rank (1 is the smallest item).  * @return the node containing the kth smallest item in the subtree.  * @throws IllegalArgumentException if k is less  *     than 1 or more than the size of the subtree.  */     protected BinaryNode<AnyType> findKth( int k, BinaryNode<AnyType> t )     {         if( t == null )             throw new IllegalArgumentException( );         int leftSize = ( t.left != null ) ?                        ((BinaryNodeWithSize<AnyType>) t.left).size : 0;         if( k <= leftSize )             return findKth( k, t.left );         if( k == leftSize + 1 )             return t;         return findKth( k - leftSize - 1, t.right );     } The findKth operation is easily implemented once the  size members are  known. The insert and  remove operations  are potentially tricky  because we do not  update the size  information if the  operation is unsuccessful.

19.2 order statistics /**  * Internal method to insert into a subtree.  * @param x the item to insert.  * @param tt the node that roots the tree.  * @return the new root.  * @throws DuplicateItemException if x is already present.  */     protected BinaryNode<AnyType> insert( AnyType x, BinaryNode<AnyType> tt )     {         BinaryNodeWithSize<AnyType> t = (BinaryNodeWithSize<AnyType>) tt;         if( t == null )             t = new BinaryNodeWithSize<AnyType>( x );         else if( x.compareTo( t.element ) < 0 )             t.left = insert( x, t.left );         else if( x.compareTo( t.element ) > 0 )             t.right = insert( x, t.right );         else             throw new DuplicateItemException( x.toString( ) );         t.size++;         return t;     } figure 19.16 The insert operation for a search tree with order statistics figure 19.17 The removeMin operation for a search tree with order statistics /**  * Internal method to remove the smallest item from a subtree,  *     adjusting size fields as appropriate.  * @param t the node that roots the tree.  * @return the new root.  * @throws ItemNotFoundException if the subtree is empty.  */     protected BinaryNode<AnyType> removeMin( BinaryNode<AnyType> tt )     {         BinaryNodeWithSize<AnyType> t = (BinaryNodeWithSize<AnyType>) tt;         if( t == null )             throw new ItemNotFoundException( );         if( t.left == null )             return t.right;         t.left = removeMin( t.left );         t.size--;         return t;     }

chapter 19 binary search trees 19.3 analysis of binary  search tree operations The cost of an  operation is  proportional to the  depth of the last  accessed node.  The cost is logarithmic for a well-  balanced tree, but it  could be as bad as  linear for a  degenerate tree. The cost of each binary search tree operation (insert, find, and remove) is proportional to the number of nodes accessed during the operation. We can thus charge the access of any node in the tree a cost of 1 plus its depth (recall that the depth measures the number of edges on a path rather than the number of nodes), which gives the cost of a successful search. Figure 19.19 shows two trees. Figure 19.19(a) shows a balanced tree of 15 nodes. The cost to access any node is at most 4 units, and some nodes require fewer accesses. This situation is analogous to the one that occurs in the binary search algorithm. If the tree is perfectly balanced, the access cost is logarithmic. figure 19.18 The remove operation for a search tree with order statistics /**  * Internal method to remove from a subtree.  * @param x the item to remove.  * @param t the node that roots the tree.  * @return the new root.  * @throws ItemNotFoundException if x is not found.  */     protected BinaryNode<AnyType> remove( AnyType x, BinaryNode<AnyType> tt )     {         BinaryNodeWithSize<AnyType> t = (BinaryNodeWithSize<AnyType>) tt;         if( t == null )             throw new ItemNotFoundException( x.toString( ) );         if( x.compareTo( t.element ) < 0 )             t.left = remove( x, t.left );         else if( x.compareTo( t.element ) > 0 )             t.right = remove( x, t.right );         else if( t.left != null && t.right != null ) // Two children         {             t.element = findMin( t.right ).element;             t.right = removeMin( t.right );         }         else             return ( t.left != null ) ? t.left : t.right;         t.size--;         return t;     }

19.3 analysis of binary search tree operations Unfortunately, we have no guarantee that the tree is perfectly balanced. The tree shown in Figure 19.19(b) is the classic example of an unbalanced tree. Here, all N nodes are on the path to the deepest node, so the worst-case search time is O(N). Because the search tree has degenerated to a linked list, the average time required to search in this particular instance is half the cost of the worst case and is also O(N). So we have two extremes: In the best case, we have logarithmic access cost, and in the worst case we have linear access cost. What, then, is the average? Do most binary search trees tend toward the balanced or unbalanced case, or is there some middle ground, such as  ? The answer is identical to that for quicksort: The average is 38 percent worse than the best case. On average, the  depth is 38 percent  worse than the best  case. This result is  identical to that  obtained using  quicksort. We prove in this section that the average depth over all nodes in a binary search tree is logarithmic, under the assumption that each tree is created as a result of random insertion sequences (with no remove operations). To see what that means, consider the result of inserting three items in an empty binary search tree. Only their relative ordering is important, so we can assume without loss of generality that the three items are 1, 2, and 3. Then there are six possible insertion orders: (1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), and (3, 2, 1). We assume in our proof that each insertion order is equally likely. The binary search trees that can result from these insertions are shown in Figure 19.20. Note that the tree with root 2, shown in Figure 19.20(c), is formed from either the insertion sequence (2, 3, 1) or the sequence (2, 1, 3). Thus some trees are more likely to result than others, and as we show, balanced trees are more likely to occur than unbalanced trees (although this result is not evident from the three-element case). We begin with the following deﬁnition. deﬁnition: The internal path length of a binary tree is the sum of the depths of its nodes. (a) (b) figure 19.19 (a) The balanced tree  has a depth of  ⎣log N⎦; (b) the  unbalanced tree has a  depth of N – 1. N

chapter 19 binary search trees The internal path  length is used to  measure the cost  of a successful  search. When we divide the internal path length of a tree by the number of nodes in the tree, we obtain the average node depth. Adding 1 to this average gives the average cost of a successful search in the tree. Thus we want to compute the average internal path length for a binary search tree, where the average is taken over all (equally probable) input permutations. We can easily do so by viewing the tree recursively and by using techniques from the analysis of quicksort given in Section 8.6. The average internal path length is established in Theorem 19.1. The insertion algorithm implies that the cost of an insert equals the cost of an unsuccessful search, which is measured by using the external path length. In an insertion or unsuccessful search, we eventually reach the test t==null. (a) (b) (c) (d) (e) figure 19.20 Binary search trees that can result from inserting a permutation 1, 2, and 3; the balanced tree shown in  part (c) is twice as likely to result as any of the others. Theorem 19.1 The internal path length of a binary search tree is approximately 1.38 N log N on  average, under the assumption that all permutations are equally likely. Proof Let D(N) be the average internal path length for trees of N nodes, so D(1) = 0. An Nnode tree T consists of an i-node left subtree and an (N – i – 1)-node right subtree,  plus a root at depth 0 for 0 ≤i < N. By assumption, each value of i is equally likely. For  a given i, D(i) is the average internal path length of the left subtree with respect to its  root. In T, all these nodes are one level deeper. Thus the average contribution of the  nodes in the left subtree to the internal path length of T is  , plus 1  for each node in the left subtree. The same holds for the right subtree. We thus  obtain the recurrence formula D(N) = (2 / N)( ) + N – 1, which is identical  to the quicksort recurrence solved in Section 8.6. The result is an average internal  path length of O(N log N). 1 N ⁄ ( ) D i( ) i = N – ∑ D i( ) i = N – ∑ The external path  length is used to  measure the cost of  an unsuccessful  search.

19.3 analysis of binary search tree operations Recall that in a tree of N nodes there are N + 1 null links. The external path length measures the total number of nodes that are accessed, including the null node for each of these N + 1 null links. The null node is sometimes called an external tree node, which explains the term external path length. As we show later in the chapter, replacing the null node with a sentinel may be convenient. deﬁnition: The external path length of a binary search tree is the sum of the depths of the N + 1 null links. The terminating null node is considered a node for these purposes. One plus the result of dividing the average external path length by N + 1 yields the average cost of an unsuccessful search or insertion. As with the binary search algorithm, the average cost of an unsuccessful search is only slightly more than the cost of a successful search, which follows from Theorem 19.2. It is tempting to say immediately that these results imply that the average running time of all operations is O(log N). This implication is true in practice, but it has not been established analytically because the assumption used to prove the previous results do not take into account the deletion algorithm. In fact, close examination suggests that we might be in trouble with our deletion algorithm because the remove operation always replaces a two-child deleted node with a node from the right subtree. This result would seem to have the effect of eventually unbalancing the tree and tending to make it left-heavy. It has been shown that if we build a random binary search tree and then perform roughly N2 pairs of random insert/remove combinations, the binary search trees will have an expected depth of  . However, a reasonable number of random insert and remove operations (in which the order of insert and remove is also random) does not unbalance the tree in any observable way. In fact, for small search trees, the remove algorithm seems to balance the tree. Consequently, we can reasonably assume that for random input all operations behave in logarithmic average time, although this result has not been proved mathematically. In Exercise 19.25 we describe some alternative deletion strategies. For any tree T, let IPL(T) be the internal path length of T and let EPL(T) be its external path length. Then, if T has N nodes, EPL(T) = IPL(T) + 2N. Theorem 19.2 This theorem is proved by induction and is left as Exercise 19.7. Proof O N ( ) Random remove operations do not  preserve the randomness of a tree.  The effects are not  completely understood theoretically,  but they apparently  are negligible in  practice.

chapter 19 binary search trees The most important problem is not the potential imbalance caused by the remove algorithm. Rather, it is that, if the input sequence is sorted, the worst-case tree occurs. When that happens, we are in deep trouble: We have linear time per operation (for a series of N operations) rather than logarithmic cost per operation. This case is analogous to passing items to quicksort but having an insertion sort executed instead. The resulting running time is completely unacceptable. Moreover, it is not just sorted input that is problematic, but also any input that contains long sequences of nonrandomness. One solution to this problem is to insist on an extra structural condition called balance: No node is allowed to get too deep. A balanced binary  search tree has an  added structure  property to guarantee logarithmic  depth in the worst  case. Updates are  slower, but  accesses are faster. Any of several algorithms can be used to implement a balanced binary search tree, which has an added structure property that guarantees logarithmic depth in the worst case. Most of these algorithms are much more complicated than those for the standard binary search trees, and all take longer on average for insertion and deletion. They do, however, provide protection against the embarrassingly simple cases that lead to poor performance for (unbalanced) binary search trees. Also, because they are balanced, they tend to give faster access time than those for the standard trees. Typically, their internal path lengths are very close to the optimal N log N rather than 1.38N log N, so searching time is roughly 25 percent faster. 19.4 avl trees The AVL tree was  the ﬁrst balanced  binary search tree.  It has historical signiﬁcance and also  illustrates most of  the ideas that are  used in other  schemes. The ﬁrst balanced binary search tree was the AVL tree (named after its discoverers, Adelson-Velskii and Landis), which illustrates the ideas that are thematic for a wide class of balanced binary search trees. It is a binary search tree that has an additional balance condition. Any balance condition must be easy to maintain and ensures that the depth of the tree is O(log N). The simplest idea is to require that the left and right subtrees have the same height. Recursion dictates that this idea apply to all nodes in the tree because each node is itself a root of some subtree. This balance condition ensures that the depth of the tree is logarithmic. However, it is too restrictive because inserting new items while maintaining balance is too difﬁcult. Thus the deﬁnition of an AVL tree uses a notion of balance that is somewhat weaker but still strong enough to guarantee logarithmic depth. deﬁnition: An AVL tree is a binary search tree with the additional balance property that, for any node in the tree, the height of the left and right subtrees can differ by at most 1. As usual, the height of an empty subtree is –1.

19.4 avl trees 19.4.1   properties Every node in an  AVL tree has subtrees whose  heights differ by at  most 1. An empty  subtree has height  –1. Figure 19.21 shows two binary search trees. The tree shown in Figure 19.21(a) satisﬁes the AVL balance condition and is thus an AVL tree. The tree shown in Figure 19.21(b), which results from inserting 1, using the usual algorithm, is not an AVL tree because the darkened nodes have left subtrees whose heights are 2 larger than their right subtrees. If 13 were inserted, using the usual binary search tree insertion algorithm, node 16 would also be in violation. The reason is that the left subtree would have height 1, while the right subtree would have height –1. The AVL tree has  height at most  roughly 44 percent  greater than the  minimum. The AVL balance condition implies that the tree has only logarithmic depth. To prove this assertion we need to show that a tree of height H must have at least CH nodes for some constant C > 1. In other words, the minimum number of nodes in a tree is exponential in its height. Then the maximum depth of an N-item tree is given by logCN. Theorem 19.3 shows that every AVL tree of height H has many nodes.     (a) (b) figure 19.21 Two binary search  trees: (a) an AVL tree;  (b) not an AVL tree  (unbalanced nodes  are darkened) figure 19.22 Minimum tree of  height H H H – 1 H – 2 SH – 1 SH – 2

chapter 19 binary search trees From Exercise 7.8,  , where  . Consequently, an AVL tree of height H has at least (roughly)   nodes. Hence its depth is at most logarithmic. The height of an AVL tree satisﬁes (19.1) so the worst-case height is at most roughly 44 percent more than the minimum possible for binary trees. The depth of a typical node in an AVL  tree is very close to  the optimal log N. The depth of an average node in a randomly constructed AVL tree tends to be very close to log N. The exact answer has not yet been established analytically. We do not even know whether the form is log N + C or , for some   that would be approximately 0.01. Simulations have been unable to demonstrate convincingly that one form is more plausible than the other. An update in an  AVL tree could  destroy the balance. It must then  be rebalanced  before the operation can be considered complete. A consequence of these arguments is that all searching operations in an AVL tree have logarithmic worst-case bounds. The difﬁculty is that operations that change the tree, such as insert and remove, are not quite as simple as before. The reason is that an insertion (or deletion) can destroy the balance of several nodes in the tree, as shown in Figure 19.21. The balance must then be restored before the operation can be considered complete. The insertion algorithm is described here, and the deletion algorithm is left for Exercise 19.9. Only nodes on the  path from the root  to the insertion  point can have their  balances altered. A key observation is that after an insertion, only nodes that are on the path from the insertion point to the root might have their balances altered because only those nodes have their subtrees altered. This result applies to almost all the balanced search tree algorithms. As we follow the path up to the root and update the balancing information, we may ﬁnd a node whose new balance violates the AVL condition. In this section we show how to rebalance the tree at the ﬁrst (i.e., the deepest) such node and prove that this rebalancing guarantees that the entire tree satisﬁes the AVL property. Theorem 19.3 An AVL tree of height H has at least FH + 3 – 1 nodes, where Fi is the ith Fibonacci  number (see Section 7.3.4). Proof Let SH be the size of the smallest AVL tree of height H. Clearly, S0 = 1 and S1 = 2. Figure 19.22 shows that the smallest AVL tree of height H must have subtrees of  height H – 1 and H – 2. The reason is that at least one subtree has height H – 1 and  the balance condition implies that subtree heights can differ by at most 1. These  subtrees must themselves have the fewest number of nodes for their heights, so  SH = SH – 1 + SH – 2 + 1. The proof can be completed by using an induction  argument. Fi φi ⁄ ≈ φ + ( ) 2 ⁄ 1.618 ≈ = φH + ⁄ H 1.44   N + ( ) log 1.328 – < ε + ( ) N log C + ε

19.4 avl trees The node to be rebalanced is X. Because any node has at most two children and a height imbalance requires that the heights of X’s two subtrees differ by 2, a violation might occur in any of four cases: 1. An insertion in the left subtree of the left child of X 2. An insertion in the right subtree of the left child of X 3. An insertion in the left subtree of the right child of X 4. An insertion in the right subtree of the right child of X Cases 1 and 4 are mirror-image symmetries with respect to X, as are cases 2 and 3. Consequently, there theoretically are two basic cases. From a programming perspective, of course, there are still four cases and numerous special cases. Balance is restored  by tree rotations. A  single rotation switches the roles  of the parent and  child while maintaining the search  order. The ﬁrst case, in which the insertion occurs on the outside (i.e., left–left or right–right), is ﬁxed by a single rotation of the tree. A single rotation switches the roles of the parent and child while maintaining search order. The second case, in which the insertion occurs on the inside (i.e., left–right or right–left), is handled by the slightly more complex double rotation. These fundamental operations on the tree are used several times in balanced tree algorithms. In the remainder of this section we describe these rotations and prove that they sufﬁce to maintain the balance condition. 19.4.2   single rotation A single rotation  handles the outside cases (1 and  4). We rotate  between a node  and its child. The  result is a binary  search tree that  satisﬁes the AVL  property. Figure 19.23 shows the single rotation that ﬁxes case 1. In Figure 19.23(a), node k2 violates the AVL balance property because its left subtree is two levels deeper than its right subtree (the dashed lines mark the levels in this section). The situation depicted is the only possible case 1 scenario that allows k2 to satisfy the AVL property before the insertion but violate it afterward. Subtree A has grown to an extra level, causing it to be two levels deeper than C. Subtree B cannot be at the same level as the new A because then k2 would have been out of balance before the insertion. Subtree B cannot be at the same level as C because then k1 would have been the ﬁrst node on the path that was in violation of the AVL balancing condition (and we are claiming that k2 is). Ideally, to rebalance the tree, we want to move A up one level and C down one level. Note that these actions are more than the AVL property requires. To do so we rearrange nodes into an equivalent search tree, as shown in Figure 19.23(b). Here is an abstract scenario: Visualize the tree as being ﬂexible, grab the child node k1, close your eyes, and shake the tree, letting gravity take hold. The result is that k1 will be the new root. The binary search tree property tells us that in the original tree, k2 > k1, so k2 becomes the right child of k1 in the new tree. Subtrees A and C remain as the left child of k1 and the right child of k2, respectively. Subtree B, which holds items between k1 and k2 If we ﬁx the balance  at the deepest  unbalanced node,  we rebalance the  entire tree. There  are four cases that  we might have to  ﬁx; two are mirror  images of the  other two.

chapter 19 binary search trees in the original tree, can be placed as k2’s left child in the new tree and satisfy all the ordering requirements. One rotation  sufﬁces to ﬁx cases  1 and 4 in an AVL  tree. This work requires only the few child link changes shown as pseudocode in Figure 19.24 and results in another binary tree that is an AVL tree. This outcome occurs because A moves up one level, B stays at the same level, and C moves down one level. Thus k1 and k2 not only satisfy the AVL requirements, but they also have subtrees that are the same height. Furthermore, the new height of the entire subtree is exactly the same as the height of the original subtree before the insertion that caused A to grow. Thus no further updating of the heights on the path to the root is needed, and consequently, no further rotations are needed. We use this single rotation often in other balanced tree algorithms in this chapter. Figure 19.25(a) shows that after the insertion of 1 into an AVL tree, node 8 becomes unbalanced. This is clearly a case 1 problem because 1 is in 8’s left– left subtree. Thus we do a single rotation between 8 and 4, thereby obtaining the tree shown in Figure 19.25(b). As mentioned earlier in this section, case 4 represents a symmetric case. The required rotation is shown in Figure 19.26, (a) Before rotation k1 k2 B C A (b) After rotation k2 k1 B A C figure 19.23 Single rotation to fix  case 1 figure 19.24 Pseudocode for a  single rotation  (case 1) /**  * Rotate binary tree node with left child.  * For AVL trees, this is a single rotation for case 1.  */     static BinaryNode rotateWithLeftChild( BinaryNode k2 )     {         BinaryNode k1 = k2.left;         k2.left = k1.right;         k1.right = k2;         return k1;     }

19.4 avl trees and the pseudocode that implements it is shown in Figure 19.27. This routine, along with other rotations in this section, is replicated in various balanced search trees later in this text. These rotation routines appear in the online code for several balanced search tree implementations. (b) After rotation B C A k2 k1 (a) Before rotation A B C k1 k2 figure 19.25 Single rotation fixes  an AVL tree after  insertion of 1. figure 19.26 Symmetric single  rotation to fix case 4 (a) After rotation k1 k2 B C A (b) Before rotation k2 k1 B A C figure 19.27 Pseudocode for a  single rotation  (case 4) /**  * Rotate binary tree node with right child.  * For AVL trees, this is a single rotation for case 4.  */     static BinaryNode rotateWithRightChild( BinaryNode k1 )     {         BinaryNode k2 = k1.right;         k1.right = k2.left;         k2.left = k1;         return k2;     }

chapter 19 binary search trees 19.4.3   double rotation The single rotation  does not ﬁx the  inside cases (2 and  3). These cases  require a double  rotation, involving  three nodes and  four subtrees. The single rotation has a problem: As Figure 19.28 shows, it does not work for case 2 (or, by symmetry, for case 3). The problem is that subtree Q is too deep, and a single rotation does not make it any less deep. The double rotation that solves the problem is shown in Figure 19.29. The fact that subtree Q in Figure 19.28 has had an item inserted into it guarantees that it is not empty. We may assume that it has a root and two (possibly empty) subtrees, so we may view the tree as four subtrees connected by three nodes. We therefore rename the four trees A, B, C, and D. As Figure 19.29 suggests, either subtree B or subtree C is two levels deeper than subtree D (unless both are empty, in which case both are), but we cannot be sure which one. Actually it does not matter; here, both B and C are drawn at 1.5 levels below D. To rebalance, we cannot leave k3 as the root. In Figure 19.28 we showed that a rotation between k3 and k1 does not work, so the only alternative is to k1 k2 Q R P (a) Before rotation (b) After rotation k2 k1 Q P R figure 19.28 Single rotation does  not fix case 2. k1 k2 k3 B C D A (a) Before rotation (b) After rotation k3 k1 k2 B C D A figure 19.29 Left–right double  rotation to fix case 2

19.4 avl trees place k2 as the new root. Doing so forces k1 to be k2’s left child and k3 to be k2’s right child. It also determines the resulting locations of the four subtrees, and the resulting tree satisﬁes the AVL property. Also, as was the case with the single rotation, it restores the height to the height before the insertion, thus guaranteeing that all rebalancing and height updating are complete.  As an example, Figure 19.30(a) shows the result of inserting 5 into an AVL tree. A height imbalance is caused at node 8, resulting in a case 2 problem. We perform a double rotation at that node, thereby producing the tree shown in Figure 19.30(b). A double rotation is  equivalent to two  single rotations. Figure 19.31 shows that the symmetric case 3 can also be ﬁxed by a double rotation. Finally, note that, although a double rotation appears complex, it turns out to be equivalent to the following sequence: n A rotation between X’s child and grandchild n A rotation between X and its new child B (b) After rotation D C A k3 k1 k2 (a) Before rotation A B C D k1 k2 k3 figure 19.30 Double rotation fixes  AVL tree after the  insertion of 5. figure 19.31 Right–Left double  rotation to fix case 3. k3 k2 k1 C B A D (a) Before rotation (b) After rotation k3 k1 k2 B C D A

chapter 19 binary search trees The pseudocode to implement the case 2 double rotation is compact and is shown in Figure 19.32. The mirror-image pseudocode for case 3 is shown in Figure 19.33. 19.4.4

chapt er hash tables  In Chapter 19 we discussed the binary search tree, which allows various operations on a set of elements. In this chapter we discuss the hash table, which supports only a subset of the operations allowed by binary search trees. The implementation of hash tables is frequently called hashing, and it performs insertions, deletions, and ﬁnds in constant average time. Unlike with the binary search tree, the average-case running time of hash table operations is based on statistical properties rather than the expectation of random-looking input. This improvement is obtained at the expense of a loss of ordering information among the elements: Operations such as findMin and findMax and the printing of an entire table in sorted order in linear time are not supported. Consequently, the hash table and binary search tree have somewhat different uses and performance properties. In this chapter, we show n Several methods of implementing the hash table n Analytical comparisons of these methods n Some applications of hashing n Comparisons of hash tables and binary search trees

chapter 20 hash tables 20.1 basic ideas The hash table is  used to implement  a set in constant  time per operation. The hash table supports the retrieval or deletion of any named item. We want to be able to support the basic operations in constant time, as for the stack and queue. Because the accesses are much less restricted, this support seems like an impossible goal. That is, surely when the size of the set increases, searches in the set should take longer. However, that is not necessarily the case. Suppose that all the items we are dealing with are small nonnegative integers, ranging from 0 to 65,535. We can use a simple array to implement each operation as follows. First, we initialize an array a that is indexed from 0 to 65,535 with all 0s. To perform insert(i), we execute a[i]++. Note that a[i] represents the number of times that i has been inserted. To perform find(i), we verify that a[i] is not 0. To perform remove(i), we make sure that a[i] is positive and then execute a[i]--. The time for each operation is clearly constant; even the overhead of the array initialization is a constant amount of work (65,536 assignments). There are two problems with this solution. First, suppose that we have 32bit integers instead of 16-bit integers. Then the array a must hold 4 billion items, which is impractical. Second, if the items are not integers but instead are strings (or something even more generic), they cannot be used to index an array. The second problem is not really a problem at all. Just as a number 1234 is a collection of digits 1, 2, 3, and 4, the string "junk" is a collection of characters 'j', 'u', 'n', and 'k'. Note that the number 1234 is just . Recall from Section 12.1 that an ASCII character can typically be represented in 7 bits as a number between 0 and 127. Because a character is basically a small integer, we can interpret a string as an integer. One possible representation is 'j' ⋅1283 + 'u' ⋅1282 + 'n' ⋅1281 + 'k' ⋅1280. This approach allows the simple array implementation discussed previously. The problem with this strategy is that the integer representation described generates huge integers: The representation for "junk" yields 224,229,227, and longer strings generate much larger representations. This result brings us back to the ﬁrst problem: How do we avoid using an absurdly large array? We do so by using a function that maps large numbers (or strings interpreted as numbers) into smaller, more manageable numbers. A function that maps an item into a small index is known as a hash function. If x is an arbitrary (nonnegative) integer, then x%tableSize generates a number between 0 and tableSize-1 suitable for indexing into an array of size tableSize. If s is a string, we can convert s to a large integer x by using the method suggested previously and then apply the mod operator (%) to get a suitable index. Thus, if 1 103 2 102 3 101 ⋅ 4 100 ⋅ + + ⋅ + ⋅ A hash function converts the item  into an integer suitable to index an  array where the  item is stored. If the  hash function were  one to one, we  could access the  item by its array  index.

20.2 hash function tableSize is 10,000, "junk" would be indexed to 9,227. In Section 20.2 we discuss implementation of the hash function for strings in detail. Because the hash  function is not one  to one, several  items collide at the  same index and  cause a collision. The use of the hash function introduces a complication: Two or more different items can hash out to the same position, causing a collision. This situation can never be avoided because there are many more items than positions. However, many methods are available for quickly resolving a collision. We investigate three of the simplest: linear probing, quadratic probing, and separate chaining. Each method is simple to implement, but each yields a different performance, depending on how full the array is. 20.2 hash function Computing the hash function for strings has a subtle complication: The conversion of the String s to x generates an integer that is almost certainly larger than the machine can store conveniently—because 1284 = 228. This integer size is only a factor of 8 from the largest int. Consequently, we cannot expect to compute the hash function by directly computing powers of 128. Instead, we use the following observation. A general polynomial (20.1) can be evaluated as (20.2) By using a trick, we  can evaluate the  hash function efﬁciently and without  overﬂow. Note that in Equation 20.2, we avoid computation of the polynomial directly, which is good for three reasons. First, it avoids a large intermediate result, which, as we have shown, overﬂows. Second, the calculation in the equation involves only three multiplications and three additions; an N-degree polynomial is computed in N multiplications and additions. These operations compare favorably with the computation in Equation 20.1. Third, the calculation proceeds left to right (A3 corresponds to 'j', A2 to 'u', and so on, and X is 128). However, an overﬂow problem persists: The result of the calculation is still the same and is likely to be too large. But, we need only the result taken mod tableSize. By applying the % operator after each multiplication (or addition), we can ensure that the intermediate results remain small.1 The resulting hash function is shown in Figure 20.1. An annoying feature of this hash function is that the mod computation is expensive. Because overﬂow is allowed (and its results are consistent on a given platform), we can make the hash A3X3 A2X2 A1X1 A0X0 + + + A3 ( )X A2 + ( )X A1 + ( )X A0 + 1. Section 7.4 contains the properties of the mod operation.

chapter 20 hash tables function somewhat faster by performing a single mod operation immediately prior to the return. Unfortunately, the repeated multiplication by 128 would tend to shift the early characters to the left—out of the answer. To alleviate this situation, we multiply by 37 instead of 128, which slows the shifting of early characters. The result is shown in Figure 20.2. It is not necessarily the best function possible. Also, in some applications (e.g., if long strings are involved), we may want to tinker with it. Generally speaking, however, the function is quite good. Note that overﬂow could introduce negative numbers. Thus if the mod generates a negative value, we make it positive (lines 15 and 16). Also note that the result obtained by allowing overﬂow and doing a ﬁnal mod is not the same as performing the mod after every step. Thus we have slightly altered the hash function—which is not a problem. figure 20.1 A first attempt at a  hash function  implementation // Acceptable hash function     public static int hash( String key, int tableSize )     {         int hashVal = 0;         for( int i = 0; i < key.length( ); i++ )             hashVal = ( hashVal * 128 + key.charAt( i ) )                                               % tableSize;         return hashVal;     } figure 20.2 A faster hash function  that takes advantage  of overflow /**  * A hash routine for String objects.  * @param key the String to hash.  * @param tableSize the size of the hash table.  * @return the hash value.  */     public static int hash( String key, int tableSize )     {         int hashVal = 0;         for( int i = 0; i < key.length( ); i++ )             hashVal = 37 * hashVal + key.charAt( i );         hashVal %= tableSize;         if( hashVal < 0 )             hashVal += tableSize;         return hashVal;     } The hash function  must be simple to  compute but also  distribute the keys  equitably. If there are  too many collisions,  the performance of  the hash table will  suffer dramatically.

20.2 hash function Although speed is an important consideration in designing a hash function, we also want to be sure that it distributes the keys equitably. Consequently, we must not take our optimizations too far. An example is the hash function shown in Figure 20.3. It simply adds the characters in the keys and returns the result mod tableSize. What could be simpler? The answer is that little could be simpler. The function is easy to implement and computes a hash value very quickly. However, if tableSize is large, the function does not distribute the keys well. For instance, suppose that tableSize is 10,000. Also suppose that all keys are 8 or fewer characters long. Because an ASCII char is an integer between 0 and 127, the hash function can assume values only between 0 and 1,016 (127 × 8). This restriction certainly does not permit an equitable distribution. Any speed gained by the quickness of the hash function calculation is more than offset by the effort taken to resolve a larger than expected number of collisions. However, a reasonable alternative is described in Exercise 20.14. The table runs  from 0 to  tableSize-1. Finally, note that 0 is a possible result of the hash function, so hash tables are indexed starting at 0. 20.2.1   hashCode in java.lang.String In Java, library types that can be reasonably inserted into a HashSet or as keys into a HashMap already have equals and hashCode deﬁned. In particular the String class has a hashCode whose implementation is critical for performance of HashSets and HashMaps involving Strings. The history of the String hashCode method is in itself fairly instructive. The earliest versions of Java used essentially the same implementation as Figure 20.2, including the constant multiplier 37, but without lines 14−16. But later on the implementation was “optimized” so that if the String was longer than 15 characters, only 8 or 9 characters, somewhat evenly spaced in the String, would be used to compute the hashCode. This version was used in Java 1.0.2 and Java 1.1, but it turned out to be a bad idea because there were many applications containing large groups of long Strings that were figure 20.3 A bad hash function if  tableSize is large // A poor hash function when tableSize is large     public static int hash( String key, int tableSize )     {         int hashVal = 0;         for( int i = 0; i < key.length( ); i++ )            hashVal += key.charAt( i );         return hashVal % tableSize;     }

chapter 20 hash tables somewhat similar. Two such examples were maps in which the keys were URLs such as http://www.cnn.com/ and maps in which the keys were complete ﬁlenames, such as  /a/file.cs.fiu.edu./disk/storage137/user/weiss/public_html/dsj4/code. Performance in these maps degraded considerably because the keys generated relatively few unique hash codes. In Java 1.2, the hashCode was returned back to the simpler version, with 31 used as the constant multiplier. Needless to say, the programmers who designed the Java library are among the most gifted on the planet, and so it is easy to see that designing a top-notch hash function can be laden with pitfalls and is not as simple as it may seem. In Java 1.3, a new idea was attempted, with more success. Because the expensive part of the hash table operations is computing the hashCode, the hashCode method in the String class contains an important optimization: Each String object stores internally the value of its hashCode. Initially it is 0, but if hashCode is invoked, the value is remembered. Thus if hashCode is computed on the same String object a second time, we can avoid the expensive recomputation. This technique is called caching the hash code, and represents another classic time-space tradeoff. Figure 20.4 shows an implementation of the String class that caches the hash code. Caching the hash code works only because Strings are immutable: if the String were allowed to change, it would invalidate the hashCode, and the hashCode would have to be reset back to 0. Although two String objects with the same state must have their hash codes computed independently, there are many situations in which the same String object keeps having its hash code queried. One situation where caching the hash code helps occurs during rehashing, because all the Strings involved in the rehashing have already had their hash codes cached. figure 20.4 Excerpt of String class hashCode public final class String {     public int hashCode( )     {         if( hash != 0 )             return hash;         for( int i = 0; i < length( ); i++ )                 hash = hash * 31 + (int) charAt( i );             return hash;     }     private int hash = 0; }

20.3 linear probing 20.3 linear probing In linear probing, collisions are  resolved by  sequentially scanning an array (with  wraparound) until  an empty cell is  found. Now that we have a hash function, we need to decide what to do when a collision occurs. Speciﬁcally, if X hashes out to a position that is already occupied, where do we place it? The simplest possible strategy is linear probing, or searching sequentially in the array until we ﬁnd an empty cell. The search wraps around from the last position to the ﬁrst, if necessary. Figure 20.5 shows the result of inserting the keys 89, 18, 49, 58, and 9 in a hash table when linear probing is used. We assume a hash function that returns the key X mod the size of the table. Figure 20.5 includes the result of the hash function. The ﬁrst collision occurs when 49 is inserted; the 49 is put in the next available spot—namely, spot 0, which is open. Then 58 collides with 18, 89, and 49 before an empty spot is found three slots away in position 1. The collision for element 9 is resolved similarly. So long as the table is large enough, a free cell can always be found. However, the time needed to ﬁnd a free cell can get to be quite long. For example, if there is only one free cell left in the table, we may have to search the entire table to ﬁnd it. On average we would expect to have to search half the table to ﬁnd it, which is far from the constant time per access that we are hoping for. But, if the table is kept relatively empty, insertions should not be so costly. We discuss this approach shortly. figure 20.5 Linear probing hash  table after each  insertion After insert 89 After insert 18 After insert 49 After insert 58 After insert 9 hash ( 89, 10 ) = 9 hash ( 18, 10 ) = 8 hash ( 49, 10 ) = 9 hash ( 58, 10 ) = 8 hash (  9, 10 ) = 9

chapter 20 hash tables The load factor of a  probing hash table  is the fraction of  the table that is full.  It ranges from 0  (empty) to 1 (full). The find algorithm  follows the same  probe sequence  as the insert algorithm. The find algorithm merely follows the same path as the insert algorithm. If it reaches an empty slot, the item we are searching for is not found; otherwise, it ﬁnds the match eventually. For example, to ﬁnd 58, we start at slot 8 (as indicated by the hash function). We see an item, but it is the wrong one, so we try slot 9. Again, we have an item, but it is the wrong one, so we try slot 0 and then slot 1 until we ﬁnd a match. A find for 19 would involve trying slots 9, 0, 1, and 2 before ﬁnding the empty cell in slot 3. Thus 19 is not found. We must use lazy deletion. Standard deletion cannot be performed because, as with a binary search tree, an item in the hash table not only represents itself, but it also connects other items by serving as a placeholder during collision resolution. Thus, if we removed 89 from the hash table, virtually all the remaining find operations would fail. Consequently, we implement lazy deletion, or marking items as deleted rather than physically removing them from the table. This information is recorded in an extra data member. Each item is either active or deleted. 20.3.1   naive analysis of linear probing The simplistic analysis of linear probing is based on the  assumption that  successive probes  are independent.  This assumption is  not true and thus  the analysis underestimates the costs  of searching and  insertion. To estimate the performance of linear probing, we make two assumptions: 1. The hash table is large 2. Each probe in the hash table is independent of the previous probe. Assumption 1 is reasonable; otherwise, we would not be bothering with a hash table. Assumption 2 says that, if the fraction of the table that is full is λ, each time we examine a cell the probability that it is occupied is also λ, independent of any previous probes. Independence is an important statistical property that greatly simpliﬁes the analysis of random events. Unfortunately, as discussed in Section 20.3.2, the assumption of independence is not only unjustiﬁed, but it also is erroneous. Thus the naive analysis that we perform is incorrect. Even so, it is helpful because it tells us what we can hope to achieve if we are more careful about how collisions are resolved. As mentioned earlier in the chapter, the performance of the hash table depends on how full the table is. Its fullness is given by the load factor. deﬁnition: The load factor, λ, of a probing hash table is the fraction of the table that is full. The load factor ranges from 0 (empty) to 1 (completely full). We can now give a simple but incorrect analysis of linear probing in Theorem 20.1.

20.3 linear probing In the proof of Theorem 20.1 we use the fact that, if the probability of some event’s occurring is p, then on average 1/p trials are required until the event occurs, provided that the trials are independent. For example, the expected number of coin ﬂips until a heads occurs is two, and the expected number of rolls of a single six-sided die until a 4 occurs is six, assuming independence. 20.3.2   what really happens: primary clustering The effect of primary clustering is  the formation of  large clusters of  occupied cells,  making insertions  into the cluster  expensive (and  then the insertion  makes the cluster  even larger). Unfortunately, independence does not hold, as shown in Figure 20.6. Part (a) shows the result of ﬁlling a hash table to 70 percent capacity, if all successive probes are independent. Part (b) shows the result of linear probing. Note the group of clusters: the phenomenon known as primary clustering. In primary clustering, large blocks of occupied cells are formed. Any key that hashes into this cluster requires excessive attempts to resolve the collision, and then it adds to the size of the cluster. Not only do items that collide because of identical hash functions cause degenerate performance, but also an item that collides with an alternative location for another item causes poor performance. The mathematical analysis required to take this phenomenon into account is complex but has been solved, yielding Theorem 20.2. For a half-full table, we obtain 2.5 as the average number of cells examined during an insertion. This outcome is almost the same as what the naive If independence of probes is assumed, the average number of cells examined in an  insertion using linear probing is 1/(1 – λ). Theorem 20.1 For a table with a load factor of λ, the probability of any cell’s being empty is 1 – λ. Consequently, the expected number of independent trials required to ﬁnd an empty  cell is 1/(1 – λ). Proof The average number of cells examined in an insertion using linear probing is roughly  . Theorem 20.2 The proof is beyond the scope of this text. See reference [6]. Proof λ – ( )2 ⁄ + ( ) 2 ⁄

chapter 20 hash tables Primary clustering  is a problem at high  load factors. For  half-empty tables,  the effect is not  disastrous. analysis indicated. The main difference occurs as λ gets close to 1. For instance, if the table is 90 percent full, λ = 0.9. The naive analysis suggests that 10 cells would have to be examined—a lot but not completely out of the question. However, by Theorem 20.2, the real answer is that some 50 cells need to be examined. That is excessive (especially as this number is only an average and thus some insertions must be worse). 20.3.3   analysis of the find operation An unsuccessful  find costs the  same as an  insertion. The cost of an insertion can be used to bound the cost of a find. There are two types of find operations: unsuccessful and successful. An unsuccessful find is easy to analyze. The sequence of slots examined for an unsuccessful search of X is the same as the sequence examined to insert X. Thus we have an immediate answer for the cost of an unsuccessful find. The cost of a successful find is an  average of the  insertion costs over  all smaller load  factors. For successful finds, things are slightly more complicated. Figure 20.5 shows a table with λ = 0.5. Thus the average cost of an insertion is 2.5. The average cost to find the newly inserted item would then be 2.5, no matter how many insertions follow. The average cost to ﬁnd the ﬁrst item inserted in the table is always 1.0 probe. Thus, in a table with λ = 0.5, some searches are easy and some are hard. In particular, the cost of a successful search of X is equal to the cost of inserting X at the time X was inserted. To ﬁnd the average time to perform a successful search in a table with load factor λ, we must compute the average insertion cost by averaging over all the load factors leading to λ. With this groundwork, we can figure 20.6 Illustration of primary clustering in linear probing (b) versus no clustering (a) and the less significant secondary  clustering in quadratic probing (c). Long lines represent occupied cells, and the load factor is 0.7. (a) (b) (c)

20.3 linear probing compute the average search times for linear probing, as asserted and proved in Theorem 20.3. We can apply the same technique to obtain the cost of a successful find under the assumption of independence (by using   in Theorem 20.3). If there is no clustering, the average cost of a successful find for linear probing is  . If the load factor is 0.5, the average number of probes for a successful search using linear probing is 1.5, whereas the nonclustering analysis suggests 1.4 probes. Note that this average does not depend on any ordering of the input keys; it depends only on the fairness of the hash function. Note also that, even when we have good hash functions, both longer and shorter probe sequences are bound to contribute to the average. For instance, there are certain to be some The average number of cells examined in an unsuccessful search using linear probing is roughly  . The average  number of cells examined in a successful search is approximately  . Theorem 20.3 The cost of an unsuccessful search is the same as the cost of an insertion. For a successful search, we compute the average insertion cost over the sequence of insertions. Because the table is large, we can compute this average by evaluating  In other words, the average cost of a successful search for a table with a load factor  of λ equals the cost of an insertion in a table of load factor x, averaged from load factors 0 through λ. From Theorem 20.2, we can derive the following equation: Proof λ – ( )2 ⁄ + ( ) 2 ⁄ λ – ( ) ⁄ + ( ) 2 ⁄ S λ ( ) λ-- I x ( ) x d x = 0 λ∫ = S λ ( ) λ-- 2-- 1 x – ( )2 ------------------ + ⎝ ⎠ ⎛ ⎞ x d x = 0 λ∫ = 2λ ------ x x – ( ) --------------- + ⎝ ⎠ ⎛ ⎞ x = 0 λ = 2λ ------ λ λ – ( ) ---------------- + ⎝ ⎠ ⎛ ⎞ – ⎝ ⎠ ⎛ ⎞ = 2-- 2 λ – λ – ----------- ⎝ ⎠ ⎛ ⎞ = 2-- 1 λ – ( ) ---------------- + ⎝ ⎠ ⎛ ⎞ = I x ( ) x – ( ) ⁄ = λ – ( ) ln λ ⁄ –

chapter 20 hash tables sequences of length 4, 5, and 6, even in a hash table that is half empty. (Determining the expected longest probe sequence is a challenging calculation.) Primary clustering not only makes the average probe sequence longer, but it also makes a long probe sequence more likely. The main problem with primary clustering therefore is that performance degrades severely for insertion at high load factors. Also, some of the longer probe sequences typically encountered (those at the high end of the average) are made more likely to occur. To reduce the number of probes, we need a collision resolution scheme that avoids primary clustering. Note, however, that, if the table is half empty, removing the effects of primary clustering would save only half a probe on average for an insertion or unsuccessful search and one-tenth a probe on average for a successful search. Even though we might expect to reduce the probability of getting a somewhat lengthier probe sequence, linear probing is not a terrible strategy. Because it is so easy to implement, any method we use to remove primary clustering must be of comparable complexity. Otherwise, we expend too much time in saving only a fraction of a probe. One such method is quadratic probing. 20.4 quadratic probing Quadratic probing examines cells 1, 4,  9, and so on, away  from the original  probe point. Quadratic probing is a collision resolution method that eliminates the primary clustering problem of linear probing by examining certain cells away from the original probe point. Its name is derived from the use of the formula   to resolve collisions. Speciﬁcally, if the hash function evaluates to H and a search in cell H is inconclusive, we try cells  , ,  ,  ,   (employing wraparound) in sequence. This strategy differs from the linear probing strategy of searching  ,  , ,  ,  . Remember that  subsequent probe  points are a quadratic number of  positions from the  original probe point. Figure 20.7 shows the table that results when quadratic probing is used instead of linear probing for the insertion sequence shown in Figure 20.5. When 49 collides with 89, the ﬁrst alternative attempted is one cell away. This cell is empty, so 49 is placed there. Next, 58 collides at position 8. The cell at position 9 (which is one away) is tried, but another collision occurs. A vacant cell is found at the next cell tried, which is   positions away from the original hash position. Thus 58 is placed in cell 2. The same thing happens for 9. Note that the alternative locations for items that hash to position 8 and the alternative locations for the items that hash to position 9 are not the same. The long probe sequence to insert 58 did not affect the subsequent insertion of 9, which contrasts with what happened with linear probing. F i( ) i2 = H + H + H + … H i2 + H + H + H + … H i + =

20.4 quadratic probing We need to consider a few details before we write code. n In linear probing, each probe tries a different cell. Does quadratic probing guarantee that, when a cell is tried, we have not already tried it during the course of the current access? Does quadratic probing guarantee that, when we are inserting X and the table is not full, X will be inserted? n Linear probing is easily implemented. Quadratic probing appears to require multiplication and mod operations. Does this apparent added complexity make quadratic probing impractical? n What happens (in both linear probing and quadratic probing) if the load factor gets too high? Can we dynamically expand the table, as is typically done with other array-based data structures? Fortunately, the news is relatively good on all cases. If the table size is prime and the load factor never exceeds 0.5, we can always place a new item X and no cell is probed twice during an access. However, for these guarantees to hold, we need to ensure that the table size is a prime number. We prove this case in Theorem 20.4. For completeness, Figure 20.8 shows a figure 20.7 A quadratic probing  hash table after each  insertion (note that  the table size was  poorly chosen  because it is not a  prime number). After insert 89 After insert 18 After insert 49 After insert 58 After insert 9 hash ( 89, 10 ) = 9 hash ( 18, 10 ) = 8 hash ( 49, 10 ) = 9 hash ( 58, 10 ) = 8 hash (  9, 10 ) = 9 If the table size is  prime and the load  factor is no larger  than 0.5, all probes  will be to different  locations and an  item can always be  inserted.

chapter 20 hash tables routine that generates prime numbers, using the algorithm shown in Figure 9.8 (a more complex algorithm is not warranted). If the table is even 1 more than half full, the insertion could fail (although failure is extremely unlikely). If we keep the table size prime and the load factor below 0.5, we have a guarantee of success for the insertion. If the table size is not prime, the number of alternative locations can be severely reduced. For example, Theorem 20.4 If quadratic probing is used and the table size is prime, then a new element can  always be inserted if the table is at least half empty. Furthermore, in the course of the  insertion, no cell is probed twice. Proof Let M be the size of the table. Assume that M is an odd prime greater than 3. We  show that the ﬁrst   alternative locations (including the original) are distinct.  Two of these locations are   and  , where  0 ≤i, j ≤⎣M / 2⎦. Suppose, for the sake of contradiction, that these two locations are  the same but that  . Then Because M is prime, it follows that either   or   is divisible by M. As i and j are distinct and their sum is smaller than M, neither of these possibilities can occur. Thus  we obtain a contradiction. It follows that the ﬁrst   alternatives (including the  original location) are all distinct and guarantee that an insertion must succeed if the  table is at least half empty. M 2 ⁄ H i2 mod M ( ) + H j2 mod M ( ) + i j ≠ H i2 + H j2 mod M ( ) + ≡ i2 j2 mod M ( ) ≡ i2 j2 – 0 mod M ( ) ≡ i j – ( ) i j + ( ) 0 mod M ( ) ≡ i j – i j + M 2 ⁄ figure 20.8 A routine used in  quadratic probing to  find a prime greater  than or equal to N /**  * Method to find a prime number at least as large as n.  * @param n the starting number (must be positive).  * @return a prime number larger than or equal to n.  */     private static int nextPrime( int n )     {         if( n % 2 == 0 )             n++;         for( ; !isPrime( n ); n += 2 )             ;         return n;     }

20.4 quadratic probing if the table size was 16, the only alternative locations would be at distances 1, 4, or 9 from the original probe point. Again, size is not really an issue: Although we would not have a guarantee of   alternatives, we would usually have more than we need. However, it is best to play it safe and use the theory to guide us in selecting parameters. Furthermore, it has been shown empirically that prime numbers tend to be good for hash tables because they tend to remove some of the nonrandomness that is occasionally introduced by the hash function. The second important consideration is efﬁciency. Recall that, for a load factor of 0.5, removing primary clustering saves only 0.5 probe for an average insertion and 0.1 probe for an average successful search. We do get some additional beneﬁts: Encountering a long probe sequence is signiﬁcantly less likely. However, if performing a probe using quadratic probing takes twice as long, doing so is hardly worth the effort. Linear probing is implemented with a simple addition (by 1), a test to determine whether wraparound is needed, and a very rare subtraction (if we need to do the wraparound). The formula for quadratic probing suggests that we need to do an addition by 1 (to go from   to i), a multiplication (to compute i2), another addition, and then a mod operation. Certainly this calculation appears to be much too expensive to be practical. However, we can use the following trick, as explained in Theorem 20.5. M 2 ⁄ i – Quadratic probing can be implemented without expensive multiplications and divisions. Theorem 20.5 Let   be the most recently computed probe (  is the original hash position) and   be the probe we are trying to compute. Then we have (20.3) If we subtract these two equations, we obtain (20.4) Equation 20.4 tells us that we compute the new value   from the previous value  Hi – 1 without squaring i. Although we still have a multiplication, the multiplication is  by 2, which is a trivially implemented bit shift on most computers. What about the mod  operation? That, too, is not really needed because the expression   must be  smaller than M. Therefore, if we add it to Hi – 1, the result will be either still smaller  than M (in which case, we do not need the mod) or just a little larger than M (in which  case, we can compute the mod equivalent by subtracting M). Proof Hi – H0 Hi Hi H0 i2 mod M ( ) + = Hi – H0 i – ( )2 mod M ( ) + = Hi Hi – 2i 1 mod M ( ) – + = Hi 2i – Quadratic probing  can be implemented without  multiplications and  mod operations.  Because it does not  suffer from primary  clustering, it outperforms linear probing  in practice.

chapter 20 hash tables When expanding a  hash table, reinsert  in the new table by  using the new hash  function. Expand the table as  soon as the load  factor reaches 0.5,  which is called  rehashing. Always  double to a prime  number. Prime  numbers are easy  to ﬁnd. Theorem 20.5 shows that we can compute the next position to probe by using an addition (to increment i), a bit shift (to evaluate 2i), a subtraction by 1 (to evaluate 2i – 1), another addition (to increment the old position by 2i – 1), a test to determine whether wraparound is needed, and a very rare subtraction to implement the mod operation. The difference is thus a bit shift, a subtraction by 1, and an addition per probe. The cost of this operation is likely to be less than the cost of doing an extra probe if complex keys (such as strings) are involved. The ﬁnal detail to consider is dynamic expansion. If the load factor exceeds 0.5, we want to double the size of the hash table. This approach raises a few issues. First, how hard will it be to ﬁnd another prime number? The answer is that prime numbers are easy to ﬁnd. We expect to have to test only O(log N) numbers until we ﬁnd a number that is prime. Consequently, the routine shown in Figure 20.8 is very fast. The primality test takes at most O(N1/2) time, so the search for a prime number takes at most O(N1/2 log N) time.2 This cost is much less than the O(N) cost of transferring the contents of the old table to the new. Once we have allocated a larger array, do we just copy everything over? The answer is most deﬁnitely no. The new array implies a new hash function, so we cannot use the old array positions. Thus we have to examine each element in the old table, compute its new hash value, and insert it in the new hash table. This process is called rehashing. Rehashing is easily implemented in Java. 20.4.1   java implementation The user must provide an appropriate  hashCode method  for objects.  We are now ready to give a complete Java implementation of a quadratic probing hash table. We will do so by implementing most of HashSet and HashMap from the Collections API. Recall that HashSet and HashMap both require a hashCode method. hashCode has no tableSize parameter; the hash table algorithms perform a ﬁnal mod operation internally after using the user-supplied hash function. The class skeleton for HashSet is shown in Figure 20.9. For the algorithms to work correctly, equals and hashCode must be consistent. That is, if two objects are equal, their hash values must be equal. The hash table consists of an array of HashEntry references. Each HashEntry reference is either null or an object that stores an item and a data member that tells us that the entry is either active or deleted. Because arrays of generic 2. This routine is also required if we add a constructor that allows the user to specify an approximate initial size for the hash table. The hash table implementation must ensure that a prime number is used.

20.4 quadratic probing figure 20.9 The class skeleton for a quadratic probing  hash table package weiss.util; public class HashSet<AnyType> extends AbstractCollection<AnyType>                               implements Set<AnyType> {     private class HashSetIterator implements Iterator<AnyType>       { /* Figure 20.18 */ }     private static class HashEntry implements java.io.Serializable       { /* Figure 20.10 */ }     public HashSet( )       { /* Figure 20.11 */ }     public HashSet( Collection<? extends AnyType> other )       { /* Figure 20.11 */ }     public int size( )       { return currentSize; }     public Iterator<AnyType> iterator( )       { return new HashSetIterator( ); }     public boolean contains( Object x )       { /* Figure 20.12 */ }     private static boolean isActive( HashEntry [ ] arr, int pos )       { /* Figure 20.13 */ }     public AnyType getMatch( AnyType x )       { /* Figure 20.12 */ }     public boolean remove( Object x )       { /* Figure 20.14 */ }     public void clear( )       { /* Figure 20.14 */ }     public boolean add( AnyType x )       { /* Figure 20.15 */ }     private void rehash( )       { /* Figure 20.16 */ }     private int findPos( Object x )       { /* Figure 20.17 */ }     private void allocateArray( int arraySize )       { array = new HashEntry[ arraySize ]; }     private static int nextPrime( int n )       { /* Figure 20.8 */ }     private static boolean isPrime( int n )       { /* See online code */ }     private int currentSize = 0;     private int occupied = 0;     private int modCount = 0;     private HashEntry [ ] array; }

chapter 20 hash tables types are illegal, HashEntry is not generic. The HashEntry nested class is shown in Figure 20.10. The array is declared at line 49. We need to keep track of both the logical size of the HashSet and the number of items in the hash table (including elements marked as deleted); these values are stored in currentSize and occupied, respectively, which are declared at lines 46 and 47. The general layout  is similar to that for  TreeSet. The rest of the class contains declarations for the hash table routines and iterator. The general layout is similar to that for TreeSet. Three private methods are declared; we describe them when they are used in the class implementation. We can now discuss the implementation of the HashSet class. Most routines are  just a few lines of  code because they  call findPos to perform quadratic  probing. The hash table constructors are shown in Figure 20.11; nothing special is going on here. The searching routine, contains, and the nonstandard getMatch are shown in Figure 20.12. contains uses the private method isActive, shown in Figure 20.13. Both contains and getMatch also call findPos, shown later, to implement quadratic probing. The findPos method is the only place in the entire code that depends on quadratic probing. Then contains and getMatch are easy to implement: An element is found if the result of findPos is an active cell (if findPos stops on an active cell, there must be a match). Similarly, the remove routine shown in Figure 20.14 is short. We check whether findPos takes us to an active cell; if so, the cell is marked deleted. Otherwise, false is returned immediately. Note that this lowers currentSize, but not occupied. Also, if there are many deleted items, the hash table is resized, at lines 16–17. The maintenance of modCount is identical to the other Collections API components previously implemented. clear removes all items from the HashSet. The add routine is shown in Figure 20.15. At line 8 we call findPos. If x is found, we return false at line 10 because duplicates are not allowed. Otherwise, findPos gives the place to insert x. The insertion is performed at line 12. figure 20.10 The HashEntry nested  class private static class HashEntry implements java.io.Serializable 2     {         public Object  element;   // the element         public boolean isActive;  // false if marked deleted         public HashEntry( Object e )         {             this( e, true );         }         public HashEntry( Object e, boolean i )         {             element  = e;             isActive = i;         }     }

20.4 quadratic probing figure 20.11 Hash table  initialization private static final int DEFAULT_TABLE_SIZE = 101;     /**      * Construct an empty HashSet.      */     public HashSet( )     {         allocateArray( DEFAULT_TABLE_SIZE );         clear( );     }     /**      * Construct a HashSet from any collection.      */     public HashSet( Collection<? extends AnyType> other )     {         allocateArray( nextPrime( other.size( ) * 2 ) );         clear( );         for( AnyType val : other )             add( val );     } figure 20.12 The searching  routines for a  quadratic probing  hash table /**  * This method is not part of standard Java.  * Like contains, it checks if x is in the set.  * If it is, it returns the reference to the matching  * object; otherwise it returns null.  * @param x the object to search for.  * @return if contains(x) is false, the return value is null;  * otherwise, the return value is the object that causes  * contains(x) to return true.  */     public AnyType getMatch( AnyType x )     {         int currentPos = findPos( x );         if( isActive( array, currentPos ) )             return (AnyType) array[ currentPos ].element;         return null;     }     /**      * Tests if some item is in this collection.      * @param x any object.      * @return true if this collection contains an item equal to x.      */     public boolean contains( Object x )     {         return isActive( array, findPos( x ) );     }

chapter 20 hash tables We adjust currentSize, occupied, and modCount at lines 13–15 and return unless a rehash is in order; otherwise, we call the private method rehash. The code that implements rehashing is shown in Figure 20.16. Line 7 saves a reference to the original table. We create a new, empty hash table at lines 10–12 that will have a 0.25 load factor when rehash terminates. Then we figure 20.13 The isActive method for a quadratic probing hash table /**  * Tests if item in pos is active.  * @param pos a position in the hash table.  * @param arr the HashEntry array (can be oldArray during rehash).  * @return true if this position is active.  */   private static boolean isActive( HashEntry [ ] arr, int pos )   {       return arr[ pos ] != null && arr[ pos ].isActive;   } figure 20.14 The remove and clear routines for a  quadratic probing  hash table /**  * Removes an item from this collection.  * @param x any object.  * @return true if this item was removed from the collection.  */     public boolean remove( Object x )     {         int currentPos = findPos( x );         if( !isActive( array, currentPos ) )             return false;         array[ currentPos ].isActive = false;         currentSize--;         modCount++;         if( currentSize < array.length / 8 )             rehash( );         return true;     }     /**      * Change the size of this collection to zero.      */     public void clear( )     {         currentSize = occupied = 0;         modCount++;         for( int i = 0; i < array.length; i++ )             array[ i ] = null;     }

20.4 quadratic probing scan through the original array and add any active elements in the new table. The add routine uses the new hash function (as it is logically based on the size of array, which has changed) and automatically resolves all collisions. We can be sure that the recursive call to add (at line 17) does not force another rehash. Alternatively, we could replace line 17 with two lines of code surrounded by braces (see Exercise 20.13). figure 20.15 The add routine for a  quadratic probing  hash table     /**      * Adds an item to this collection.      * @param x any object.      * @return true if this item was added to the collection.      */     public boolean add( AnyType x )     {         int currentPos = findPos( x );         if( isActive( array, currentPos ) )             return false;         if( array[ currentPos ] == null )              occupied++;         array[ currentPos ] = new HashEntry( x, true );         currentSize++;         modCount++;         if( occupied > array.length / 2 )             rehash( );         return true;     } figure 20.16 The rehash method  for a quadratic  probing hash table /**  * Private routine to perform rehashing.  * Can be called by both add and remove.  */     private void rehash( )     {         HashEntry [ ] oldArray = array;             // Create a new, empty table         allocateArray( nextPrime( 4 * size( ) ) );         currentSize = 0;         occupied = 0;             // Copy table over         for( int i = 0; i < oldArray.length; i++ )             if( isActive( oldArray, i ) )                  add( (AnyType) oldArray[ i ].element );     } The add routine  performs rehashing if the table is  (half) full.

chapter 20 hash tables So far, nothing that we have done depends on quadratic probing. Figure 20.17 implements findPos, which ﬁnally deals with the quadratic probing algorithm. We keep searching the table until we ﬁnd an empty cell or a match. Lines 22–25 directly implement the methodology described in Theorem 20.5, using two additions. There are additional complications because null is a valid item in the HashSet; the code illustrates why it is preferable to assume that null is invalid. Figure 20.18 gives the implementation of the iterator inner class. It is relatively standard fare, though quite tricky. visited represents the number of calls to next, while currentPos represents the index of the last object returned by next. Finally, Figure 20.19 implements HashMap. It is much like TreeMap, except that Pair is a nested class rather than an inner class (it does not need access to an outer object), and implements both equals and hashCode methods instead of the Comparable interface. figure 20.17 The routine that finally deals with quadratic probing /**  * Method that performs quadratic probing resolution.  * @param x the item to search for.  * @return the position where the search terminates.  */     private int findPos( Object x )     {         int offset = 1;         int currentPos = ( x == null ) ?                            0 : Math.abs( x.hashCode( ) % array.length );         while( array[ currentPos ] != null )         {             if( x == null )             {                 if( array[ currentPos ].element == null )                     break;             }             else if( x.equals( array[ currentPos ].element ) )                 break;              currentPos += offset;                  // Compute ith probe offset += 2;             if( currentPos >= array.length )       // Implement the mod                 currentPos -= array.length;         }         return currentPos;     }

20.4 quadratic probing figure 20.18 The HashSetIterator inner class /**  * This is the implementation of the HashSetIterator.  * It maintains a notion of a current position and of  * course the implicit reference to the HashSet.  */     private class HashSetIterator implements Iterator<AnyType>     {         private int expectedModCount = modCount;         private int currentPos = -1;         private int visited = 0;         public boolean hasNext( )         {             if( expectedModCount != modCount )                 throw new ConcurrentModificationException( );             return visited != size( );         }         public AnyType next( )         {             if( !hasNext( ) )                 throw new NoSuchElementException( );             do             {                 currentPos++;             } while( currentPos < array.length &&                                     !isActive( array, currentPos ) );             visited++;             return (AnyType) array[ currentPos ].element;         }         public void remove( )         {             if( expectedModCount != modCount ) throw new ConcurrentModificationException( );                           if( currentPos == -1 || !isActive( array, currentPos ) )                 throw new IllegalStateException( );             array[ currentPos ].isActive = false;             currentSize--;             visited--;             modCount++;             expectedModCount++;         }     }

chapter 20 hash tables figure 20.19 The HashMap class package weiss.util; public class HashMap<KeyType,ValueType> extends MapImpl<KeyType,ValueType> {     public HashMap( )       { super( new HashSet<Map.Entry<KeyType,ValueType>>( ) ); }     public HashMap( Map<KeyType,ValueType> other )       { super( other ); }     protected Map.Entry<KeyType,ValueType> makePair( KeyType key, ValueType value )       { return new Pair<KeyType,ValueType>( key, value ); }     protected Set<KeyType> makeEmptyKeySet( )       { return new HashSet<KeyType>( ); }     protected Set<Map.Entry<KeyType,ValueType>>     clonePairSet( Set<Map.Entry<KeyType,ValueType>> pairSet )     {         return new HashSet<Map.Entry<KeyType,ValueType>>( pairSet );     }     private static final class Pair<KeyType,ValueType>                                extends MapImpl.Pair<KeyType,ValueType>     {         public Pair( KeyType k, ValueType v )           { super( k, v ); }         public int hashCode( )         {             KeyType k = getKey( );             return k == null ? 0 : k.hashCode( );         }         public boolean equals( Object other )         {             if( other instanceof Map.Entry )             {                 KeyType thisKey = getKey( );                 KeyType otherKey = ((Map.Entry<KeyType,ValueType>) other).getKey( );                 if( thisKey == null )                     return thisKey == otherKey;                 return thisKey.equals( otherKey );             }             else                 return false;         }     } }

20.5 separate chaining hashing Quadratic probing  is implemented in  findPos. It uses the  previously described trick to  avoid multiplications and mods. In secondary clustering, elements that hash to the  same position  probe the same  alternative cells.  Secondary clustering is a minor theoretical blemish. Double hashing is a  hashing technique  that does not suffer from secondary  clustering. A second hash function  is used to drive the  collision resolution. 20.4.2   analysis of quadratic probing Quadratic probing has not yet been mathematically analyzed, although we know that it eliminates primary clustering. In quadratic probing, elements that hash to the same position probe the same alternative cells, which is known as secondary clustering. Again, the independence of successive probes cannot be assumed. Secondary clustering is a slight theoretical blemish. Simulation results suggest that it generally causes less than an extra one-half probe per search and that this increase is true only for high load factors. Figure 20.6 illustrates the difference between linear probing and quadratic probing and shows that quadratic probing does not suffer from as much clustering as does linear probing. Techniques that eliminate secondary clustering are available. The most popular is double hashing, in which a second hash function is used to drive the collision resolution. Speciﬁcally, we probe at a distance Hash2(X), 2Hash2(X), and so on. The second hash function must be carefully chosen (e.g., it should never evaluate to 0), and all cells must be capable of being probed. A function such as Hash2(X) = R – (X mod R), with R a prime smaller than M, generally works well. Double hashing is theoretically interesting because it can be shown to use essentially the same number of probes as the purely random analysis of linear probing would imply. However, it is somewhat more complicated than quadratic probing to implement and requires careful attention to some details. There seems to be no good reason not to use a quadratic probing strategy, unless the overhead of maintaining a half-empty table is burdensome. That would be the case in other programming languages if the items being stored were very large. 20.5 separate chaining hashing Separate chaining  hashing is a spaceefﬁcient alternative  to quadratic probing in which an  array of linked lists  is maintained. It is  less sensitive to  high load factors.  A popular and space-efﬁcient alternative to quadratic probing is separate chaining hashing in which an array of linked lists is maintained. For an array of linked lists, L0, L1, ..., LM – 1, the hash function tells us in which list to insert an item X and then, during a find, which list contains X. The idea is that, although searching a linked list is a linear operation, if the lists are sufﬁciently short, the search time will be very fast. In particular, suppose that the load factor, N/M, is λ, which is not bounded by 1.0. Thus the average list has length λ, making the expected number of probes for an insertion or unsuccessful search λ and the expected number of probes for a successful search  . The reason is that a successful search must occur in a nonempty list, and in such a list we expect to have to traverse halfway down the list. The relative cost of a successful search λ 2 ⁄ +

chapter 20 hash tables For separate chaining hashing, a reasonable load factor  is 1.0. A lower load  factor does not signiﬁcantly improve  performance; a  moderately higher  load factor is  acceptable and can  save space. versus an unsuccessful search is unusual in that, if  , the successful search is more expensive than the unsuccessful search. This condition makes sense, however, because many unsuccessful searches encounter an empty linked list. A typical load factor is 1.0; a lower load factor does not signiﬁcantly enhance performance, but it costs extra space. The appeal of separate chaining hashing is that performance is not affected by a moderately increasing load factor; thus rehashing can be avoided. For languages that do not allow dynamic array expansion, this consideration is signiﬁcant. Furthermore, the expected number of probes for a search is less than in quadratic probing, particularly for unsuccessful searches. We can implement separate chaining hashing by using our existing linked list classes. However, because the header node adds space overhead and is not really needed, if space were at a premium we could elect not to reuse components and instead implement a simple stacklike list. The coding effort turns out to be remarkably light. Also, the space overhead is essentially one reference per node, plus an additional reference per list; for example, when the load factor is 1.0, it is two references per item. This feature could be important in other programming languages if the size of an item is large. In that case, we have the same trade-offs as with the array and linked list implementations of stacks. The Java Collections API uses seperate chaining hashing, with a default load factor of 0.75. To illustrate the complexity (or rather, the relative lack of complexity) of the separate chaining hash table, Figure 20.20 provides a short sketch of the basic implementation of separate chaining hash tables. It avoids issues such as rehashing, and does not implement remove and does not even keep track of the current size. Nonetheless, it shows the basic logic of add and contains, both of which use the hash code to select an appropriate singly-linked list. 20.6 hash tables versus  binary search trees Use a hash table  instead of a binary  search tree if you  do not need order  statistics and are  worried about nonrandom inputs. We can also use binary search trees to implement insert and find operations. Although the resulting average time bounds are O(log N), binary search trees also support routines that require order and thus are more powerful. Using a hash table, we cannot efﬁciently ﬁnd the minimum element or extend the table to allow computation of an order statistic. We cannot search efﬁciently for a string unless the exact string is known. A binary search tree could quickly ﬁnd all items in a certain range, but this capability is not supported by a hash table. Furthermore, the O(log N) bound is not necessarily that much more than O(1), especially since no multiplications or divisions are required by search trees. λ <

20.6 hash tables versus binary search trees figure 20.20 Simplified implementation of separate chaining hash table class MyHashSet<AnyType> {     public MyHashSet( )       { this( 101 ); }     public MyHashSet( int numLists )       { lists = new Node[ numLists ]; }     public boolean contains( AnyType x )     {         for( Node<AnyType> p = lists[ myHashCode( x ) ]; p != null; p = p.next )             if( p.data.equals( x ) )                 return true;         return false;     }     public boolean add( AnyType x )     {         int whichList = myHashCode( x );         for( Node<AnyType> p = lists[ whichList ]; p != null; p = p.next )             if( p.data.equals( x ) )                  return false;         lists[ whichList ] = new Node<AnyType>( x, lists[ whichList ] );         return true;     }     private int myHashCode( AnyType x )       { return Math.abs( x.hashCode( ) % lists.length ); }     private Node<AnyType> [ ] lists;     private static class Node<AnyType>     {         Node( AnyType d, Node<AnyType> n )         {             data = d;             next = n;         }         AnyType data;         Node<AnyType> next;     } }

chapter 20 hash tables The worst case for hashing generally results from an implementation error, whereas sorted input can make binary search trees perform poorly. Balanced search trees are quite expensive to implement. Hence, if no ordering information is required and there is any suspicion that the input might be sorted, hashing is the data structure of choice. 20.7 hashing applications Hashing applications are abundant. Hashing applications are abundant. Compilers use hash tables to keep track of declared variables in source code. The data structure is called a symbol table. Hash tables are the ideal application for this problem because only insert and find operations are performed. Identiﬁers are typically short, so the hash function can be computed quickly. In this application, most searches are successful. Another common use of hash tables is in game programs. As the program searches through different lines of play, it keeps track of positions that it has encountered by computing a hash function based on the position (and storing its move for that position). If the same position recurs, usually by a simple transposition of moves, the program can avoid expensive recomputation. This general feature of all game-playing programs is called the transposition table. We discussed this feature in Section 10.2, where we implemented the tic-tactoe algorithm. A third use of hashing is in online spelling checkers. If misspelling detection (as opposed to correction) is important, an entire dictionary can be prehashed and words can be checked in constant time. Hash tables are well suited for this purpose because the words do not have to be alphabetized. Printing out misspellings in the order they occurred in the document is acceptable.

chapt er a priority queue:  the binary heap The priority queue is a fundamental data structure that allows access only to the minimum item. In this chapter we discuss one implementation of the priority queue data structure, the elegant binary heap. The binary heap supports the insertion of new items and the deletion of the minimum item in logarithmic worst-case time. It uses only an array and is easy to implement. In this chapter, we show n The basic properties of the binary heap n How the insert and deleteMin operations can be performed in logarithmic time n A linear-time heap construction algorithm n A Java 5 implementation of class PriorityQueue n An easily implemented sorting algorithm, heapsort, that runs in O(N log N) time but uses no extra memory n The use of heaps to implement external sorting

chapter 21 a priority queue: the binary heap 21.1 basic ideas A linked list or array  requires that some  operation use linear  time. As discussed in Section 6.9, the priority queue supports the access and deletion of the minimum item with findMin and deleteMin, respectively. We could use a simple linked list, performing insertions at the front in constant time, but then ﬁnding and/or deleting the minimum would require a linear scan of the list. Alternatively, we could insist that the list always be kept sorted. This condition makes the access and deletion of the minimum cheap, but then insertions would be linear. An unbalanced  binary search tree  does not have a  good worst case. A  balanced search  tree requires lots of  work. Another way of implementing priority queues is to use a binary search tree, which gives an O(log N) average running time for both operations. However, a binary search tree is a poor choice because the input is typically not sufﬁciently random. We could use a balanced search tree, but the structures shown in Chapter 19 are cumbersome to implement and lead to sluggish performance in practice. (In Chapter 22, however, we cover a data structure, the splay tree, that has been shown empirically to be a good alternative in some situations.) The priority queue  has properties that  are a compromise  between a queue  and a binary search  tree. On the one hand, because the priority queue supports only some of the search tree operations, it should not be more expensive to implement than a search tree. On the other hand, the priority queue is more powerful than a simple queue because we can use a priority queue to implement a queue as follows. First, we insert each item with an indication of its insertion time. Then, a deleteMin on the basis of minimum insertion time implements a dequeue. Consequently, we can expect to obtain an implementation with properties that are a compromise between a queue and a search tree. This compromise is realized by the binary heap, which n Can be implemented by using a simple array (like the queue) n Supports insert and deleteMin in O(log N) worst-case time (a compromise between the binary search tree and the queue) n Supports insert in constant average time and findMin in constant worst-case time (like the queue) The binary heap is  the classic method  used to implement  priority queues. The binary heap is the classic method used to implement priority queues and— like the balanced search tree structures in Chapter 19—has two properties: a structure property and an ordering property. And as with balanced search trees, an operation on a binary heap can destroy one of the properties, so a binary heap operation must not terminate until both properties are in order. This outcome is simple to achieve. (In this chapter, we use the word heap to refer to the binary heap.) 21.1.1   structure property The only structure that gives dynamic logarithmic time bounds is the tree, so it seems natural to organize the heap’s data as a tree. Because we want

21.1 basic ideas the logarithmic bound to be a worst-case guarantee, the tree should be balanced. The heap is a complete binary tree, allowing representation by a simple  array and guaranteeing logarithmic  depth. A complete binary tree is a tree that is completely ﬁlled, with the possible exception of the bottom level, which is ﬁlled from left to right and has no missing nodes. An example of a complete binary tree of 10 items is shown in Figure 21.1. Had the node J been a right child of E, the tree would not be complete because a node would be missing. The complete tree has a number of useful properties. First, the height (longest path length) of a complete binary tree of N nodes is at most ⎣log N⎦. The reason is that a complete tree of height H has between 2H and 2H + 1 – 1 nodes. This characteristic implies that we can expect logarithmic worst-case behavior if we restrict changes in the structure to one path from the root to a leaf. The parent is in  position ⎣i/2⎦, the  left child is in position 2i, and the  right child is in  position 2i + 1. Second and equally important, in a complete binary tree, left and right links are not needed. As shown in Figure 21.1, we can represent a complete binary tree by storing its level-order traversal in an array. We place the root in position 1 (position 0 is often left vacant, for a reason discussed shortly). We also need to maintain an integer that tells us the number of nodes currently in the tree. Then for any element in array position i, its left child can be found in position 2i. If this position extends past the number of nodes in the tree, we know that the left child does not exist. Similarly, the right child is located immediately after the left child; thus it resides in position 2i + 1. We again test against the actual tree size to be sure that the child exists. Finally, the parent is in position  .  Note that every node except the root has a parent. If the root were to have a parent, the calculation would place it in position 0. Thus we reserve position 0 for a dummy item that can serve as the root’s parent. Doing so can simplify one of the operations. If instead we choose to place the root in position 0, the locations of the children and parent of the node in position i change slightly (in Exercise 21.15 you are asked to determine the new locations). Using an array to store a tree is called implicit representation. As a result of this representation, not only are child links not required, but also the operations figure 21.1 A complete binary  tree and its array  representation A B C D E F G H I J 10 11 12 13 A B C D E F G H I J 11 12 13 i 2 ⁄ Using an array to  store a tree is  called implicit representation.

chapter 21 a priority queue: the binary heap required to traverse the tree are extremely simple and likely to be very fast on most computers. The heap entity consists of an array of objects and an integer representing the current heap size. In this chapter, heaps are drawn as trees to make the algorithms easier to visualize. In the implementation of these trees we use an array. We do not use the implicit representation for all search trees. Some of the problems with doing so are covered in Exercise 21.8. 21.1.2   heap-order property The heap-order property states that,  in a heap, the item  in the parent is  never larger than  the item in a node. The property that allows operations to be performed quickly is the heap-order property. We want to be able to ﬁnd the minimum quickly, so it makes sense that the smallest element should be at the root. If we consider that any subtree should also (recursively) be a heap, any node should be smaller than all of its descendants. Applying this logic, we arrive at the heap-order property. heap-order property In a heap, for every node X with parent P, the key in P is smaller than or equal to the key in X. The heap-order property is illustrated in Figure 21.2. In Figure 21.3(a), the tree is a heap, but in Figure 21.3(b), the tree is not (the dashed line figure 21.2 Heap-order property P X P <– X figure 21.3 Two complete trees:  (a) a heap; (b) not a  heap (a) (b) The root’s parent  can be stored in  position 0 and  given a value of  negative inﬁnity.

21.1 basic ideas shows the violation of heap order). Note that the root does not have a parent. In the implicit representation, we could place the value   in position 0 to remove this special case when we implement the heap. By the heap-order property, we see that the minimum element can always be found at the root. Thus findMin is a constant time operation. A max heap supports access of the maximum instead of the minimum. Minor changes can be used to implement max heaps. 21.1.3   allowed operations Now that we have settled on the representation, we can start writing code for our implementation of java.util.PriorityQueue. We already know that our heap supports the basic insert, findMin, and deleteMin operations and the usual isEmpty and makeEmpty routines. Figure 21.4 shows the class skeleton using the naming conventions in java.util.PriorityQueue. We will refer to the operations using both the historic names and their java.util equivalents. We provide a constructor that  accepts a collection  containing an initial  set of items and  calls buildHeap. We begin by examining the public methods. A trio of constructors are declared at lines 9 to 14. The third constructor accepts a collection of items that should initially be in the priority queue. Why not just insert the items one at a time? The reason is that in numerous applications we can add many items before the next deleteMin occurs. In those cases, we do not need to have heap order in effect until the deleteMin occurs. The buildHeap operation, declared at line 32, reinstates the heap order—no matter how messed up the heap is—and we will see that it works in linear time. Thus, if we need to place N items in the heap before the ﬁrst deleteMin, placing them in the array sloppily and then doing one buildHeap is more efﬁcient than doing N insertions. The add method is declared at line 25. It adds a new item x into the heap, performing the necessary operations to maintain the heap-order property.  The remaining operations are as expected. The element routine is declared at line 23 and returns the minimum item in the heap. remove is declared at line 27 and removes and then returns the minimum item. The usual size, clear, and iterator routines are declared at lines 16 to 21.  The constructors are shown in Figure 21.5. All initialize the array, the size, and the comparator; the third constructor additionally copies in the collection passed as a parameter and then calls buildHeap. Figure 21.6 shows element. ∞ –

chapter 21 a priority queue: the binary heap figure 21.4 The PriorityQueue class skeleton 1 package weiss.util; 3 /** 4  * PriorityQueue class implemented via the binary heap. 5  */ 6 public class PriorityQueue<AnyType> extends AbstractCollection<AnyType> 7                                     implements Queue<AnyType> 8 { 9     public PriorityQueue( ) 10       { /* Figure 21.5 */ } 11     public PriorityQueue( Comparator<? super AnyType> c ) 12       { /* Figure 21.5 */ } 13     public PriorityQueue( Collection<? extends AnyType> coll ) 14       { /* Figure 21.5 */ } 16     public int size( ) 17       { return currentSize; } 18     public void clear( ) 19       { currentSize = 0; } 20     public Iterator<AnyType> iterator( ) 21       { /* See online code */ } 23     public AnyType element( ) 24       { /* Figure 21.6 */ } 25     public boolean add( AnyType x ) 26       { /* Figure 21.9 */ } 27     public AnyType remove( ) 28       { /* Figure 21.13 */ } 30     private void percolateDown( int hole ) 31       { /* Figure 21.14 */ } 32     private void buildHeap( ) 33       { /* Figure 21.16 */ } 35     private int currentSize;   // Number of elements in heap 36     private AnyType [ ] array; // The heap array 37     private Comparator<? super AnyType> cmp; 39     private void doubleArray( ) 40       { /* See online code */ } 41     private int compare( AnyType lhs, AnyType rhs ) 42       { /* Same code as in TreeSet; see Figure 19.70 */ } 43 }

21.1 basic ideas figure 21.5 Constructors for the PriorityQueue class private static final int DEFAULT_CAPACITY = 100; 3     /** 4      * Construct an empty PriorityQueue. 5      */ 6     public PriorityQueue( ) 7     { 8         currentSize = 0; 9         cmp = null; 10         array = (AnyType[]) new Object[ DEFAULT_CAPACITY + 1 ]; 11     } 13     /** 14      * Construct an empty PriorityQueue with a specified comparator. 15      */ 16     public PriorityQueue( Comparator<? super AnyType> c ) 17     { 18         currentSize = 0; 19         cmp = c; 20         array = (AnyType[]) new Object[ DEFAULT_CAPACITY + 1 ]; 21     } 24     /** 25      * Construct a PriorityQueue from another Collection. 26      */ 27     public PriorityQueue( Collection<? extends AnyType> coll ) 28     { 29         cmp = null; 30         currentSize = coll.size( ); 31         array = (AnyType[]) new Object[ ( currentSize + 2 ) * 11 / 10 ]; 33         int i = 1; 34         for( AnyType item : coll ) 35             array[ i++ ] = item; 36         buildHeap( ); 37     } figure 21.6 The element routine /**  * Returns the smallest item in the priority queue.  * @return the smallest item.  * @throws NoSuchElementException if empty.  */     public AnyType element( )     {         if( isEmpty( ) )             throw new NoSuchElementException( );         return array[ 1 ];     }

chapter 21 a priority queue: the binary heap 21.2 implementation of the  basic operations The heap-order property looks promising so far because easy access to the minimum is provided. We must now show that we can efﬁciently support insertion and deleteMin in logarithmic time. Performing the two required operations is easy (both conceptually and practically): The work merely involves ensuring that the heap-order property is maintained. 21.2.1   insertion Insertion is implemented by creating  a hole at the next  available location  and then percolating it up until the  new item can be  placed in it without  introducing a heaporder violation with  the hole’s parent. To insert an element X in the heap, we must ﬁrst add a node to the tree. The only option is to create a hole in the next available location; otherwise, the tree is not complete and we would violate the structure property. If X can be placed in the hole without violating heap order, we do so and are done. Otherwise, we slide the element that is in the hole’s parent node into the node, bubbling the hole up toward the root. We continue this process until X can be placed in the hole. Figure 21.7 shows that to insert 14, we create a hole in the next available heap location. Inserting 14 into the hole would violate the heaporder property, so 31 is slid down into the hole. This strategy is continued in Figure 21.8 until the correct location for 14 is found. This general strategy is called percolate up, in which insertion is implemented by creating a hole at the next available location and bubbling it up the heap until the correct location is found. Figure 21.9 shows the add method, which implements the percolate up strategy by using a very tight loop. At line 13, we place x as the –∞sentinel in position 0. The statement at line 12 increments the current size and sets the hole to the newly added node. We iterate the loop at line 15 as long as the item in the parent node is larger than x. figure 21.7 Attempt to insert 14,  creating the hole and  bubbling the hole up (a) (b)

21.2 implementation of the basic operations Line 16 moves the item in the parent down into the hole, and then the third expression in the for loop moves the hole up to the parent. When the loop terminates, line 17 places x in the hole. Insertion takes constant time on average but logarithmic  time in the worst  case. The time required to do the insertion could be as much as O(log N) if the element to be inserted is the new minimum. The reason is that it will be percolated up all the way to the root. On average the percolation terminates early: It has been shown that 2.6 comparisons are required on average to perform the add, so the average add moves an element up 1.6 levels. figure 21.8 The remaining two  steps required to  insert 14 in the  original heap shown in  Figure 21.7 (a) (b) figure 21.9 The add method /**  * Adds an item to this PriorityQueue.  * @param x any object.  * @return true.  */     public boolean add( AnyType x )     {         if( currentSize + 1 == array.length )             doubleArray( );             // Percolate up         int hole = ++currentSize;         array[ 0 ] = x;         for( ; compare( x, array[ hole / 2 ] ) < 0; hole /= 2 )             array[ hole ] = array[ hole / 2 ];         array[ hole ] = x;         return true;     }

chapter 21 a priority queue: the binary heap 21.2.2   the deleteMin operation  The deleteMin operation is handled in a similar manner to the insertion operation. As shown already, ﬁnding the minimum is easy; the hard part is removing it. When the minimum is removed, a hole is created at the root. The heap now becomes one size smaller, and the structure property tells us that the last node must be eliminated. Figure 21.10 shows the situation: The minimum item is 13, the root has a hole, and the former last item needs to be placed in the heap somewhere. If the last item could be placed in the hole, we would be done. That is impossible, however, unless the size of the heap is two or three, because elements at the bottom are expected to be larger than elements on the second level. We must play the same game as for insertion: We put some item in the hole and then move the hole. The only difference is that for the deleteMin we move down the tree. To do so, we ﬁnd the smaller child of the hole, and if that child is smaller than the item that we are trying to place, we move the child into the hole, pushing the hole down one level and repeating these actions until the item can be correctly placed—a process called percolate down. In Figure 21.11, we place the smaller child (14) in the hole, sliding the hole down one level. We repeat this action, placing 19 in the hole and creating a new hole one level deeper. We then place 26 in the hole and create a new figure 21.10 Creation of the hole at  the root 32 31 Min = 13 figure 21.11 The next two steps in  the deleteMin operation 32 31 Deletion of the minimum involves placing the former last  item in a hole that  is created at the  root. The hole is  percolated down the tree through  minimum children  until the item can  be placed without  violating the heaporder property. The deleteMin operation is logarithmic in both the  worst and average  cases.

21.2 implementation of the basic operations hole on the bottom level. Finally, we are able to place 31 in the hole, as shown in Figure 21.12. Because the tree has logarithmic depth, deleteMin is a logarithmic operation in the worst case. Not surprisingly, percolation rarely terminates more than one or two levels early, so deleteMin is logarithmic on average, too. Figure 21.13 shows this method, which is named remove in the standard library. The test for emptiness in remove is automatically done by the call to element, which is named remove in the standard library, at line 8. The real work is done in percolateDown, shown in Figure 21.14. The code shown there is similar in spirit to the percolation up code in the add routine. However, because there are two children rather than one parent, the code is a bit more complicated. The percolateDown method takes a single parameter that indicates where the hole is to be placed. The item in the hole is then moved out, and the percolation begins. For remove, hole will be position 1. The for loop at line 10 terminates when there is no left child. The third expression moves the hole to the child. The smaller child is found at lines 13–15. We have to be careful because the last node in an even-sized heap is an only child; we cannot always assume that there are two children, which is why we have the ﬁrst test at line 13. figure 21.12 The last two steps in  the deleteMin operation figure 21.13 The remove method /**  * Removes the smallest item in the priority queue.  * @return the smallest item.  * @throws NoSuchElementException if empty.  */     public AnyType remove( )     {         AnyType minItem = element( );         array[ 1 ] = array[ currentSize-- ];         percolateDown( 1 );         return minItem;     }

chapter 21 a priority queue: the binary heap 21.3 the buildHeap operation:  linear-time heap construction The buildHeap operation can be done  in linear time by  applying a percolate down routine to  nodes in reverse  level order. The buildHeap operation takes a complete tree that does not have heap order and reinstates it. We want it to be a linear-time operation, since N insertions could be done in O(N log N) time. We expect that O(N) is attainable because N successive insertions take a total of O(N) time on average, based on the result stated at the end of Section 21.2.1. The N successive insertions do more work than we require because they maintain heap order after every insertion and we need heap order only at one instant. The easiest abstract solution is obtained by viewing the heap as a recursively deﬁned structure, as shown in Figure 21.15: We recursively call buildHeap on the left and right subheaps. At that point, we are guaranteed that heap order has been established everywhere except at the root. We can establish heap order everywhere by calling percolateDown for the root. The recursive routine works by guaranteeing that when we apply percolateDown(i), all descendants of i have been processed recursively by their own calls to percolateDown. The recursion, however, is not necessary, for the following figure 21.14 The percolateDown method used for remove and buildHeap /**  * Internal method to percolate down in the heap.  * @param hole the index at which the percolate begins.  */     private void percolateDown( int hole )     {         int child;         AnyType tmp = array[ hole ];         for( ; hole * 2 <= currentSize; hole = child )         {             child = hole * 2;             if( child != currentSize &&                     compare( array[ child + 1 ], array[ child ] ) < 0 )                 child++;             if( compare( array[ child ], tmp ) < 0 )                 array[ hole ] = array[ child ];             else                 break;         }         array[ hole ] = tmp;     }

21.3 the buildHeap operation: linear-time heap construction reason. If we call percolateDown on nodes in reverse level order, then at the point percolateDown(i) is processed, all descendants of node i will have been processed by a prior call to percolateDown. This process leads to an incredibly simple algorithm for buildHeap, which is shown in Figure 21.16. Note that percolateDown need not be performed on a leaf. Thus we start at the highest numbered nonleaf node. The tree in Figure 21.17(a) is the unordered tree. The seven remaining trees in Figures 21.17(b) through 21.20 show the result of each of the seven percolateDown operations. Each dashed line corresponds to two comparisons: one to ﬁnd the smaller child and one to compare the smaller child with the node. Notice that the ten dashed lines in the algorithm correspond to 20 comparisons. (There could have been an eleventh line.) figure 21.15 Recursive view of the  heap R figure 21.16 Implementation of the  linear-time buildHeap method /**  * Establish heap order property from an arbitrary  * arrangement of items. Runs in linear time.  */     private void buildHeap( )     {         for( int i = currentSize / 2; i > 0; i-- )             percolateDown( i );     } figure 21.17 (a) Initial heap;  (b) after  percolateDown(7) (b) (a)

chapter 21 a priority queue: the binary heap The linear-time  bound can be  shown by computing the sum of the  heights of all the  nodes in the heap. To bound the running time of buildHeap, we must bound the number of dashed lines. We can do so by computing the sum of the heights of all the nodes in the heap, which is the maximum number of dashed lines. We expect a small number because half the nodes are leaves and have height 0 and a quarter of the nodes have height 1. Thus only a quarter of the nodes (those not already counted in the ﬁrst two cases) can contribute more than 1 unit of height. In particular, only one node contributes the maximum height of ⎣log N⎦. figure 21.18 (a) After  percolateDown(6); (b) after  percolateDown(5) (b) (a) figure 21.19 (a) After  percolateDown(4); (b) after  percolateDown(3) (b) (a) figure 21.20 (a) After  percolateDown(2); (b) after  percolateDown(1) and buildHeap terminates (b) (a)

21.3 the buildHeap operation: linear-time heap construction We prove the  bound for perfect  trees by using a  marking argument. To obtain a linear-time bound for buildHeap, we need to establish that the sum of the heights of the nodes of a complete binary tree is O(N). We do so in Theorem 21.1, proving the bound for perfect trees by using a marking argument. A complete binary tree is not a perfect binary tree, but the result we have obtained is an upper bound on the sum of the heights of the nodes in a complete binary tree. A complete binary tree has between 2H and 2H + 1 – 1 figure 21.21 Marking the left  edges for height 1  nodes For the perfect binary tree of height H containing N = 2H + 1 – 1 nodes, the sum of the  heights of the nodes is N – H – 1. Theorem 21.1 We use a tree-marking argument. (A more direct brute-force calculation could also  be done, as in Exercise 21.10.) For any node in the tree that has some height h, we  darken h tree edges as follows. We go down the tree by traversing the left edge and  then only right edges. Each edge traversed is darkened. An example is a perfect tree  of height 4. Nodes that have height 1 have their left edge darkened, as shown in  Figure 21.21. Next, nodes of height 2 have a left edge and then a right edge darkened on the path from the node to the bottom, as shown in Figure 21.22. In  Figure 21.23, three edges are darkened for each node of height 3: the ﬁrst left edge  leading out of the node and then the two right edges on the path to the bottom.  Finally, in Figure 21.24 four edges are darkened: the left edge leading out of the root  and the three right edges on the path to the bottom. Note that no edge is ever darkened twice and that every edge except those on the right path is darkened. As there  are (N – 1) tree edges (every node has an edge coming into it except the root) and H edges on the right path, the number of darkened edges is N – H – 1. This proves the  theorem. Proof

chapter 21 a priority queue: the binary heap nodes, so this theorem implies that the sum is O(N). A more careful argument establishes that the sum of the heights is N – v(N), where v(N) is the number of 1s in the binary representation of N. A proof of this is left for you to do as Exercise 21.12. figure 21.22 Marking the first left  edge and the  subsequent right edge  for height 2 nodes figure 21.23 Marking the first left  edge and the  subsequent two right  edges for height 3  nodes figure 21.24 Marking the first left  edge and the  subsequent two right  edges for the  height 4 node

21.5 internal sorting: heapsort 21.4 advanced operations: decreaseKey and merge In Chapter 23 we examine priority queues that support two additional operations. The decreaseKey operation lowers the value of an item in the priority queue. The item’s position is presumed known. In a binary heap this operation is easily implemented by percolating up until heap order is reestablished. However, we must be careful because by assumption each item’s position is being stored separately, and all items involved in the percolation have their positions altered. It is possible to incorporate decreaseKey into the PriorityQueue class. This is left as Exercise 21.30. The decreaseKey operation is useful in implementing graph algorithms (e.g., Dijkstra’s algorithm presented in Section 14.3). The merge routine combines two priority queues. Because the heap is array-based, the best we can hope to achieve with a merge is to copy the items from the smaller heap to the larger heap and do some rearranging. Doing so takes at least linear time per operation. If we use general trees with nodes connected by links, we can reduce the bound to logarithmic cost per operation. Merging has uses in advanced algorithm design. 21.5 internal sorting: heapsort The priority queue can be used to sort N items by the following: 1. Inserting every item into a binary heap  2. Extracting every item by calling deleteMin N times, thus sorting the result Using the observation in Section 21.4, we can more efﬁciently implement this procedure by 1. Tossing each item into a binary heap 2. Applying buildHeap 3. Calling deleteMin N times, with the items exiting the heap in sorted order Step 1 takes linear time total, and step 2 takes linear time. In step 3, each call to deleteMin takes logarithmic time, so N calls take O(N log N) time. Consequently, we have an O(N log N) worst-case sorting algorithm, called heapsort, which is as good as can be achieved by a comparison-based algorithm (see Section 8.8). One problem with the algorithm as it stands now is that sorting A priority queue  can be used to sort  in O(N log N ) time. An algorithm based  on this idea is  heapsort.

chapter 21 a priority queue: the binary heap an array requires the use of the binary heap data structure, which itself carries the overhead of an array. Emulating the heap data structure on the array that is input—rather than going through the heap class apparatus—would be preferable. We assume for the rest of this discussion that this is done. Even though we do not use the heap class directly, we still seem to need a second array. The reason is that we have to record the order in which items exit the heap equivalent in a second array and then copy that ordering back into the original array. The memory requirement is doubled, which could be crucial in some applications. Note that the extra time spent copying the second array back to the ﬁrst is only O(N), so, unlike mergesort, the extra array does not affect the running time signiﬁcantly. The problem is space. By using empty  parts of the array,  we can perform the  sort in place. A clever way to avoid using a second array makes use of the fact that, after each deleteMin, the heap shrinks by 1. Thus the cell that was last in the heap can be used to store the element just deleted. As an example, suppose that we have a heap with six elements. The ﬁrst deleteMin produces A1. Now the heap has only ﬁve elements, so we can place A1 in position 6. The next deleteMin produces A2. As the heap now has only four elements, we can place A2 in position 5. If we use a max  heap, we obtain  items in increasing  order. When we use this strategy, after the last deleteMin the array will contain the elements in decreasing sorted order. If we want the array to be in the more typical increasing sorted order, we can change the ordering property so that the parent has a larger key than the child does. Thus we have a max heap. For example, let us say that we want to sort the input sequence 59, 36, 58, 21, 41, 97, 31, 16, 26, and 53. After tossing the items into the max heap and applying buildHeap, we obtain the arrangement shown in Figure 21.25. (Note that there is no sentinel; we presume the data starts in position 0, as is typical for the other sorts described in Chapter 8.) Figure 21.26 shows the heap that results after the ﬁrst deleteMax. The last element in the heap is 21; 97 has been placed in a part of the heap array that is technically no longer part of the heap. figure 21.25 Max heap after the  buildHeap phase 10 11 12 13 97 53 59 26 41 58 31 16 21 36 J 11 12 13

21.5 internal sorting: heapsort Figure 21.27 shows that after a second deleteMax, 16 becomes the last element. Now only eight items remain in the heap. The maximum element removed, 59, is placed in the dead spot of the array. After seven more deleteMax operations, the heap represents only one element, but the elements left in the array will be sorted in increasing order. Minor changes are  required for heapsort because the  root is stored in  position 0. Implementation of the heapsort operation is simple because it basically follows the heap operation. There are three minor differences between the two operations. First, because we are using a max heap, we need to reverse the logic of the comparisons from > to <. Second, we can no longer assume that there is a sentinel position 0. The reason is that all our other sorting algorithms store data at position 0, and we must assume that heapSort is no different. Although the sentinel is not needed anyway (there are no percolate up operations), its absence affects calculations of the child and parent. That is, for a node in position i, the parent is in position  , the left child is in position  , and the right child is next to the left child. Third, percDown needs to be informed of the current heap size (which is lowered by 1 in each iteration of deleteMax). The implementation of percDown is left for you to do as figure 21.26 Heap after the first  deleteMax operation 10 11 12 13 59 53 58 26 41 36 31 16 21 97 J 11 12 13 figure 21.27 Heap after the second  deleteMax operation 10 11 12 13 58 53 36 26 41 21 31 16 59 97 J 11 12 13 i – ( ) 2 ⁄ 2i +

chapter 21 a priority queue: the binary heap Exercise 21.23. Assuming that we have written percDown, we can easily express heapSort as shown in Figure 21.28. Although heapsort is not as fast as quicksort, it can still be useful. As discussed in Section 8.6 (and detailed in Exercise 8.20), in quicksort we can keep track of each recursive call’s depth, and switch to an O(N log N) worst-case sort for any recursive call that is too deep (roughly 2log N nested calls). Exercise 8.20 suggested mergesort, but actually heapsort is the better candidate. 21.6 external sorting External sorting is  used when the  amount of data is  too large to ﬁt in  main memory. So far, all the sorting algorithms examined require that the input ﬁt in main memory. However, the input for some applications is much too large to ﬁt in main memory. In this section we discuss external sorting, which is used to handle such very large inputs. Some of the external sorting algorithms involve the use of heaps. 21.6.1   why we need new algorithms Most of the internal sorting algorithms take advantage of the fact that memory is directly accessible. Shellsort compares elements a[i] and a[i-gap] in one time unit. Heapsort compares a[i] and a[child=i*2] in one time unit. Quicksort, with median-of-three pivoting, requires comparing a[first], a[center], and a[last] in a constant number of time units. If the input is on a tape, all these operations lose their efﬁciency because elements on a tape can be accessed only sequentially. Even if the data are on a disk, efﬁciency still suffers because of the delay required to spin the disk and move the disk head. figure 21.28 The heapSort routine     // Standard heapsort.     public static <AnyType extends Comparable<? super AnyType>>     void heapsort( AnyType [ ] a )     {         for( int i = a.length / 2 - 1; i >= 0; i-- )  // Build heap             percDown( a, i, a.length );         for( int i = a.length - 1; i > 0; i-- )         {             swapReferences( a, 0, i );                // deleteMax             percDown( a, 0, i );         }     }

21.6 external sorting To demonstrate how slow external accesses really are, we could create a random ﬁle that is large but not too big to ﬁt in main memory. When we read in the ﬁle and sort it by using an efﬁcient algorithm, the time needed to read the input is likely to be signiﬁcant compared to the time required to sort the input, even though sorting is an O(N log N) operation (or worse for Shellsort) and reading the input is only O(N). 21.6.2   model for external sorting We assume that  sorts are performed  on tape. Only  sequential access  of the input is  allowed. The wide variety of mass storage devices makes external sorting much more device-dependent than internal sorting. The algorithms considered here work on tapes, which are probably the most restrictive storage medium. Access to an element on tape is gained by winding the tape to the correct location, so tapes can be efﬁciently accessed only in sequential order (in either direction). Let us assume that we have at least three tape drives for performing the sort. We need two drives to do an efﬁcient sort; the third drive simpliﬁes matters. If only one tape drive is present, we are in trouble: Any algorithm will require Ω(N2) tape accesses. 21.6.3   the simple algorithm The basic external  sort uses repeated  two-way merging.  Each group of  sorted records is a  run. As a result of a  pass, the length of  the runs doubles  and eventually only  a single run  remains. The basic external sorting algorithm involves the use of the merge routine from mergesort. Suppose that we have four tapes A1, A2, B1, and B2, which are two input and two output tapes. Depending on the point in the algorithm, the A tapes are used for input and the B tapes for output, or vice versa. Suppose further that the data are initially on A1 and that the internal memory can hold (and sort) M records at a time. The natural ﬁrst step is to read M records at a time from the input tape, sort the records internally, and then write the sorted records alternately to B1 and B2. Each group of sorted records is called a run. When done, we rewind all the tapes. If we have the same input as in our example for Shellsort, the initial conﬁguration is as shown in Figure 21.29. If figure 21.29 Initial tape  configuration A1 A2 B1 B2

chapter 21 a priority queue: the binary heap M = 3, after the runs have been constructed, the tapes contain the data, as shown in Figure 21.30. Now B1 and B2 contain a group of runs. We take the ﬁrst runs from each tape, merge them, and write the result—which is a run twice as long—to A1. Then we take the next runs from each tape, merge them, and write the result to A2. We continue this process, alternating output to A1 and A2 until either B1 or B2 is empty. At this point, either both are empty or one (possibly short) run is left. In the latter case, we copy this run to the appropriate tape. We rewind all four tapes and repeat the same steps, this time using the A tapes as input and the B tapes as output. This process gives runs of length 4M. We continue this process until we get one run of length N, at which point the run represents the sorted arrangement of the input. Figures 21.31– 21.33 show how this process works for our sample input. The algorithm will require ⎡log(N/M)⎤passes, plus the initial runconstructing pass. For instance, if we have 10,000,000 records of 6,400 bytes each and 200 MB of internal memory, the ﬁrst pass creates 320 runs. We figure 21.30 Distribution of length  3 runs to two tapes A1 A2 B1 B2 figure 21.31 Tapes after the first  round of merging (run  length = 6) A1 A2 B1 B2 figure 21.32 Tapes after the  second round of  merging (run length  = 12) A1 A2 B1 B2 We need  ⎡log(N/M)⎤passes over the input  before we have one  giant run.

21.6 external sorting would then need nine more passes to complete the sort. This formula also correctly tells us that our example in Figure 21.30 requires ⎡log(13/3)⎤, or three more passes. 21.6.4   multiway merge K-way merging  reduces the number of passes. The  obvious implementation uses 2K tapes. If we have extra tapes, we can reduce the number of passes required to sort our input with a multiway (or K-way) merge. We do so by extending the basic (two-way) merge to a K-way merge and use 2K tapes. Merging two runs is done by winding each input tape to the beginning of each run. Then the smaller element is found and placed on an output tape, and the appropriate input tape is advanced. If there are K input tapes, this strategy works in the same way; the only difference is that ﬁnding the smallest of the K elements is slightly more complicated. We can do so by using a priority queue. To obtain the next element to write on the output tape, we perform a deleteMin operation. The appropriate input tape is advanced, and if the run on that input tape has not yet been completed, we insert the new element in the priority queue. Figure 21.34 shows how the input from the previous example is distributed onto three tapes. Figures 21.35 and 21.36 show the two passes of three-way merging that complete the sort. figure 21.33 Tapes after the third  round of merging A1 A2 B1 B2 figure 21.34 Initial distribution of  length 3 runs to three  tapes A1 A2 A3 B1 B2 B3

chapter 21 a priority queue: the binary heap After the initial run-construction phase, the number of passes required using K-way merging is ⎡logK(N/M)⎤because the length of the runs gets K times larger in each pass. For our example, the formula is veriﬁed because . If we have 10 tapes, K = 5. For the large example in Section 21.6.3, 320 runs would require   passes. 21.6.5   polyphase merge The K-way merging strategy requires the use of 2K tapes, which could be prohibitive for some applications. We can get by with only K + 1 tapes, called a polyphase merge. An example is performing two-way merging with only three tapes. Suppose that we have three tapes—T1, T2, and T3—and an input ﬁle on T1 that can produce 34 runs. One option is to put 17 runs each on T2 and T3. We could then merge this result onto T1, thereby obtaining one tape with 17 runs. The problem is that, as all the runs are on one tape, we must now put some of these runs on T2 to perform another merge. The logical way to do that is to copy the figure 21.35 After one round of  three-way merging  (run length = 9) A1 A2 A3 B1 B2 B3 figure 21.36 After two rounds of  three-way merging A1 A2 A3 B1 B2 B3 13 3 ⁄ log = log = The polyphase merge implements  a K-way merge with  K + 1 tapes

21.6 external sorting ﬁrst eight runs from T1 to T2 and then perform the merge. This approach adds an extra half pass for every pass that we make. The question is, can we do better? The distribution of  runs affects performance. The best  distribution is  related to the  Fibonacci  numbers. An alternative method is to split the original 34 runs unevenly. If we put 21 runs on T2 and 13 runs on T3, we could merge 13 runs on T1 before T3 was empty. We could then rewind T1 and T3 and merge T1, with 13 runs, and T2, with 8 runs, on T3. Next, we could merge 8 runs until T2 was empty, leaving 5 runs on T1 and 8 runs on T3. We could then merge T1 and T3, and so on. Figure 21.37 shows the number of runs on each tape after each pass. The original distribution of runs makes a great deal of difference. For instance, if 22 runs are placed on T2, with 12 on T3, after the ﬁrst merge we obtain 12 runs on T1 and 10 runs on T2. After another merge, there are 10 runs on T1 and 2 runs on T3. At this point, the going gets slow because we can merge only two sets of runs before T3 is exhausted. Then T1 has 8 runs and T2 has 2 runs. Again we can merge only two sets of runs, obtaining T1 with 6 runs and T3 with 2 runs. After three more passes, T2 has 2 runs and the other tapes are empty. We must copy 1 run to another tape. Then we can ﬁnish the merge. Our ﬁrst distribution turns out to be optimal. If the number of runs is a Fibonacci number, FN, the best way to distribute them is to split them into two Fibonacci numbers, FN – 1 and FN – 2. Otherwise, the tape must be padded with dummy runs in order to increase the number of runs to a Fibonacci number. We leave the details of how to place the initial set of runs on the tapes for you to handle as Exercise 21.22. We can extend this technique to a K-way merge, in which we need Kth-order Fibonacci numbers for the distribution. The Kth-order Fibonacci number is deﬁned as the sum of the K previous Kth-order Fibonacci numbers: figure 21.37 The number of runs for a polyphase merge Run Const. After T3 + T2 T1 + T2 T1 + T3 T2 + T3 T1 + T2 T1 + T3 T2 + T3 T1 T2 T3 F K ( ) N ( ) F K ( ) N – ( ) F K ( ) N – ( ) … F K ( ) N K – ( ) + + + = F K ( ) 0 N K – ≤ ≤ ( ) = F K ( ) K – ( ) =

chapter 21 a priority queue: the binary heap 21.6.6   replacement selection The last topic we consider in this chapter is construction of the runs. The strategy used so far is the simplest: We read as many elements as possible and sort them, writing the result to a tape. This seems like the best approach possible, until we realize that as soon as the ﬁrst element is written to the output tape, the memory it used becomes available for another element. If the next element on the input tape is larger than the element just output, it can be included in the run. If we are clever, we  can make the  length of the runs  that we initially construct larger than  the amount of available main memory.  This technique is  called replacement selection. Using this observation, we can write an algorithm for producing runs, commonly called replacement selection. Initially, M elements are read into memory and placed in a priority queue efﬁciently with a single buildHeap. We perform a deleteMin, writing the smallest element to the output tape. We read the next element from the input tape. If it is larger than the element just written, we can add it to the priority queue; otherwise, it cannot go into the current run. Because the priority queue is smaller by one element, this element is stored in the dead space of the priority queue until the run has been completed and is then used for the next run. Storing an element in the dead space is exactly what is done in heapsort. We continue doing this process until the size of the priority queue is 0, at which point the run is over. We start a new run by rebuilding a new priority queue with a buildHeap operation, in the process using all of the elements in the dead space. Figure 21.38 shows the run construction for the small example we have been using, with M = 3. Elements that are reserved for the next run are shaded. Elements 11, 94, and 81 are placed with buildHeap. Element 11 is output, and then 96 is placed in the heap by an insertion because it is larger than 11. Element 81 is output next, and then 12 is read. As 12 is smaller than the 81 just output, it cannot be included in the current run. Thus it is placed in the heap dead space. The heap now logically contains only 94 and 96. After they are output, we have only dead space elements, so we construct a heap and begin run 2.  In this example, replacement selection produces only 3 runs, compared to the 5 runs obtained by sorting. As a result, a three-way merge ﬁnishes in one pass instead of two. If the input is randomly distributed, replacement selection produces runs of average length 2M. For our large example, we would expect 160 runs instead of 320 runs, so a ﬁve-way merge would still require four passes. In this case, we have not saved a pass, although we might if we get lucky and have 125 runs or fewer. Because external sorts take so long, every pass saved can make a signiﬁcant difference in the running time. As we have shown, replacement selection may do no better than the standard algorithm. However, the input is frequently nearly sorted to start with, in

chapt er splay trees In this chapter we describe a remarkable data structure called the splay tree, which supports all the binary search tree operations but does not guarantee O(log N) worst-case performance. Instead, its bounds are amortized, meaning that, although individual operations can be expensive, any sequence of operations is guaranteed to behave as though each operation in the sequence exhibited logarithmic behavior. Because this guarantee is weaker than that provided by balanced search trees, only the data and two links per node are required for each item and the operations are somewhat simpler to code. The splay tree has some other interesting properties, which we reveal in this chapter. In this chapter, we show n The concepts of amortization and self-adjustment n The basic bottom-up splay tree algorithm and a proof that it has logarithmic amortized cost per operation n Implementation of splay trees with a top-down algorithm, using a complete splay tree implementation (including a deletion algorithm) n Comparisons of splay trees with other data structures

chapter 22 splay trees 22.1 self-adjustment and amortized analysis Although balanced search trees provide logarithmic worst-case running time per operation, they have several limitations. n They require storing an extra piece of balancing information per node. n They are complicated to implement. As a result, insertions and deletions are expensive and potentially error-prone. n They do not provide a win when easy inputs occur. The real problem is  that the extra data  members add complications. Let us examine the consequences of each of these deﬁciencies. First, balanced search trees require an extra data member. Although in theory this member can be as small as a single bit (as in a red–black tree), in practice the extra data member uses an entire integer for storage in order to satisfy hardware restrictions. Because computer memories are becoming huge, we must ask whether worrying about memory is a large issue. The answer in most cases is probably not, except that maintaining the extra data members requires more complex code and tends to lead to longer running times and more errors. Indeed, identifying whether the balancing information for a search tree is correct is difﬁcult because errors lead only to an unbalanced tree. If one case is slightly wrong, spotting the errors might be difﬁcult. Thus, as a practical matter, algorithms that allow us to remove some complications without sacriﬁcing performance deserve serious consideration. The 90–10 rule states that 90 percent of the  accesses are to  10 percent of the  data items. However, balanced  search trees do not  take advantage of  this rule. Second, the worst-case, average-case, and best-case performances of a balanced search are essentially identical. An example is a find operation for some item X. We could reasonably expect that, not only the cost of the find will be logarithmic, but also that if we perform an immediate second find for X, the second access will be cheaper than the ﬁrst. However, in a red–black tree, this condition is not true. We would also expect that, if we perform an access of X, Y, and Z in that order, a second set of accesses for the same sequence would be easy. This assumption is important because of the 90–10 rule. As suggested by empirical studies, the 90–10 rule states that in practice 90 percent of the accesses are to 10 percent of the data items. Thus we want easy wins for the 90 percent case, but balanced search trees do not take advantage of this rule. The 90–10 rule has been used for many years in disk I/O systems. A cache stores in main memory the contents of some of the disk blocks. The hope is that when a disk access is requested, the block can be found in the main memory cache and thus save the cost of an expensive disk access. Of course, only relatively few disk blocks can be stored in memory. Even so, storing the most recently

22.1 self-adjustment and amortized analysis accessed disk blocks in the cache enables large improvements in performance because many of the same disk blocks are accessed repeatedly. Browsers make use of the same idea: A cache stores locally the previously visited Web pages. 22.1.1   amortized time bounds We are asking for a lot: We want to avoid balancing information and, at the same time, we want to take advantage of the 90–10 rule. Naturally, we should expect to have to give up some feature of the balanced search tree. Amortized analysis bounds the cost of  a sequence of  operations and distributes this cost  evenly to each  operation in the  sequence. We choose to sacriﬁce the logarithmic worst-case performance. We are hoping that we do not have to maintain balance information, so this sacriﬁce seems inevitable. However, we cannot accept the typical performance of an unbalanced binary search tree. But there is a reasonable compromise: O(N) time for a single access may be acceptable so long as it does not happen too often. In particular, if any M operations (starting with the ﬁrst operation) take a total of O(M log N) worst-case time, the fact that some operations are expensive might be inconsequential. When we can show that a worst-case bound for a sequence of operations is better than the corresponding bound obtained by considering each operation separately and can be spread evenly to each operation in the sequence, we have performed an amortized analysis and the running time is said to be amortized. In the preceding example, we have logarithmic amortized cost. That is, some single operations may take more than logarithmic time, but we are guaranteed compensation by some cheaper operations that occur earlier in the sequence. However, amortized bounds are not always acceptable. Speciﬁcally, if a single bad operation is too time consuming, we really do need worst-case bounds rather than amortized bounds. Even so, in many cases a data structure is used as part of an algorithm and only the total amount of time used by the data structure in the course of running an algorithm is important. We have already presented one example of an amortized bound. When we implement array doubling in a stack or queue, the cost of a single operation can be either constant, if no doubling is needed, or O(N), if doubling is needed. However, for any sequence of M stack or queue operations, the total cost is guaranteed to be O(M), yielding constant amortized cost per operation. The fact that the array doubling step is expensive is inconsequential because its cost can be distributed to many earlier inexpensive operations. 22.1.2   a simple self-adjusting strategy (that does not work) In a binary search tree, we cannot expect to store the frequently accessed items in a simple table. The reason is that the caching technique beneﬁts from the great discrepancy between main memory and disk access times. Recall

chapter 22 splay trees that the cost of an access in a binary search tree is proportional to the depth of the accessed node. Thus we can attempt to restructure the tree by moving frequently accessed items toward the root. Although this process costs extra time during the ﬁrst find operation, it could be worthwhile in the long run. The easiest way to move a frequently accessed item toward the root is to rotate it continually with its parent, moving the item closer to the root, a process called the rotate-to-root strategy. Then, if the item is accessed a second time, the second access is cheap, and so on. Even if a few other operations intervene before the item is reaccessed, that item will remain close to the root and thus will be quickly found. An application of the rotate-to-root strategy to node 3 is shown in Figure 22.1.1 As a result of the rotation, future accesses of node 3 are cheap (for a while). Unfortunately, in the process of moving node 3 up two levels, nodes 4 and 5 each move down a level. Thus, if access patterns do not follow the 90– 10 rule, a long sequence of bad accesses can occur. As a result, the rotate-toroot rule does not exhibit logarithmic amortized behavior, which is likely unacceptable. A bad case is illustrated in Theorem 22.1. 1. An insertion counts as an access. Thus an item would always be inserted as a leaf and then immediately rotated to the root. An unsuccessful search counts as an access on the leaf at which the search terminates. Theorem 22.1 There are arbitrarily long sequences for which M rotate-to-root accesses use Θ(MN) time. Proof Consider the tree formed by the insertion of 1, 2, 3,  , N in an initially empty tree.  The result is a tree consisting of only left children. This outcome is not bad, as the  time to construct the tree is only O(N) total. As illustrated in Figure 22.2, each newly added item is made a child of the root. Then,  only one rotation is needed to place the new item at the root. The bad part, as shown  in Figure 22.3, is that accessing the node with key 1 takes N units of time. After the  rotations have been completed, access of the node with key 2 takes N units of time  and access of key 3 takes N – 1 units of time. The total for accessing the N keys in  order is  . After they have been accessed, the tree reverts to its  original state and we can repeat the sequence. Thus we have an amortized bound of  only Θ(N). … N i i = N ∑ + Θ N 2 ( ) = The rotate-to-root strategy rearranges  a binary search tree  after each access  so as to move frequently accessed  items closer to  the root. The rotate-to-root  strategy is good if  the 90–10 rule  applies. It can be a  bad strategy when  the rule does not  apply.

22.2 the basic bottom-up splay tree 22.2 the basic bottom-up splay tree In a basic bottomup splay tree, items are rotated to the  root by using a  slightly more complicated method  than that used for a  simple rotate-toroot strategy. Achieving logarithmic amortized cost seems impossible because, when we move an item to the root via rotations, other items are pushed deeper. Seemingly, that would always result in some very deep nodes if no balancing information is maintained. Amazingly, we can apply a simple ﬁx to the rotate-to-root strategy that allows the logarithmic amortized bound to be obtained. Implementation of this slightly more complicated rotate-to-root method called splaying leads to the basic bottom-up splay tree. figure 22.1 Rotate-to-root strategy applied when  node 3 is accessed figure 22.2 Insertion of 4 using  the rotate-to-root  strategy figure 22.3 Sequential access of  items takes quadratic  time

chapter 22 splay trees The splaying strategy is similar to the simple rotate-to-root strategy, but it has one subtle difference. We still rotate from the bottom up along the access path (later in the chapter we describe a top-down strategy). If X is a nonroot node on the access path on which we are rotating and the parent of X is the root of the tree, we merely rotate X and the root, as shown in Figure 22.4. This rotation is the last along the access path, and it places X at the root. Note that this action is exactly the same as that in the rotate-to-root algorithm and is referred to as the zig case. The zig and zig-zag cases are identical  to rotate-to-root. Otherwise, X has both a parent P and a grandparent G, and we must consider two cases and symmetries. The ﬁrst case is the so called zig-zag case, which corresponds to the inside case for AVL trees. Here X is a right child and P is a left child (or vice versa). We perform a double rotation exactly like an AVL double rotation, as shown in Figure 22.5. Note that, as a double rotation is the same as two bottom-up single rotations, this case is no different than the rotate-to-root strategy. In Figure 22.1, the splay at node 3 is a single zig-zag rotation. The zig-zig case is  unique to the splay  tree. The ﬁnal case, the zig-zig case, is unique to the splay tree and is the outside case for AVL trees. Here, X and P are either both left children or both right children. In this case, we transform the left-hand tree of Figure 22.6 to the right-hand tree. Note that this method differs from the rotate-to-root strategy. The zig-zig splay rotates between P and G and then X and P, whereas the rotate-to-root strategy rotates between X and P and then between X and G. figure 22.4 The zig case (normal  single rotation) B C P X C A B X A P figure 22.5 The zig-zag case  (same as a double  rotation); the  symmetric case has  been omitted G P D A B C X X P G A B C D

22.2 the basic bottom-up splay tree The difference seems quite minor, and the fact that it matters is somewhat surprising. To see this difference consider the sequence that gave the poor results in Theorem 22.1. Again, we insert keys 1, 2, 3, ..., N in an initially empty tree in linear total time and obtain an unbalanced left-child-only tree. However, the result of a splay is somewhat better, as shown in Figure 22.7. After the splay at node 1, which takes N node accesses, a splay at node 2 takes only roughly   accesses, rather than N – 1 accesses. Splaying not only moves the accessed node to the root, but it also roughly halves the depth of most nodes on the access path (some shallow nodes are pushed down at most two levels). A subsequent splay at node 2 brings nodes to within   of the root. Splaying is repeated until the depth becomes roughly log N. In fact, a complicated analysis shows that what used to be a bad case for the rotate-toroot algorithm is a good case for splaying: Sequential access of the N items in the splay tree takes a total of only O(N) time. Thus we win on easy input. In Section 22.4 we show, by subtle accounting, that there are no bad access sequences. figure 22.6 Zig-zig case (unique  to the splay tree); the  symmetric case has  been omitted A B G P D X C D C X P A G B figure 22.7 Result of splaying at  node 1 (three zig-zigs) N 2 ⁄ N 4 ⁄ Splaying has the  effect of roughly  halving the depth of  most nodes on the  access path and  increasing by at  most two levels the  depth of a few  other nodes.

chapter 22 splay trees 22.3 basic splay tree operations After an item has  been inserted as a  leaf, it is splayed to  the root. As mentioned earlier, a splay operation is performed after each access. When an insertion is performed, we perform a splay. As a result, the newly inserted item becomes the root of the tree. Otherwise, we could spend quadratic time constructing an N item tree. All searching operations incorporate a  splay. For the find, we splay at the last node accessed during the search. If the search is successful, the node found is splayed and becomes the new root. If the search is unsuccessful, the last node accessed prior to reaching the null reference is splayed and becomes the new root. This behavior is necessary because, otherwise, we could repeatedly perform a find for 0 in the initial tree in Figure 22.7 and use linear time per operation. Likewise, operations such as findMin and findMax perform a splay after the access. The interesting operations are the deletions. Recall that the deleteMin and deleteMax are important priority queue operations. With splay trees, these operations become simple. We can implement deleteMin as follows. First, we perform a findMin. This brings the minimum item to the root, and by the binary search tree property, there is no left child. We can use the right child as the new root. Similarly, deleteMax can be implemented by calling findMax and setting the root to the post-splay root’s left child. Deletion operations are much simpler than usual.  They also contain a  splaying step  (sometimes two). Even the remove operation is simple. To perform deletion, we access the node to be deleted, which puts the node at the root. If it is deleted, we get two subtrees, L and R (left and right). If we ﬁnd the largest element in L, using a findMax operation, its largest element is rotated to L’s root and L’s root has no right child. We ﬁnish the remove operation by making R the right child of L’s root. An example of the remove operation is shown in Figure 22.8. The cost of the remove operation is two splays. All other operations cost one splay. Thus we need to analyze the cost of a series of splay steps. The next section shows that the amortized cost of a splay is at most 3 log N + 1 single rotations. Among other things, this means we do not have to worry that the remove algorithm described previously is biased. The splay tree’s amortized figure 22.8 The remove operation  applied to node 6:  First, 6 is splayed to  the root, leaving two  subtrees; a findMax is  performed on the left  subtree, raising 5 to  the root of the left  subtree; then the right  subtree can be  attached (not shown).

22.4 analysis of bottom-up splaying The rank of a node  is the logarithm of  its size. Ranks and  sizes are not maintained but are  merely accounting  tools for the proof.  Only nodes on the  splay path have  their ranks  changed. bound guarantees that any sequence of M splays will use at most 3 M log N + M tree rotations. Consequently, any sequence of M operations starting from an empty tree will take a total of at most O(M log N) time. 22.4 analysis of bottom-up splaying The analysis of the  splay tree is complicated and is part  of a much larger  theory of amortized  analysis. The analysis of the splay tree algorithm is complicated because each splay can vary from a few rotations to O(N) rotations. Each splay can drastically change the structure of the tree. In this section we prove that the amortized cost of a splay is at most 3log N + 1 single rotations. The splay tree’s amortized bound guarantees that any sequence of M splays use at most 3M log N + M tree rotations, and consequently any sequence of M operations starting from an empty tree take a total of at most O(M log N) time. The potential function is an accounting device used to  establish the  required time  bound. To prove this bound, we introduce an accounting function called the potential function. Not maintained by the algorithm, the potential function is merely an accounting device used to establish the required time bound. Its choice is not obvious and is the result of a large amount of trial and error. For any node i in the splay tree, let S(i) be the number of descendants of i (including i itself). The potential function is the sum, over all nodes i in the tree T, of the logarithm of S(i). Speciﬁcally, To simplify the notation, we let R(i) = log S(i), which gives The term R(i) represents the rank of node i, or the logarithm of its size. Note that the rank of the root is log N.  Recall that neither ranks nor sizes are maintained by splay tree algorithms (unless, of course, order statistics are needed). When a zig rotation is performed, only the ranks of the two nodes involved in the rotation change. When a zig-zig or a zig-zag rotation is performed, only the ranks of the three nodes involved in the rotation change. And ﬁnally, a single splay consists of some number of zig-zig or zig-zag rotations followed by perhaps one zig rotation. Each zig-zig or zig-zag rotation can be counted as two single rotations. For Theorem 22.2 we let   be the potential function of the tree immediately after the ith splay and   be the potential prior to the ﬁrst splay. Φ T ( ) S i( ) log i T ∈∑ = Φ T ( ) R i( ) ∑ = Φi Φ0

chapter 22 splay trees In all the proofs in  this section we use  the concept of telescoping sums. Before proving Theorem 22.2, let us determine what it means. The cost of M splays can be taken as   rotations. If the M splays are consecutive (i.e., no insertions or deletions intervene), the potential of the tree after the ith splay is the same as prior to the (i + 1)th splay. Thus we can use Theorem 22.2 M times to obtain the following sequence of equations: (22.1) These equations telescope, so if we add them, we obtain (22.2) which bounds the total number of rotations as Now consider what happens when insertions are intermingled with ﬁnd operations. The potential of an empty tree is 0, so when a node is inserted in the tree as a leaf, prior to the splay the potential of the tree increases by at most log N (which we prove shortly). Suppose that ri rotations are used for an insertion and that the potential prior to the insertion is  . After the insertion, the potential is at most  . After the splay that moves the inserted node to the root, the new potential will satisfy (22.3) Suppose further that there are F ﬁnds and I insertions and that   represents the potential after the ith operation. Then, because each find is governed by Theorem 22.2 and each insertion is governed by Equation 22.3, the telescoping logic indicates that (22.4) Theorem 22.2 If the ith splay operation uses   rotations,  . ri Φi Φi – – ri + N log + ≤ ri i = M ∑ Φ1 Φ0 – r1 + N log + ≤ Φ2 Φ1 – r2 + N log + ≤ Φ3 Φ2 – r3 + N log + ≤ … ΦM ΦM – – rM + N log + ≤ ΦM Φ0 – ri i = M ∑ + N log + ( )M ≤ ri i = M ∑ N log + ( )M ΦM Φ0 – ( ) – ≤ Φi – Φi – log N + Φi Φi – log N + ( ) – ri + N log + ≤ Φi Φi – – ri + N log + ≤ Φi ri i = M ∑ N log + ( )F N log + ( )I ΦM Φ0 – ( ) – + ≤

22.4 analysis of bottom-up splaying Moreover, before the ﬁrst operation the potential is 0, and since it can never be negative,  . Consequently, we obtain (22.5) showing that the cost of any sequence of ﬁnds and insertions is at most logarithmic per operation. A deletion is equivalent to two splays, so it too is logarithmic. Thus we must prove the two dangling claims—namely, Theorem 22.2 and the fact that an insertion of a node adds at most log N to the potential. We prove both theorems by using telescoping arguments. We take care of the insertion claim ﬁrst, as Theorem 22.3. To prove Theorem 22.2, we break each splay step into its constituent zig, zig-zag, and zig-zig parts and establish a bound for the cost of each type of rotation. By telescoping these bounds, we obtain a bound for the splay. Before continuing, we need a technical theorem, Theorem 22.4. We are now ready to prove Theorem 22.2. ΦM Φ0 – ≥ ri i = M ∑ N log + ( )F N log + ( )I + ≤ Insertion of the Nth node in a tree as a leaf adds at most log N to the potential of the  tree. Theorem 22.3 The only nodes whose ranks are affected are those on the path from the inserted  leaf to the root. Let   be their sizes prior to the insertion and note that   and  . Let   be the sizes after the insertion. Clearly,   for  , since  . Consequently,  . The  change in potential is thus Proof S1 S2 … Sk , , , Sk N – = S1 S2 … Sk < < < S′1 S′2 … S′k , , , S′i Si + ≤ i k < S′i Si + = R′i Ri + ≤ R′i Ri – ( ) i = k ∑ R′k Rk – Ri + Ri – ( ) i = k – ∑ + log N R1 – log N. ≤ ≤ ≤ If   and a and b are both positive integers, then  . Theorem 22.4 By the arithmetic–geometric mean inequality,  . Thus  .  Squaring both sides gives  . Then taking logarithms of both sides proves  the theorem. Proof a b + c ≤ a log b log + c log – ≤ ab a b + ( ) 2 ⁄ ≤ ab c 2 ⁄ ≤ ab c2 4 ⁄ ≤

chapter 22 splay trees 22.4.1   proof of the splaying bound First, if the node to splay is already at the root, there are no rotations and no potential change. Thus the theorem is trivially true, and we may assume at least one rotation. We let X be the node involved in the splay. We need to show that, if r rotations are performed (a zig-zig or zig-zag counts as two rotations), r plus the change in potential is at most 3 log N + 1. Next, we let Δ be the change in potential caused by any of the splay steps zig, zig-zag, or zig-zig. Finally, we let Ri(X) and Si(X) be the rank and size of any node X immediately before a splay step and Rf (X) and Sf (X) be the rank and size of any node X immediately after a splay step. Following are the bounds that are to be proven. For a zig step that promotes node X, Δ ≤3(Rf (X) – Ri(X)); for the other two steps, Δ ≤3(Rf (X) – Ri(X)) – 2. When we add these bounds over all the steps that comprise a splay, the sum telescopes to the desired bound. We prove each bound separately in Theorems 22.5–22.7. Then we can complete the proof of Theorem 22.2 by applying a telescoping sum. The zig-zag and zig-zig steps are more complicated because the ranks of three nodes are affected. First, we prove the zig-zag case. Theorem 22.5 For a zig step, Δ ≤3(Rf (X) – Ri(X)). Proof As mentioned earlier in this section, the only nodes whose ranks change in a zig step  are X and P. Consequently, the potential change is Rf (X) – Ri(X) + Rf (P) – Ri (P). From Figure 22.4, Sf (P) < Si (P); thus it follows that Rf (P) – Ri (P) < 0. Consequently,  the potential change satisﬁes Δ ≤Rf (X) – Ri (X). As Sf (X) > Si (X), it follows that  Rf (X) – Ri (X) > 0; hence Δ ≤ 3(Rf (X) – Ri (X)). Theorem 22.6 For a zig-zag step, Δ ≤3(Rf (X) – Ri(X)) – 2. Proof As before, we have three changes, so the potential change is given by From Figure 22.5,  , so their ranks must be equal. Thus we obtain (continues next page) Δ Rf X ( ) Ri X ( ) – Rf P ( ) Ri P ( ) – Rf G ( ) Ri G ( ) – + + = Sf X ( ) Si G ( ) = Δ R – i X ( ) Rf P ( ) R – i P ( ) Rf G ( ) + + =

22.4 analysis of bottom-up splaying Finally, we prove the bound for the zig-zig case. (continued from previous page) Also,  . Consequently,  . Making this substitution and rearranging terms gives (22.6) From Figure 22.5,  . Applying Theorem 22.4, we obtain  log , which by the deﬁnition of rank, becomes (22.7) Substituting Equation 22.7 into Equation 22.6 yields (22.8) As for the zig rotation,  , so we can add it to the right side of Equation 22.8, factor, and obtain the desired Proof of Theorem 22.6 Si P ( ) Si X ( ) ≥ Ri P ( ) Ri X ( ) ≥ Δ Rf P ( ) Rf G ( ) 2Ri X ( ) – + ≤ Sf P ( ) Sf G ( ) Sf X ( ) ≤ + Sf P ( ) log Sf G ( ) 2 log Sf X ( ) – ≤ + Rf P ( ) Rf G ( ) 2Rf X ( ) – ≤ + Δ 2Rf X ( ) 2Ri X ( ) – – ≤ Rf X ( ) Ri X ( ) > – Δ 3 Rf X ( ) Ri X ( ) – ( ) – ≤ For a zig-zig step,  Theorem 22.7 As before, we have three changes, so the potential change is given by From Figure 22.6,   their ranks must be equal, so we obtain We also can obtain   and   Making this substitution and  rearranging gives (22.9) (continues next page) Proof Δ 3(Rf X ( ) Ri X ( )) – 2. – ≤ Δ Rf X ( ) Ri X ( ) – Rf P ( ) Ri P ( ) – Rf G ( ) Ri G ( ) – + + = Sf X ( ) Si G ( ); = Δ R – i X ( ) Rf P ( ) Ri P ( ) – Rf G ( ) + + = Ri P ( ) Ri X ( ) > Rf P ( ) Rf X ( ). < Δ Rf X ( ) Rf G ( ) 2Ri X ( ) – + <

chapter 22 splay trees Now that we have established bounds for each splaying step, we can ﬁnally complete the proof of Theorem 22.2. Although it is complex, the proof of the splay tree bound illustrates several interesting points. First, the zig-zig case is apparently the most expensive: It contributes a leading constant of 3, whereas the zig-zag contributes 2. The proof would fall apart if we tried to adapt it to the rotate-to-root algorithm because, in the zig case, the number of rotations plus the potential change is The 1 at the end does not telescope out, so we would not be able to show a logarithmic bound. This is fortunate because we already know that a logarithmic bound would be incorrect. Proof of Theorem 22.7 (continued from previous page) From Figure 22.6,   so applying Theorem 22.4 yields (22.10) Rearranging Equation 22.10, we obtain (22.11) When we substitute Equation 22.11 into Equation 22.9, we get Si X ( ) Sf G ( ) Sf X ( ), ≤ + Ri X ( ) Rf G ( ) 2Rf X ( ) – ≤ + Rf G ( ) 2Rf X ( ) Ri X ( ) – – ≤ Δ 3 Rf X ( ) Ri X ( ) – ( ) – ≤ Proof of Theorem 22.2 Let R0(X) be the rank of X prior to the splay. Let Ri(X) be X’s rank after the ith splaying step. Prior to the last splaying step, all splaying steps must be zig-zags or zig-zigs.  Suppose that there are k such steps. Then the total number of rotations performed at  that point is 2k. The total potential change is   This  sum telescopes to   At this point, the total number of rotations  plus the total potential change is bounded by   because the 2k term cancels  and the initial rank of X is not negative. If the last rotation is a zig-zig or a zig-zag,  then a continuation of the telescoping sum gives a total of 3R(root). Note that here,  on the one hand, the –2 in the potential increase cancels the cost of two rotations.  On the other hand, this cancellation does not happen in the zig, so we would get a  total of 3R(root) +1. The rank of the root is log N, so then—in the worst case—the total  number of rotations plus the change in potential during a splay is at most 3 log N + 1. Σi = k (3(Ri X ( ) Ri – X ( )) – 2. – 3(Rk X ( ) R0 X ( )) – 2k. – 3Rk X ( ) Rf X ( ) Ri X ( ) – 1. +

22.5 top-down splay trees The technique of amortized analysis is very interesting, and some general principles have been developed to formalize the framework. Check the references for more details. 22.5 top-down splay trees As for red–black  trees, top-down splay trees are  more efﬁcient in  practice than their  bottom-up counterparts. A direct implementation of the bottom-up splay strategy requires a pass down the tree to perform an access and then a second pass back up the tree. These passes can be made by maintaining parent links, by storing the access path on a stack, or by using a clever trick to store the path (using the available links in the accessed nodes). Unfortunately, all these methods require expending a substantial amount of overhead and handling many special cases. Recall from Section 19.5 that implementing search tree algorithms with a single top-down pass is a better approach and we can use dummy nodes to avoid special cases. In this section we describe a top-down splay tree that maintains the logarithmic amortized bound, is faster in practice, and uses only constant extra space. It is the method recommended by the inventors of the splay tree. The basic idea behind the top-down splay tree is that, as we descend the tree searching for some node X, we must take the nodes that are on the access path and move them and their subtrees out of the way. We must also perform some tree rotations to guarantee the amortized time bound. We maintain three  trees during the  top-down pass. At any point in the middle of the splay, a current node X is the root of its subtree; it is represented in the diagrams as the middle tree. Tree L stores nodes  that are less than X; similarly, tree R stores nodes that are larger than X. Initially, X is the root of T, and L and R are empty. Descending the tree two levels at a time, we encounter a pair of nodes. Depending on whether these nodes are smaller or larger than X, we place them in L or R, along with subtrees that are not on the access path to X. Thus the current node on the search path is always the root of the middle tree. When we ﬁnally reach X, we can then attach L and R to the bottom of the middle tree. As a result, X has been moved to the root. The remaining tasks then are to place nodes in L and R and to perform the reattachment at the end, as illustrated in the trees shown in Figure 22.9. As is customary, three symmetric cases are omitted. In all the diagrams, X is the current node, Y is its child, and Z is a grandchild (should an applicable node exist). (The precise meaning of the term applicable is made clear during the discussion of the zig case.) If the rotation should be a zig, the tree rooted at Y becomes the new root of the middle tree. Node X and subtree B are attached as a left child of the smallest

chapter 22 splay trees item in R; X’s left child is logically made null.2 As a result, X is the new smallest element in R, making future attachments easy. Note that Y does not have to be a leaf for the zig case to apply. If the item sought is found in Y, a zig case will apply even if Y has children. A zig case also applies if the item sought is smaller than Y and Y has no left child, even if Y has a right child, and also for the symmetric case. A similar dissection applies to the zig-zig case. The crucial point is that a rotation between X and Y is performed. The zig-zag case brings the bottom node Z to the top of the middle tree and attaches subtrees X and Y to R and L, respectively. Note that Y is attached to, and then becomes, the largest item in L. 2. In the code written here, the smallest node in R does not have a null left link because it is not needed. figure 22.9 Top-down splay  rotations: (a) zig,  (b) zig-zig, and  (c) zig-zag L A B R L R B X Y A Y X L R L R B C C B X A Z A Z Y Y X L R L R C C B X A Z B Z X A Y Y (a) (b) (c)

22.5 top-down splay trees The zig-zag step can be simpliﬁed somewhat because no rotations are performed. Instead of making Z the root of the middle tree, we make Y the root, as shown in Figure 22.10. This action simpliﬁes the coding because the action for the zig-zag case becomes identical to the zig case and would seem advantageous, as testing for a host of cases is time-consuming. The disadvantage is that a descent of only one level results in more iterations in the splaying procedure. Eventually, the  three trees are  reassembled into  one. Once we have performed the ﬁnal splaying step, L, R, and the middle tree are arranged to form a single tree, as shown in Figure 22.11. Note that the result is different from that obtained with bottom–up splaying. The crucial fact is that the O(log N) amortized bound is preserved (see Exercise 22.3). An example of the simpliﬁed top-down splaying algorithm is shown in Figure 22.12. When we attempt to access 19, the ﬁrst step is a zig-zag. In accordance with a symmetric version of Figure 22.10, we bring the subtree rooted at 25 to the root of the middle tree and attach 12 and its left subtree to L. Next, we have a zig-zig: 15 is elevated to the root of the middle tree, and a rotation between 20 and 25 is performed, with the resulting subtree being attached to R. The search for 19 then results in a terminal zig. The middle’s new root is 18, and 15 and its left subtree are attached as a right child of L’s largest node. The reassembly, in accordance with Figure 22.11, terminates the splay step. figure 22.10 Simplified top-down  zig-zag L R L R C C B X A Z X Y B A Z Y figure 22.11 Final arrangement for  top-down splaying L R B X A R X L B A

chapter 22 splay trees 22.6 implementation of top-down splay trees The splay tree class skeleton is shown in Figure 22.13. We have the usual methods, except that find is a mutator rather than an accessor. The BinaryNode class is our standard package-visible node class that contains data and two child references, but it is not shown. To eliminate annoying special cases, figure 22.12 Steps in a top-down  splay (accessing 19 in  the top tree) Empty Empty Empty Simplified zig-zag Zig-zig Zig Reassemble

22.6 implementation of top-down splay trees we maintain a nullNode sentinel. We allocate and initialize the sentinel in the constructor, as shown in Figure 22.14.  Figure 22.15 shows the method for insertion of an item x. A new node (newNode) is allocated, and if the tree is empty, a one-node tree is created. Otherwise, we splay around x. If the data in the tree’s new root equal x, we have a duplicate. In this case, we do not want to insert x; we throw an exception instead at line 39. We use an instance variable so that the next call to insert figure 22.13 The top-down SplayTree class skeleton package weiss.nonstandard; // SplayTree class // // CONSTRUCTION: with no initializer // // ******************PUBLIC OPERATIONS********************* // void insert( x )       --> Insert x // void remove( x )       --> Remove x // Comparable find( x )   --> Return item that matches x // boolean isEmpty( )     --> Return true if empty; else false // void makeEmpty( )      --> Remove all items // ******************ERRORS******************************** // Exceptions are thrown by insert and remove if warranted public class SplayTree<AnyType extends Comparable<AnyType>> {     public SplayTree( )       { /* Figure 22.14 */ }     public void insert( AnyType x )       { /* Figure 22.15 */ }     public void remove( AnyType x )       { /* Figure 22.16 */ }     public AnyType find( AnyType x )       { /* Figure 22.18 */ }     public void makeEmpty( )       { root = nullNode; }     public boolean isEmpty( )       { return root == nullNode; }     private BinaryNode<AnyType> splay( AnyType x, BinaryNode<AnyType> t )       { /* Figure 22.17 */ }     private BinaryNode<AnyType> root;     private BinaryNode<AnyType> nullNode; }

chapter 22 splay trees figure 22.14 The SplayTree class  constructor /**  * Construct the tree.  */     public SplayTree( )     {         nullNode = new BinaryNode<AnyType>( null );         nullNode.left = nullNode.right = nullNode;         root = nullNode;     } figure 22.15 The top-down  SplayTree class  insertion routine   // Used between different inserts     private BinaryNode<AnyType> newNode = null;     /**      * Insert into the tree.      * @param x the item to insert.      * @throws DuplicateItemException if x is already present.      */     public void insert( AnyType x )     {         if( newNode == null )             newNode = new BinaryNode<AnyType>( null );         newNode.element = x;         if( root == nullNode )         {             newNode.left = newNode.right = nullNode;             root = newNode;         }         else         {             root = splay( x, root );             if( x.compareTo( root.element ) < 0 )             {                 newNode.left = root.left;                 newNode.right = root;                 root.left = nullNode;                 root = newNode;             }             else             if( x.compareTo( root.element ) > 0 )             {                 newNode.right = root.right;                 newNode.left = root;                 root.right = nullNode;                 root = newNode;             }             else                 throw new DuplicateItemException( x.toString( ) );         }         newNode = null;   // So next insert will call new     }

22.6 implementation of top-down splay trees can avoid calling new, in the case that the insert fails because of a duplicate item. (Normally, we would not be so concerned with this exceptional case; however, a reasonable alternative is to use a Boolean return value rather than using exceptions.) If the new root contains a value larger than x, the new root and its right subtree become a right subtree of newNode, and the root’s left subtree becomes a left subtree of newNode. Similar logic applies if the new root contains a value smaller than x. In either case, newNode is assigned to root to indicate that it is the new root. Then we make newNode null at line 41 so that the next call to insert will call new. Figure 22.16 shows the deletion routine for splay trees. A deletion procedure rarely is shorter than the corresponding insertion procedure. Next, is the top-down splaying routine.  Our implementation, shown in Figure 22.17, uses a header with left and right links to contain eventually the roots of the left and right trees. These trees are initially empty; a header is used to correspond to the min or max node of the right or left tree, respectively, in this initial state. In this way we can avoid checking for empty trees. The ﬁrst time the left tree becomes nonempty, the header’s right link is initialized and does not change in the future. figure 22.16 The top-down  SplayTree class  deletion routine /**  * Remove from the tree.  * @param x the item to remove.  * @throws ItemNotFoundException if x is not found.  */     public void remove( AnyType x )     {         BinaryNode<AnyType> newTree;             // If x is found, it will be at the root         root = splay( x, root );         if( root.element.compareTo( x ) != 0 )             throw new ItemNotFoundException( x.toString( ) );         if( root.left == nullNode )             newTree = root.right;         else         {             // Find the maximum in the left subtree             // Splay it to the root; and then attach right child             newTree = root.left;             newTree = splay( x, newTree );             newTree.right = root.right;         }         root = newTree;     }

chapter 22 splay trees figure 22.17 A top-down splay algorithm private BinaryNode<AnyType> header = new BinaryNode<AnyType>( null );     /**      * Internal method to perform a top-down splay.      * The last accessed node becomes the new root.      * @param x the target item to splay around.      * @param t the root of the subtree to splay.      * @return the subtree after the splay.      */     private BinaryNode<AnyType> splay( AnyType x, BinaryNode<AnyType> t )     {         BinaryNode<AnyType> leftTreeMax, rightTreeMin;         header.left = header.right = nullNode;         leftTreeMax = rightTreeMin = header;         nullNode.element = x;   // Guarantee a match         for( ; ; )             if( x.compareTo( t.element ) < 0 )             {                 if( x.compareTo( t.left.element ) < 0 )                     t = Rotations.rotateWithLeftChild( t );                 if( t.left == nullNode )                     break;                 // Link Right                 rightTreeMin.left = t;                 rightTreeMin = t;                 t = t.left;             }             else if( x.compareTo( t.element ) > 0 )             {                 if( x.compareTo( t.right.element ) > 0 )                     t = Rotations.rotateWithRightChild( t );                 if( t.right == nullNode )                     break;                 // Link Left                 leftTreeMax.right = t;                 leftTreeMax = t;                 t = t.right;             }             else                 break;         leftTreeMax.right = t.left;         rightTreeMin.left = t.right;         t.left = header.right;         t.right = header.left;         return t;     }

22.7 comparison of the splay tree with other search trees Thus it contains the root of the left tree at the end of the top-down search. Similarly, the header’s left link eventually contains the root of the right tree. The header variable is not local because we want to allocate it only once over the entire sequence of splays.  Before the reassembly at the end of the splay, header.left and header.right reference R and L, respectively (this is not a typo—follow the links). Note that we are using the simpliﬁed top-down splay. The find method, shown in Figure 22.18, completes the implementation of the splay tree. 22.7 comparison of the splay tree  with other search trees The implementation just presented suggests that splay trees are not as complicated as red–black trees and almost as simple as AA-trees. Are they worth using? The answer has yet to be resolved completely, but if the access patterns are nonrandom, splay trees seem to perform well in practice. Some properties relating to their performances also can be proved analytically. Nonrandom accesses include those that follow the 90–10 rule, as well as several special cases such as sequential access, double-ended access, and apparently access patterns that are typical of priority queues during some types of event simulations. In the exercises you are asked to examine this question in more detail. Splay trees are not perfect. One problem with them is that the find operation is expensive because of the splay. Hence when access sequences are random and uniform, splay trees do not perform as well as other balanced trees. figure 22.18 The find routine, for  top-down splay trees /**  * Find an item in the tree.  * @param x the item to search for.  * @return the matching item or null if not found.  */     public AnyType find( AnyType x )     {         root = splay( x, root );         if( isEmpty( ) || root.element.compareTo( x ) != 0 )             return null;         return root.element;     }

chapt er merging priority queues In this chapter we examine priority queues that support an additional operation: The merge operation, which is important in advanced algorithm design, combines two priority queues into one (and logically destroys the originals). We represent the priority queues as general trees, which simpliﬁes somewhat the decreaseKey operation and is important in some applications. In this chapter, we show n How the skew heap—a mergeable priority queue implemented with binary trees—works. n How the pairing heap—a mergeable priority queue based on the M-ary tree—works. The pairing heap appears to be a practical alternative to the binary heap even if the merge operation is not needed. 23.1 the skew heap The skew heap is a heap-ordered binary tree without a balancing condition. Without this structural constraint on the tree—unlike with the heap or the balanced binary search trees—there is no guarantee that the depth of the tree is The skew heap is a  heap-ordered binary tree without  a balancing condition and supports all  operations in logarithmic amortized  time.

chapter 23 merging priority queues logarithmic. However, it supports all operations in logarithmic amortized time. The skew heap is thus somewhat similar to the splay tree. 23.1.1   merging is fundamental If a heap-ordered, structurally unconstrained binary tree is used to represent a priority queue, merging becomes the fundamental operation. This is because we can perform other operations as follows: n h.insert( x ): Create a one-node tree containing x and merge that tree into the priority queue. n h.findMin( ): Return the item at the root. n h.deleteMin( ): Delete the root and merge its left and right subtrees. The decreaseKey operation is implemented by detaching a subtree from  its parent and then  using merge. n h.decreaseKey( p, newVal ): Assuming that p is a reference to a node in the priority queue, we can lower p’s key value appropriately and then detach p from its parent. Doing so yields two priority queues that can be merged. Note that p (meaning the position) does not change as a result of this operation (in contrast to the equivalent operation in a binary heap). We need show only how to implement merging; the other operations become trivial. The decreaseKey operation is important in some advanced applications. We presented one illustration in Section 14.3—Dijkstra’s algorithm for shortest paths in a graph. We did not use the decreaseKey operation in our implementation because of the complications of maintaining the position of each item in the binary heap. In a merging heap, the position can be maintained as a reference to the tree node, and unlike in the binary heap, the position never changes. In this section we discuss one implementation of a mergeable priority queue that uses a binary tree: the skew heap. First, we show that, if we are not concerned with efﬁciency, merging two heap-ordered trees is easy. Next, we cover a simple modiﬁcation (the skew heap) that avoids the obvious inefﬁciencies in the original algorithm. Finally, we give a proof that the merge operation for skew heaps is logarithmic in an amortized sense and comment on the practical signiﬁcance of this result. 23.1.2   simplistic merging of heap-ordered trees Two trees are easily  merged recursively. Let us assume that we have two heap-ordered trees, H1 and H2, that need to be merged. Clearly, if either of the two trees is empty, the other tree is the result of the merge. Otherwise, to merge the two trees, we compare their roots. We

23.1 the skew heap recursively merge the tree with the larger root into the right subtree of the tree with the smaller root.1 The result is that  right paths are  merged. We must  be careful not to  create unduly long  right paths. Figure 23.1 shows the effect of this recursive strategy: The right paths of the two priority queues are merged to form the new priority queue. Each node on the right path retains its original left subtree, and only the nodes on the right path are touched. The outcome shown in Figure 23.1 is unattainable by using only insertions and merges because, as just mentioned, left children cannot be added by a merge. The practical effect is that what seems to be a heap-ordered binary tree is in fact an ordered arrangement consisting only of a single right path. Thus all operations take linear time. Fortunately, a simple modiﬁcation ensures that the right path is not always long. 23.1.3   the skew heap: a simple modification To avoid the problem of unduly long  right paths, we  make the resulting  right path after a  merge a left path.  Such a merge  results in a skew  heap. The merge shown in Figure 23.1 creates a temporary merged tree. We can make a simple modiﬁcation in the operation as follows. Prior to the completion of a merge, we swap the left and right children for every node in the resulting right path of the temporary tree. Again, only those nodes on the original right paths are on the right path in the temporary tree. As a result of the swap, shown in Figure 23.2, these nodes then form the left path of the resulting tree. When a merge is performed in this way, the heap-ordered tree is also called a skew heap. 1. Clearly, either subtree could be used. We arbitrarily use the right subtree. figure 23.1 Simplistic merging of  heap-ordered trees:  Right paths are  merged. 8 + figure 23.2 Merging of skew  heap; right paths are  merged, and the result  is made a left path. 8 +

chapter 23 merging priority queues A recursive viewpoint is as follows. If we let L be the tree with the smaller root and R be the other tree, the following is true. 1. If one tree is empty, the other can be used as the merged result. 2. Otherwise, let Temp be the right subtree of L. 3. Make L’s left subtree its new right subtree. 4. Make the result of the recursive merge of Temp and R the new left subtree of L. A long right path is  still possible. However, it rarely occurs  and must be preceded by many  merges involving  short right paths. We expect the result of the child swapping to be that the length of the right path will not be unduly large all the time. For instance, if we merge a pair of long right-path trees, the nodes involved in the path do not reappear on a right path for quite some time in the future. Obtaining trees that have the property that every node appears on a right path is still possible, but that can be done only as a result of a large number of relatively inexpensive merges. In Section 23.1.4, we prove this assertion rigorously by establishing that the amortized cost of a merge operation is only logarithmic. 23.1.4   analysis of the skew heap The actual cost of a  merge is the number of nodes on the  right paths of the  two trees that are  merged. Suppose that we have two heaps, H1 and H2, and that there are r1 and r2 nodes on their respective right paths. Then the time required to perform the merge is proportional to r1 + r2. When we charge 1 unit for each node on the right paths, the cost of the merge is proportional to the number of charges. Because the trees have no structure, all the nodes in both trees may lie on the right path. This condition would give a Θ(N) worst-case bound for merging the trees (in Exercise 23.4 you are asked to construct such a tree). As we demonstrate shortly, the amortized time needed to merge two skew heaps is O(log N). As with the splay tree, we introduce a potential function that cancels the varying costs of skew heap operations. We want the potential function to increase by a total of O(log N) – (r1 + r2) so that the total of the merge cost and potential change is only O(log N). If the potential is minimal prior to the ﬁrst operation, applying the telescoping sum guarantees that the total spent for any M operations is O(M log N), as with the splay tree. What we need is some potential function that captures the effect of skew heap operations. Finding such a function is quite challenging. Once we have found one, however, the proof is relatively short. deﬁnition: A node is a heavy node if the size of its right subtree is larger than the size of its left subtree. Otherwise, it is a light node; a node is light if its subtrees are of equal size.

23.1 the skew heap The potential function is the number  of heavy nodes.  Only nodes on the  merged path have  their heavy or light  status changed.  The number of light  nodes on a right  path is logarithmic. In Figure 23.3, prior to the merge, nodes 3 and 4 are heavy. After the merge, only node 3 is heavy. Three facts are easily shown. First, as a result of a merge, only nodes on the right path can have their heavy or light status changed because no other nodes have their subtrees altered. Second, a leaf is light. Third, the number of light nodes on the right path of an N node tree is at most ⎣log N⎦+ 1. The reason is that the right child of a light node is less than half the size of the light node itself, and the halving principle applies. The additional +1 is a result of the leaf’s being light. With these preliminaries, we can now state and prove Theorems 23.1 and 23.2.  figure 23.3 Change in the heavy  or light status of  nodes after a merge 8 + L H L L L L L L Let H1 and H2 be two skew heaps with N1 and N2 nodes, respectively, and let N be their  combined size (that is, N1 + N2). Suppose that the right path of H1 has l1 light nodes and  h1 heavy nodes, for a total of l1 + h1, whereas the right path of H2 has l2 light nodes and  h2 heavy nodes, for a total of l2 + h2. If the potential is deﬁned as the total number of heavy  nodes in the collection of skew heaps, then the merge costs at most 2 log N + (h1 + h2), but the change in potential is at most 2 log N – (h1 + h2). Theorem 23.1 The cost of the merge is merely the total number of nodes on the right paths,  l1 + l2 + h1 + h2. The number of light nodes is logarithmic, so   and  . Thus l1 + l2 ≤log N1 + log N2 + 2 ≤2 log N, where the last inequality  follows from Theorem 22.4. The merge cost is thus at most 2 log N + (h1 + h2). The  bound on the potential change follows from the fact that only the nodes involved in  the merge can have their heavy/light status changed and from the fact that any  heavy node on the path must become light because its children are swapped. Even if  all the light nodes became heavy, the potential change would still be limited to  l1 + l2 – (h1 + h2). Based on the same argument as before, that is at most  2 log N – (h1 + h2). Proof l1 N1 log + ≤ l2 N2 log + ≤

chapter 23 merging priority queues Finding a useful  potential function is  the most difﬁcult  part of the analysis. The skew heap is a remarkable example of a simple algorithm with an analysis that is not obvious. The analysis, however, is easy to perform once we have identiﬁed the appropriate potential function. Unfortunately, there is still no general theory that allows us to decide on a potential function. Typically, many different functions have to be tried before a usable one is found. A nonrecursive  algorithm should be  used because of  the possibility that  we could run out of  stack space. One comment is in order: Although the initial description of the algorithm uses recursion and recursion provides the simplest code, it cannot be used in practice. The reason is that the linear worst-case time for an operation could cause an overﬂow of the runtime stack when the recursion is implemented. Consequently, a nonrecursive algorithm must be used. Rather than explore those possibilities, we discuss an alternative data structure that is slightly more complicated: the pairing heap. This data structure has not been completely analyzed, but it seems to perform well in practice. 23.2 the pairing heap The pairing heap is  a heap-ordered  M-ary tree with no  structural constraints. Its analysis  is incomplete, but it  appears to perform  well in practice. The pairing heap is a structurally unconstrained heap-ordered M-ary tree for which all operations except deletion take constant worst-case time. Although deleteMin could take linear worst-case time, any sequence of pairing heap operations has logarithmic amortized performance. It has been conjectured— but not proved—that even better performance is guaranteed. However, the best possible scenario—namely, that all operations except for deleteMin have constant amortized cost, while deleteMin has logarithmic amortized cost—has recently been shown to be untrue. Theorem 23.2 The amortized cost of the skew heap is at most 4 log N for the merge, insert, and  deleteMin operations. Proof Let   be the potential in the collection of skew heaps immediately following the ith operation. Note that   and  . An insertion creates a single node tree  whose root is by deﬁnition light and thus does not alter the potential prior to the  resulting merge. A deleteMin operation discards the root prior to the merge, so it cannot raise the potential (it may, in fact, lower it). We need to consider only the merging  costs. Let   be the cost of the merge that occurs as a result of the ith operation.  Then  . Telescoping over any M operations yields   because   is not negative. Φi Φ0 = Φi ≥ ci ci Φi Φi – – + N log ≤ ci i = M ∑ 4M N log ≤ ΦM Φ0 –

23.2 the pairing heap The pairing heap is  stored by using a  left child/right sibling representation.  A third link is used  for decreaseKey. Figure 23.4 shows an abstract pairing heap. The actual implementation uses a left child/right sibling representation (see Chapter 18). The decreaseKey method, as we discuss shortly, requires that each node contain an additional link. A node that is a leftmost child contains a link to its parent; otherwise, the node is a right sibling and contains a link to its left sibling. This representation is shown in Figure 23.5, where the darkened line indicates that two links (one in each direction) connect pairs of nodes. 23.2.1   pairing heap operations Merging is simple:  Attach the larger  root tree as a left  child of the smaller  root tree. Insertion  and decreasing are  also simple. In principle, the basic pairing heap operations are simple, which is why the pairing heap performs well in practice. To merge two pairing heaps, we make the heap with the larger root the new ﬁrst child of the heap with the smaller root. Insertion is a special case of merging. To perform a decreaseKey operation, we lower the value of the requested node. Because we are not maintaining parent links for all nodes, we do not know if this action violates the heap order. Thus we detach the adjusted node from its parent and complete decreaseKey by merging the two pairing heaps that result. Figure 23.5 shows that detaching a node from its parent means removing it from what is essentially a linked list of children. So far we are in great shape: Every operation described takes constant time. However, we are not so lucky with the deleteMin operation. figure 23.4 Abstract representation of a  sample pairing heap figure 23.5 Actual representation  of the pairing heap  shown in Figure 23.4;  the dark lines  represent a pair of  links that connect  nodes in both  directions

chapter 23 merging priority queues The deleteMin operation is expensive because the  new root could be  any of the c children of the old  root. We need  c  – 1 merges. To perform a deleteMin, we must remove the root of the tree, creating a collection of heaps. If there are c children of the root, combining these heaps into one heap requires c – 1 merges. Hence, if there are lots of children of the root, the deleteMin operation costs lots of time. If the insertion sequence is 1, 2, ..., N, then 1 is at the root and all the other items are in nodes that are children of the root. Consequently, deleteMin is O(N) time. The best that we can hope to do is to arrange the merges so that we do not have repeatedly expensive deleteMin operations. The order in which  pairing heap subtrees are merged is  important. The simplest algorithm is  two-pass merging. The order in which pairing heap subtrees are merged is important. The simplest and most practical of the many variants of doing so that have been proposed is two-pass merging, in which a ﬁrst scan merges pairs of children from left to right2 and then a second scan, right to left, is performed to complete the merging. After the ﬁrst scan, we have half as many trees to merge.  In the second scan, at each step, we merge the rightmost tree that remains from the ﬁrst scan with the current merged result. For example, if we have children c1 through c8, the ﬁrst scan performs the merges c1 and c2, c3 and c4, c5 and c6, and c7 and c8. The result is d1, d2, d3, and d4. We perform the second pass by merging d3 and d4; d2 is then merged with that result, and d1 is then merged with the result of that merge, completing the deleteMin operation. Figure 23.6 shows the result of using deleteMin on the pairing heap shown in Figure 23.5. Several alternatives have been  proposed. Most are  indistinguishable, but using a single  left-to-right pass is  a bad idea. Other merging strategies are possible. For instance, we can place each subtree (corresponding to a child) on a queue, repeatedly dequeue two trees, and then enqueue the result of merging them. After c – 1 merges, only one tree remains on the queue, which is the result of the deleteMin. However, using a stack instead of a queue is a disaster because the root of the resulting tree may possibly have c – 1 children. If that occurs in a sequence, the deleteMin operation will have linear, rather than logarithmic, amortized cost per operation. In Exercise 23.8 you are asked to construct such a sequence. 23.2.2   implementation of the pairing heap The prev data  member links to  either a left sibling  or a parent.  The PairingHeap class skeleton is shown in Figure 23.7. The nested class PairNode implements the nested Position interface that is declared at lines 16 and 17. In the pairing heap, insert returns a Position which is the newly created PairNode. The basic node of a pairing heap, PairNode, is shown in Figure 23.8 and consists of an item and three links. Two of these links are the left child and the 2. Care must be exercised if there is an odd number of children. When that happens, we merge the last child with the result of the rightmost merge to complete the ﬁrst scan.

23.2 the pairing heap next sibling. The third link is prev, which references the parent if the node is a ﬁrst child or to a left sibling otherwise. The findMin routine is coded in Figure 23.9. The minimum is at the root, so this routine is easily implemented. The insert routine, shown in Figure 23.10, creates a one-node tree and merges it with the root to obtain a new tree. As mentioned earlier in the section, insert returns a reference to the newly allocated node. Note that we must handle the special case of an insertion in an empty tree. figure 23.6 Recombination of  siblings after a  deleteMin. In each  merge, the larger root  tree is made the left  child of the smaller  root tree: (a) the  resulting trees;  (b) after the first pass;  (c) after the first  merge of the second  pass; (d) after the  second merge of the  second pass (a) (b) (c) (d)

chapter 23 merging priority queues figure 23.7 The PairingHeap class skeleton package weiss.nonstandard; // PairingHeap class // // CONSTRUCTION: with no initializer // // ******************PUBLIC OPERATIONS********************* // General methods for priority queues and also: // void decreaseKey( Position p, newVal ) //                        --> Decrease value in node p // ******************ERRORS******************************** // Exceptions thrown as warranted public class PairingHeap<AnyType extends Comparable<? super AnyType>> {     public interface Position<AnyType>       { AnyType getValue( ); }     private static class PairNode<AnyType> implements Position<AnyType>        { /* Figure 23.8 */ }     private PairNode<AnyType> root;     private int      theSize;     public PairingHeap( )       { root = null; theSize = 0; }     public boolean isEmpty( )       { return root == null; }     public int size( )       { return theSize; }     public void makeEmpty( )       { root = null; theSize = 0; }     public Position<AnyType> insert( AnyType x )       { /* Figure 23.10 */ }     public AnyType findMin( )       { /* Figure 23.9 */ }     public AnyType deleteMin( )       { /* Figure 23.11 */ }     public void decreaseKey( Position<AnyType> pos, AnyType newVal )       { /* Figure 23.12 */ }     private PairNode<AnyType> compareAndLink( PairNode<AnyType> first,                                               PairNode<AnyType> second )       { /* Figure 23.14 */ }     private PairNode [ ] doubleIfFull( PairNode [ ] array, int index )       { /* Implementation is as usual; see online code */ }     private PairNode<AnyType> combineSiblings( PairNode<AnyType> firstSibling )       { /* Figure 23.15 */ } }

23.2 the pairing heap figure 23.8 The PairNode nested class /**  * Private static class for use with PairingHeap.  */     private static class PairNode<AnyType> implements Position<AnyType>      {         /**          * Construct the PairNode.          * @param theElement the value stored in the node.          */         public PairNode( AnyType theElement )         {             element     = theElement;             leftChild   = null;             nextSibling = null;             prev        = null;         }         /**          * Returns the value stored at this position.          */         public AnyType getValue( )         {             return element;         }         public AnyType           element;         public PairNode<AnyType> leftChild;         public PairNode<AnyType> nextSibling;         public PairNode<AnyType> prev;     } figure 23.9 The findMin method  for the PairingHeap class /**  * Find the smallest item in the priority queue.  * @return the smallest item.  * @throws UnderflowException if pairing heap is empty.  */     public AnyType findMin( )     {         if( isEmpty( ) )             throw new UnderflowException( );         return root.element;     }

chapter 23 merging priority queues The deleteMin operation is implemented as a call to  combineSiblings. Figure 23.11 implements the deleteMin routine. If the pairing heap is empty, we have an error. After saving the value found in the root (at line 11) and clearing the value at line 12, we make a call to combineSiblings at line 16 to merge the root’s subtrees and set the result to the new root. If there are no subtrees, we merely set root to null at line 14. figure 23.10 The insert routine for  the PairingHeap class /**  * Insert into the priority queue, and return a Position  * that can be used by decreaseKey.  * Duplicates are allowed.  * @param x the item to insert.  * @return the node containing the newly inserted item.  */     public Position<AnyType> insert( AnyType x )     {         PairNode<AnyType> newNode = new PairNode<AnyType>( x );         if( root == null )             root = newNode;         else             root = compareAndLink( root, newNode );         theSize++;         return newNode;     } figure 23.11 The deleteMin method for the PairingHeap class /**  * Remove the smallest item from the priority queue.  * @return the smallest item.  * @throws UnderflowException if pairing heap is empty.  */     public AnyType deleteMin( )     {         if( isEmpty( ) )             throw new UnderflowException( ); AnyType x = findMin( ); root.element = null; // So decreaseKey can detect stale Position if( root.leftChild == null )             root = null;         else             root = combineSiblings( root.leftChild );         theSize--;         return x;     }

23.2 the pairing heap The decreaseKey method is implemented in Figure 23.12. If the new value is larger than the original, we might destroy the heap order. We have no way of knowing that without examining all the children. Because many children may exist, doing so would be inefﬁcient. Thus we assume that it is always an error to attempt to increase the key by using the decreaseKey. (In Exercise 23.9 you are asked to describe an algorithm for increaseKey.) After performing this test, we lower the value in the node. If the node is the root, we are done. Otherwise, we splice the node out of the list of children that it is in, using the code in lines 21 to 28. After doing that, we merely merge the resulting tree with the root. The two remaining routines are compareAndLink, which combines two trees, and combineSiblings, which combines all the siblings, when given the ﬁrst sibling. Figure 23.13 shows how two subheaps are combined. The procedure is generalized to allow the second subheap to have siblings (which is needed for the second pass in the two-pass merge). As mentioned earlier in figure 23.12 The decreaseKey method for the  PairingHeap class /**  * Change the value of the item stored in the pairing heap.  * @param pos any Position returned by insert.  * @param newVal the new value, which must be smaller  *    than the currently stored value.  * @throws IllegalArgumentException if pos is null.  * @throws IllegalValueException if new value is larger than old.  */     public void decreaseKey( Position<AnyType> pos, AnyType newVal )     {         if( pos == null )             throw new IllegalArgumentException( );         PairNode<AnyType> p = (PairNode<AnyType>) pos;         if( p.element == null || p.element.compareTo( newVal ) < 0 )             throw new IllegalValueException( );         p.element = newVal;         if( p != root )         {             if( p.nextSibling != null )                 p.nextSibling.prev = p.prev;             if( p.prev.leftChild == p )                 p.prev.leftChild = p.nextSibling;             else                 p.prev.nextSibling = p.nextSibling;             p.nextSibling = null;             root = compareAndLink( root, p );         }     }

chapter 23 merging priority queues the chapter, the subheap with the larger root is made a leftmost child of the other subheap, the code for which is shown in Figure 23.14. Note that in several instances a link reference is tested against null before it accesses its prev data member. This action suggests that having a nullNode sentinel—as was customary in the advanced search tree implementations—might be useful. This possibility is left for you to explore as Exercise 23.12. Finally, Figure 23.15 implements combineSiblings. We use the array treeArray to store the subtrees. We begin by separating the subtrees and storing them in treeArray, using the loop at lines 16 to 22. Assuming that we have more than one sibling to merge, we make a left-to-right pass at lines 28 and 29. The special case of an odd number of trees is handled at lines 31–36. We ﬁnish the merging with a right-to-left pass at lines 40 and 41. Once we have ﬁnished, the result appears in array position 0 and can be returned. 23.2.3   application: dijkstra’s shortest  weighted path algorithm The decreaseKey operation is an  improvement for  Dijkstra’s algorithm  in instances for  which there are  many calls  to it. As an example of how the decreaseKey operation is used, we rewrite Dijkstra’s algorithm (see Section 14.3). Recall that at any point we are maintaining a priority queue of Path objects, ordered by the dist data member. For each vertex in the graph, we needed only one Path object in the priority queue at any instant, but for convenience we had many. In this section, we rework the code so that if a vertex w’s distance is lowered, its position in the priority queue is found, and a decreaseKey operation is performed for its corresponding Path object. The new code is shown in Figure 23.16, and all the changes are relatively minor. First, at line 6 we declare that pq is a pairing heap rather than a binary heap. Note that the Vertex object has an additional data member pos figure 23.13 The compareAndLink method merges two  trees + F A S B C F A B S C S B A F C F > S F<– S

23.2 the pairing heap that represents its position in the priority queue (and is null if the Vertex is not in the priority queue). Initially, all the positions are null (which is done in clearAll). Whenever a vertex is inserted in the pairing heap, we adjust its pos data member—at lines 13 and 35. The algorithm itself is simpliﬁed. Now we merely call deleteMin so long as the pairing heap is not empty, figure 23.14 The compareAndLink routine /**  * Internal method that is the basic operation to maintain order.  * Links first and second together to satisfy heap order.  * @param first root of tree 1, which may not be null.  *    first.nextSibling MUST be null on entry.  * @param second root of tree 2, which may be null.  * @return result of the tree merge.  */     private PairNode<AnyType> compareAndLink( PairNode<AnyType> first,                                               PairNode<AnyType> second )     {         if( second == null )             return first;         if( second.element.compareTo( first.element ) < 0 )         {             // Attach first as leftmost child of second             second.prev = first.prev;             first.prev = second;             first.nextSibling = second.leftChild;             if( first.nextSibling != null )                 first.nextSibling.prev = first;             second.leftChild = first;             return second;         }         else         {             // Attach second as leftmost child of first             second.prev = first;             first.nextSibling = second.nextSibling;             if( first.nextSibling != null )                 first.nextSibling.prev = first;             second.nextSibling = first.leftChild;             if( second.nextSibling != null )                 second.nextSibling.prev = second;             first.leftChild = second;             return first;         }     }

chapter 23 merging priority queues figure 23.15 The heart of the pairing heap algorithm: implementing a two-pass merge to combine all the siblings,  given the first sibling    // The tree array for combineSiblings     private PairNode [ ] treeArray =  new PairNode[ 5 ];     /**      * Internal method that implements two-pass merging.      * @param firstSibling the root of the conglomerate;      *     assumed not null.      */     private PairNode<AnyType> combineSiblings( PairNode<AnyType> firstSibling )     {         if( firstSibling.nextSibling == null )             return firstSibling;             // Store the subtrees in an array         int numSiblings = 0;         for( ; firstSibling != null; numSiblings++ )         {             treeArray = doubleIfFull( treeArray, numSiblings );             treeArray[ numSiblings ] = firstSibling;             firstSibling.prev.nextSibling = null;  // break links             firstSibling = firstSibling.nextSibling;         }         treeArray = doubleIfFull( treeArray, numSiblings );         treeArray[ numSiblings ] = null;             // Combine subtrees two at a time, going left to right         int i = 0;         for( ; i + 1 < numSiblings; i += 2 )             treeArray[ i ] = compareAndLink( treeArray[ i ], treeArray[ i + 1 ] );         int j = i - 2;             // j has the result of last compareAndLink.             // If an odd number of trees, get the last one.         if( j == numSiblings - 3 )             treeArray[ j ] = compareAndLink( treeArray[ j ], treeArray[ j + 2 ] );             // Now go right to left, merging last tree with             // next to last. The result becomes the new last.         for( ; j >= 2; j -= 2 )             treeArray[ j - 2 ] = compareAndLink( treeArray[ j - 2 ], treeArray[ j ] );         return (PairNode<AnyType>) treeArray[ 0 ];     }

23.2 the pairing heap rather than repeatedly calling deleteMin until an unseen vertex emerges. Consequently, we no longer need the scratch data member. Compare lines 15–18 to the corresponding code presented in Figure 14.27. All that remains figure 23.16 Dijkstra’s algorithm, using the pairing heap and the decreaseKey operation /**  * Single-source weighted shortest-path algorithm using pairing heaps.  */     public void dijkstra( String startName )     {         PairingHeap<Path> pq = new PairingHeap<Path>( );         Vertex start = vertexMap.get( startName );         if( start == null )             throw new NoSuchElementException( "Start vertex not found" );         clearAll( );         start.pos = pq.insert( new Path( start, 0 ) ); start.dist = 0;         while ( !pq.isEmpty( ) )         {             Path vrec = pq.deleteMin( );             Vertex v = vrec.dest;             for( Edge e : v.adj )             {                 Vertex w = e.dest;                 double cvw = e.cost;                 if( cvw < 0 )                     throw new GraphException( "Graph has negative edges" );                 if( w.dist > v.dist + cvw )                 {                     w.dist = v.dist + cvw;                     w.prev = v;                     Path newVal = new Path( w, w.dist );                     if( w.pos == null )                         w.pos = pq.insert( newVal );                     else                         pq.decreaseKey( w.pos, newVal );                  }             }         }     }

chapter 23 merging priority queues to be done are the updates after line 28 that indicate a change is in order. If the vertex has never been placed in the priority queue, we insert it for the ﬁrst time, updating its pos data member. Otherwise, we merely call decreaseKey at line 37. Whether the binary heap implementation of Dijkstra’s algorithm is faster than the pairing heap implementation depends on several factors. One study (see the Reference section), suggests that the pairing heap is slightly better than the binary heap when both are carefully implemented. The results depend heavily on the coding details and the frequency of the decreaseKey operations. More study is needed to decide when the pairing heap is suitable in practice.

chapt er the disjoint  set class In this chapter we describe an efﬁcient data structure for solving the equivalence problem: the disjoint set class. This data structure is simple to implement, with each routine requiring only a few lines of code. Its implementation is also extremely fast, requiring constant average time per operation. This data structure is also very interesting from a theoretical point of view because its analysis is extremely difﬁcult; the functional form of the worst case is unlike any discussed so far in this text. In this chapter, we show n Three simple applications of the disjoint set class n A way to implement the disjoint set class with minimal coding effort n A method for increasing the speed of the disjoint set class, using two simple observations n An analysis of the running time of a fast implementation of the disjoint set class

chapter 24 the disjoint set class 24.1 equivalence relations A relation is deﬁned  on a set if every  pair of elements  either is related or  is not. An equivalence relation is  reﬂexive,  symmetric, and  transitive. A relation R is deﬁned on a set S if for every pair of elements (a, b), a R b is either true or false. If a R b is true, we say that a is related to b. An equivalence relation is a relation R that satisﬁes three properties. 1. Reﬂexive: a R a is true for all  2. Symmetric: a R b if and only if b R a. 3. Transitive: a R b and b R c implies that a R c. Electrical connectivity, where all connections are by metal wires, is an equivalence relation. The relation is clearly reﬂexive, as any component is connected to itself. If a is electrically connected to b, then b must be electrically connected to a, so the relation is symmetric. Finally, if a is connected to b and b is connected to c, then a is connected to c. Likewise, connectivity through a bidirectional network forms equivalence classes of connected components. However, if the connections in the network are directed (i.e., a connection from v to w does not imply one from w to v), we do not have an equivalence relation because the symmetric property does not hold. An example is a relation in which town a is related to town b if traveling from a to b by road is possible. This relationship is an equivalence relation if the roads are two-way. 24.2 dynamic equivalence and applications For any equivalence relation, denoted ~, the natural problem is to decide for any a and b whether a ~ b. If the relation is stored as a two-dimensional array of Boolean variables, equivalence can be tested in constant time. The problem is that the relation is usually implicitly, rather than explicitly, deﬁned. The equivalence class of an element x in set S is  the subset of S that contains all the elements related to x. The equivalence  classes form  disjoint sets. For example, an equivalence relation is deﬁned over the ﬁve-element set . This set yields 25 pairs of elements, each of which either is or is not related. However, the information that a1 ~ a2, a3 ~ a4, a1 ~ a5, and a4 ~ a2 are all related implies that all pairs are related. We want to be able to infer this condition quickly. The equivalence class of an element   is the subset of S that contains all the elements related to x. Note that the equivalence classes form a partition of S: Every member of S appears in exactly one equivalence class. To decide whether a ~ b, we need only check whether a and b are in the same a b , S, ∈ a S. ∈ a1 a2 a3 a4 a5 , , , , { } x S ∈

24.2 dynamic equivalence and applications equivalence class. This information provides the strategy to solve the equivalence problem. The input is initially a collection of N sets, each with one element. In this initial representation all relations (except reﬂexive relations) are false. Each set has a different element, so   and such sets (in which any two sets contain no common elements) are called disjoint sets. The two basic  disjoint set class operations are  union and find. The two basic disjoint set class operations are find, which returns the name of the set (i.e., the equivalence class) containing a given element, and the union, which adds relations. If we want to add the pair (a, b) to the list of relations, we ﬁrst determine whether a and b are already related. We do so by performing find operations on both a and b and ﬁnding out whether they are in the same equivalence class; if they are not, we apply union. This operation merges the two equivalence classes containing a and b into a new equivalence class. In terms of sets the result is a new set  , which we create by simultaneously destroying the originals and preserving the disjointedness of all the sets. The data structure to do this is often called the disjoint set union/ ﬁnd data structure. The union/ﬁnd algorithm is executed by processing union/ ﬁnd requests within the disjoint set data structure. In an online algorithm, an answer  must be provided  for each query  before the next  query can be  viewed. The algorithm is dynamic because, during the course of algorithm execution, the sets can change via the union operation. The algorithm must also operate as an online algorithm so that, when a find is performed, an answer must be given before the next query can be viewed. Another possibility is an offline algorithm in which the entire sequence of union and find requests are made visible. The answer it provides for each find must still be consistent with all the unions performed before the find. However, the algorithm can give all its answers after it has dealt with all the questions. This distinction is similar to the difference between taking a written exam (which is generally offline because you only have to give the answers before time expires) and taking an oral exam (which is online because you must answer the current question before proceeding to the next question). The set elements  are numbered  sequentially, starting from 0. Note that we do not perform any operations to compare the relative values of elements but merely require knowledge of their location. For this reason, we can assume that all elements have been numbered sequentially, starting from 0, and that the numbering can be determined easily by some hashing scheme. Before describing how to implement the union and find operations, we provide three applications of the data structure. 24.2.1   application: generating mazes An example of the use of the union/ﬁnd data structure is to generate mazes, such as the one shown in Figure 24.1. The starting point is the top-left corner, Si Sj ∩ ∅ = Sk Si Sj ∪ =

chapter 24 the disjoint set class and the ending point is the bottom-right corner. We can view the maze as a 50 × 88 rectangle of cells in which the top-left cell is connected to the bottomright cell, and cells are separated from their neighboring cells via walls. A simple algorithm to generate the maze is to start with walls everywhere (except for the entrance and exit). We then continually choose a wall randomly and knock it down if the cells that the wall separates are not already connected to each other. If we repeat this process until the starting and ending cells are connected, we have a maze. Continuing to knock down walls until every cell is reachable from every other cell is actually better because doing so generates more false leads in the maze. We illustrate the algorithm with a 5 × 5 maze, and Figure 24.2 shows the initial conﬁguration. We use the union/ﬁnd data structure to represent sets of cells that are connected to each other. Initially, walls are everywhere, and each cell is in its own equivalence class. Figure 24.3 shows a later stage of the algorithm, after a few walls have been knocked down. Suppose, at this stage, that we randomly target the wall that connects cells 8 and 13. Because 8 and 13 are already connected (they are in the same set), we would not remove the wall because to do so would simply trivialize the maze. Suppose that we randomly target cells 18 and 13 next. By performing two find operations, we determine that these cells are in different sets; thus 18 and 13 are not already connected. Therefore we knock down the wall that separates them, as shown in Figure 24.4. As a figure 24.1 A 50 × 88 maze

24.2 dynamic equivalence and applications figure 24.2 Initial state: All walls  are up, and all cells  are in their own sets. {0} {1} {2} {3} {4} {5} {6} {7} {8} {9} {10} {11} {12} {13} {14} {15} {16} {17} {18} {19} {20} {21} {22} {23} {24} figure 24.3 At some point in the  algorithm, several  walls have been  knocked down and  sets have been  merged. At this point,  if we randomly select  the wall between 8  and 13, this wall is not  knocked down  because 8 and 13 are  already connected. {0, 1} {2} {3} {4, 6, 7, 8, 9, 13,14} {5} {10, 11, 15} {12} {16, 17, 18, 22} {19} {20} {21} {22} {23} {24} figure 24.4 We randomly select  the wall between  squares 18 and 13 in  Figure 24.3; this wall  has been knocked  down because 18 and  13 were not already  connected, and their  sets have been  merged. {0,1} {2} {3} {5} {10, 11, 15} {12} {4, 6, 7, 8, 9, 13, 14, 16, 17, 18, 22} {19} {20} {21} {23} {24}

chapter 24 the disjoint set class result of this operation, the sets containing cells 18 and 13 are combined by a union operation. The reason is that all the cells previously connected to 18 are now connected to all the cells previously connected to 13. At the end of the algorithm, as depicted in Figure 24.5, all the cells are connected, and we are done. The running time of the algorithm is dominated by the union/ﬁnd costs. The size of the union/ﬁnd universe is the number of cells. The number of find operations is proportional to the number of cells because the number of removed walls is 1 less than the number of cells. If we look carefully, however, we can see that there are only about twice as many walls as cells in the ﬁrst place. Thus, if N is the number of cells and as there are two finds per randomly targeted wall, we get an estimate of between (roughly) 2N and 4N find operations throughout the algorithm. Therefore the algorithm’s running time depends on the cost of O(N) union and O(N) find operations. 24.2.2   application: minimum spanning trees The minimum spanning tree is a  connected subgraph of G that  spans all vertices at  minimum total cost. A spanning tree of an undirected graph is a tree formed by graph edges that connect all the vertices of the graph. Unlike the graphs in Chapter 14, an edge (u, v) in a graph G is identical to an edge (v, u). The cost of a spanning tree is the sum of the costs of the edges in the tree. The minimum spanning tree is a connected subgraph of G that spans all vertices at minimum cost. A minimum spanning tree exists only if the subgraph of G is connected. As we show shortly, testing a graph’s connectivity can be done as part of the minimum spanning tree computation. In Figure 24.6(b), the graph is a minimum spanning tree of the graph in Figure 24.6(a) (it happens to be unique, which is unusual if the graph has many edges of equal cost). Note that the number of edges in the minimum spanning tree is   The minimum spanning tree is a tree because it is figure 24.5 Eventually, 24 walls  have been knocked  down, and all the  elements are in the  same set. {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24} V 1. –

24.2 dynamic equivalence and applications acyclic, it is spanning because it covers every vertex, and it is minimum for the obvious reason. Suppose that we need to connect several towns with roads, minimizing the total construction cost, with the provision that we can transfer to another road only at a town (in other words, no extra junctions are allowed). Then we need to solve a minimum spanning tree problem, where each vertex is a town, and each edge is the cost of building a road between the two cities it connects. A related problem is the minimum Steiner tree problem, which is like the minimum spanning tree problem, except that junctions can be created as part of the solution. The minimum Steiner tree problem is much more difﬁcult to solve. However, it can be shown that if the cost of a connection is proportional to the Euclidean distance, the minimum spanning tree is at most 15 percent more expensive than the minimum Steiner tree. Thus a minimum spanning tree, which is easy to compute, provides a good approximation for the minimum Steiner tree, which is hard to compute. Kruskal’s algorithm is used to select  edges in order of  increasing cost and  adds an edge to  the tree if it does  not create a cycle. A simple algorithm, commonly called Kruskal’s algorithm, is used to select edges continually in order of smallest weight and to add an edge to the tree if it does not cause a cycle. Formally, Kruskal’s algorithm maintains a forest—a collection of trees. Initially, there are   single-node trees. Adding an edge merges two trees into one. When the algorithm terminates, there is only one tree, which is the minimum spanning tree.1 By counting the number of accepted edges, we can determine when the algorithm should terminate. Figure 24.7 shows the action of Kruskal’s algorithm on the graph shown in Figure 24.6. The ﬁrst ﬁve edges are all accepted because they do not create cycles. The next two edges, (v1, v3) (of cost 3) and then (v0, v2) (of cost 4), are rejected because each would create a cycle in the tree. The next edge considered is accepted, and because it is the sixth edge in a seven-vertex graph, we can terminate the algorithm. figure 24.6 (a) A graph G and  (b) its minimum  spanning tree V2 V0 V3 V4 V1 V5 V6 V2 V0 V3 (a) (b) V4 V1 V5 V6 1. If the graph is not connected, the algorithm will terminate with more than one tree. Each tree then represents a minimum spanning tree for each connected component of the graph. V

chapter 24 the disjoint set class The edges can be  sorted, or a priority  queue can be used. Ordering the edges for testing is simple enough to do. We can sort them at a cost of  and then step through the ordered array of edges. Alternatively, we can construct a priority queue of   edges and repeatedly obtain edges by calling deleteMin. Although the worst-case bound is unchanged, using a priority queue is sometimes better because Kruskal’s algorithm tends to test only a small fraction of the edges on random graphs. Of course, in the worst case, all the edges may have to be tried. For instance, if there were an extra vertex v8 and edge (v5, v8) of cost 100, all the edges would have to be examined. In this case, a quicksort at the start would be faster. In effect, the choice between a priority queue and an initial sort is a gamble on how many edges are likely to have to be examined. More interesting is the issue of how we decide whether an edge (u, v) should be accepted or rejected. Clearly, adding the edge (u, v) causes a cycle if (and only if) u and v are already connected in the current spanning forest, figure 24.7 Kruskal’s algorithm  after each edge has  been considered. The  stages proceed leftto-right, top-tobottom, as numbered. V2 V0 V3 V4 V1 V5 V6 V2 V0 V3 V4 V1 V5 V6 V2 V0 V3 V4 V1 V5 V6 V2 V0 V3 V4 V1 V5 V6 V2 V0 V3 V4 V1 V5 V6 V2 V0 V3 V4 V1 V5 V6 V2 V0 V3 V4 V1 V5 V6 V2 V0 V3 V4 V1 V5 V6 E E log E

24.2 dynamic equivalence and applications The test for cycles  is done by using a  union/ﬁnd data  structure. which is a collection of trees. Thus we merely maintain each connected component in the spanning forest as a disjoint set. Initially, each vertex is in its own disjoint set. If u and v are in the same disjoint set, as determined by two find operations, the edge is rejected because u and v are already connected. Otherwise, the edge is accepted and a union operation is performed on the two disjoint sets containing u and v, in effect, combining the connected components. This result is what we want because once edge (u, v) has been added to the spanning forest, if w was connected to u and x was connected to v, x and w must be connected and thus belong in the same set. 24.2.3   application: the nearest  common ancestor problem Another illustration of the union/ﬁnd data structure is the offline nearest common ancestor (NCA) problem. offline nearest common ancestor problem Given a tree and a list of pairs of nodes in the tree, ﬁnd the nearest common ancestor for each pair of nodes. Solution of the  NCA is important in  graph algorithm  and computational  biology applications. As an example, Figure 24.8 shows a tree with a pair list containing ﬁve requests. For the pair of nodes u and z, node C is the nearest ancestor of both. (A and B are also ancestors, but they are not the closest.) The problem is offline because we can see the entire request sequence prior to providing the ﬁrst answer. Solution of this problem is important in graph theory applications and computational biology (where the tree represents evolution) applications. A postorder traversal can be  used to solve  the problem. The algorithm works by performing a postorder tree traversal. When we are about to return from processing a node, we examine the pair list to determine whether any ancestor calculations are to be performed. If u is the current figure 24.8 The nearest common  ancestor for each  request in the pair  sequence  (x, y), (u, z), (w, x), (z, w), and  (w, y) is A, C, A, B,  and y, respectively. A C B y w u D z x

chapter 24 the disjoint set class node, (u, v) is in the pair list and we have already ﬁnished the recursive call to v, we have enough information to determine NCA(u, v). The anchor of a visited (but not necessarily marked) node  v is the node on the  current access path  that is closest to v. Figure 24.9 helps in understanding how this algorithm works. Here, we are about to ﬁnish the recursive call to D. All shaded nodes have been visited by a recursive call, and except for the nodes on the path to D, all the recursive calls have already ﬁnished. We mark a node after its recursive call has been completed. If v is marked, then NCA(D, v) is some node on the path to D. The anchor of a visited (but not necessarily marked) node v is the node on the current access path that is closest to v. In Figure 24.9, p’s anchor is A, q’s anchor is B, and r is unanchored because it has yet to be visited; we can argue that r’s anchor is r at the point that r is ﬁrst visited. Each node on the current access path is an anchor (of at least itself). Furthermore, the visited nodes form equivalence classes: Two nodes are related if they have the same anchor, and we can regard each unvisited node as being in its own class. Now suppose once again that (D, v) is in the pair list. Then we have three cases. 1. v is unmarked, so we have no information to compute NCA(D, v). However, when v is marked, we are able to determine NCA(v, D). 2. v is marked but not in D’s subtree, so NCA(D, v) is v’s anchor. 3. v is in D’s subtree, so NCA(D, v) = D. Note that this is not a special case because v’s anchor is D. The union/ﬁnd  algorithm is used to  maintain the sets of  nodes with common anchors. All that remains to be done is to ensure that, at any instant, we can determine the anchor of any visited node. We can easily do so with the union/ﬁnd algorithm. After a recursive call returns, we call union. For instance, after the recursive call to D in Figure 24.9 returns, all nodes in D have their anchor figure 24.9 The sets immediately  prior to the return  from the recursive call  to D; D is marked as  visited and NCA(D, v) is v ’s anchor to the  current path. r B q p D A C

24.2 dynamic equivalence and applications changed from D to C. The new situation is shown in Figure 24.10. Thus we need to merge the two equivalence classes into one. At any point, we can obtain the anchor for a vertex v by a call to a disjoint set find. Because find returns a set number, we use an array anchor to store the anchor node corresponding to a particular set. The pseudocode is  compact. A pseudocode implementation of the NCA algorithm is shown in Figure 24.11. As mentioned earlier in the chapter, the find operation generally is based on the assumption that elements of the set are 0, 1,  ,   so we store a preorder number in each tree node in a preprocessing step that computes the size of the tree. An object-oriented approach might attempt to incorporate a mapping into the find, but we do not do so. We also assume that we have an array of lists in which to store the NCA requests; that is, list i stores the requests for tree node i. With those details taken care of, the code is remarkably short. When a node u is ﬁrst visited, it becomes the anchor of itself, as in line 18 of Figure 24.11. It then recursively processes its children v by making the call at line 23. After each recursive call returns, the subtree is combined into u’s current equivalence class and we ensure that the anchor is updated at lines 24 and 25. When all the children have been processed recursively, we can mark u as processed at line 29 and ﬁnish by checking all NCA requests involving u at lines 30 to 33.2 figure 24.10 After the recursive call  from D returns, we  merge the set  anchored by D into  the set anchored by  C and then compute  all NCA(C, v) for  nodes v marked prior  to completing C’s recursive call. A B C D 2. Strictly speaking, u should be marked at the last statement, but marking it earlier handles the annoying request NCA(u, u). … N 1, –

chapter 24 the disjoint set class 24.3 the quick-find algorithm In this section and Section 24.4 we lay the groundwork for the efﬁcient implementation of the union/ﬁnd data structure. There are two basic strategies for solving the union/ﬁnd problem. The ﬁrst approach, the quick-ﬁnd algorithm, ensures that the find instruction can be executed in constant worst-case time. The other approach, the quick-union algorithm, ensures figure 24.11 Pseudocode for the nearest common ancestors problem // Nearest Common Ancestors algorithm     // // Preconditions (and global objects):     //  1. union/find structure is initialized     //  2. All nodes are initially unmarked     //  3. Preorder numbers are already assigned in num field     //  4. Each node can store its marked status     //  5. List of pairs is globally available DisjSets s = new DisjSets( treeSize );  // union/find Node [ ] anchor = new Node[ treeSize ]; // Anchor node for each set // main makes the call NCA( root ) // after required initializations void NCA( Node u ) {         anchor[ s.find( u.num ) ] = u;         // Do postorder calls         for( each child v of u )         {             NCA( v );             s.union( s.find( u.num ), s.find( v.num ) );             anchor[ s.find( u.num ) ] = u;         }         // Do nca calculation for pairs involving u         u.marked = true;         for( each v such that NCA( u, v ) is required )             if( v.marked )                 System.out.println( "NCA( " + u + ", " + v +                        " ) is " + anchor[ s.find( v.num ) ] ); }

24.4 the quick-union algorithm that the union operation can be executed in constant worst-case time. It has been shown that both cannot be done simultaneously in constant worstcase (or even amortized) time. For the find operation to be fast, in an array we could maintain the name of the equivalence class for each element. Then find is a simple constanttime lookup. Suppose that we want to perform union(a, b). Suppose, too, that a is in equivalence class i and that b is in equivalence class j. Then we can scan down the array, changing all i’s to j’s. Unfortunately, this scan takes linear time. Thus a sequence of N – 1 union operations (the maximum because then everything is in one set) would take quadratic time. In the typical case in which the number of finds is subquadratic, this time is clearly unacceptable. One possibility is to keep all the elements that are in the same equivalence class in a linked list. This approach saves time when we are updating because we do not have to search the entire array. By itself that does not reduce the asymptotic running time, as performing Θ(N 2) equivalence class updates over the course of the algorithm is still possible. The argument that  an equivalence  class can change  at most log N times per item is  also used in the  quick-union algorithm. Quick-ﬁnd is  a simple algorithm, but quickunion is better. If we also keep track of the size of the equivalence classes—and when performing a union change the name of the smaller class to the larger—the total time spent for N unions is O(N log N). The reason is that each element can have its equivalence class changed at most log N times because every time its class is changed, its new equivalence class is at least twice as large as its old class (so the repeated doubling principle applies). This strategy provides that any sequence of at most M find and N – 1 union operations take at most O(M + N log N) time. If M is linear (or slightly nonlinear), this solution is still expensive. It also is a bit messy because we must maintain linked lists. In Section 24.4 we examine a solution to the union/ﬁnd problem that makes union easy but find hard—the quick-union algorithm. Even so, the running time for any sequence of at most M find and N – 1 union operations is only negligibly more than O(M + N) time and, moreover, only a single array of integers is used. 24.4 the quick-union algorithm Recall that the union/ﬁnd problem does not require a find operation to return any speciﬁc name; it requires just that finds on two elements return the same answer if and only if they are in the same set. One possibility might be to use a tree to represent a set, as each element in a tree has the same root and the root can be used to name the set.

chapter 24 the disjoint set class A tree is represented by an array  of integers representing parent  nodes. The set  name of any node  in a tree is the root  of a tree. Each set is represented by a tree (recall that a collection of trees is called a forest). The name of a set is given by the node at the root. Our trees are not necessarily binary trees, but their representation is easy because the only information we need is the parent. Thus we need only an array of integers: Each entry p[i] in the array represents the parent of element i, and we can use –1 as a parent to indicate a root. Figure 24.12 shows a forest and the array that represents it. The union operation is constant  time. To perform a union of two sets, we merge the two trees by making the root of one tree a child of the root of the other. This operation clearly takes constant time. Figures 24.13–24.15 represent the forest after each of union(4, 5), union(6, 7), and union(4, 6), where we have adopted the convention that the new root after union(x, y) is x. The cost of a find depends on the  depth of the  accessed node and  could be linear. A find operation on element x is performed by returning the root of the tree containing x. The time for performing this operation is proportional to the number of nodes on the path from x to the root. The union strategy outlined previously enables us to create a tree whose every node is on the path to x, resulting in a worst-case running time of Θ(N) per find. Typically (as shown in the preceding figure 24.12 A forest and its eight  elements, initially in  different sets -1 -1 -1 -1 -1 -1 -1 -1 figure 24.13 The forest after the  union of trees with  roots 4 and 5 -1 -1 -1 -1 -1 -1 -1

24.4 the quick-union algorithm applications), the running time is computed for a sequence of M intermixed instructions. In the worst case, M consecutive operations could take Θ(MN) time. Quadratic running time for a sequence of operations is generally unacceptable. Fortunately, there are several ways to easily ensure that this running time does not occur. 24.4.1   smart union algorithms We performed the previous unions rather arbitrarily by making the second tree a subtree of the ﬁrst. A simple improvement is always to make the smaller tree a subtree of the larger, breaking ties by any method, an approach called union-by-size. The preceding three union operations were all ties, so we can consider that they were performed by size. If the next operation is union(3, 4), the forest shown in Figure 24.16 forms. Had the size heuristic not been used, a deeper forest would have been formed (three nodes rather than one would have been one level deeper). figure 24.14 The forest after the  union of trees with  roots 6 and 7 -1 -1 -1 -1 -1 -1 figure 24.15 The forest after the  union of trees with  roots 4 and 6 -1 -1 -1 -1 -1

chapter 24 the disjoint set class Union-by-size guarantees logarithmic  ﬁnds. If the union operation is done by size, the depth of any node is never more than log N. A node is initially at depth 0, and when its depth increases as a result of a union, it is placed in a tree that is at least twice as large as before. Thus its depth can be increased at most log N times. (We used this argument in the quick-ﬁnd algorithm in Section 24.3.) This outcome implies that the running time for a find operation is O(log N) and that a sequence of M operations takes at most O(M log N) time. The tree shown in Figure 24.17 illustrates the worst tree possible after 15 union operations and is obtained if all the unions are between trees of equal size. (The worst-case tree is called a binomial tree. Binomial trees have other applications in advanced data structures.) Instead of –1 being  stored for roots, the  negative of the size  is stored. To implement this strategy, we need to keep track of the size of each tree. Since we are just using an array, we can have the array entry of the root contain the negative of the size of the tree, as shown in Figure 24.16. Thus the initial representation of the tree with all –1s is reasonable. When a union operation is performed, we check the sizes; the new size is the sum of the old. Thus union-by-size is not at all difﬁcult to implement and requires no extra space. It is also fast on average because, when random union operations are figure 24.16 The forest formed by  union-by-size, with the  sizes encoded as  negative numbers -1 -1 -1 -5 figure 24.17 Worst-case tree for  N = 16

24.4 the quick-union algorithm performed, generally very small (usually one-element) sets are merged with large sets throughout the algorithm. Mathematical analysis of this process is quite complex; the references at the end of the chapter provide some pointers to the literature. Union-by-height also guarantees  logarithmic find operations. An alternative implementation that also guarantees logarithmic depth is union-by-height in which we keep track of the height of the trees instead of the size and perform union operations by making a shallower tree a subtree of the deeper tree. This algorithm is easy to write and use because the height of a tree increases only when two equally deep trees are joined (and then the height goes up by 1). Thus union-by-height is a trivial modiﬁcation of unionby-size. As heights start at 0, we store the negative of the number of nodes rather than the height on the deepest path, as shown in Figure 24.18. 24.4.2   path compression The union/ﬁnd algorithm, as described so far, is quite acceptable for most cases. It is very simple and linear on average for a sequence of M instructions. However, the worst case is still unappealing. The reason is that a sequence of union operations occurring in some particular application (such as the NCA problem) is not obviously random (in fact, for certain trees, it is far from random). Hence we have to seek a better bound for the worst case of a sequence of M operations. Seemingly, no more improvements to the union algorithm are possible because the worst case is achievable when identical trees are merged. The only way to speed up the algorithm then, without reworking the data structure entirely, is to do something clever with the find operation. That something clever is path compression. Clearly, after we perform a find on x, changing x’s parent to the root would make sense. In that way, a second find on x or any item in x’s subtree becomes easier. There is no need to stop there, however. We might as well change the parents for all the nodes on the access path. In path compression every node on the path from x to the root figure 24.18 A forest formed by  union-by-height, with  the height encoded as  a negative number -1 -1 -1 -3 Path compression makes every  accessed node a  child of the root  until another union occurs.

chapter 24 the disjoint set class has its parent changed to the root. Figure 24.19 shows the effect of path compression after find(14) on the generic worst tree shown in Figure 24.17. With an extra two parent changes, nodes 12 and 13 are now one position closer to the root and nodes 14 and 15 are now two positions closer. The fast future accesses on the nodes pay (we hope) for the extra work to do the path compression. Note that subsequent unions push the nodes deeper. Path compression  guarantees logarithmic amortized cost for the  find operation.  When unions are done arbitrarily, path compression is a good idea because of the abundance of deep nodes; they are brought near the root by path compression. It has been proved that when path compression is done in this case, a sequence of M operations requires at most O(M log N) time, so path compression by itself guarantees logarithmic amortized cost for the find operation. Path compression  and a smart union  rule guarantee  essentially constant amortized  cost per operation  (i.e., a long  sequence can be  executed in almost  linear time). Path compression is perfectly compatible with union-by-size. Thus both routines can be implemented at the same time. However, path compression is not entirely compatible with union-by-height because path compression can change the heights of the trees. We do not know how to recompute them efﬁciently, so we do not attempt to do so. Then the heights stored for each tree become estimated heights, called ranks, which is not a problem. The resulting algorithm, union-by-rank, is thus obtained from union-by-height when compression is performed. As we show in Section 24.6, the combination of a smart union rule and path compression gives an almost linear guarantee on the running time for a sequence of M operations. 24.5 java implementation Disjoint sets are  relatively simple to  implement. The class skeleton for a disjoint sets class is given in Figure 24.20, and the implementation is completed in Figure 24.21. The entire algorithm is amazingly short. figure 24.19 Path compression  resulting from a  find(14) on the tree  shown in  Figure 24.17

24.5 java implementation In our routine, union is performed on the roots of the trees. Sometimes the operation is implemented by passing any two elements and having union perform the find operation to determine the roots. The interesting procedure is find. After the find has been performed recursively, array[x] is set to the root and then is returned. Because this procedure is recursive, all nodes on the path have their entries set to the root. figure 24.20 The disjoint sets class  skeleton package weiss.nonstandard; // DisjointSets class // // CONSTRUCTION: with int representing initial number of sets // // ******************PUBLIC OPERATIONS********************* // void union( root1, root2 ) --> Merge two sets // int find( x )              --> Return set containing x // ******************ERRORS******************************** // Error checking or parameters is performed public class DisjointSets {     public DisjointSets( int numElements )       { /* Figure 24.21 */ }     public void union( int root1, int root2 )       { /* Figure 24.21 */ }     public int find( int x )       { /* Figure 24.21 */ }     private int [ ] s;     private void assertIsRoot( int root )     {         assertIsItem( root );         if( s[ root ] >= 0 )             throw new IllegalArgumentException( );     }     private void assertIsItem( int x )     {         if( x < 0 || x >= s.length )             throw new IllegalArgumentException( );     } }

chapter 24 the disjoint set class figure 24.21 Implementation of a  disjoint sets class /**  * Construct the disjoint sets object.  * @param numElements the initial number of disjoint sets.  */     public DisjointSets( int numElements )     {         s = new int[ numElements ];         for( int i = 0; i < s.length; i++ )             s[ i ] = -1;     }     /**      * Union two disjoint sets using the height heuristic.      * root1 and root2 are distinct and represent set names.      * @param root1 the root of set 1.      * @param root2 the root of set 2.      * @throws IllegalArgumentException if root1 or root2      * are not distinct roots.      */     public void union( int root1, int root2 )     {         assertIsRoot( root1 );         assertIsRoot( root2 );         if( root1 == root2 )             throw new IllegalArgumentException( );         if( s[ root2 ] < s[ root1 ] )  // root2 is deeper             s[ root1 ] = root2;        // Make root2 new root         else         {             if( s[ root1 ] == s[ root2 ] )                 s[ root1 ]--;          // Update height if same             s[ root2 ] = root1;        // Make root1 new root         }     }     /**      * Perform a find with path compression.      * @param x the element being searched for.      * @return the set containing x.      * @throws IllegalArgumentException if x is not valid.      */     public int find( int x )     {         assertIsItem( x );         if( s[ x ] < 0 )             return x;         else             return s[ x ] = find( s[ x ] );     }

24.6 worst case for union-by-rank and path compression 24.6 worst case for union-by-rank  and path compression When both heuristics are used, the algorithm is almost linear in the worst case. Speciﬁcally, the time required to process a sequence of at most N – 1 union operations and M find operations in the worst case is Θ(Mα(M, N)) (provided that M ≥N), where α(M, N) is a functional inverse of Ackermann’s function, which grows very quickly and is deﬁned as follows:3 From the preceding, we deﬁne Ackermann’s function grows very  quickly, and its  inverse is essentially at most 4. You might want to compute some values, but for all practical purposes, α(M, N) ≤4, which is all that really matters here. For instance, for any j > 1, we have where the number of 2s in the exponent is j. The function F(N) = A(2, N) is commonly called a single-variable Ackermann’s function. The single-variable inverse of Ackermann’s function, sometimes written as log*N, is the number of times the logarithm of N needs to be applied until N ≤1. Thus log*65536 = 4, because log log log log 65536 = 1, and log*265536 = 5. However, keep in mind that 265536 has more than 20,000 digits. The function α(M, N) grows even slower than log*N. For instance, A(3, 1) = A(2, 2) = 222 = 16. Thus for N < 216, α(M, N) ≤3. Further, because A(4, 1) = A(3, 2) = A(2, A(3, 1)) = A(2, 16), which is 2 raised to a power of 16 stacked 2s, in practice, α(M, N) ≤4. However, α(M, N) is not a constant when M is slightly more than N, so the running time is not linear.4 3. Ackermann’s function is frequently deﬁned with A(1, j) = j + 1 for j ≥1. The form we use in this text grows faster; thus the inverse grows more slowly. A 1 j, ( ) j = A i 1 , ( ) A(i 1 2) , – = A i j, ( ) A(i 1 A i j – , ( )) , – = j ≥ i ≥ i j, ≥ α M N , ( ) min i 1 (A i M N ⁄ ) , N log > ( ) ≥ { } = 4. Note, however, that if M = N log*N, then α(M, N) is at most 2. Thus, so long as M is slightly more than linear, the running time is linear in M. A 2 j, ( ) A 1 A 2 j – , ( ) , ( ) = A 2 j – , ( ) = 222… =

chapter 24 the disjoint set class In the remainder of this section, we prove a slightly weaker result. We show that any sequence of M = Ω(N) union and find operations takes a total of O(M log*N) time. The same bound holds if we replace union-by-rank with union-by-size. This analysis is probably the most complex in this text and is one of the ﬁrst truly complex analyses ever performed for an algorithm that is essentially trivial to implement. By extending this technique, we can show the stronger bound claimed previously. 24.6.1   analysis of the union/find algorithm In this section, we establish a fairly tight bound on the running time of a sequence of M = Ω(N) union and find operations. The union and find operations may occur in any order, but union is done by rank and find is done with path compression. We begin with some theorems concerning the number of nodes of rank r. Intuitively, because of the union-by-rank rule, there are many more nodes of small rank than of large rank. In particular, there can be at most one node of rank log N. What we want to do is to produce as precise a bound as possible on the number of nodes of any particular rank r. Because ranks change only when union operations are performed (and then only when the two trees have the same rank), we can prove this bound by ignoring path compression. We do so in Theorem 24.1. Theorem 24.1 says that if no path compression is performed, any node of rank r must have at least 2r descendants. Path compression can change this condition, of course, because it can remove descendants from a node. However, Theorem 24.1 In the absence of path compression, when a sequence of union instructions is being  executed, a node of rank r must have 2r descendants (including itself). Proof The proof is by induction. The basis r = 0 is clearly true. Let T be the tree of rank r with the fewest number of descendants and x be T’s root. Suppose that the last  union with which x was involved was between T1 and T2. Suppose that T1’s root was  x. If T1 had rank r, then T1 would be a tree of rank r with fewer descendants than T. This condition contradicts the assumption that T is the tree with the smallest number  of descendants. Hence the rank of T1 is at most r – 1. The rank of T2 is at most the  rank of T1 because of union-by-rank. As T has rank r and the rank could only  increase because of T2, it follows that the rank of T2 is r – 1. Then the rank of T1 is  also r – 1. By the induction hypothesis, each tree has at least 2r – 1 descendants,  giving a total of 2r and establishing the theorem.

24.6 worst case for union-by-rank and path compression when union operations are performed—even with path compression—we are using ranks, or estimated heights. These ranks behave as if there is no path compression. Thus when the number of nodes of rank r are being bounded, path compression can be ignored, as in Theorem 24.2. Theorem 24.3 seems somewhat obvious, but it is crucial to the analysis. There are not too  many nodes of  large rank, and the  ranks increase on  any path up toward  a root. The following is a

Introduction This chapter emphasizes on brush up of the fundamentals of the JAVA Programming language. It will talk about variables, references, classes, loops, recursion, arrays etc. We assume that the reader is familiar with the syntax of the JAVA programming language and knows the basics of Object-Orientation.

First JAVA Program It is tradition to discuss a HelloWorld program in the start which will print the phrase “Hello, World!” to the output screen. So let us start discussing it. This is a small program but it contains many common features of all the JAVA programs. 1.    This program begins with “public class HelloWorld”: a.    A class is the basic unit of encapsulation. Keyword class is used to create a new class in our case class HelloWorld is created. The class name is same as the name of the file, which will contain this class. Therefore, in our case the name of the file is HelloWorld.java. A class contains data and methods. b.    The keyword public is an access specifier, which sets the accessibility of classes. A public class can be accessed from any class within the JAVA program. 2.    Next are the comments, which are for readability of the program and are ignored by the compiler. a.    A single line comment begins with two forward slash // b.    A multiline comment begins with /* and ends with */. 3.     “public static void main(String[] args)” : a.    A method is a set of statements that are executed to give desire result. b.    main() is a special method of a class this is the entry point of the application. c.    The keyword public with the method means that it can be accessed from any class of the application. d.    A keyword static in the method specifies that the method is a class method and will be called directly from the class name. It does not require any object to call this method.

e.    The void keyword means that this method is not going to return anything. 4.    System.out.println(“Hello, World!”) is a system provided method which will print “Hello, World! ” to the standard output.

Object An Object is an entity with state and behavior. A baby, a cat, a dog, a bulb etc are all examples of objects. A baby has properties & states (name, hungry, crying, sleeping etc.) and behaviors (feed, play, etc.). Another example a bulb have two states (on, off) and the two behaviors (turn on, turn off). Software objects are just like real world objects. They have state in the form of member variables called fields (isOn) and they expose behavior in the form of member functions called methods (turn on, turn off). Hiding internal details (state) of the object and allowing all the actions to be performed over the objects using methods is known has data-encapsulation. A Class is a prototype (blueprint) of objects. An object is an instance of a class.  Human is a class of living being and a person named John is an instance of human class. Example 1.1: class Bulb {           boolean isOn=false;           public void turnOn() {                    isOn = true;           }           public void turnOff() {                    isOn = false;           } } In this example, we have a class name Bulb. It has a member variable isOn, which indicates its state that the bulb is on or off. It has two methods turnOn() and turnoff() which will change the state of the object from off to on and vice versa.

Variable "Variables" are simply storage locations for data. For every variable, some memory is allocated. The size of this memory depends on the type of the variable.  Example 1.2: public class variableExample {           public static void main(String[] args) {                    int var1,var2,var3; //Declaring three variables declared                    var1=100;                    var2=200;                    var3=var1+var2;                    System.out.println("Adding"+var1+"and "+var2+"will give"+var3);           } } Analysis: ·         Memory is allocated for variables var1, var2 and var3. Whenever we declare a variable (premative type variable), then memory is allocated for storing the value in the variable. In our example, 4 bytes are allocated for each of the variable. ·         Value 100 is stored in variable var1 and value 200 is stored in variable var2. ·         Value of var1 and var2 is added and stored in var3. ·         Finally, the value of var1, var2 and var3 is printed to screen using System.out.println(); built-in method.

Data Types The various types of variable are defined as Data Types. There are two varieties of data types available in JAVA: 1.    Primitive/ Basic Data Types 2.    Reference Data Types

Primitive Data Type Explanation Boolean The value represents one bit of information. The allowed values are true or false. Int 32-bit signed two's complement integer value range from -2,147,483,648 to 2,147,483,647 Char Single 16-bit Unicode character. The value ranges from '\u0000' (or 0) to '\uffff' (or 65,535) Byte 8-bit signed two's complement integer value range from -128 to 127 Short 16-bit signed two’s complement integer. The value ranges from   -32,768 to 32,767 Long 64-bit signed two's complement integer. The value ranges from -2^63 to (2^63 -1) Float 32-bit single-precision floating-point number. Double 64-bit double-precision floating-point number. Primitive/ Basic Data Types Primitive data types are the basic data types, which are defined in the JAVA language. There are 8 different primitive data types - byte, short, int, long, float, double, boolean, char. All primitive values can be stored in a fixed amount of memory (between one and eight bytes) Assignment of primitive data type is done by value. The content in primitive data type is copied to another variable. Comparison (==) in primitive variables in done by comparing the value stored in the variable. Parameter passing of primitive variable is done by copying the content of the variable. (i.e. pass-by-value). Since value is copied changes done on the parameter value inside a method is not reflected to the caller of the function. Returning primitive variable from a method return value stored in variable. Local variable are destroyed when return from a method. Example 1.3: Program demonstrating range of Primitive data type public class MinMaxValueTest {           public static void main(String args[]) {                    byte maxByte = Byte.MAX_VALUE;                    byte minByte = Byte.MIN_VALUE;                    short maxShort = Short.MAX_VALUE;                    short minShort = Short.MIN_VALUE;                    int maxInteger = Integer.MAX_VALUE;                    int minInteger = Integer.MIN_VALUE;                    long maxLong = Long.MAX_VALUE;                    long minLong = Long.MIN_VALUE;                    float maxFloat = Float.MAX_VALUE;                    float minFloat = Float.MIN_VALUE;                    double maxDouble = Double.MAX_VALUE;                    double minDouble = Double.MIN_VALUE;                    System.out.println("Range of byte :: " + minByte + " to " + maxByte);                    System.out.println("Range of short :: " + minShort + " to " + maxShort);

System.out.println("Range of integer :: "+minInteger + " to "+maxIntege);                   System.out.println("Range of long :: " + minLong + " to " + maxLong);                   System.out.println("Range of float :: "      + minFloat + " to " + maxFloat);                   System.out.println("Range of double :: " + minDouble + " to " + maxDouble);          } } Output: Range of byte :: -128 to 127 Range of short :: -32768 to 32767 Range of integer :: -2147483648 to 2147483647 Range of long :: -9223372036854775808 to 9223372036854775807 Range of float :: 1.4E-45 to 3.4028235E38 Range of double :: 4.9E-324 to 1.7976931348623157E308

Parameter passing, Call by value  Arguments can be passed from one method to other using parameters. All the basic data types when passed as parameters are by the passed-by-value. That means a separate copy is created inside the called method and the variable in the calling method remains unchanged. Example 1.4: public static void increment(int var) {           var++; } public static void main(String[] args) {           int i = 10;           System.out.println("Value of i before increment is :  " +  i);           increment(i);           System.out.println("Value of i before increment is :  " +  i); } Output: Value of i before increment is :  10 Value of i after increment is :  10 Analysis: ·         Variable ”i” is declared and the value 10 is initialized to it. ·         Value of ”i” is printed. ·         Increment method is called. When a method is called the value of the parameter is copied into another variable of the called method. Flow of control goes to increase() function. ·         Value of var is incremented by 1. But remember, it is just a copy inside the increment method. ·         When the method exits, the value of ”i” is still 10. Points to remember: 1.    Pass by value just creates a copy of variable. 2.    Pass by value, value before and after the method call remain same.

Reference Data Types Reference variables are used to store memory address of classes as well as arrays. Any variable other them 8 primitive data types are reference data type. Parameter passing of a reference variable is done by copying the address of the variable. This method is called pass-by-reference. Since the object is not copied, it is shared, the changes done in a called function are also reflected to the caller of the function. Comparison (==) in reference variable is done by comparing the address of the variable. The default value of any reference variable is null. Returning reference variable from a method return address of the variable. If the returned address is stored in some other variable the locally created object is not destroyed.

Parameter passing, Call by Reference If you need to change the value of the parameter inside the method, then you should use call by reference. JAVA language by default passes by value. Therefore, to make it happen, you need to pass the address of a variable and changing the value of the variable using this address inside the called method. Example 1.5: private static class MyInt {                    int value; }; public static void increment(MyInt value) {           (value.value)++; } public static void main(String[] args) {           MyInt x = new MyInt();           x.value = 10;           System.out.println("Value of i before increment is: "+ x.value);           increment(x);           System.out.println("Value of i after increment is: "+ x.value); } Output: Value of i before increment is :  10 Value of i after increment is :  11 Analysis: Object of class MyInt is passed to the method. Since the objects are passed by reference. Value change in increment() function is reflected to the original object of the caller function. Points to remember: 1.    Call by reference is implemented indirectly by passing the address of an instance of class or array to the function.

Kinds of Variables The JAVA programming language defines three kinds of variables: 1.    Instance Variables (Non-Static): They are instance variables so they are unique to each instance/object of a class.   2.    Class Variables (Static): A class variable is any field with the static modifier. These variables are linked with the class not with the objects of the class. There is exactly one copy of these variables regardless of how many instances of the class are created. 3.    Local Variables: the temporary variables in a method are called local variables. The local variables are only visible to the method in which they are declared. The parameters that are passed to the methods are also local variables of the called method.  Example 1.6: class Bulb {           //Class Variables           private static int TotalBulbCount = 0;           //Instance Variables           private boolean isOn=false;           //Constructor           public Bulb(){                    TotalBulbCount++;           }           //Class Method           public static int getBulbCount(){                    return TotalBulbCount;           }           //Instance Method           public void turnOn() {                    isOn = true;           }           //Instance Method           public void turnOff() {                    isOn = false;           }           //Instance Method           public boolean isOnFun(){                    return isOn;           } }

Methods There are three types of methods. Class Methods, Instance Methods and Constructors. By default, all the methods are instance methods. Class Methods (Static): The static modifier is used to create class methods. Class methods with static modifier with them should be invoked with the class name without the need of creating even a single instance of the class Instance Methods (Non-Static): These methods can only be invoked over an instance of the class. Some points regarding Instance methods and Class methods: 1.    Instance methods can access other instance methods and instance variables directly. 2.    Instance methods can access class methods and variables directly. 3.    Class methods can access other class methods and class variables directly. 4.    Class methods cannot access instance methods and instance variables directly. To access instance variable and methods they need to create and instance (object) of class. 5.    The special keyword “this” is valid only inside instance methods (and invalid inside class methods ) as “this“ refers to the current instance.    Constructor: It is a special kind of method, which is invoked over objects when they are created. Constructor methods have the same name as the class. Constructor method is used to initialize the various fields of the object. For the class that does not have constructors, JAVA language provides default constructors for them.

Access Modifiers Access modifiers are used to set the visibility level to the class, variables and methods. JAVA provide four types access modifiers: default, public, protected, private. 1.    Default modifier (No modifier needed.) has visibility to the package. 2.    Private modifier has visibility only within its own class. 3.    Public modifier has visibility to all the classes in the package. 4.    Protected modifier has visibility within its own class and the subclasses its own class.

Interface Objects define their interface as the interaction with the outside world. For example, in the bulb case switch is the interface between you and the bulb. You press the button turn on and the bulb start glowing.  Example 1.7: public interface  BulbInterface {           public void turnOn();           public void turnOff();             public boolean isOnFun(); } class Bulb implements BulbInterface {           private boolean isOn=false;           @Override           public void turnOn() {                    isOn = true;           }           @Override           public void turnOff() {                    isOn = false;           }           @Override           public boolean isOnFun() {                    return isOn;           } } public class BulbDemo {           public static void main(String[] args) {                    Bulb b = new Bulb();                    System.out.println("bulb is on return : " + b.isOnFun());                    b.turnOn();                    System.out.println("bulb is on return : " + b.isOnFun());           } } Analysis: In this example, BulbInterface is the interface of Bulb class. Bulb class needs to implement all the methods of BulbInterface to implement it.

Relationship These are the various relationships that exist between two classes: Dependency: Objects of one class use objects of another class temporarily. When a class creates an instance of another class inside its member method and use it and when the method exits, then the instance of the other class is deleted. Association: Objects of one class work with objects of another class for some amount of time. The association is of two kinds - Aggregation and Composition. Aggregation: One class object share a reference to objects of another class. When a class stores the reference of another class inside it. Just a reference is kept inside the class. Composition: One class contains objects of another class. In Composition, the containing object is responsible for the creation and deletion of the contained object. Inheritance: One class is a sub-type of another class. Inheritance is a straightforward relationship to explain the parent-child relationship. Example 1.8: class AdvanceBulb extends Bulb {           int intensity;           public void setIntersity(int i) {                    intensity = i;           } } In our example, AdvanceBulb is a sub-class of Bulb. When an object of AdvanceBulb is created, all public and protected methods of Bulb class are also accessible.

General Prototype of a Class Example 1.9: class ChildClass extends ParentClass implements SomeInterface {     // fields     // methods } A ChildClass inherited from ParentClass and implements SomeInterface.

Abstract Class & Methods An abstract method is a method which does not have a definition. Such methods are declared with abstract keyword. A class which has at least one abstract method need to be declared as abstract. We cannot create objects of an abstract class. (A class which does not have any abstract method can also be declared as abstract to prevent its object creation.). Example 1.10: //Abstract Class public abstract class Shape {           //Abstract Method           public abstract double area();               //Abstract Method           public abstract double perimeter(); } Shape is an abstract class. And its instance cannot be created. Those classes, which extend it, need to implement these two functions to become concrete class whose instances can be created. public class Circle extends Shape {             private double radius;             public Circle() {                    this(1);           }             public Circle(double r) {                    radius = r;           }             public void setRadius(double r) {                    radius = r;           }           @Override             public double area() {                    // Area = πr^2                         return Math.PI * Math.pow(radius, 2);           }           @Override

public double perimeter() {                   // Perimeter = 2πr                        return 2 * Math.PI * radius;          } } Circle is a class which extends shape class and implement area() and parimeter() methods. public class Rectangle extends Shape {          private double width, length;          public Rectangle() {                   this(1,1);          }          public Rectangle(double w, double l) {                   width = w;                   length = l;          }          public void setWidth(double w) {                   width = w;          }          public void setLength(double l) {                   length = l;          }          @Override          public double area() {                   // Area = width * length                   return width * length;          }          @Override          public double perimeter() {                   // Perimeter = 2(width + length)                   return 2*(width + length);          } } Same as Circle class, Rectangle class also extends Shape class and implements its abstract functions. public class ShapeDemo {            public static void main(String[] args) {                    double width = 2, length = 3;                    Shape rectangle = new Rectangle(width, length);                    System.out.println("Rectangle width: " + width + " and length: " + length                                      + " Area: " + rectangle.area()                                       + " Perimeter: " + rectangle.perimeter());                        double radius = 10;                    Shape circle = new Circle(radius);                    System.out.println("Circle radius: " + radius                                       + " Area: " + circle.area()

+ " Perimeter: " + circle.perimeter());          } } Shape demo creates an instance of the Rectangle and the Circle class and assign it to a reference of type Shape. Finally area() and perimeter() functions are called over instance of Rectangle and Circle class.

Nested Class A class within another class is called a nested class. Compelling reasons for using nested classes include the following: 1.    It is a way of logically grouping classes that are only used in one place: If a class is useful to only one other class, then it is logical to embed it in that class and keep the two together. Nesting such "helper classes" makes their package more streamlined. 2.    It increases encapsulation: Consider two top-level classes, A and B, where B needs access to members of A that would otherwise be declared private. By hiding class B within class A, A's members can be declared private and B can access them. In addition, B itself can be hidden from the outside world. 3.    It can lead to more readable and maintainable code: Nesting small classes within top-level classes places the code closer to where it is used. A nested class has an independent set of modifiers from the outer class. Visibility modifiers (public, private and protected) effect whether the nested class definition is accessible beyond the outer class definition. For example, a private nested class can be used by the outer class, but by no other classes. Example 1.11: Demonstrating Nested Class public class OuterClass {           class NestedClass {                    // NestedClass fields and methods.           }           // OuterClass fields and methods. } Let us take example of LinkedList and Tree class. Both the linked list and tree have nodes to store new element. Both the linked list and tree have their nodes different, so it is best to declare their corresponding nodes class inside their own class to prevent name conflict and increase encapsulation. Example 1.12: public class LinkedList {           private static class Node {                     private int value;                    private Node next;                    // Nested Class Node other fields and methods.           }           private Node head;           // Outer Class LinkedList other fields and methods. } public class Tree {           private static class Node {

private int value;                   private Node lChild;                   private Node rChild;                   // Nested Class Node other fields and methods.             }          private Node root;          // Outer Class Tree other fields and methods. }

Enums Enums restrict a variable to have one of the few predefined values. Example 1.13: class Bulb {           //Enums             enum  BulbSize{ SMALL, MEDIUM, LARGE }           BulbSize  size;           //Other bulb class fields and methods. } public class BulbDemo {           public static void main(String[] args) {                    Bulb b = new Bulb();                    b.size = Bulb.BulbSize.MEDIUM ;                      System.out.println("Bulb Size :" + b.size);           } } In the above code, we made some change to Bulb class. It has one more field size and size is of type enum BulbSize. And the allowed values of the size are SMALL, MEDIUM and LARGE.

Constants Constants are defined by using static modifier in combination with the final modifier. The final modifier indicates that the value of this field cannot be changed. The static modifier indicates that there will be only one instance of the variable and is a class variable. For example, the following variable declaration defines a constant named PI, whose value approximates pi. static final double  PI = 3.141592653589793; Another example, in this we had created a constant of string type. static final String text = "Hello, World!";

Conditions and Loops IF Condition If condition consists of a Boolean condition followed by one or more statements. It allows you to take different paths of logic, depending on a given Boolean condition. if (boolean_expression) {           // statements } If statement can be followed by else statements and an optional else statement which is executed when the Boolean condition is false. if(boolean_expression) {           // if condition statements boolean condition true } else {

// else condition statements, boolean condition false } While Loop A while-loop is used to repeatedly execute some block of code as long as a given condition is true. Example 1.14: public static void main(String[] args) {          int[] numbers = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10};          int sum = 0;          int i = 0;          while(i < numbers.length)          {                   sum += numbers[i];                   i++;          }          System.out.println("Sum is :: " + sum); } Analysis: All the variables stored in array are added to sum variabale one by on in a while loop. Do..While Loop A do..while-loop is similar to while-loop, but the only difference is that the conditional code is executed before the test condition. do..while-loop is used where you want to execute some conditional code at least once.

For Loop For loop is just another loop in which initialization, condition check and increment are bundled together. for ( initialization; condition; increment ) {          statements } Example 1.15: public static void main(String[] args) {          int[] numbers = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10};          int sum = 0;          for(int i = 0; i < numbers.length; i++)                   sum += numbers[i];          System.out.println("Sum is :: " + sum); }

ForEach Loop For loop works well with basic types, but it does not handle collections objects well. For this, an alternate syntax foreach loop is provided. JAVA does not have keyword foreach it uses for keyword. for ( declaration : collection / array ) {          statements } Example 1.16: public static void main(String[] args) {          int[] numbers = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10};          int sum = 0;          for(int n : numbers)                    sum += n;          System.out.println("Sum is :: " + sum); }

Array Arrays are the most basic data structures used to store information. An array is a data structure used to store multiple data elements of the same data type. All the data is stored sequentially. The value stored at any index can be accessed in constant time. Example 1.17: public class Introduction {           public static void main(String[] args) {                    arrayExample();           }           public static void arrayExample() {                    int[] arr = new int[10];                    for (int i = 0; i < 10; i++)                    {                              arr[i] = i;                    }                    printArray(arr,10);           } } Analysis: Defines an array of integer arr. The array is of size 10 - which means that it can store 10 integers inside it. Array elements are accessed using the subscript operator []. Lowest subscript is 0 and highest subscript is (size of array – 1). Value 0 to 9 is stored in the array at index 0 to 9. Array and its size are passed to printArray() method. Example 1.18: public static void printArray(int arr[], int count) {           System.out.println("Values stored in array are : ");           for (int i = 0; i < count; i++)           {                    System.out.println(" " + arr[i]);           } } Analysis: ·         Array variable arr and its variable count are passed as arguments to printArray() method. ·         Finally array values are printed to screen using the System.out.println() method in a for loop. Point to Remember: 1.    Array index always starts from 0 index and highest index is size -1. 2.    The subscript operator has highest precedence if you write arr[2]++. Then the value of arr[2] will be incremented.

Two Dimensional Array We can define two dimensional or multidimensional array. It is an array of array. Example 1.19: public class Introduction {           public static void main(String[] args) {                    twoDArrayExample();           }           public static void twoDArrayExample()           {                    int[][] arr = new int[4][2];                    int count = 0;                    for (int i = 0; i < 4; i++)                              for (int j = 0; j < 2; j++)                                       arr[i][j] = count++;                    print2DArray(arr, 4, 2);           }           public static void print2DArray(int[][] arr, int row, int col) {                    for (int i = 0; i < row; i++)                              for (int j = 0; j < col; j++)                                       System.out.println(" " + arr[i][j]);           } } Analysis: ·         An array is created with dimension 4 x 2. The array will have 4 rows and 2 columns. ·         Value is assigned to the array ·         Finally, the value stored in an array is printed to screen by using print2DArray() method.

Array Interview Questions The following section will discuss the various algorithms that are applicable to arrays and will follow by list of practice problems with similar approaches. Sum Array Write a method that will return the sum of all the elements of the integer array given array and its size as an argument. Example 1.20: public static int SumArray(int arr[]) {           int size = arr.length;           int total=0;           int index=0;           for(index=0;index<size;index++)                    total = total + arr[index];           return total; } public static void main(String[] args) {           int[] arr = {1,2,3,4,5,6,7,8,9};           System.out.println("sum of all the values in array:" + SumArray(arr)); } Sequential Search Write a method, which will search an array for some given value. Example 1.21: public static int SequentialSearch(int arr[], int size, int value) {           int i = 0;           for(i = 0; i < size; i++) {                    if(value == arr[i] )                              return i;            }           return -1; } Analysis: ·         Since we have no idea about the data stored in the array, or if the data is not sorted then we have to search the array in sequential manner one by one. ·         If we find the value, we are looking for we return that index. ·         Else, we return -1 index, as we did not found the value we are looking for. In the above example, the data are not sorted. If the data is sorted, a binary search may be done. We examine the middle position at each step. Depending upon the data that we are searching is

greater or smaller than the middle value. We will search either the left or the right portion of the array. At each step, we are eliminating half of the search space there by making this algorithm very efficient the linear search. Binary Search Example 1.22: Binary search in a sorted array. // Binary Search Algorithm – Iterative Way  public static int BinarySearch (int arr[], int size, int value) {          int mid;          int low = 0;          int high = size-1;          while (low <= high)          {                   mid = low + (high-low)/2;   // To avoid the overflow                   if (arr[mid] == value)                             return mid;                   else if (arr[mid] < value)                             low = mid + 1;                   else                             high = mid - 1;          }          return -1; } Analysis: ·         Since we have data sorted in increasing / decreasing order, we can apply more efficient binary search. At each step, we reduce our search space by half. ·         At each step, we compare the middle value with the value we are searching for. If mid value is equal to the value we are searching for then we return the middle index. ·         If the value is smaller than the middle value, we search the left half of the array. ·         If the value is grater then the middle value then we search the right half of the array. ·         If we find the value we are looking for then its index is returned or -1 is returned otherwise. Rotating an Array by K positions. For example, an array [10,20,30,40,50,60] rotate by 2 positions to [30,40,50,60,10,20] Example 1.23: public static void rotateArray(int[] a,int n,int k) {          reverseArray(a,0,k-1);          reverseArray(a,k,n-1);          reverseArray(a,0,n-1); } public static void reverseArray(int[] a,int start, int end) {

for(int i=start,j=end;i<j;i++,j--)          {                   int temp = a[i];                   a[i]=a[j];                   a[j]=temp;          } } 1,2,3,4,5,6,7,8,9,10                 =>                 5,6,7,8,9,10,1,2,3,4 1,2,3,4,5,6,7,8,9,10      =>      4,3,2,1,10,9,8,7,6,5         =>  5,6,7,8,9,10,1,2,3,4 Analysis: ·         Rotating array is done in two parts trick. In the first part, we first reverse elements of array first half and then second half. ·         Then we reverse the whole array there by completing the whole rotation. Find the largest sum contiguous subarray. Given an array of positive and negative integer s, find a contiguous subarray whose sum (sum of elements) is maximized.  Example 1.24: public class Introduction {          public static void main(String[] args) {                   int[] arr = {1,-2,3,4,-4,6,-14,8,2};                   System.out.println("Max sub array sum :" + maxSubArraySum(arr, 9));          }          public static int maxSubArraySum(int[] a, int size) {                   int maxSoFar = 0, maxEndingHere = 0;                   for (int i = 0; i < size; i++) {                             maxEndingHere = maxEndingHere + a[i];                             if (maxEndingHere < 0)                                      maxEndingHere = 0;                             if (maxSoFar < maxEndingHere)                                      maxSoFar = maxEndingHere;                   }                   return maxSoFar;          } } Analysis: ·         Maximum subarray in an array is found in a single scan. We keep track of global maximum sum so far and the maximum sum, which include the current element. ·         When we find global maximum value so far is less than the maximum value containing current value we update the global maximum value. ·         Finally return the global maximum value.

Concept of Stack A stack is a memory in which values are stored and retrieved in “last in first out” manner. Data is added to stack using push operation and data is taken out of stack using pop operation. 1.    Initially the stack was empty. Then we have added value 1 to stack using push(1) operator. 2.    Similarly, push(2) and push(3) 3.    Pop operation take the top of the stack. In Stack data is added and deleted in “last in, first out” manner. 4.    First pop() operation will take 3 out of the stack. 5.    Similarly, other pop operation will take 2 then 1 out of the stack 6.    In the end, the stack is empty when all the elements are taken out of the stack.

System stack and Method Calls When the method is called, the current execution is stopped and the control goes to the called method. After the called method exits / returns, the execution resumes from the point at which the execution was stopped. To get the exact point at which execution should be resumed, the address of the next instruction is stored in the stack. When the method call completes, the address at the top of the stack is taken out. Example 1.25: public static void function2() {           System.out.println("fun2 line 1"); } public static void function1() {           System.out.println("fun1 line 1");           function2();           System.out.println("fun1 line 2"); } public static void main(String[] args) {           System.out.println("main line 1");           function1();           System.out.println("main line 2"); } Output: main line 1 fun1 line 1 fun2 line 1 fun1 line 2 main line 2 Analysis: ·         Every program starts with main() method. ·         The first statement of main() will be executed. And we will print “main line 1” as output. ·         function1() is called. Before control goes to function1() then next instruction that is address of next line is stored in the system stack. ·         Control goes to function1() method. ·         The first statement inside function1() is executed, this will print “fun1 line 1” to output. ·         function2() is called from function1(). Before control goes to function2() address of the next instruction that is address of next line is added to the system stack.

·         Control goes to function2() method. ·         “fun2 line 1” is printed to screen. ·         When function2() exits, control come back to function1(). And the program reads the next instruction from the stack, and the next line is executed. And print “fun1 line 2” to screen. ·         When fun1 exits, control comes back to the main method. And program reads the next instruction from the stack and executed it and finally “main line 2” is printed to screen. Points to remember: 1.    Methods are implemented using a stack. 2.    When a method is called the address of the next instruction is pushed into the stack. 3.    When a method is finished the address of the execution is taken out of the stack.

Recursive Function A recursive function is a function that calls itself, directly or indirectly. A recursive method consists of two parts: Termination Condition and Body (which include recursive expansion). 1.    Termination Condition:  A recursive method always contains one or more terminating condition. A condition in which recursive method is processing a simple case and will not call itself. 2.    Body (including recursive expansion): The main logic of the recursive method contained in the body of the method. It also contains the recursion expansion statement that in turn calls the method itself. Three important properties of recursive algorithm are: 1)    A recursive algorithm must have a termination condition. 2)    A recursive algorithm must change its state, and move towards the termination condition. 3)    A recursive algorithm must call itself. Note: The speed of a recursive program is slower because of stack overheads. If the same task can be done using an iterative solution (loops), then we should prefer an iterative solution (loops) in place of recursion to avoid stack overhead. Note: Without termination condition, the recursive method may run forever and will finally consume all the stack memory. Factorial Example 1.26: Factorial Calculation. N! = N* (N-1)…. 2*1. public static int factorial(int i) {           /* Termination Condition */           if(i <= 1)                     return 1;           /* Body, Recursive Expansion */           return i * factorial(i - 1); } Analysis: Each time method fn is calling fn-1. Time Complexity is O(N) Print Base 10 Integer s Example 1.27: public  static void printInt1(int number) {           char digit = (char) (number % 10 + '0');

number = number / 10;          if (number != 0)                   printInt1(number/10);          System.out.print(" " + digit); } Analysis: ·         Each time remainder is calculated and stored its char equivalent in digit. ·         If the number is greater than 10 then the number divided by 10 is passed to printInt() method. ·         Number will be printed with higher order first than the lower order digits.   Time Complexity is O(N) Print Base 16 Integer s Example 1.28: Generic print to some specific base method. public static void printInt2(int number, final int base) {          String conversion = "0123456789ABCDEF";          char digit = (char) (number % base) ;          number = number / base;          if (number != 0)                   printInt2(number,base);          System.out.print(" " + conversion.charAt(digit)); } Analysis: ·         Base value is provided along with the number in the function parameter. ·         Remainder of the number is calculated and stored in digit. ·         If the number is greater than base then, number divided by base is passed as an argument to the printInt() method recursively. ·         Number will be printed with higher order first than the lower order digits.   Time Complexity is O(N) Tower of Hanoi The Tower of Hanoi (also called the Tower of Brahma) We are given three rods and N number of disks, initially all the disks are added to first rod (the leftmost one) in decreasing size order. The objective is to transfer the entire stack of disks from first tower to third tower (the rightmost one), moving only one disk at a time and never a larger one onto a smaller.

Example 1.29: public static void towerOfHanoi(int num, char src, char dst, char temp) {          if (num < 1)                   return;          towerOfHanoi(num - 1, src, temp, dst);          System.out.println("Move"+num+"disk  from peg"+src+" to peg " + dst);          towerOfHanoi(num - 1, temp, dst, src); } public static void main(String[] args) {          int num = 4;          System.out.println("The sequence of moves in the Tower of Hanoi are : ");          towerOfHanoi(num, 'A', 'C', 'B'); } Analysis: TowerOfHanoi problem if we want to move N disks from source to destination, then we first move N-1 disks from source to temp, then move the lowest Nth disk from source to destination. Then will move N-1 disks from temp to destination. Greatest common divisor (GCD) Example 1.30: public static int GCD(int m, int n) {          if(m<n)                   return (GCD(n, m));          if(m%n == 0)                   return (n);          return(GCD(n, m%n)); } Analysis: Euclid’s algorithm is used to find gcd. GCD(n,m) == GCD(m, n mod m)

Fibonacci number Example 1.31: public static int fibonacci(int n) {          if (n <= 1)                   return n;          return fibonacci(n - 1) + fibonacci(n - 2); } Analysis: Fibonacci number are calculated by adding sum of the previous two number. There is an inefficiency in the solution we will look better solution in coming chapters. All permutations of an integer array Example 1.32: public static void printArray(int[] arr, int count) {          System.out.print("\n Values stored in array are : ");          for (int i = 0; i < count; i++)          {                   System.out.print(" " + arr[i]);          } }        public static void swap(int[] arr, int x, int y){          int temp = arr[x];          arr[x] = arr[y];          arr[y] = temp;          return; } public static void permutation(int[] arr, int i, int length) {          if (length == i){                   printArray(arr, length);                   return;          }          int j = i;          for (j = i; j < length; j++) {                   swap(arr, i, j);                   permutation(arr, i + 1, length);                   swap(arr, i, j);          }          return; } public static void main(String[] args) {          int[] arr = new int[5];          for (int i = 0; i < 5; i++)          {                   arr[i] = i;          }          permutation(arr, 0, 5); }

Analysis: In permutation method at each recursive call number at index, “i” is swapped with all the numbers that are right of it. Since the number is swapped with all the numbers in its right one by one it will produce all the permutation possible. Binary search using recursion Example 1.33: // Binary Search Algorithm – Recursive Way public static int BinarySearchRecursive(int arr[], int low, int high, int value) {          int mid = low + (high-low)/2;        // To avoid the overflow          if (arr[mid] == value)                   return mid;          else if (arr[mid] < value)                   return BinarySearchRecursive (arr, mid + 1, high, value);          else                   return BinarySearchRecursive (arr, low, mid - 1 , value); } Analysis: Similar iterative solution we had already seen. Now lets look into the recursive solution of the same problem in this solution also we are diving the search space into half and doing the same what we had done in the iterative solution.

Introduction Computer programmer learn by experience. We learn by seeing solved problems and solving new problems by ourselves. Studying various problem-solving techniques and by understanding how different algorithms are designed helps us to solve the next problem that is given to us. By considering a number of different algorithms, we can begin to develop pattern so that the next time a similar problem arises, we are better able to solve it. When an interviewer asks to develop a program in an interviewer, what are the steps that an interviewee should follow? We will be taking a systematic approach to handle the problem and finally reaching to the solution. Algorithm An algorithm is a set of steps to accomplish a task. An algorithm in a computer program is a set of steps applied over a set of input to produce a set of output. Knowledge of algorithm helps us to get our desired result faster by applying the right algorithm. The most important properties of an algorithm are: 1.    Correctness: The algorithm should be correct. It should be able to process all the given inputs and provide correct output. 2.    Efficiency: The algorithm should be efficient in solving problems. Algorithmic complexity is defined as how fast a particular algorithm performs. Complexity is represented by function T (n) - time versus the input size n.

Asymptotic analysis Asymptotic analysis is used to compare the efficiency of algorithm independently of any particular data set or programming language. We are generally interested in the order of growth of some algorithm and not interested in the exact time required for running an algorithm. This time is also called Asymptotic-running time.

Big-O Notation Definition: “f(n) is big-O of g(n)” or f(n) = O(g(n)), if there are two +ve constants c and n0 such that f(n) ≤ c g(n) for all n ≥ n0, In other words, c g(n) is an upper bound for f(n) for all n ≥ n0 The function f(n) growth is slower than c g(n) Example:

Omega-Ω Notation Definition: “f(n) is omega of g(n).” or f(n) = Ω(g(n)) if there are two +ve constants c and n0 such that c g(n) ≤ f(n) for all n ≥ n0 In other words, c g(n) is lower bound for f(n) Function f(n) growth is faster than c g(n) Find relationship of   and

Theta-Θ Notation Definition: “f(n) is theta of g(n).” or f(n) = Θ(g(n)) if there are three +ve constants c1, c2 and n0 such that  c1 g(n) ≤ f(n) ≤ c2 g(n) for all n ≥ n0 g(n) is an asymptotically tight bound on f(n). Function f(n) grows at the same rate as g(n). Example:  Example:  Find relationship of   and  Note:- Asymptotic Analysis is not perfect, but that is the best way available for analyzing algorithms. For example, say there are two sorting algorithms first take   and   time. The asymptotic analysis says that the first algorithm is better (as it ignores constants) but actually for a small set of data when n is small then 10000, the second algorithm will perform better. To consider this drawback of asymptotic analysis case analysis of the algorithm is introduced.

Complexity analysis of algorithms 1) Worst Case complexity: It is the complexity of solving the problem for the worst input of size n. It provides the upper bound for the algorithm. This is the most common analysis done. 2) Average Case complexity: It is the complexity of solving the problem on an average. We calculate the time for all the possible inputs and then take an average of it. 3) Best Case complexity: It is the complexity of solving the problem for the best input of size n.

Time Complexity Order A list of commonly occurring algorithm Time Complexity in increasing order: Name Notation Constant Logarithmic Linear N-LogN Quadratic Polynomial         c is a constant & c>1 Exponential        c is a constant & c>1 Factorial or N-power-N  or  Constant Time: O(1) An algorithm is said to run in constant time regardless of the input size. Examples: 1.    Accessing   element of an array 2.    Push and pop of a stack. 3.    Enqueue and remove of a queue. 4.    Accessing an element of Hash-Table. 5.    Bucket sort Linear Time: O(n) An algorithm is said to run in linear time if the execution time of the algorithm is directly proportional to the input size. Examples: 1.    Array operations like search element, find min, find max etc. 2.    Linked list operations like traversal, find min, find max etc. Note: when we need to see/ traverse all the nodes of a data-structure for some task then complexity is no less than O(n) Logarithmic Time: O(logn) An algorithm is said to run in logarithmic time if the execution time of the algorithm is proportional to the logarithm of the input size. Each step of an algorithm, a significant portion of the input is pruned out without traversing it. Example: 1.    Binary search

Note: We will read about these algorithms in this book. N-LogN Time: O(nlog(n)) An algorithm is said to run in logarithmic time if the execution time of an algorithm is proportional to the product of input size and logarithm of the input size. Example: 1.    Merge-Sort 2.    Quick-Sort (Average case) 3.    Heap-Sort Note: Quicksort is a special kind of algorithm to sort a list of numbers. Its worst-case complexity is   and average case complexity is . Quadratic Time: O( ) An algorithm is said to run in logarithmic time if the execution time of an algorithm is proportional to the square of the input size. Examples: 1.    Bubble-Sort 2.    Selection-Sort 3.    Insertion-Sort

Deriving the Runtime Function of an Algorithm Constants Each statement takes a constant time to run. Time Complexity is  Loops The running time of a loop is a product of running time of the statement inside a loop and number of iterations in the loop. Time Complexity is  Nested Loop The running time of a nested loop is a product of running time of the statements inside loop multiplied by a product of the size of all the loops. Time Complexity is  Where c is a number of loops. For two loops, it will be  Consecutive Statements Just add the running times of all the consecutive statements If-Else Statement Consider the running time of the larger of if block or else block. And ignore the other one. Logarithmic statement If each iteration the input size is decreases by a constant factors.  .

Time Complexity Examples Example 1 int fun1(int n) {           int m = 0;           for (int i = 0; i<n; i++)                    m += 1;           return m; } Time Complexity: O(n) Example 2 int fun2(int n) {           int i=0, j=0, m = 0;           for (i = 0; i<n; i++)                    for (j = 0; j<n; j++)                              m += 1;           return m; } Time Complexity:  Example 3 int fun3(int n) {           int i=0, j=0, m = 0;           for (i = 0; i<n; i++)                    for (j = 0; j<i; j++)                              m += 1;           return m; } Time Complexity:  O(N+(N-1)+(N-2)+...) == O(N(N+1)/2)  == Example 4 int fun4(int n) {           int i = 0, m = 0;           i = 1;           while (i < n) {                    m += 1;                    i = i * 2;           }           return m; } Each time problem space is divided into half. Time Complexity: O(log(n)) Example 5 int fun5(int n) {           int i = 0, m = 0;           i = n;

while (i > 0) {                   m += 1;                   i = i / 2;          }          return m; } Same as above each time problem space is divided into half. Time Complexity: O(log(n)) Example 6 int fun6(int n) {          int i = 0, j = 0, k = 0, m = 0;          i = n;          for (i = 0; i<n; i++)                   for (j = 0; j<n; j++)                             for (k = 0; k<n; k++)                                      m += 1;          return m; } Outer loop will run for n number of iterations. In each iteration of the outer loop, inner loop will run for n iterations of their own. Final complexity will be n*n*n. Time Complexity:  Example 7 int fun7(int n) {          int i = 0, j = 0, k = 0, m = 0;          i = n;          for (i = 0; i<n; i++)                   for (j = 0; j<n; j++)                             m += 1;          for (i = 0; i<n; i++)                   for (k = 0; k<n; k++)                             m += 1;          return m; } These two groups of for loop are in consecutive so their complexity will add up to form the final complexity of the program. Time Complexity: T(n) =   =  Example 8 int fun8(int n) {          int i = 0, j = 0, m = 0;          for (i = 0; i<n; i++)                   for (j = 0; j<   Math.sqrt(n); j++)                             m += 1;          return m; } Time Complexity:

Example 9 int fun9(int n) {          int i = 0, j = 0, m = 0;          for ( i = n; i > 0; i /= 2)                   for ( j = 0; j < i; j++)                             m += 1;          return m; } Each time problem space is divided into half.   Time Complexity: O(log(n)) Example 10 int fun10(int n) {          int i = 0, j = 0, m = 0;          for ( i = 0; i < n; i++)                   for ( j = i; j > 0; j--)                             m += 1;          return m; } O(N+(N-1)+(N-2)+...) = O(N(N+1)/2)  // arithmetic progression. Time Complexity: Example 11 int fun11(int n) {          int i = 0, j = 0, k = 0, m = 0;          for (i = 0; i<n; i++)                   for (j = i; j<n; j++)                             for (k = j+1; k<n; k++)                                      m += 1;          return m; } Time Complexity:  Example 12 int fun12(int n) {          int i = 0, j = 0, m = 0;          for (i = 0; i<n; i++)                   for (; j<n; j++)                             m += 1;          return m; } Think carefully once again before finding a solution, j value is not reset at each iteration. Time Complexity: O(n) Example 13 int fun13(int n) {          int i = 1, j = 0, m = 0;          for (i = 1; i<=n; i *= 2)

for (j = 0; j<=i; j++)                             m += 1;          return m; } The inner loop will run for 1, 2, 4, 8,… n times in successive iteration of the outer loop. Time Complexity: T(n) = O(1+ 2+ 4+ ….+n/2+n) = O(n)

Master Theorem The master theorem solves recurrence relations of the form: Where a ≥ 1 and b > 1. "n" is the size of the problem. "a" is a number of sub problem in the recursion. “n/b” is the size of each sub-problem. "f(n)" is the cost of the division of the problem into sub problem or merge of results of sub problems to get the final result. It is possible to determine an asymptotic tight bound in these three cases: Case 1: when  ) and constant  , than the final Time Complexity will be: Case 2: when  ) and constant k ≥ 0, than the final Time Complexity will be: ) Case 3: when   and constant  , Then the final Time Complexity will be: Example 14: Take an example of Merge-Sort,  Sol:-     =  = 1 ) Case 2 applies and  ) Example 15: Binary Search

Sol:-    =  = 0 ) Case 2 applies and  ) Example 16: Binary tree traversal  Sol:-    =  = 1 ) Case 1 applies and  Example 17: Take an example  Sol:-    =  = 1 Case 3 applies and  ) Example 18: Take an example  Sol:-    =  = 2 ) Case 2 applies and  )

Modified Master theorem This is a shortcut to solving the same problem easily and fast. If the recurrence relation is in the form of T(n) Example 19:  Sol:-   s = 2 Case 3:    =  Example 20: T (n) = T (n/2) + 2n Sol:-  s = 1 Case 3 T(n)=  Example 21: T (n) = 16T (n/4) + n Sol:- r = 2 s = 1 Case 1 T(n)=  Example 22: T (n) = 2T (n/2) + n log n Sol:-  There is logn in f(n) so use master theorem shortcut will not word. ) ) = )) Example 23:

Sol:-    0.5 = s Case 2:   ) = ) Example 24:  Sol:-  Case 1:  =  Example 25: T (n) = 3T (n/3) + √ n Sol:-    s = ½ Case 1 Example 26:T (n) = 3T (n/4) + n log n Sol:-  There is logn in f(n) so see if master theorem. f(n) = n log n =  ) Case 3: Example 27: T (n) = 3T (n/3) + n/2 Sol:-   r=1=s Case 2:

Abstract data type (ADT) An abstract data type (ADT) is a logical description of how we view the data and the operations that are allowed on it. ADT is defined as a user point of view of a data type. ADT concerns about the possible values of the data and what are interface exposed by it. ADT does not concern about the actual implementation of the data structure. For example, a user wants to store some integers and find a mean of it. Does not talk about how exactly it will be implemented.

Data-Structure Data structures are concrete representations of data and are defined as a programmer point of view. Data-structure represents how data will be stored in memory. All data-structures have their own pros and cons. And depending upon the problem at hand, we pick a data-structure that is best suited for it. For example, we can store data in an array, a linked-list, stack, queue, tree, etc.

JAVA Collection Framework JAVA programming language provides a JAVA Collection Framework, which is a set of high quality, high performance & reusable data-structures and algorithms. The following advantages of using a JAVA collection framework: 1.    Programmers do not have to implement basic data structures and algorithms again and again. Thereby it prevents the reinvention the wheel. Thus the programmer can devote more effort in business logic. 2.    The JAVA Collection Framework code is well-tested, high quality, high performance code there by increasing the quality of the programs. 3.    Development cost is reduced as basic data structures and algorithms are implemented in Collections framework. 4.    Easy for the review and understanding others programs as other developers also use the Collection framework. In addition, collection framework is well documented.

Array Array represents a collection of multiple elements of the same datatype. Arrays are fixed size data structure, the size of this data structure is fixed at the time of its creation. Arrays are the most common data structure used to store data. As we cannot change the size of an array, we generally declare a large size array to handle any future data expansion. This ends up in creating a large size array, where most of the space is unused. Note: - Arrays can store a fixed number of elements, whereas a collection stores object dynamically so there is no size restrictions it grows and shrinks automatically. Array ADT Operations Below is the API of array: 1.    Adds an element at kth position Value can be stored in array at Kth position in O(1) constant time. We just need to store value at arr[k]. 2.    Reading the value stored at kth position. Accessing value stored a some location in array is also O(1) constant time. We just need to read value stored at arr[k]. Example 4.1 public class ArrayDemo {           public static void main(String[] args) {                    int[] arr = new int[10];                    for (int i = 0; i < 10; i++)                    {                              arr[i] = i;                    }           } } JAVA standard arrays are of fixed length. Sometime we do not know how much memory we need so we create a bigger size array. There by wasting space to avoid this situation JAVA Collection framework had implemented ArrayList to solve this problem. ArrayList implementation in JAVA Collections

ArrayList<E> in by JAVA Collections is a data structure which implements List<E> interface which means that it can have duplicate elements in it. ArrayList is an implementation as dynamic array that can grow or shrink as needed. (Internally array is used when it is full a bigger array is allocated and the old array values are copied to it.) Example 4.2 import java.util.ArrayList; public class ArrayListDemo {          public static void main(String[] args) {                   ArrayList<Integer> al = new ArrayList<Integer>();                   al.add(1); // add 1 to the end of the list                   al.add(2); // add 2 to the end of the list                   al.add(3); // add 3 to the end of the list                   al.add(4); // add 4 to the end of the list                   System.out.println("Contents of Array: " + al); // array is converted to                   // string and printed to screen                   al.add(2,9); // 9 is added to 2nd index.                   al.add(5,9); // 9 is added to 5th index.                   System.out.println("Contents of Array: " + al);                   System.out.println("Array Size: " + al.size()); // array size printed                   System.out.println("Array IsEmpty: " + al.isEmpty());                   al.remove(al.size() -1); // last element of array is removed.                   System.out.println("Array Size: " + al.size());                   al.removeAll(al); // all the elements of array are removed.                   System.out.println("Array IsEmpty: " + al.isEmpty());          } } Output Contents of Array: [1, 2, 3, 4] Contents of Array: [1, 2, 9, 3, 4, 9] Array Size: 6 Array IsEmpty: false Array Size: 5 Array IsEmpty: true

Linked List Linked lists are dynamic data structure and memory is allocated at run time. The concept of linked list is not to store data contiguously. Use links that point to the next elements. Performance wise linked lists are slower than arrays because there is no direct access to linked list elements. The linked list is a useful data structure when we do not know the number of elements to be stored ahead of time. There are many flavors of linked list that you will see: linear, circular, doubly, and doubly circular. Linked List ADT Operations Below is the API of Linked list. Insert(k): adds k to the start of the list Insert an element at the start of the list. Just create a new element and move references. So that this new element becomes the new element of the list. This operation will take O(1) constant time. Delete(): delete element at the start of the list Delete an element at the start of the list. We just need to move one reference. This operation will also take O(1) constant time. PrintList(): display all the elements of the list. Start with the first element and then follow the references. This operation will take O(N) time. Find(k): find the position of element with value k Start with the first element and follow the reference until we get the value we are looking for or reach the end of the list. This operation will take O(N) time. Note: binary search does not work on linked lists. FindKth(k): find element at position k Start from the first element and follow the links until you reach the kth element. This operation

will take O(N) time. IsEmpty(): check if the number of elements in the list are zero. Just check the head reference of the list it should be Null. Which means list is empty. This operation will take O(1) time. LinkedList implementation in JAVA Collections LinkedList<E> in by JAVA Collections is a data structure which also implements List<E> interface. Example 4.3 import java.util.LinkedList; public class LinkedListDemo {          public LinkedListDemo() {          }          public static void main(String[] args) {                   LinkedList<Integer> ll = new LinkedList<Integer>();                   ll.addFirst(8); // 8 is added to the list                   ll.addLast(9); // 9 is added to last of the list.                   ll.addFirst(7); // 7 is added to first of the list.                   ll.addLast(20); // 20 is added to last of the list                   ll.addFirst(2); // 2 is added to first of list.                   ll.addLast(22); // 22 is added to last of the list.                   System.out.println("Contents of Linked List: " + ll);                   ll.removeFirst();                   ll.removeLast();                   System.out.println("Contents of Linked List: " + ll);          } } Output: Contents of Linked List: [2, 7, 8, 9, 20, 22] Contents of Linked List: [7, 8, 9, 20]

Stack Stack is a special kind of data structure that follows Last-In-First-Out (LIFO) strategy. This means that the element that is added to stack last will be the first to be removed. The various applications of stack are: 1.    Recursion: recursive calls are preformed using system stack. 2.    Postfix evaluation of expression. 3.    Backtracking 4.    Depth-first search of trees and graphs. 5.    Converting a decimal number into a binary number etc. Stack ADT Operations Push(k): Adds a new item to the top of the stack Pop(): Remove an element from the top of the stack and return its value. Top(): Returns the value of the element at the top of the stack Size(): Returns the number of elements in the stack IsEmpty(): determines whether the stack is empty. It returns 1 if the stack is empty or return 0. Note: All the above Stack operations are implemented in O(1) Time Complexity. Stack implementation in JAVA Collection Stack is implemented by calling push and pop methods of ArrayDeque<T> class. JDK also provides Stack<T>, but we should not use this class and prefer Deque which implement collection interface. 1.    First reason is that Stack<T> does not drive from Collection interface. 2.    Second Stack<T> drives from Vector<T> so random access is possible so it brakes abstraction of a stack.

3.    Third ArrayDeque is more efficient as compared to Stack<T>. Example 4.4 import java.util.ArrayDeque; public class DequeStack<T> {          private ArrayDeque<T> deque = new ArrayDeque<T>();          public void push(T obj){                   deque.push(obj);          }          public T pop(){                   return deque.pop();          }          public T top(){                   return deque.peekLast();          }          public int size(){                   return deque.size();          }                 public boolean isEmpty(){                   return deque.isEmpty();          } } Note: - in the coming chapters we will directly use ArrayDeque object. We will not be making DequeStack wrapper class. When we use push() and pop() methods to insert and remove from a ArrayDeque we will be doing stack operations.

Queue A queue is a First-In-First-Out (FIFO) kind of data structure. The element that is added to the queue first will be the first to be removed from the queue and so on. Queue has the following application uses: 1.    Access to shared resources (e.g., printer) 2.    Multiprogramming 3.    Message queue Queue ADT Operations: Add(): Add a new element to the back of the queue. Remove(): remove an element from the front of the queue and return its value. Front(): return the value of the element at the front of the queue. Size(): returns the number of elements inside the queue. IsEmpty(): returns 1 if the queue is empty otherwise return 0 Note: All the above Queue operations are implemented in O(1) Time Complexity. Queue implementation in JAVA Collection ArrayDeque<T> is the class implementation of doubly ended queue. If we use add(), remove() and peekFirst() it will behave as a queue. ( And if we use push(), pop() and peekLast() it behave as a stack.) Example 4.5 import java.util.ArrayDeque; public class DequeQueue<T> {           private ArrayDeque<T> deque = new ArrayDeque<T>();           public void add(T obj){                    deque.add(obj);           }

public T remove(){                   return deque.remove();          }          public T peek(){                   return deque.peekFirst();          }          public int size(){                   return deque.size();          }          public boolean isEmpty(){                   return deque.isEmpty();          } } Note:-  In the coming chapter we will not be making wrapper class DequeQueue<E>. We will directly use ArrayDeque<E> and will call add() and remove() methods to use it as a queue.

Trees Tree is a hierarchical data structure. The top element of a tree is called the root of the tree. Except the root element, every element in a tree has a parent element, and zero or more child elements. The tree is the most useful data structure when you have hierarchical information to store. There are many types of trees, for example, binary-tree, Red-black tree, AVL tree, etc.

Binary Tree A binary tree is a type of tree in which each node has at most two children (0, 1 or 2) which are referred as left child and right child.

Binary Search Trees (BST) A binary search tree (BST) is a binary tree on which nodes are ordered in the following way: 1.    The key in the left subtree is less than the key in its parent node. 2.    The key in the right subtree is greater or equal the key in its parent node. Binary Search Tree ADT Operations Insert(k): Insert an element k into the tree. Delete(k): Delete an element k from the tree. Search(k): Search a particular value k into the tree if it is present or not. FindMax(): Find the maximum value stored in the tree. FindMin(): Find the minimum value stored in the tree. The average Time Complexity of all the above operations on a binary search tree is O(log n), the case when the tree is balanced. The worst-case Time Complexity will be O(n) when the tree is skewed. A binary tree is skewed when tree is not balanced. There are two types of skewed tree. 1.    Right Skewed binary tree: A binary tree in which each node is having either only a right child or no child at all. 2.    Left Skewed binary tree: A binary tree in which each node is having either only a left child or no child at all. Balanced Binary search tree There are few binary search tree, which always keeps themselves balanced. Most important among them are Red-Black Tree (RB-Tree) and AVL tree.

The standard template library (STL) is implemented using this Red-Black Tree (RB-Tree). TreeSet implementation in JAVA Collections TreeSet<> is a class which implements Set<> interface which means that it store only unique elements. TreeSet<> is implemented using a red-black balanced binary search tree in JAVA Collections. Since TreeSet<> is implemented using a binary search tree its elements are stored in sequential order. Example 4.6 import java.util.TreeSet; public class TreeSetDemo {          public static void main(String[] args) {                   // Create a tree set.                   TreeSet<String> ts = new TreeSet<String>();                   // Add elements to the tree set.                   ts.add("India");                   ts.add("USA");                   ts.add("Brazil");                   ts.add("Canada");                   ts.add("UK");                   ts.add("China");                   ts.add("France");                   ts.add("Spain");                   ts.add("Italy");                   System.out.println(ts);          } } Output [Brazil, Canada, China, France, India, Italy, Spain, UK, USA] Note:- TreeSet is implemented using a binary search tree so add, remove, and contains methods have logarithmic time complexity O(log (n)), where n is the number of elements in the set. TreeMap implementation in JAVA Collection A Map<> is an interface that maps keys to values. Also called a dictionary. A TreeMap<> is an implementation of Map<> and is implemented using red-black balanced binary tree so the key value pairs are stored in sorted order. Example 4.7 import java.util.TreeMap; public class TreeMapDemo { public static void main(String[] args) {                   // create a tree map.                   TreeMap<String, Integer> tm = new TreeMap<String, Integer>();                   // Put elements into the map

tm.put("Mason", new Integer(55));                   tm.put("Jacob", new Integer(77));                   tm.put("William", new Integer(99));                   tm.put("Alexander", new Integer(80));                   tm.put("Michael", new Integer(50));                   tm.put("Emma", new Integer(65));                   tm.put("Olivia", new Integer(77));                   tm.put("Sophia", new Integer(88));                   tm.put("Emily", new Integer(99));                   tm.put("Isabella", new Integer(100));                   System.out.println("Total number of students in class :: " + tm.size());                   for(String key : tm.keySet()){                             System.out.println(key + " score marks :" + tm.get(key));                   }                   System.out.println("Emma score present::" + tm.containsKey("Emma"));                   System.out.println("John score present:: " + tm.containsKey("John"));                   tm.remove("Emma");                   System.out.println("Emma score present::" + m.containsKey("Emma"));          } } Output Total number of students in class :: 10 Alexander score marks :80 Emily score marks :99 Emma score marks :65 Isabella score marks :100 Jacob score marks :77 Mason score marks :55 Michael score marks :50 Olivia score marks :77 Sophia score marks :88 William score marks :99 Emma present in class :: true John present in class :: false Emma present in class :: false

Priority Queue (Heap) Priority queue is implemented using a binary heap data structure. In a heap, the records are stored in an array so that each key is larger than its two children keys. Each node in the heap follows the same rule that the parent value is greater than its two children. There are two types of the heap data structure: 1.    Max heap: each node should be greater than or equal to each of its children. 2.    Min heap: each node should be smaller than or equal to each of its children. A heap is a useful data structure when you want to get max/min one by one from data. Heap-Sort uses max heap to sort data in increasing/decreasing order. Heap ADT Operations Insert() - Adding a new element to the heap. The Time Complexity of this operation is O(log(n)) remove() - Extracting max for max heap case (or min for min heap case). The Time Complexity of this operation is O(log(n)) Heapify() – To convert a list of numbers in an array into a heap. This operation has a Time Complexity O(n)

PriorityQueue implementation in JAVA Collection Min heap implementation of Priority Queue Example 4.8 import java.util.PriorityQueue; public class PriorityQueueDemo {          public static void main(String[] args) {                   PriorityQueue<Integer> pq = new PriorityQueue<Integer>();                   int[] arr = {1,2,10,8,7,3,4,6,5,9};                   for(int i: arr){                             pq.add(i);                   }                   System.out.println("Printing Priority Queue Heap : " + pq);                   System.out.print("remove elements of Priority Queue ::");                   while(pq.isEmpty() == false){                             System.out.print(" " + pq.remove());                   }          } } Output Printing Priority Queue Heap : [1, 2, 3, 5, 7, 10, 4, 8, 6, 9] Dequeue elements of Priority Queue :: 1 2 3 4 5 6 7 8 9 10 Max heap implementation of Priority Queue We just need to change collection order to make max heap from PriorityQueue<> collection. Example 4.9 PriorityQueue<Integer> pq = new PriorityQueue<Integer>(Collections.reverseOrder()); Output Printing Priority Queue Heap : [10, 9, 4, 6, 8, 2, 3, 1, 5, 7] Dequeue elements of Priority Queue :: 10 9 8 7 6 5 4 3 2 1

Hash-Table A Hash-Table is a data structure that maps keys to values. Each position of the Hash-Table is called a slot. The Hash-Table uses a hash function to calculate an index of an array of slots. We use the Hash-Table when the number of keys actually stored is small relatively to the number of possible keys. The process of storing objects using a hash function is as follows: 1. Create an array of size M to store objects, this array is called Hash-Table. 2. Find a hash code of an object by passing it through the hash function. 3. Take module of hash code by the size of Hash-Table to get the index of the table where objects will be stored. 4. Finally store these objects in the designated index. The process of searching objects in Hash-Table using a hash function is as follows: 1. Find a hash code of the object we are searching for by passing it through the hash function. 2. Take module of hash code by the size of Hash-Table to get the index of the table where objects are stored. 3. Finally, retrieve the object from the designated index. Hash-Table Abstract Data Type (ADT) ADT of Hash-Table contains the following functions: Insert(x): Add object x to the data set. Delete(x): Delete object x from the data set. Search(x): Search object x in data set.

The Hash-Table is a useful data structure for implementing dictionary. The average time to search for an element in a Hash-Table is O(1). A Hash Table generalizes the notion of an array. HashSet implementation of JAVA Collections HashSet <> is a class which implements Set<> interface which means that it store only unique elements. HashSet <> is implemented using a hash table. Since HashSet<> is implemented using a hash table its elements are not stored in sequential order. Example 4.10 import java.util.HashSet; public class HashSetDemo {          public static void main(String[] args) {                   // Create a hash set.                   HashSet<String> hs = new HashSet<String>();                   // Add elements to the hash set.                   hs.add("India");                   hs.add("USA");                   hs.add("Brazil");                   hs.add("Canada");                   hs.add("UK");                   hs.add("China");                   hs.add("France");                   hs.add("Spain");                   hs.add("Italy");                   System.out.println(hs);                                  System.out.println("Hash Table contains USA : " + hs.contains("USA"));                   System.out.println("Hash Table contains Russia : " +                   hs.contains("Russia"));                   hs.remove("USA");                   System.out.println(hs);                   System.out.println("Hash Table contains USA : " + hs.contains("USA"));          } } Output [Canada, USA, UK, China, Italy, Brazil, France, India, Spain] Hash Table contains USA : true Hash Table contains Russia : false [Canada, UK, China, Italy, Brazil, France, India, Spain] Hash Table contains USA : false LinkedHashSet implementation of JAVA Collections LinkedHashSet <> is a class which implements Set<> interface which means that it store only unique elements. LinkedHashSet <> is implemented using a hash table and a linked list.  Linked list is used to preserver order of elements based on insertion. Traversing the elements of the hash table is done in order of insertion.

Example 4.11 import java.util.HashSet; import java.util.LinkedHashSet; public class LinkedHashSetDemo {          public static void main(String[] args) {                   // Create a hash set.                   HashSet<String> hs = new HashSet<String>();                   // Add elements to the hash set.                   hs.add("India");                   hs.add("USA");                   hs.add("Brazil");                   hs.add("Canada");                   hs.add("UK");                   hs.add("China");                   hs.add("France");                   hs.add("Spain");                   hs.add("Italy");                   System.out.println("HashSet value:: " + hs);                   // Create a linked hash set.                   LinkedHashSet<String> lhs = new LinkedHashSet<String>();                   // Add elements to the linked hash set.                   lhs.add("India");                   lhs.add("USA");                   lhs.add("Brazil");                   lhs.add("Canada");                   lhs.add("UK");                   lhs.add("China");                   lhs.add("France");                   lhs.add("Spain");                   lhs.add("Italy");                   System.out.println("LinkedHashSet value:: " + lhs);                } } Output HashSet value:: [Canada, USA, UK, China, Italy, Brazil, France, India, Spain] LinkedHashSet value:: [India, USA, Brazil, Canada, UK, China, France, Spain, Italy] Comparison of various Set classes. TreeSet HashSet LinkedHashSet Storage Red-Black Tree Hash Table Hash Table with Linked List. Performance Slower than HashSet, O(log(N)) Fastest, constant time More expensive then HashSet, have to maintain linked list. Order of Iteration Increasing Order No order guarantee Order of insertion HashMap implementation  in JAVA Collection A Map<> is a data structure that maps keys to values. Also called a dictionary. A HashMap<> is an implementation of Map<> and is implemented using a hash table so the key value pairs are not stored in sorted order. Map<> does not allow duplicate keys buts values can be duplicate.

Example 4.12 import java.util.HashMap; public class HashMapDemo {          public static void main(String[] args) {                   // Create a hash map.                   HashMap<String, Integer> hm = new HashMap<String, Integer>();                   // Put elements into the map                   hm.put("Mason", new Integer(55));                   hm.put("Jacob", new Integer(77));                   hm.put("William", new Integer(99));                   hm.put("Alexander", new Integer(80));                   hm.put("Michael", new Integer(50));                   hm.put("Emma", new Integer(65));                   hm.put("Olivia", new Integer(77));                   hm.put("Sophia", new Integer(88));                   hm.put("Emily", new Integer(99));                   hm.put("Isabella", new Integer(100));                   System.out.println("Total number of students in class :: " + hm.size());                   for(String key : hm.keySet()){                             System.out.println(key + " score marks :" + hm.get(key));                   }                   System.out.println("Emma score present::" + hm.containsKey("Emma"));                   System.out.println("John score  present:: " + hm.containsKey("John"));          } } Output Total number of students in class :: 10 Olivia score marks :77 Emily score marks :99 Alexander score marks :80 Mason score marks :55 Michael score marks :50 Isabella score marks :100 William score marks :99 Emma score marks :65 Sophia score marks :88 Jacob score marks :77 Emma present in class :: true John present in class :: false

Dictionary / Symbol Table A symbol table is a mapping between a string(key) and a value, which can be of any data type. A value can be an integer such as occurrence count, dictionary meaning of a word and so on. Binary Search Tree (BST) for Strings Binary Search Tree (BST) is the simplest way to implement symbol table. Simple string compare function can be used to compare two strings. If all the keys are random, and the tree is balanced. Then on an average key lookup can be done in logarithmic time. Hash-Table The Hash-Table is another data structure, which can be used for symbol table implementation. Below Hash-Table diagram, we can see the name of that person is taken as the key, and their meaning is the value of the search. The first key is converted into a hash code by passing it to appropriate hash function. Inside hash function the size of Hash-Table is also passed, which is used to find the actual index where values will be stored. Finally, the value that is meaning of name is stored in the Hash-Table, or you can store a reference to the string which store meaning can be stored into the Hash-Table.

Hash-Table has an excellent lookup of constant time. Let us suppose we want to implement autocomplete the box feature of Google search. When you type some string to search in google search, it proposes some complete string even before you have done typing. BST cannot solve this problem as related strings can be in both right and left subtree.  The Hash-Table is also not suited for this job. One cannot perform a partial match or range query on a Hash-Table. Hash function transforms string to a number. Moreover, a good hash function will give a distributed hash bode even for partial string and there is no way to relate two strings in a Hash-Table.  Trie and Ternary Search tree are a special kind of tree, which solves partial match, and range query problem well. Trie Trie is a tree, in which we store only one character at each node. This final key value pair is stored in the leaves. Each node has K children, one for each possible character. For simplicity purpose, let us consider that the character set is 26, corresponds to different characters of English alphabets. Trie is an efficient data structure. Using Trie, we can search the key in O(M) time. Where M is the maximum string length. Trie is also suitable for solving partial match and range query problems.

Ternary Search Trie/ Ternary Search Tree Tries having a very good search performance of O(M) where M is the maximum size of the search string. However, tries having very high space requirement. Every node Trie contain references to multiple nodes, each reference corresponds to possible characters of the key.  To avoid this high space requirement Ternary Search Trie (TST) is used. A TST avoid the heavy space requirement of the traditional Trie while still keeping many of its advantages. In a TST, each node contains a character, an end of key indicator, and three references. The three references are corresponding to current char hold by the node(equal), characters less than and character greater than.  The Time Complexity of ternary search tree operation is proportional to the height of the ternary search tree. In the worst case, we need to traverse up to 3 times that many links. However, this case is rare. Therefore, TST is a very good solution for implementing Symbol Table, Partial match and range query.



Graphs A graph is a data structure which represents a network, that connects a collection of nodes called vertices, and their connections, called edges. An edge can be seen as a path between two nodes. These edges can be either directed or undirected. If a path is directed then you can move only in one direction, while in an undirected path you can move in both the directions.

Graph Algorithms Depth-First Search (DFS) The DFS algorithm we start from starting point and go into depth of graph until we reach a dead end and then move up to parent node (Backtrack).  In DFS, we use stack to get the next vertex to start a search. Alternatively, we can use recursion (system stack) to do the same. Breadth-First Search (BFS) In BFS algorithm, a graph is traversed in layer-by-layer fashion. The graph is traversed closer to the starting point. The queue is used to implement BFS.



Sorting Algorithms Sorting is the process of placing elements from a collection into ascending or descending order. Sorting arranges data elements in order so that searching become easier. There are good sorting functions available which does sorting in O(nlogn) time, so in this book when we need sorting we will use sort() function and will assume that the sorting is done in O(nlogn) time.

Counting Sort Counting sort is the simplest and most efficient type of sorting. Counting sort has a strict requirement of a predefined range of data. Like, sort how many people are in which age group. We know that the age of people can vary between 1 and 130. If we know the range of input, then sorting can be done using counting in O(n+k).

Different Searching Algorithms ·         Linear Search – Unsorted Input ·         Linear Search – Sorted Input ·         Binary Search (Sorted Input) ·         String Search: Tries, Suffix Trees, Ternary Search. ·         Hashing and Symbol Tables

Linear Search – Unsorted Input When elements of an array are not ordered or sorted and we want to search for a particular value, we need to scan the full array unless we find the desired value. This kind of algorithm known as unordered linear search. The major problem with this algorithm is less performance or high Time Complexity in worst case. Example 5.1 boolean linearSearchUnsorted(int[] arr, int size, int value) {           int i = 0;             for(i = 0 ; i < size ; i++) {                    if(value == arr[i] )                    return true;           }             return false; } Time Complexity: O(n). As we need to traverse the complete array in worst case. Worst case is when your desired element is at the last position of the array. Here, ‘n’ is the size of the array. Space Complexity: O(1). No extra memory is used to allocate the array.

Linear Search – Sorted If elements of the array are sorted either in increasing order or in decreasing order, searching for a desired element will be much more efficient than unordered linear search. In many cases, we do not need to traverse the complete array. Following example explains when you encounter a greater element from the increasing sorted array, you stop searching further. This is how this algorithm saves the time and improves the performance.  Example 5.2 boolean linearSearchSorted(int[] arr, int size, int value) {           int i = 0;           for(i = 0 ; i < size ; i++)           {                    if(value == arr[i] )                              return true;                    else if( value < arr[i] )                              return false;           }           return false; } Time Complexity: O(n). As we need to traverse the complete array in worst case. Worst case is when your desired element is at the last position of the sorted array. However, in the average case this algorithm is more efficient even though the growth rate is same as unsorted. Space Complexity: O(1). No extra memory is used to allocate the array.

Binary Search How do we search a word in a dictionary? In general, we go to some approximate page (mostly middle) and start searching from that point. If we see the word that we are searching is same then we are done with the search. Else, if we see the page is before the selected pages, then apply the same procedure for the first half otherwise to the second half. Binary Search also works in the same way. We get the middle point from the sorted array and start comparing with the desired value. Note: Binary search requires the array to be sorted otherwise binary search cannot be applied. Example 5.3 // Binary Search Algorithm – Iterative Way boolean Binarysearch(int[] arr, int size, int value) {           int low = 0;           int high = size-1;           int mid;           while (low <= high)           {                    mid = low + (high-low)/2; // To avoid the overflow                     if (arr[mid] == value)                              return true;                    else if (arr[mid] < value)                              low = mid + 1;                    else                              high = mid - 1;           }           return false; } Time Complexity: O(logn). We always take half input and throwing out the other half. So the recurrence relation for binary search is T(n) = T(n/2) + c. Using master theorem (divide and conquer), we get T(n) = O(logn) Space Complexity: O(1) Example 5.4 // Binary Search Algorithm – Recursive Way boolean BinarySearchRecursive(int[] arr, int low, int high,int value)   {           if(low > high)                    return false;             int mid = low + (high-low)/2; // To avoid the overflow             if (arr[mid] == value)                    return true;           else if (arr[mid] < value)                    return BinarySearchRecursive (arr, mid + 1, high, value);           else                    return BinarySearchRecursive (arr, low, mid - 1 , value);

} Time Complexity: O(logn). Space Complexity: O(logn) For system stack in recursion

String Searching Algorithms Refer String chapter.

Hashing and Symbol Tables Refer Hash-Table chapter.

How sorting is useful in Selection Algorithm? Selection problems can be converted to sorting problems. Once the array is sorted, it is easy to find the minimum/maximum (or desired element) from the sorted array. The method ‘Sorting and then Selecting’ is inefficient for selecting a single element, but it is efficient when many selections need to be made from the array. It is because only one initial expensive sort is needed, followed by many cheap selection operations. For example, if we want to get the maximum element from an array. After sorting the array, we can simply return the last element from the array. What if we want to get second maximum. Now, we do not have to sort the array again and we can return the second last element from the sorted array. Similarly, we can return the kth maximum element by just one scan of the sorted list. So with the above discussion, sorting is used to improve the performance. In general this method requires O(nlogn) (for sorting) time. With the initial sorting, we can answer any query in one scan, O(n).

Problems in Searching Print Duplicates in Array Given an array of n numbers, print the duplicate elements in the array. First approach:  Exhaustive search or Brute force, for each element in array find if there is some other element with the same value. This is done using two for loop, first loop to select the element and second loop to find its duplicate entry. Example 5.5 void printRepeating(int[] arr, int size) {           int i, j;           System.out.println(" Repeating elements are ");           for(i = 0; i < size; i++)                    for(j = i+1; j < size; j++)                              if(arr[i] == arr[j])                                       System.out.println(" " + arr[i]); } The Time Complexity is  and Space Complexity is  Second approach: Sorting, Sort all the elements in the array and after this in a single scan, we can find the duplicates. Example 5.6 void printRepeating2(int[] arr, int size) {           int i;           Arrays.sort(arr);      // Sort(arr,size);           System.out.println(" Repeating elements are ");           for(i = 1; i < size; i++)           {                    if(arr[i] == arr[i-1])                              System.out.println(" " + arr[i]);           } } Sorting algorithms take   time and single scan take   time. The Time Complexity of an algorithm is  and Space Complexity is  Third approach: Hash-Table, using Hash-Table, we can keep track of the elements we have already seen and we can find the duplicates in just one scan.  Example 5.7 void printRepeating3(int[] arr, int size) {           HashSet<Integer> hs = new HashSet<Integer>();           int i;

System.out.println(" Repeating elements are ");          for(i = 0; i < size; i++)          {                   if(hs.contains(arr[i]))                             System.out.println(" " + arr[i]);                   else                             hs.add(arr[i]);          } } Hash-Table insert and find take constant time  so the total Time Complexity of the algorithm is  time. Space Complexity is also  Forth approach: Counting, this approach is only possible if we know the range of the input. If we know that, the elements in the array are in the range 0 to n-1. We can reserve and array of length n and when we see an element we can increase its count. In just one single scan, we know the duplicates. If we know the range of the elements, then this is the fastest way to find the duplicates. Example 5.8 void printRepeating4(int[] arr, int size) {          int[] count = new int[size];          int i;          for(i = 0; i < size; i++)                   count[i]=0;          System.out.println(" Repeating elements are ");          for(i = 0; i < size; i++)          {                   if(count[arr[i]] == 1)                             System.out.println(" " + arr[i]);                   else                             count[arr[i]]++;          } } Counting approach just uses an array so insert and find take constant time  so the total Time Complexity of the algorithm is   time. Space Complexity for creating count array is also  Find max, appearing element in an array Given an array of n numbers, find the element, which appears maximum number of times. First approach:  Exhaustive search or Brute force, for each element in array find how many times this particular value appears in array. Keep track of the maxCount and when some element count is greater than it then update the maxCount. This is done using two for loop, first loop to select the element and second loop to count the occurrence of that element. The Time Complexity is  and Space Complexity is  Example 5.9 int getMax(int[] arr, int size) {          int i, j;

int max = arr[0], count = 1, maxCount = 1;          for (i = 0; i < size; i++)          {                   count = 1;                   for (j = i + 1; j < size; j++)                             if (arr[i] == arr[j])                                      count++;                   if (count > maxCount)                    {                             max = arr[i];                             maxCount = count;                   }          }          return max; } Second approach: Sorting, Sort all the elements in the array and after this in a single scan, we can find the counts. Sorting algorithms take   time and single scan take   time. The Time Complexity of an algorithm is  and Space Complexity is  Example 5.10 int getMax2(int[] arr, int size) {          int max = arr[0], maxCount = 1;          int curr = arr[0], currCount = 1;          int i;          Arrays.sort(arr);      // Sort(arr,size);          for (i = 1; i < size; i++)          {                   if (arr[i] == arr[i - 1])                             currCount++;                   else                   {                             currCount = 1;                             curr = arr[i];                   }                   if (currCount > maxCount)                   {                             maxCount = currCount;                             max = curr;                   }          }          return max; } Third approach: Counting, This approach is only possible if we know the range of the input. If we know that, the elements in the array are in the range 0 to n-1. We can reserve and array of length n and when we see an element we can increase its count. In just one single scan, we know the duplicates. If we know the range of the elements, then this is the fastest way to find the max count. Counting approach just uses array so to increase count take constant time  so the total Time Complexity of the algorithm is   time. Space Complexity for creating count array is also

Example 5.11 int getMax(int[] arr, int size, int range) {          int max = arr[0], maxCount = 1;          int[] count = new int[range];          int i;          for (i = 0; i < size; i++)          {                   count[arr[i]]++;                   if (count[arr[i]] > maxCount)                   {                             maxCount = count[arr[i]];                             max = arr[i];                   }          }          return max; } Majority element in an Array Given an array of n elements. Find the majority element, which appears more than n/2 times. Return 0 in case there is no majority element. First approach:  Exhaustive search or Brute force, for each element in array find how many times this particular value appears in array. Keep track of the maxCount and when some element count is greater than it then update the maxCount. This is done using two for loop, first loop to select the element and second loop to count the occurrence of that element. Once we have the final, maxCount we can see if it is greater than n/2, if it is greater than we have a majority if not we do not have any majority. The Time Complexity is  and Space Complexity is  Example 5.12 int getMajority(int[] arr, int size) {          int i, j;          int max=0, count=0 , maxCount=0;            for(i = 0; i < size; i++)          {                   for(j = i+1; j < size; j++)                             if(arr[i] == arr[j])                                      count++;                   if(count > maxCount)                   {                             max = arr[i];                             maxCount = count;                   }          }          if (maxCount > size/2)

return max;          else                   return Integer.MIN_VALUE; } Second approach: Sorting, Sort all the elements in the array. If there is a majority than the middle element at the index n/2 must be the majority number. So just single scan can be used to find its count and see if the majority is there or not. Sorting algorithms take   time and single scan take   time. The Time Complexity of an algorithm is  and Space Complexity is  Example 5.13 int getMajority2(int[] arr, int size) {          int majIndex = size/2, count = 1;          int i;          int candidate;          Arrays.sort(arr); // Sort(arr,size);          candidate = arr[majIndex];          count = 0;          for (i = 0; i < size; i++)                   if(arr[i] == candidate)                             count++;          if (count > size/2)                   return arr[majIndex];          else                   return Integer.MIN_VALUE; } Third approach: This is a cancelation approach (Moore’s Voting Algorithm), if all the elements stand against the majority and each element is cancelled with one element of majority if there is majority then majority prevails. ·         Set the first element of the array as majority candidate and initialize the count to be 1. ·         Start scanning the array. o   If we get some element whose value same as a majority candidate, then we increase the count. o   If we get an element whose value is different from the majority candidate, then we decrement the count. o   If count become 0, that means we have a new majority candidate. Make the current candidate as majority candidate and reset count to 1. o   At the end, we will have the only probable majority candidate. ·         Now scan through the array once again to see if that candidate we found above have appeared more than n/2 times. Counting approach just scans throw array two times.  The Time Complexity of the algorithm is   time. Space Complexity for creating count array is also

Example 5.14 int getMajority3(int[] arr, int size) {          int majIndex = 0, count = 1;          int i;          int candidate;          for(i = 1; i < size; i++)          {                   if(arr[majIndex] == arr[i])                             count++;                   else                             count--;                   if(count == 0)                   {                             majIndex = i;                             count = 1;                   }          }          candidate = arr[majIndex];          count = 0;          for (i = 0; i < size; i++)                   if(arr[i] == candidate)                             count++;          if (count > size/2)                   return arr[majIndex];          else                   return Integer.MIN_VALUE; } Find the missing number in an Array Given an array of n-1 elements, which are in the range of 1 to n. There are no duplicates in the array. One of the integer is missing. Find the missing element. First approach:  Exhaustive search or Brute force, for each value in the range 1 to n, find if there is some element in array which have the same value. This is done using two for loop, first loop to select value in the range 1 to n and the second loop to find if this element is in the array or not. The Time Complexity is  and Space Complexity is  Example 5.15 int findMissingNumber(int[] arr, int size) {          int i, j, found = 0;          for (i = 1; i <= size; i++)          {                   found = 0;                   for (j = 0; j < size; j++)                   {                             if (arr[j] == i)                             {                                      found = 1;                                      break;

}                   }                   if (found == 0)                             return i;          }          return Integer.MAX_VALUE; } Second approach: Sorting, Sort all the elements in the array and after this in a single scan, we can find the duplicates. Sorting algorithms take   time and single scan take   time. The Time Complexity of an algorithm is  and Space Complexity is  Third approach: Hash-Table, using Hash-Table, we can keep track of the elements we have already seen and we can find the missing element in just one scan. Hash-Table insert and find take constant time  so the total Time Complexity of the algorithm is  time. Space Complexity is also  Forth approach: Counting, we know the range of the input so counting will work. As we know that, the elements in the array are in the range 0 to n-1. We can reserve and array of length n and when we see an element we can increase its count. In just one single scan, we know the missing element. Counting approach just uses an array so insert and find take constant time  so the total Time Complexity of the algorithm is   time. Space Complexity for creating count array is also  Fifth approach: You are allowed to modify the given input array. Modify the given input array in such a way that in the next scan you can find the missing element. When you scan through the array. When at index “index”, the value stored in the array will be arr[index] so add the number “n + 1” to arr[ arr[ index]]. Always read the value from the array using a reminder operator “%”. When you scan the array for the first time and modified all the values, then one single scan you can see if there is some value in the array which is smaller than “n+1” that index is the missing number.  In this approach, the array is scanned two times and the Time Complexity of this algorithm is . Space Complexity is  Sixth approach: Summation formula to find the sum of n numbers from 1 to n. Subtract the values stored in the array and you will have your missing number. The Time Complexity of this algorithm is . Space Complexity is  Seventh approach: XOR approach to find the sum of n numbers from 1 to n. XOR the values

stored in the array and you will have your missing number. The Time Complexity of this algorithm is . Space Complexity is  Example 5.16 int findMissingNumber(int[] arr, int size, int range) {          int i;          int xorSum = 0;          //get the XOR of all the numbers from 1 to range          for (i = 1; i <= range; i++)                   xorSum ^= i;          //loop through the array and get the XOR of elements          for (i = 0; i<size; i++)                   xorSum ^= arr[i];          return xorSum; } Note: Same problem can be asked in many forms (sometimes you have to do the xor of the range sometime you do not): 1.    There are numbers in the range of 1-n out of which all appears single time but one that appear two times. 2.    All the elements in the range 1-n are appearing 16 times and one element appear 17 times. Find the element that appears 17 times. Find Pair in an Array Given an array of n numbers, find two elements such that their sum is equal to “value” First approach:  Exhaustive search or Brute force, for each element in array find if there is some other element, which sum up to the desired value. This is done using two for loop, first loop to select the element and second loop to find another element. The Time Complexity is  and Space Complexity is  Example 5.17 int FindPair(int[] arr, int size, int value) {          int i, j;          for (i = 0; i < size; i++)                   for (j = i + 1; j < size; j++)                             if ((arr[i] + arr[j] ) == value)                             {                             System.out.println("The pair is : "+arr[i]+","+arr[j]);                             return 1;                             }          return 0; } Second approach: Sorting, Steps are as follows: 1.    Sort all the elements in the array.

2.    Take two variable first and second. Variable first= 0 and second = size -1 3.    Compute sum = arr[first]+arr[second] 4.    If the sum is equal to the desired value, then we have the solution 5.    If the sum is less than the desired value, then we will increase first 6.    If the sum is greater than the desired value, then we will decrease the second 7.    We repeat the above process till we get the desired pair or we get first >= second (don’t have a pair) Sorting algorithms take   time and single scan take   time. The Time Complexity of an algorithm is  and Space Complexity is  Example 5.18 int FindPair2(int[] arr, int size, int value) {          int first = 0, second = size - 1;          int curr;          Arrays.sort(arr);//Sort(arr, size);          while (first < second) {                   curr = arr[first] + arr[second];                   if (curr == value)                   {                             System.out.println("The pair is " + arr[first]+ "," +  arr[second]);                             return 1;                   }                   else if (curr < value)                             first++;                   else                             second--;          }          return 0; }        Third approach: Hash-Table, using Hash-Table, we can keep track of the elements we have already seen and we can find the pair in just one scan. 1.    For each element, insert the value in Hashtable. Let say current value is arr[index] 2.    And see if the value - arr[index] is already in a Hashtable. 3.    If value - arr[index] is in the Hashtable then we have the desired pair. 4.    Else, proceed to the next entry in the array. Hash-Table insert and find take constant time  so the total Time Complexity of the algorithm is  time. Space Complexity is also  Example 5.19 int FindPair3(int[] arr, int size, int value) {          HashSet<Integer> hs = new HashSet<Integer>();          int i;          for (i = 0; i < size; i++)          {                   if ( hs.contains(value - arr[i]))                   {                             System.out.println("The pair is : "+arr[i]+" , "+(value - arr[i]));

return 1;                   }                   hs.add(arr[i]);          }          return 0; } Forth approach: Counting, This approach is only possible if we know the range of the input. If we know that, the elements in the array are in the range 0 to n-1. We can reserve and array of length n and when we see an element we can increase its count. In place of the Hashtable in the above approach, we will use this array and will find out the pair. Counting approach just uses an array so insert and find take constant time  so the total Time Complexity of the algorithm is   time. Space Complexity for creating count array is also  Find the Pair in two Arrays Given two array X and Y. Find a pair of elements (xi, yi) such that xi∈X and yi∈Y where xi+yi=value. First approach:  Exhaustive search or Brute force, loop through element xi of X and see if you can find (value – xi) in Y. Two for loop. The Time Complexity is  and Space Complexity is  Second approach: Sorting, Sort all the elements in the second array Y. For each element if X you can see if that element is there in Y by using binary search. Sorting algorithms take   and searching will take  time. The Time Complexity of an algorithm is  and Space Complexity is  Third approach: Sorting, Steps are as follows: 1.    Sort the elements of both X and Y in increasing order. 2.    Take the sum of the smallest element of X and the largest element of Y. 3.    If the sum is equal to value, we got our pair. 4.    If the sum is smaller than value, take next element of X 5.    If the sum is greater than value, take the previous element of Y Sorting algorithms take   for sorting and searching will take  time. The Time Complexity of an algorithm is    Space Complexity is  Forth approach: Hash-Table, Steps are as follows: 1.    Scan through all the elements in the array Y and insert them into Hashtable. 2.    Now scan through all the elements of array X, let us suppose the current element is xi see if you can find (value - xi) in the Hashtable.

3.    If you find the value, you got your pair. 4.    If not, then go to the next value in the array X. Hash-Table insert and find take constant time  so the total Time Complexity of the algorithm is  time. Space Complexity is also  Fifth approach: Counting, This approach is only possible if we know the range of the input. Same as Hashtable implementation just use a simple array in place of Hashtable and you are done. Counting approach just uses an array so insert and find take constant time  so the total Time Complexity of the algorithm is   time. Space Complexity for creating count array is also  Two elements whose sum is closest to zero Given an   Array of integer s, both +ve and -ve. You need to find the two elements such that their sum is closest to zero. First approach:  Exhaustive search or Brute force, for each element in array find the other element whose value when added will give minimum absolute value. This is done using two for loop, first loop to select the element and second loop to find the element that should be added to it so that the absolute of the sum will be minimum or close to zero. The Time Complexity is  and Space Complexity is  Example 5.20 void minabsSumPair(int[] arr, int size) {          int l, r, minSum, sum, minFirst, minSecond;          // Array should have at least two elements          if(size < 2) {                   System.out.println("Invalid Input");                   return;          }          // Initialization of values          minFirst = 0;          minSecond = 1;          minSum = Math.abs(arr[0] + arr[1]);          for(l = 0; l < size - 1; l++)          {                   for(r = l+1; r < size; r++)                   {                             sum = Math.abs(arr[l] + arr[r]);                             if(sum < minSum)                             {                                      minSum = sum;                                      minFirst = l;                                      minSecond = r;                             }                   }          }

System.out.println(" The two elements with minimum sum are : " + arr[minFirst] + " , "+ arr[minSecond]); } Second approach: Sorting Steps are as follows: 1.    Sort all the elements in the array. 2.    Take two variable firstIndex = 0 and secondIndex = size -1 3.    Compute sum = arr[firstIndex]+arr[secondIndex] 4.    If the sum is equal to the 0 then we have the solution 5.    If the sum is less than the 0 then we will increase first 6.    If the sum is greater than the 0 then we will decrease the second 7.    We repeat the above process 3 to 6, till we get the desired pair or we get first >= second Example 5.21 void minabsSumPair2(int[] arr, int size) {          int l, r, minSum, sum, minFirst, minSecond;          // Array should have at least two elements          if (size < 2)          {                   System.out.println("Invalid Input");                   return;          }          Arrays.sort(arr);      //Sort(arr, size);          // Initialization of values          minFirst = 0;          minSecond = size - 1;          minSum = Math.abs(arr[minFirst] + arr[minSecond]);          for (l = 0, r = size - 1; l < r;)          {                   sum = (arr[l] + arr[r]);                   if (Math.abs(sum) < minSum)                   {                             minSum = Math.abs(sum);                             minFirst = l;                             minSecond = r;                   }                   if (sum < 0)                             l++;                   else if (sum > 0)                             r++;                   else                             break;          }          System.out.println(" The two elements with minimum sum are : " + arr[minFirst] + " , "+ arr[minSecond]); } Find maxima in a bitonic array A bitonic array comprises of an increasing sequence of integers immediately followed by a decreasing sequence of integer s. Since the elements are sorted in some order, we should go for

algorithm similar to binary search. The steps are as follows: 1.    Take two variable for storing start and end index. Variable start=0 and end=size-1 2.    Find the middle element of the array. 3.    See if the middle element is the maxima. If yes, return the middle element. 4.    Alternatively, If the middle element in increasing part, then we need to look for in mid+1 and end. 5.    Alternatively, if the middle element is in the decreasing part, then we need to look in the start and mid-1. 6.    Repeat step 2 to 5 until we get the maxima. Example 5.22 int SearchBotinicArrayMax(int[] arr, int size) {          int start = 0, end = size - 1;          int mid = (start + end) / 2;          int maximaFound = 0;            if (size < 3)          {                   System.out.println("error");                   return 0;          }          while (start <= end)          {                   mid = (start + end) / 2;                   if (arr[mid - 1] < arr[mid] && arr[mid + 1] < arr[mid])  //maxima                   {                             maximaFound = 1;                             break;                   }                   else if (arr[mid - 1] < arr[mid] && arr[mid] < arr[mid + 1])//increasing                   {                             start = mid + 1;                   }                   else if (arr[mid - 1] > arr[mid] && arr[mid] > arr[mid + 1])//decreasing                   {                             end = mid - 1;                   }                   else                   {                             break;                   }          }          if (maximaFound == 0)          {                   System.out.println("error");                   return 0;          }            return arr[mid]; } Search element in a bitonic array  A bitonic array comprises of an increasing sequence of integers immediately followed by a decreasing sequence of integer s. To search an element in a bitonic array:

1.    Find the index or maximum element in the array. By finding the end of increasing part of the array, using modified binary search. 2.    Once we have the maximum element, search the given value in increasing part of the array using binary search. 3.    If the value is not found in increasing part, search the same value in decreasing part of the array using binary search. Example 5.23 int SearchBitonicArray(int[] arr, int size, int key) {          int max = FindMaxBitonicArray(arr, size);          int k = BinarySearch(arr, 0, max, key, true);          if (k != -1)                   return k;          else                   return BinarySearch(arr, max + 1, size - 1, key, false); } int FindMaxBitonicArray(int[] arr, int size) {          int start = 0, end = size - 1, mid;          if (size < 3)          {                   System.out.println("error");                   return 0;          }          while (start <= end)          {                   mid = (start + end) / 2;                   if (arr[mid - 1] < arr[mid] && arr[mid + 1] < arr[mid])//maxima                   {                             return mid;                   }                   else if (arr[mid - 1] < arr[mid] && arr[mid] < arr[mid + 1])//increasing                   {                             start = mid + 1;                   }                   else if (arr[mid - 1] > arr[mid] && arr[mid] > arr[mid + 1])//increasing                   {                             end = mid - 1;                   }                   else                   {                             break;                   }          }          System.out.println("error");          return 0; } int BinarySearch(int[] arr, int start, int end, int key, boolean isInc) {          int mid;          if (end < start)                   return -1;          mid = (start + end) / 2;          if (key == arr[mid])                   return mid;

if (isInc != false && key < arr[mid] ||                             isInc == false && key > arr[mid])          {                   return BinarySearch(arr, start, mid - 1, key, isInc);          }          else          {                   return BinarySearch(arr, mid + 1, end, key, isInc);          } } Occurrence counts in sorted Array Given a sorted array arr[] find the number of occurrences of a number. First approach:  Brute force, Traverse the array and in linear time we will get the occurrence count of the number. This is done using one loop. The Time Complexity is  and Space Complexity is  Example 5.24 int findKeyCount(int[] arr, int size, int key) {          int i, count = 0;          for (i = 0; i < size ; i++)          {                   if (arr[i] == key)                             count++;          }          return count; } Second approach: Since we have sorted array, we should think about some binary search. 1.    First, we should find the first occurrence of the key. 2.    Then we should find the last occurrence of the key. 3.    Take the difference of these two values and you will have the solution. Example 5.25 int findKeyCount2(int[] arr, int size, int key) {          int firstIndex, lastIndex;          firstIndex = findFirstIndex(arr, 0, size -1, key);          lastIndex = findLastIndex(arr, 0, size - 1, key);          return (lastIndex - firstIndex + 1); } int findFirstIndex(int[] arr, int start, int end, int key) {          int mid;          if (end < start)                   return -1;          mid = (start + end) / 2;          if (key == arr[mid] && (mid == start || arr[mid - 1] != key))

return mid;          if (key <= arr[mid])// <= is us the number.t in sorted array.          {                   return findFirstIndex(arr, start, mid - 1, key);          }          else          {                   return findFirstIndex(arr, mid + 1, end, key);          } } int findLastIndex(int[] arr, int start, int end, int key) {          int mid;            if (end < start)                   return -1;          mid = (start + end) / 2;            if (key == arr[mid] && (mid == end || arr[mid + 1] != key))                   return mid;            if (key < arr[mid])// <          {                   return findLastIndex(arr, start, mid - 1, key);          }            else          {                   return findLastIndex(arr, mid + 1, end, key);          } } Separate even and odd numbers in Array Given an array of even and odd numbers, write a program to separate even numbers from the odd numbers. First approach: allocate a separate array, then scan through the given array, and fill even numbers from the start and odd numbers from the end. Second approach: Algorithm is as follows. 1.    Initialize the two variable left and right. Variable left=0 and right= size-1. 2.    Keep increasing the left index until the element at that index is even. 3.    Keep decreasing the right index until the element at that index is odd. 4.    Swap the number at left and right index. 5.    Repeat steps 2 to 4 until left is less than right. Example 5.26 void swap (int[] arr, int first, int second) {          int temp = arr[first];          arr[first] = arr[second];          arr[second] = temp;

} void seperateEvenAndOdd(int[] arr, int size) {          int left = 0, right = size - 1;          while (left < right)          {                   if (arr[left] % 2 == 0 )                   {                             left++;                   }                   else if (arr[right] % 2 == 1)                   {                             right--;                   }                   else                   {                             swap(arr, left, right);                             left++;                             right--;                   }          } } Stock purchase-sell problem Given an array, whose nth element is the price of the stock on nth day. You are asked to buy once and sell once, on what date you will be buying and at what date you will be selling to get maximum profit. Or Given an array of numbers, you need to maximize the difference between two numbers, such that you can subtract the number, which appear before form the number that appear after it. First approach:  Brute force, for each element in array find if there is some other element whose difference is maximum. This is done using two for loop, first loop to select, buy date index and the second loop to find its selling date entry. The Time Complexity is  and Space Complexity is  Second approach: Another clever solution is to keep track of the smallest value seen so far from the start. At each point, we can find the difference and keep track of the maximum profit. This is a linear solution. The Time Complexity of the algorithm is   time. Space Complexity for creating count array is also  Example 5.27 void maxProfit(int stocks[], int size) {          int buy = 0, sell = 0;          int curMin = 0;          int currProfit=0;          int maxProfit = 0;

int i;          for (i = 0; i < size; i++) {                   if (stocks[i] < stocks[curMin])                             curMin = i;                   currProfit = stocks[i] - stocks[curMin];                   if (currProfit > maxProfit) {                             buy = curMin;                             sell = i;                             maxProfit = currProfit;                   }          }          System.out.println("Purchase day is- "+ buy +" at price " + stocks[buy]);          System.out.println("Sell day is- "+sell+" at price " + stocks[sell]); } Find a median of an array Given an array of numbers of size n, if all the elements of the array are sorted then find the element, which lie at the index n/2. First approach: Sort the array and return the element in the middle. Sorting algorithms take . The Time Complexity of an algorithm is  and Space Complexity is  Example 5.28 int getMedian(int[] arr, int size) {          Arrays.sort(arr); //Sort(arr, size);          return arr[size / 2]; } Second approach: Use QuickSelect algorithm. This algorithm we will look into the next chapter. In QuickSort algorithm just skip the recursive call that we do not need. The average Time Complexity of this algorithm will be  Find median of two sorted arrays. First approach: Keep track of the index of both the array, say the index are i and j. keep increasing the index of the array which ever have a smaller value. Use a counter to keep track of the elements that we have already traced. The Time Complexity of an algorithm is  and Space Complexity is  Example 5.29 int findMedian(int[] arrFirst, int sizeFirst, int[] arrSecond, int sizeSecond) {          int medianIndex = ((sizeFirst + sizeSecond) +

(sizeFirst + sizeSecond) % 2) / 2;  //cealing function.          int i = 0, j = 0;          int count = 0;          while (count < medianIndex - 1)          {                   if (i < sizeFirst - 1 && arrFirst[i] < arrSecond[j])                             i++;                   else                             j++;                   count++;          }          if (arrFirst[i] < arrSecond[j])                   return arrFirst[i];          else                   return arrSecond[j]; } Search 01 Array Given an array of 0’s and 1’s. All the 0’s come before 1’s. Write an algorithm to find the index of the first 1. Or You are given an array which contains either 0 or 1, and they are in sorted order Ex. a [] = { 1,1,1,1,0,0,0} How will you count no of 1`s and 0's? First approach: Binary Search, since the array is sorted using binary search to find the desired index. The Time Complexity of an algorithm is  and Space Complexity is  Example 5.30 int BinarySearch01Wrapper(int[] arr, int size) {          if (size == 1 && arr[0] == 1)                   return 0;          return BinarySearch01(arr, 0, size - 1); } int BinarySearch01(int[] arr, int start, int end) {          int mid;          if (end < start)                   return -1;          mid = (start + end) / 2;          if (1 == arr[mid] && 0 == arr[mid - 1])                   return mid;          if (0 == arr[mid])          {                   return BinarySearch01(arr, mid + 1, end);          }          else          {                   return BinarySearch01(arr, start, mid - 1);          } }

Search in sorted rotated Array Given a sorted array s of n integer. s is rotated an unknown number of times. Find an element in the array. First approach: Since the array is sorted, we can use modified binary search to find the element. The Time Complexity of an algorithm is  and Space Complexity is  Example 5.31 int BinarySearchRotateArray(int[] arr, int start, int end, int key) {          int mid;          if (end < start)                   return -1;          mid = (start + end) / 2;          if (key == arr[mid])                   return mid;          if (arr[mid] > arr[start])          {                   if (arr[start] <= key && key < arr[mid])                   {                             return BinarySearchRotateArray(arr, start, mid - 1, key);                   }                   else                   {                             return BinarySearchRotateArray(arr, mid + 1, end, key);                   }          }          else          {                   if (arr[mid] < key && key <= arr[end])                   {                             return BinarySearchRotateArray(arr, mid + 1, end, key);                   }                   else                   {                             return BinarySearchRotateArray(arr, start, mid - 1, key);                   }          } } int BinarySearchRotateArrayWrapper(int[] arr, int size, int key) {          return BinarySearchRotateArray(arr, 0, size - 1, key); } First Repeated element in the array Given an unsorted array of n elements, find the first element, which is repeated. First approach:  Exhaustive search or Brute force, for each element in array find if there is some other element with the same value. This is done using two for loop, first loop to select the element

and second loop to find its duplicate entry. The Time Complexity is  and Space Complexity is  Example 5.32 int FirstRepeated(int[] arr, int size) {          int i, j;          for (i = 0; i < size; i++)                   for (j = i + 1; j < size; j++)                             if (arr[i] == arr[j])                                      return arr[i];          return 0; } Second approach: Hash-Table, using Hash-Table, we can keep track of the number of times a particular element came in the array. First scan just populate the Hashtable. In the second, scan just look the occurrence of the elements in the Hashtable. If occurrence is more for some element, then we have our solution and the first repeated element. Hash-Table insert and find take constant time  so the total Time Complexity of the algorithm is  time. Space Complexity is also   for maintaining hash. Transform Array How would you swap elements of an array like [a1 a2 a3 a4 b1 b2 b3 b4] to convert it into [a1 b1 a2 b2 a3 b3 a4 b4]? Approach:  ·         First swap elements in the middle pair ·         Next swap elements in the middle two pairs ·         Next swap elements in the middle three pairs  ·         Iterate n-1 steps.  Ex: with n = 4.  a1 a2 a3 a4 b1 b2 b3 b4  a1 a2 a3 b1 a4 b2 b3 b4  a1 a2 b1 a3 b2 a4 b3 b4  a1 b1 a2 b2 a3 b3 a4 b4 Example 5.33 void transformArrayAB1(int[] arr, int size) {          int N = size/2, i, j;          for (i = 1; i < N; i++) {                   for (j = 0; j < i; j++) {                             swap(arr, N-i+2*j, N-i+2*j+1);

}          } } Find 2nd largest number in an array with minimum comparisons Suppose you are given an unsorted array of n distinct elements. How will you identify the second largest element with minimum number of comparisons? First approach: Find the largest element in the array. Then replace the last element with the largest element. Then search the second largest element int the remaining n-1 elements. The total number of comparisons is: (n-1) + (n-2) Second approach: Sort the array and then give the (n-1) element. This approach is still more inefficient. Third approach: Using priority queue / Heap. This approach we will look into heap chapter. Use buildHeap() function to build heap from the array. This is done in n comparisons. Arr[0] is the largest number , and the grater among arr[1] and arr[2] is the second largest. The total number of comparisons are: (n-1) + 1 = n Check if two arrays are permutation of each other  Given two integer arrays. You have to check whether they are permutation of each other. First approach: Sorting, Sort all the elements of both the arrays and Compare each element of both the arrays from beginning to end. If there is no mismatch, return true. Otherwise, false. Sorting algorithms take   time and comparison take   time. The Time Complexity of an algorithm is  and Space Complexity is  Example 5.34 boolean checkPermutation(int[] array1, int size1, int[] array2, int size2){          if (size1 != size2)                   return false;          Arrays.sort(array1); // Sort(array1, size1);          Arrays.sort(array2); // Sort(array2, size2);          for (int i = 0; i < size1; i++) {                   if (array1[i] != array2[i])                             return false;          }          return true; } Second approach: Hash-Table (Assumption: No duplicates). 1.    Create a Hash-Table for all the elements of the first array. 2.    Traverse the other array from beginning to the end and search for each element in the Hash-

Table. 3.    If all the elements are found in, the Hash-Table return true, otherwise return false. Hash-Table insert and find take constant time  so the total Time Complexity of the algorithm is  time. Space Complexity is also  Time Complexity = O(n) (For creation of Hash-Table and look-up), Space Complexity = O(n) (For creation of Hash-Table). Example 5.35 boolean checkPermutation2(int[] array1, int size1, int[] array2, int size2) {          int i;          if (size1 != size2)                   return false;          ArrayList<Integer> al = new ArrayList<Integer>();          for (i = 0; i < size1; i++)                   al.add(array1[i]);          for (i = 0; i < size2; i++)          {                   if (al.contains(array2[i]) == false)                             return false;                   al.remove(array2[i]);          }          return true; } Remove duplicates in an integer array  First approach:  Sorting, Steps are as follows: 1.    Sort the array. 2.    Take two references. A subarray will be created with all unique elements starting from 0 to the first reference (The first reference points to the last index of the subarray). The second reference iterates through the array from 1 to the end.  Unique numbers will be copied from the second reference location to first reference location and the same elements are ignored. Time Complexity calculation:  Time to sort the array = O(nlogn).  Time to remove duplicates = O(n). Overall Time Complexity = O(nlogn). No additional space is required so Space Complexity is O(1). Example 5.36 int removeDuplicates(int array[], int size) {          int j = 0;          int i;          if (size == 0)                   return 0;          Arrays.sort(array); // Sort(array,size);          for (i = 1; i < size; i++) {                   if (array[i] != array[j]) {                             j++;

array[j] = array[i];                   }          }          return j + 1; } Searching for an element in a 2-d sorted array  Given a 2 dimensional array. Each row and column are sorted in ascending order. How would you find an element in it? The algorithm works as: 1.    Start with element at last column and first row 2.    If the element is the value we are looking for, return true. 3.    If the element is greater than the value we are looking for, go to the element at previous column but same row. 4.    If the element is less than the value we are looking for, go to the element at next row but same column. 5.    Return false, if the element is not found after reaching the element of the last row of the first column. Condition (row <  r && column >= 0 ) is false. Running time = O(N). Example 5.37 int FindElementIn2DArray(int[] arr[], int r, int c, int value) {          int row = 0;          int column = c - 1;          while (row < r && column >= 0){                   if (arr[row][column] == value)                             return 1;                   else if (arr[row][column] > value)                             column--;                   else                             row++;          }          return 0; }

Introduction Sorting is the process of placing elements from a collection into ascending or descending order. For example, when we play cards, sort cards, according to their value so that we can find the required card easily. When we go to some library, the books are arranged according to streams (Algorithm, Operating systems, Networking etc.). Sorting arranges data elements in order so that searching become easier. When books are arranged in proper indexing order, then it is easy to find a book we are looking for. This chapter discusses algorithms for sorting a set of N items. Understanding sorting algorithms are the first step towards understanding algorithm analysis. Many sorting algorithms are developed and analysed. A sorting algorithm like Bubble-Sort, Insertion-Sort and Selection-Sort are easy to implement and are suitable for the small input set. However, for large dataset they are slow. A sorting algorithm like Merge-Sort, Quick-Sort and Heap-Sort are some of the algorithms that are suitable for sorting large dataset. However, they are overkill if we want to sort the small dataset. Some algorithm, which is suitable when we have some range information on input data. Some other algorithm is there to sort a huge data set that cannot be stored in memory completely, for which external sorting technique is developed. Before we start a discussion of the various algorithms one by one. First, we should look at comparison function that is used to compare two values. Less function will return 1 if value1 is less than value2 otherwise, it will return 0. private boolean less(int value1, int value2) {           return value1 < value2; } More function will return 1 if value1 is more than value2 otherwise it will return 0. private boolean more(int value1, int value2) {           return value1 > value2; } The value in various sorting algorithms is compared using one of the above functions and it will

be swapped depending upon the return value of these functions. If more() comparison function is used, then sorted output will be increasing in order and if less() is used than resulting output will be in descending order.

Type of Sorting Internal Sorting: All the elements can be read into memory at the same time and sorting is performed in memory. 1.    Selection-Sort 2.    Insertion-Sort 3.    Bubble-Sort 4.    Quick-Sort External Sorting: In this, the dataset is so big that it is impossible to load the whole dataset into memory so sorting is done in chunks. 1.    Merge-Sort Three things to consider in choosing, sorting algorithms for application: 1.    Number of elements in list 2.    A number of different orders of list required 3.    The amount of time required to move the data or not move the data

Bubble-Sort Bubble-Sort is the slowest algorithm for sorting, but it is heavily used, as it is easy to implement. In Bubble-Sort, we compare each pair of adjacent values. We want to sort values in increasing order so if the second value is less than the first value then we swap these two values. Otherwise, we will go to the next pair. Thus, smaller values bubble to the start of the array. We will have N number of passes to get the array completely sorted. After the first pass, the largest value will be in the rightmost position. Example 6.1 public class BubbleSort {           private int[] arr;           public BubbleSort(int[] array)           {                    arr = array;           }           private boolean less(int value1, int value2)           {                    return value1 < value2;           }           private boolean more(int value1, int value2)           {                    return value1 > value2;           }           public void sort()           {                    int size = arr.length;                    int i, j, temp;

for (i = 0 ; i < ( size - 1 ); i++)                   {                             for (j = 0 ; j < size - i - 1; j++)                             {                                      if (more(arr[j], arr[j+1]))                                      {                                                /* Swapping */                                                temp= arr[j];                                                arr[j]= arr[j+1];                                                arr[j+1] = temp;                                      }                             }                   }          } } Analysis: ·         The outer for loops represents the number of swaps that are done for comparison of data. ·         The inner loop is actually used to do the comparison of data. At the end of each inner loop iteration, the largest value is moved to the end of the array. In the first iteration the largest value, in the second iteration the second largest and so on. ·         more() function is used for comparison which means when the value of the first argument is greater than the value of the second argument then perform a swap. By this we are sorting in increasing order if we have, the less() function in place of more() than we will get decreasing order sorting. Have a look into more() function in case you forgot private boolean more(int value1, int value2) {          return value1 > value2; } Complexity Analysis: Each time the inner loop execute for (n-1), (n-2), (n-3)… (n-1) + (n-2) + (n-3) + ..... + 3 + 2 + 1  =  n(n-1)/2 Worst case performance O( ) Average case performance O( ) Space Complexity O(1) as we need only one temp variable Stable Sorting Yes

Modified (improved) Bubble-Sort When there is no more swap in one pass of the outer loop. It indicates that all the elements are already in order so we should stop sorting. This sorting improvement in Bubble-Sort is extremely useful when we know that, except few elements rest of the array is already sorted. Example 6.2 public void sort() {           int size = arr.length;           int i, j, temp, swapped=1;           for (i = 0; i < (size - 1) && swapped == 1; i++)           {                    swapped = 0;                    for (j = 0; j < size - i - 1; j++)                    {                              if (more(arr[j], arr[j + 1]))                              {                                       /* Swapping */                                       temp = arr[j];                                       arr[j] = arr[j + 1];                                       arr[j + 1] = temp;                                       swapped = 1;                              }                    }           } } By applying this improvement, best case of this algorithm, when an array is nearly sorted, is improved. Best case is O(n) Complexity Analysis: Worst case performance O( ) Average case performance O( ) Space Complexity O(1) Adaptive: When array is nearly sorted O(n) Stable Sorting Yes

Insertion-Sort Insertion-Sort Time Complexity is O( ) which is same as Bubble-Sort but perform a bit better than it. It is the way we arrange our playing cards. We keep a sorted subarray. Each value is inserted into its proper position in the sorted sub-array in the left of it. Example 6.3 public class InsertionSort {           private int[] arr;           public InsertionSort(int[] array) {                    arr = array;           }           private boolean more(int value1, int value2)           {                    return value1 > value2;           }           public void sort()           {                    int size = arr.length;                    int temp,j;                    for(int i=1; i<size; i++)                    {

temp=arr[i];                             for(j=i; j>0 && more(arr[j-1], temp); j--)                             {                                      arr[j]=arr[j-1];                             }                             arr[j]=temp;                   }          } } public class InsertionSortDemo {          public static void main(String[] args)          {                   int[] array = {9,1,8,2,7,3,6,4,5};                   InsertionSort bs = new InsertionSort(array);                   bs.sort();                   for(int i=0;i<array.length ;i++)                   {                             System.out.print(array[i] + " ");                   }                           } } Analysis: ·         The outer loop is used to pick the value we want to insert into the sorted left array. ·         The value we want to insert we have picked and saved in a temp variable. ·         The inner loop is doing the comparison using the more() function. The values are shifted to the right until we find the proper position of the temp value for which we are doing this iteration. ·         Finally the value is placed into the proper position.In each iteration of the outer loop, the length of the sorted array increase by one. When we exit the outer loop the whole array is sorted. Complexity Analysis: Worst case Time Complexity O( ) Best case Time Complexity O(n) Average case Time Complexity O( ) Space Complexity O(1) Stable sorting Yes

Selection-Sort Selection-Sort searches the whole unsorted array and put the largest value at the end of it. This algorithm is having the same Time Complexity, but performs better than both bubble and Insertion-Sort as less number of comparisons required. The sorted array is created backward in Selection-Sort. Example 6.4: public class SelectionSort {           private int[] arr;           public SelectionSort(int[] array) {                    arr = array;           }           //back array           public void sort() {                    int size = arr.length;                    int i, j, max, temp;                    for (i = 0; i < size - 1; i++) {                              max = 0;                              for (j = 1; j < size -1 - i ; j++) {                                       if (arr[j] > arr[max])         {                                                 max = j;                                       }                              }                              temp = arr[size - 1 - i];                              arr[size - 1 - i] = arr[max];                              arr[max] = temp;                    }           } } Analysis: ·         The outer loop decide the number of times the inner loop will iterate. For a input of N elements the inner loop will iterate N number of times. ·         In each iteration of the inner loop the largest value is calculated and is placed placed at the end of the array. ·         This is the final replacement of the maximum value to the proper location. The sorted

array is created backward. Complexity Analysis: Worst Case Time Complexity O(n2) Best Case Time Complexity O(n2) Average case Time Complexity O(n2) Space Complexity O(1) Stable Sorting No The same algorithm can be implemented by creating the sorted array in the front of the array. Example 6.5 //front array varient void sort2() {          int size = arr.length;          int i, j, min, temp;          for (i = 0; i < size - 1; i++) {                   min = i;                   for (j = i + 1; j < size; j++) {                             if (arr[j] < arr[min]) {                                      min = j;                             }                   }                   temp = arr[i];                   arr[i] = arr[min];                   arr[min] = temp;          } }

Merge-Sort Merge sort divide the input into half recursive in each step. It sort the two parts separately recursively and finally combine the result into final sorted output. Example 6.6 public class MergeSort {           private int[] arr;           private void merge(int[] arr,int[] tempArray, int lowerIndex, int middleIndex, int upperIndex)           {                    int lowerStart=lowerIndex;                    int lowerStop=middleIndex;                    int upperStart=middleIndex+1;                    int upperStop=upperIndex;                    int count=lowerIndex;                    while(lowerStart<=lowerStop && upperStart<=upperStop)                    {                              if(arr[lowerStart]<arr[upperStart])                                       tempArray[count++]=arr[lowerStart++];                              else                                       tempArray[count++]=arr[upperStart++];                    }                    while(lowerStart<=lowerStop)                    {                              tempArray[count++]=arr[lowerStart++];                    }                    while( upperStart<=upperStop)                    {                              tempArray[count++]=arr[upperStart++];                    }                    for(int i=lowerIndex;i<=upperIndex;i++)                              arr[i]=tempArray[i];           }           private void mergeSrt(int[] arr,int[] tempArray, int lowerIndex, int upperIndex)           {                    if(lowerIndex >= upperIndex)                              return;                    int middleIndex=(lowerIndex+upperIndex)/2;                    mergeSrt(arr,tempArray,lowerIndex,middleIndex);                    mergeSrt(arr,tempArray,middleIndex+1,upperIndex);

merge(arr,tempArray,lowerIndex,middleIndex,upperIndex);          }          public void sort()          {                   int size = arr.length;                   int[] tempArray= new int[size];                   mergeSrt(arr,tempArray,0,size-1);          }          public MergeSort(int[] array)          {                   arr = array;          } } public class MergeSortDemo {          public static void main(String[] args) {                   int[] array={3,4,2,1,6,5,7,8,1,1};                   MergeSort m = new MergeSort(array);                   m.sort();                   for(int i=0;i<array.length ;i++)                   {                             System.out.print(array[i] + " ");                   }          } } ·         The Time Complexity of Merge-Sort is O(nlogn) in all 3 cases (best, average and worst) as Merge-Sort always divides the array into two halves and take linear time to merge two halves. ·         It requires the equal amount of additional space as the unsorted list. Hence, it is not at all recommended for searching large unsorted lists. ·         It is the best Sorting technique for sorting Linked Lists. Complexity Analysis: Worst Case Time Complexity O(nlogn) Best Case Time Complexity O(nlogn) Average Time Complexity O(nlogn) Space Complexity O(n) Stable Sorting Yes

Quick-Sort Quick sort is also a recursive algorithm. ·         In each step we select a pivot ( let us say first element of array). ·         Then we traverse the rest of the array and copy all the elements of the array which are smaller than the pivot to the left side of array ·         We copy all the elements of the array which are grater then pivot to the right side of the array. Obviously the pivot is at its sorted position. ·         Then we sort the left and right subarray separately. ·         When the algorithm returns the whole array is sorted. Example 6.7 public class QuickSort {           private int[] arr;           public QuickSort(int[] array) {                    arr = array;           }

private void swap(int arr[], int first, int second){                   int temp = arr[first];                   arr[first] = arr[second];                   arr[second] = temp;          }          private void quickSortUtil (int arr[], int lower, int upper)          {                   if (upper<=lower)                             return;                   int pivot = arr[lower];                   int start = lower;                   int stop = upper;                   while ( lower < upper)                   {                             while (arr[lower] <= pivot && lower < upper)                             {                                      lower++;                             }                             while (arr[upper] > pivot && lower <= upper)                             {                                      upper--;                             }                             if (lower < upper)                             {                                      swap(arr,upper,lower);                             }                   }                   swap(arr, upper, start); // upper is the pivot position                   quickSortUtil (arr, start, upper - 1); // pivot -1 is the upper for left sub array.                   quickSortUtil (arr, upper + 1, stop); // pivot + 1 is the lower for right sub array.          }          public void sort(){          int size = arr.length;          quickSortUtil (arr, 0, size - 1);          } } public class QuickSortDemo {          public static void main(String[] args) {                   int[] array={3,4,2,1,6,5,7,8,1,1};                   QuickSort m = new QuickSort(array);                   m.sort();                   for(int i=0;i<array.length ;i++) {                             System.out.print(array[i] + " ");                   }          } } ·         The space required by Quick-Sort is very less, only O(nlogn) additional space is required. ·         Quicksort is not a stable sorting technique, so it might change the occurrence of two similar elements in the list while sorting. Complexity Analysis: Worst Case Time Complexity O(n2)

Best Case Time Complexity O(nlogn) Average Time Complexity O(nlogn) Space Complexity O(nlogn) Stable Sorting No

Quick Select  Quick select is very similar to Quick-Sort in place of sorting the whole array we just ignore the one-half of the array at each step of Quick-Sort and just focus on the region of array on which we are interested. Example 6.8: public class QuickSelect {           public static void swap(int arr[], int first, int second){                    int temp = arr[first];                    arr[first] = arr[second];                    arr[second] = temp;           }           public static void quickSelect(int arr[], int lower, int upper,int k)           {                    if (upper <= lower)                              return;                    int pivot = arr[lower];                    int start = lower;                    int stop = upper;                    while (lower < upper)                    {                              while (arr[lower] <= pivot && lower < upper)                                       lower++;                              while (arr[upper] > pivot && lower <= upper)                                       upper--;                              if (lower < upper)                                       swap(arr, upper, lower);                    }                    swap(arr, upper, start); //upper is the pivot position                    if (k<upper)                              quickSelect(arr, start, upper - 1, k); //pivot -1 is the upper for left sub array.                    if (k>upper)                              quickSelect(arr, upper + 1, stop, k); // pivot + 1 is the lower for right sub array.           }           public static void quickSelect(int arr[], int k){                    quickSelect(arr, 0, arr.length - 1, k);           } } public class QuickSelectDemo {           public static void main(String[] args) {                    int[] array={3,4,2,1,6,5,7,8,10,9};                    QuickSelect.quickSelect(array, 5);                    System.out.print("value at index 5 is : "+ array[4]);           } } Complexity Analysis: Worst Case Time Complexity O(n2) Best Case Time Complexity O(logn)

Average Time Complexity O(logn) Space Complexity O(nlogn)

Bucket Sort Bucket sort is the simplest and most efficient type of sorting. Bucket sort has a strict requirement of a predefined range of data. Like, sort how many people are in which age group. We know that the age of people can vary between 1 and 130. Example 6.9 public class BucketSort {           int[] array ;           int range;           int lowerRange;           public BucketSort (int[] arr,int lowerRange, int upperRange){                    array = arr;                    range = upperRange - lowerRange;                    this.lowerRange = lowerRange;           }           public void sort()           {                    int i, j;                    int size = array.length;                    int[] count = new int[range];                    for (i = 0; i < range; i++)                    {                              count[i] = 0;                    }                    for (i = 0; i < size; i++)                    {                              count[array[i] - lowerRange]++;                    }                    j = 0;                    for (i = 0; i < range; i++)                    {                              for (; count[i]>0; (count[i])--)

{                                      array[j++] = i + lowerRange;                             }                   }          } } public class BucketSortDemo {          public static void main(String[] args) {                   int[] array={23,24,22,21,26,25,27,28,21,21};                   BucketSort m = new BucketSort(array,20,30);                   m.sort();                   for(int i=0;i<array.length ;i++)                   {                             System.out.print(array[i] + " ");                   }          } } Analysis: ·         We have created a count array to store counts. ·         count array elements are initialized to zero. ·         Index corresponding to input array is incremented. ·         Finally, the information stored in count array is saved in the array. Complexity Analysis: Data structure Array Worst case performance O(n+k) Average case performance O(n+k) Worst case Space Complexity O(k) Where k - is number of distinct elements. n – is the total number of elements in array.

Generalized Bucket Sort There are cases when the element falling into a bucket are not unique but are in the same range. When we want to sort an index of a name, we can use the reference bucket to store names. The buckets are already sorted and the elements inside each bucket can be kept sorted by using an Insertion-Sort algorithm. We are leaving this generalized bucket sort implementation to the reader of this book. The similar data structure will be defined in the coming chapter of Hash-Table using separate chaining.

Heap-Sort Heap-Sort we have already studied in the Heap chapter. Complexity Analysis: Data structure Array Worst case performance O(nlogn) Average case performance O(nlogn) Worst case Space Complexity O(1)

Tree Sorting In-order traversal of the binary search tree can also be seen as a sorting algorithm. We will see this in binary search tree section of tree chapter. Complexity Analysis: Worst Case Time Complexity O( ) Best Case Time Complexity O(nlogn) Average Time Complexity O(nlogn) Space Complexity O(n) Stable Sorting Yes

External Sort (External Merge-Sort) When data need to be sorted is huge. Moreover, it is not possible to load it completely in memory (RAM) for such a dataset we use external sorting. Specific data is sorted using external MergeSort algorithm. First data are picked in chunks and it is sorted in memory. Then this sorted data is written back to disk. Whole data are sorted in chunks using Merge-Sort. Now we need to combine these sorted chunks into final sorted data. Then we create queues for the data, which will read from the sorted chunks. Each chunk will have its own queue. We will pop from this queue and these queues are responsible for reading from the sorted chunks. Let us suppose we have K different chunks of sorted data each of length M. The third step is using a Min-Heap, which will take input data from each of this queue. It will take one element from each queue. The minimum value is taken from the Heap and added to the final sorted element output. Then queue from which this min element is inserted in the heap will again popped and one more element from that queue is added to the Heap. Finally, when the data is exhausted from some queue that queue is removed from the input list. Finally, we will get a sorted data came out from the heap. We can optimize this process further by adding an output buffer, which will store data coming out of Heap and will do a limited number of the write operation in the final Disk space. Note: No one will be asking to implement external sorting in an interview, but it is good to know about it.

Comparisons of the various sorting algorithms. Sort Average Time Best Time Worst Time Space Stability Bubble-Sort O(n2) O(n2) O(n2) O(1) Stable Modified Bubble-Sort O(n2) O(n) O(n2) O(1) Stable Selection-Sort O(n2) O(n2) O(n2) O(1) Unstable Insertion-Sort O(n2) O(n) O(n2) O(1) Stable Heap-Sort O(n*log(n)) O(n*log(n)) O(n*log(n)) O(1) UnStable Merge-Sort O(n*log(n)) O(n*log(n)) O(n*log(n)) O(n) Stable Quick-Sort O(n*log(n)) O(n*log(n)) O(n2) O(n) worst case O(log(n)) average case Unstable Bucket Sort O(n k) O(n k) O(n k) O(n k) Stable

Selection of Best Sorting Algorithm No sorting algorithm is perfect. Each of them has their own pros and cons. Let us read one by one: Quick-Sort: When you do not need a stable sort and average case performance matters more than worst-case performance. When data is random, we prefer the Quick-Sort. Average case Time Complexity of Quick-Sort is O(nlogn) and worst-case Time Complexity is O(n2). Space Complexity of Quick-Sort is  O(logn) auxiliary storage, which is stack space used in recursion. Merge-Sort: When you need a stable sort and Time Complexity of O(nlogn), Merge-Sort is used. In general, Merge-Sort is slower than Quick-Sort because of lot of copy happening in the merge phase. There are two uses of Merge-Sort when we want to merge two sorted linked lists and Merge-Sort is used in external sorting. Heap-Sort: When you do not need a stable sort and you care more about worst-case performance than average case performance. It has guaranteed to be O(nlogn), and uses O(1) auxiliary space, meaning that you will not unexpectedly run out of memory on very large inputs. Insertion-Sort: When we need a stable sort, When N is guaranteed to be small, including as the base case of a Quick-Sort or Merge-Sort. Worst-case Time Complexity is O(n2), it has a very small constant, so for smaller input size it performs better than Merge-Sort or Quick-Sort. It is also useful when the data is already pre-sorted in this case its best case running time is O(N). Bubble-Sort: Where we know the data is very nearly sorted. Say only two elements are out of place. Then in one pass, Bubble Sort will make the data sorted and in the second pass, it will see everything is sorted and then exit. Only takes 2 passes of the array. Selection-Sort: Best Worst Average Case running time all O(n2). It is only useful when you want to do something quick. They can be used when you are just doing some prototyping. Counting-Sort: When you are sorting data within a limited range. Radix-Sort: When log(N) is significantly larger than K, where K is the number of radix digits. Bucket-Sort: When your input is more or less uniformly distributed. Note: A stable sort is one that has guaranteed not to reorder elements with identical keys.

Introduction Let us suppose we have an array that contains following five elements 1, 2, 4, 5, 6. We want to insert a new element with value “3” in between “2” and “4”. In the array, we cannot do so easily. We need to create another array that is long enough to store the current values and one more space for “3”. Then we need to copy these elements in the new space. This copy operation is inefficient. To remove this fixed length constraint linked list is used.

Linked List The linked list is a list of items, called nodes. Nodes have two parts, value part and link part. Value part is used to stores the data. The value part of the node can be either a basic data-type like an integer or it can be some other data-type like an object of some class. The link part is a reference, which is used to store addresses of the next element in the list.

Types of Linked list There are different types of linked lists. The main difference among them is how their nodes refer to each other. Singly Linked List Each node (Except the last node) has a reference to the next node in the linked list. The link portion of node contains the address of the next node. The link portion of the last node contains the value null. Doubly Linked list The node in this type of linked list has reference to both previous and the next node in the list. Circular Linked List This type is similar to the singly linked list except that the last element have reference to the first node of the list. The link portion of the last node contains the address of the first node. The various parts of linked list 1.    Head: Head is a reference that holds the address of the first node in the linked list. 2.    Nodes: Items in the linked list are called nodes. 3.    Value: The data that is stored in each node of the linked list. 4.    Link: Link part of the node is used to store the reference of other node. a.    We will use “next” and “prev” to store address of next or previous node.



Singly Linked List Let us look an example of Node, in this example, the value is of type int, but it can be of some other data-type. The link is named as next in the below class definition. private static class Node {           private int value;           private Node next;           public Node( int v, Node n) {                              value = v;                              next = n;           }           public Node( int v)   {                              value = v;                              next = null;           } } Note: For a singly linked, we should always test these three test cases before saying that the code is good to go. This one node and zero node case is used to catch boundary cases. It is always to take care of these cases before submitting code to the reviewer. ·         Zero element / Empty linked list. ·         One element / Just single node case. ·         General case. The various basic operations that we can perform on linked lists, many of these operations require list traversal: ·         Insert an element in the list, this operation is used to create a linked list. ·         Print various elements of the list. ·         Search an element in the list. ·         Delete an element from the list. ·         Reverse a linked list. You cannot use Head to traverse a linked list because if we use the head, then we lose the nodes of the list. We have to use another reference variable of same data-type as the head. public class LinkedList {

private static class Node {                   private int value;                   private Node next;                   public Node( int v, Node n) {                             value = v;                             next = n;                   }          }          private Node head;          private int size = 0;          //Other Methods. } Size of List public int size(){          return size; } IsEmpty function public boolean isEmpty(){          return size == 0; } Insert element in linked list An element can be inserted into a linked list in various orders. Some of the example cases are mentioned below: 1.    Insertion of an element at the start of linked list 2.    Insertion of an element at the end of linked list 3.    Insertion of an element at the Nth position in linked list 4.    Insert element in sorted order in linked list

Insert element at the Head public void addHead(int value) {          head = new Node(value, head);          size++; } Analysis:        ·         We need to create a new node with the value passed to the function as argument. ·         While creating the new node the reference stored in head is passed as argument to Node() constructor so that the next reference will start pointing to the node or null which is referenced by the head node. ·         The newly created node will become head of the linked list. ·         Size of the list is increased by one. Insertion of an element at the end Example 7.5: Insertion of an element at the end of linked list public void addTail(int value) {          Node newNode = new Node(value, null);          Node curr=head;          if(head==null)                   head=newNode;          while(curr.next != null)          {                   curr=curr.next;          }          curr.next = newNode; } Analysis:

·         New node is created and the value is stored inside it. If the list is empty. Next of new node is null. ·         If list is empty then head will store the reference to the newly created node. ·         If list is not empty then we will traverse until the end of the list. ·         Finally, new node is added to the end of the list. Note: This operation is un-efficient as each time you want to insert an element you have to traverse to the end of the list. Therefore, the complexity of creation of the list is n2. So how to make it efficient we have to keep track of the last element by keeping a tail reference. Therefore, if it is required to always insert element at the end of linked list, then we will keep track of the tail reference also. Traversing Linked List Example 7.2: Print various elements of a linked list public void print(){          Node temp = head;          while(temp != null){                   System.out.print(temp.value+" ");                   temp = temp.next;          } } Analysis: We will store the reference of head in a temporary variable temp. We will traverse the list by printing the content of list and always incrementing the temp by pointing to its next node. Complete code for list creation and printing the list. Example 7.3: public class LinkedListDemo {          public static void main(String[] args) {                   LinkedList ll = new LinkedList();                   ll.addHead(1);                   ll.addHead(2);                   ll.addHead(3);                   ll.addHead(1);                   ll.addHead(2);                   ll.addHead(3);                   ll.print();          } } Analysis: New instance of linked list is created. Various elements are added to list by calling addHead() method.

Finally all the content of list is printed to screen by calling print() method. Sorted Insert Insert an element in sorted order in linked list given Head reference Example 7.4: public void sortedInsert(int value){          Node newNode = new Node(value, null);          Node curr = head;          if(curr == null || curr.value > value){                   newNode.next = head;                   head = newNode;                   return;          }          while(curr.next != null && curr.next.value < value){                   curr = curr.next;          }          newNode.next = curr.next;          curr.next =newNode; } Analysis: ·         Head of the list is stored in curr. ·         A new empty node of the linked list is created. And initialized by storing an argument

value into its value. Next of the node will point to null. ·         It checks if the list was empty or if the value stored in the first node is greater than the current value. Then this new created node will be added to the start of the list. And head need to be modified. ·         We iterate through the list to find the proper position to insert the node. ·         Finally, the node will be added to the list. Search Element in a Linked-List Search element in linked list. Given a head reference and value. Returns true if value found in list else returns false. Note: Search in a single linked list can be only done in one direction. Since all elements in the list have reference to the next item in the list. Therefore, traversal of linked list is linear in nature. Example 7.5: public boolean isPresent(int data){          Node temp = head;          while(temp != null){                   if(temp.value == data)                             return true;                   temp = temp.next;          }          return false; } Analysis: ·         We create a temp variable which will point to head of the list. ·         Using a while loop we will iterate through the list. ·         Value of each element of list is compared with the given value. If value is found, then the function will return true.   ·         If the value is not found, then false will be returned from the function in the end. Delete element from the linked list

Delete First element in a linked list. Example 7.6: public int removeHead() throws IllegalStateException {          if(isEmpty())                   throw new IllegalStateException("EmptyListException");          int value = head.value;          head = head.next;          size--;          return value; } Analysis: ·         First, we need to check if the list is already empty. If list is already empty then throw EmptyListException. ·         If list is not empty then store the value of head node in a temporary variable value. ·         We need to find the second element of the list and assign it as head of the linked list. ·         Since the first node is no longer referenced so it will be automatically deleted. ·         Decrease the size of list. And return the value stored in temporary variable value. Delete node from the linked list given its value.

Example 7.7: public boolean deleteNode(int delValue){          Node temp = head;          if(isEmpty())                    return false;          if(delValue == head.value){                   head=head.next;                   size--;                   return true;          }          while(temp.next != null){                   if(temp.next.value == delValue){                             temp.next = temp.next.next;                             size--;                             return true;                   }                   temp = temp.next;

}          return false; } Analysis: ·         If the list is empty then we will return false from the function which indicate that the deleteNode() method executed with error. ·         If the node that need to be deleted is head node than head reference need to be modified and point to the next node. ·         In a while loop we will traverse the link list and try to find the node that need to be deleted. If the node is found that we will point its reference to the node next to it. And return true. ·         If the node is not found then we will return false. Delete all the occurrence of particular value in linked list. Example 7.8: public void deleteNodes(int delValue) {          Node currNode = head;          Node nextNode;          while(currNode != null && currNode.value == delValue)/*first node */          {                   head = currNode.next;                   currNode = head;          }          while(currNode != null)          {                   nextNode = currNode.next;                   if(nextNode != null && nextNode.value == delValue)                   {                             currNode.next = nextNode.next;                   }                   else                   {                             currNode = nextNode;                   }          } } Analysis: ·         In the first while loop will delete all the nodes that are at the front of the list, which have valued equal to delValue. In this, we need to update head of the list. ·         In the second while loop, we will be deleting all the nodes that are having value equal to the delValue. Remember that we are not returning even though we have the node that we are looking for. Delete a single linked list

Delete all the elements of a linked list, given a reference to head of linked list. Example 7.9: public void freeList(){          head = null;          size = 0; } Analysis: We just need to point head to null. The reference to the list is lost so it will automatically deleted. Reverse a linked list. Reverse a singly linked List iteratively using three Pointers Example 7.10: public void reverse(){          Node curr=head;          Node prev=null;          Node next=null;          while(curr!=null)          {                   next = curr.next;                   curr.next = prev;                   prev=curr;                   curr=next;          }          head = prev; } Analysis: The list is iterated. Make next equal to the next node of the curr node. Make curr node’s next point to prev node. Then iterate the list by making prev point to curr and curr point to next. Recursively Reverse a singly linked List Example 7.11: Recursively Reverse a singly linked List Arguments are current node and its next value. public Node reverseRecurseUtil (Node currentNode, Node nextNode){          Node ret;          if(currentNode == null)                   return null;          if(currentNode.next == null)          {                   currentNode.next = nextNode;                   return currentNode;          }          ret= reverseRecurseUtil (currentNode.next, currentNode);          currentNode.next = nextNode;          return ret; }

public void reverseRecurse() {          head=reverseRecurseUtil(head,null); } Analysis: ·         ReverseRecurse function will call a reverseRecurseUtil function to reverse the list and the reference returned by the reverseRecurseUtil will be the head of the reversed list. ·         The current node will point to the nextNode that is previous node of the old list. Note: A linked list can be reversed using two approaches the one approach is by using three references. The Second approach is using recursion both are linear solution, but three-reference solution is more efficient. Remove duplicates from the linked list Remove duplicate values from the linked list. The linked list is sorted and it contains some duplicate values, you need to remove those duplicate values. (You can create the required linked list using SortedInsert() function) Example 7.12: public void removeDuplicate() {          curr = head;          while(curr!= null)    {                   if(curr.next!=null  &&  curr.value == curr.next.value)                             curr.next = curr.next.next;                   else                             curr = curr.next;          } } Analysis:  While loop is used to traverse the list. Whenever there is a node whose value is equal to the next node’s value, that current node next will point to the next of next node. Which will remove the next node from the list. Copy List Reversed Copy the content of linked list in another linked list in reverse order. If the original linked list contains elements in order 1,2,3,4, the new list should contain the elements in order 4,3,2,1. Example 7.13: public LinkedList CopyListReversed() {          Node tempNode = null;          Node tempNode2 = null;          Node curr = head;          while (curr != null) {                   tempNode2 = new Node(curr.value, tempNode);                   curr = curr.next;                   tempNode = tempNode2;          }

LinkedList ll2 = new LinkedList();          ll2.head = tempNode;          return ll2; } Analysis: Traverse the list and add the node’s value to the new list. Since the list is traversed in the forward direction and each node’s value is added to another list so the formed list is reverse of the given list. Copy the content of given linked list into another linked list Copy the content of given linked list into another linked list. If the original linked list contains elements in order 1,2,3,4, the new list should contain the elements in order 1,2,3,4. Example 7.14: public LinkedList copyList() {          Node headNode = null;          Node tailNode = null;          Node tempNode = null;          Node curr = head;          if (curr == null)                   return null;          headNode = new Node(curr.value, null);          tailNode = headNode;          curr = curr.next;          while (curr != null) {                   tempNode = new Node(curr.value, null);                   tailNode.next = tempNode;                   tailNode = tempNode;                   curr = curr.next;          }          LinkedList ll2 = new LinkedList();          ll2.head = headNode;          return ll2; } Analysis: Traverse the list and add the node’s value to new list, but this time always at the end of the list. Since the list is traversed in the forward direction and each node’s value is added to the end of another list. Therefore, the formed list is same as the given list. Compare List Example 7.15: Compare two list given public boolean compareList(LinkedList ll) {          return compareList(head, ll.head); } public boolean compareList(Node head1, Node head2) {

if( head1==null && head2==null )                   return true;          else if( (head1==null) || (head2==null) || (head1.value!=head2.value))                   return false;          else                   return compareList(head1.next,head2.next); } Analysis: ·         List is compared recursively. Moreover, if we reach the end of the list and both the lists are null. Then both the lists are equal and so return true. ·         List is compared recursively. If either one of the list is empty or the value of corresponding nodes is unequal, then this function will return false. ·         Recursively calls compare list function for the next node of the current nodes. Find Length Example 7.16: Find the length of given linked list. public int findLength() {          Node curr=head;          int count = 0;          while (curr!=null)    {                   count++;                   curr = curr.next;          }          return count; } Analysis: Length of linked list is found by traversing the list till we reach the end of list Nth Node from Beginning Example 7.17: :  Find Nth node from beginning public int nthNodeFromBegining(int index) {          if (index > size() || index < 1)                   return Integer.MAX_VALUE;          int count = 0;          Node curr = head;          while (curr != null && count < index - 1) {                   count++;                   curr = curr.next;          }          return curr.value; } Analysis: Nth node can be found by traversing the list N-1 number of time and then return the node. If list does not have N elements method return null. Nth Node from End

Example 7.18:  Find Nth node from end public int nthNodeFromEnd(int index) {          int size = findLength();          int startIndex;          if (size != 0 && size < index) {                   return Integer.MAX_VALUE;          }          startIndex = size - index + 1;          return nthNodeFromBegining(startIndex); } Analysis: First find the length of list, then nth node from end will be (length – nth +1) node from the beginning. Example 7.19: public int nthNodeFromEnd2(int index) {          int count = 1;          Node forward = head;          Node curr = head;          while (forward != null && count <= index ) {                    count++;                   forward = forward.next;          }          if (forward == null)                   return Integer.MAX_VALUE;          while (forward != null) {                   forward = forward.next;                   curr = curr.next;          }          return curr.value; } Analysis: Second approach is to use two references one is N steps / nodes ahead of the other when forward reference reach the end of the list then the backward reference will point to the desired node. Loop Detect 1.    Traverse through the list. 2.    If the current node is, not there in the Hash-Table then insert it into the Hash-Table. 3.    If the current node is already in the Hashtable then we have a loop. Loop Detect We have to find if there is a loop in the linked list. There are two ways to find if there is a loop in a linked list. One way is called “Slow reference and fast reference approach (SPFP)” the other is called “Reverse list approach”. Both approaches are linear in nature, but still in SPFP approach, we do not require to modify the linked list so it is preferred.

Find if there is a loop in a linked list. If there is a loop, then return 1 if not, then return 0. Use slow reference fast reference approach. Example 7.20: Find if there is a loop in a linked list. If there is a loop, then return true if not, then return false. public boolean loopDetect() {          Node slowPtr;          Node fastPtr;          slowPtr = fastPtr = head;          while(fastPtr.next != null && fastPtr.next.next != null)          {                   slowPtr = slowPtr.next;                   fastPtr = fastPtr.next.next;                   if(slowPtr == fastPtr)                   {                             System.out.println("loop found" );                             return true;                   }          }          System.out.println("loop not found" );          return false; } Analysis:

·         The list is traversed with two references, one is slow reference and another is fast reference. Slow reference always moves one-step. Fast reference always moves two steps. If there is no loop, then control will come out of while loop. So return false. ·         If there is a loop, then there came a point in a loop where the fast reference will come and try to pass slow reference and they will meet at a point. When this point arrives, we come to know that there is a loop in the list. So return true. Reverse List Loop Detect Example 7.21: Find if there is a loop in a linked list. Use reverse list approach. public boolean  reverseListLoopDetect() {          Node  tempHead=head;          reverse();          if(tempHead==head)          {                   reverse();                   System.out.println("loop found" );                   return true;          }          else          {                   reverse();                   System.out.println("loop not found" );                   return false;          } } Analysis: ·         Store reference of the head of list in a temp variable. ·         Reverse the list ·         Compare the reversed list head reference to the current list head reference. ·         If the head of reversed list and the original list are same then reverse the list back and return true. ·         If the head of the reversed list and the original list are not same, then reverse the list back and return false. Which means there is no loop. Loop Type Detect Find if there is a loop in a linked list. If there is no loop, then return 0, if there is loop return 1, if the list is circular then 2. Use slow reference fast reference approach. Example 7.22: public int loopTypeDetect() {          Node slowPtr;          Node fastPtr;          slowPtr = fastPtr = head;

while(fastPtr.next != null && fastPtr.next.next != null)          {                   if(head == fastPtr.next || head==fastPtr.next.next)                   {                             System.out.println("circular list loop found" );                             return 2;                   }                   slowPtr = slowPtr.next;                   fastPtr = fastPtr.next.next;                   if(slowPtr == fastPtr)                   {                             System.out.println("loop found" );                             return 1;                   }          }          System.out.println("loop not found" );          return 0; } Analysis: This program is same as the loop detect program only if it is a circular list than the fast reference reach the slow reference at the head of the list this means that there is a loop at the beginning of the list. Remove Loop Example 7.23: Given there is a loop in linked list remove the loop. public Node loopPointDetect() {          Node slowPtr;          Node fastPtr;          slowPtr = fastPtr = head;          while(fastPtr.next != null &&                             fastPtr.next.next != null)          {                   slowPtr=slowPtr.next;                   fastPtr=fastPtr.next.next;                   if(slowPtr==fastPtr)                   {                             return slowPtr;                   }          }          return null; } public void removeLoop() {          Node loopPoint = loopPointDetect();          if(loopPoint == null)                   return;          Node firstPtr =  head;          if(loopPoint == head)  // circular list case.          {                   while(firstPtr.next != head)

firstPtr = firstPtr.next;                   firstPtr.next = null;                   return;          }          Node secondPtr = loopPoint;          while(firstPtr.next != secondPtr.next) // general loop case.          {                   firstPtr = firstPtr.next;                   secondPtr = secondPtr.next;          }          secondPtr.next = null; } Analysis: ·         Loop through the list by two reference, one fast reference and one slow reference. Fast reference jumps two nodes at a time and slow reference jump one node at a time. The point where these two reference intersect is a point in the loop. ·         If that intersection point is head of the list, this is a circular list case and you need to again traverse through the list and make the node before head point to null. ·         In the other case you need to use two reference variable one start from head and another start form loop point. They both will meet at the point of loop. (You can mathematically prove it ;) ) Find Intersection Example 7.24: public Node findIntersection(Node head, Node head2) {          int l1 = 0;          int l2 = 0;          Node tempHead = head;          Node tempHead2 = head2;          while(tempHead != null)          {                   l1++;                   tempHead = tempHead.next;          }          while(tempHead2 != null)          {                   l2++;                   tempHead2 = tempHead2.next;          }          int diff;          if(l1<12)

{                   Node temp = head;                   head = head2;                   head2 = temp;                   diff = l2-l1;          }          else                   diff = l1-l2;          for(;diff>0;diff--)                   head = head.next;          while(head!=head2)          {                   head = head.next;                   head2 = head2.next;          }          return head; } Analysis: Find length of both the lists. Find the difference of length of both the lists. Increment the longer list by diff steps, and then increment both the lists and get the intersection point.

Doubly Linked List In a Doubly Linked list, there are two references in each node. These references are called prev and next. The prev reference of the node will point to the node before it and the next reference will point to the node next to the given node. Let us look an example of Node, in this example, the value is of type int, but it can be of some other data-type. The two link references are prev and next. Example 7.25: private static class Node {           private int value;           private Node next; private Node prev;           public Node( int v, Node nxt, Node prv)   {                    value = v;                    next = nxt;                    prev = prv;           }           public Node( int v)   {                    value = null;                    next = null;                    prev = null;           } } Search in a single linked list can be only done in one direction. Since all elements in the list has reference to the next item in the list. Therefore, traversal of linked list is linear in nature. In a doubly linked list, we keep track of both head of the linked list and tail of linked list. For a doubly linked list linked list, there are few cases that we need to keep in mind while coding: ·         Zero element case (head and tail both can be modified)

·         Only element case (head and tail both can be modified) ·         First element (head can be modified) ·         General case ·         The last element (tail can be modified) Note: Any program which is likely to change head reference or tail reference is to be passed as a double reference, which is pointing to head or tail reference. Basic operations of Linked List Basic operation of a linked list requires traversing a linked list. The various operations that we can perform on linked lists, many of these operations require list traversal: ·         Insert an element in the list, this operation is used to create a linked list. ·         Print various elements of the list. ·         Search an element in the list. ·         Delete an element from the list. ·         Reverse a linked list. For any linked list there are only three cases zero element, one element, and generally For doubly linked list, we have a few more things 1.    null values (head and tail both can be modified) 2.    Only element (head and tail both can be modified) 3.    First element (head can be modified) 4.    General case 5.    Last element (tail can be modified) Example 7.26: public class DoublyLinkedList {          private static class Node {                   ………………          }          private Node head;          private Node tail;          private int size = 0; } Example 7.27: public int size(){          return size; } Example 7.28: public boolean isEmpty(){          return size == 0; }

Example 7.29: public int peek(){          if(isEmpty())                   throw new IllegalStateException("EmptyListException");          return head.value; } Insert at Head Example 7.30: public void addHead(int value){          Node newNode = new Node(value, null, null);          if(size == 0)                   tail = head = newNode;          else{                   head.prev = newNode;                   newNode.next = head;                   head = newNode;          }          size++; } Analysis: Insert in double linked list is same as insert in a singly linked list. ·         Create a node assign null to prev reference of the node. ·         If the list is empty then tail and head will point to the new node. ·         If the list is not empty then prev of head will point to newNode and next of newNode will

point to head. Then head will be modified to point to newNode. Insert at Tail Example 7.31: Insert an element at the end of the list. public void addTail(int value){          Node newNode = new Node(value, null, null);          if(size == 0)                   head = tail = newNode;          else {                   newNode.prev = tail;                   tail.next = newNode;                   tail = newNode;          }          size++; } Analysis:  Find the proper location of the node and add it to the list. Manage next and prev reference of the node so that list always remain double linked list. Remove Head of doubly linked list Example 7.32: public int removeHead(){          if(isEmpty())            // empty list case.                   throw new IllegalStateException("EmptyListException");            int value = head.value;          head = head.next;          if(head == null)      // single node case.                   tail = null;          else                   head.prev = null;          size--;          return value; } Analysis: ·         If the list is empty then EmptyListException will be raised. ·         Now head will point to its next. ·         If head is null then this was single node list case tail also need to be made null. ·         In all the general case head. Prev will be set to null. ·         Size of list will be reduced by one and value of node is returned. Delete a node given its value

Example 7.33: Delete node in linked list public boolean removeNode(int key){          Node curr = head;          if(curr == null) //empty list                   return false;          if(curr.value == key)//head is the node with value key.          {                    head = head.next;                   size--;                   if(head == null)                             head.prev = null;                   else                             tail = null; // only one element in list.

return true;          }          while(curr.next != null){                   if(curr.next.value == key)                   {                             curr.next = curr.next.next;                             if(curr.next == null)//last element case.                                      tail = curr;                             else                                      curr.next = curr;                             size--;                             return true;                   }                   curr = curr.next;          }          return false; } Analysis: Traverse the list find the node which need to be deleted. Then remove it and adjust next reference of the node previous to it and prev reference of the node next to it. Search list Example 7.34: public boolean isPresent(int key){          Node temp = head;          while(temp != null){                   if(temp.value == key)                             return true;                   temp = temp.next;          }          return false; } Analysis: Traverse the list and find if some value is resent or not. Free List Example 7.35: public void freeList(){          head = null;          tail = null;          size = 0; } Analysis: Just head and tail references need to point to null. The rest of the list will automatically deleted by garbage collection. Print list

Example 7.36: public void print(){          Node temp = head;          while(temp != null){                   System.out.print(temp.value + " ");                   temp = temp.next;          } } Analysis: Traverse the list and print the value of each node. Reverse a doubly linked List iteratively Example 7.37: public void reverseList() {          Node curr=head;          Node tempNode;          while(curr!=null)          {                   tempNode=curr.next;                   curr.next=curr.prev;                   curr.prev = tempNode;                   if(curr.prev == null)                   {                             tail=head;                             head=curr;                             return;                   }                   curr=curr.prev;          }          return; } Analysis: Traverse the list. Swap the next and prev. then traverse to the direction curr.prev which was next before swap. If you reach the end of the list then set head and tail. Copy List Reversed Example 7.38: Copy the content of the list into another list in reverse order. public DoublyLinkedList copyListReversed() {          DoublyLinkedList dll = new DoublyLinkedList();          Node curr = head;          while(curr != null)          {                   dll.addHead(curr.value);                   curr=curr.next;          }

return dll; } Analysis: ·         Create a DoublyLinkedList  class object dll. ·         Traverse through the list and copy the value of the nodes into another list by calling addHead() method. ·         Since the new nodes are added to the head of the list, the new list formed have nodes order reverse there by making reverse list. Copy List Example 7.39: public DoublyLinkedList copyList() {          DoublyLinkedList dll = new DoublyLinkedList();          Node curr = head;          while(curr != null)          {                   dll.addTail(curr.value);                   curr=curr.next;          }          return dll; } Analysis: ·         Create a DoublyLinkedList  class object dll. ·         Traverse through the list and copy the value of the nodes into another list by calling addTail() method. ·         Since the new nodes are added to the tail of the list, the new list formed have nodes order same as the original list. Sorted Insert

Example 7.40: //SORTED INSERT DECREASING public void sortedInsert(int value) {          Node temp= new Node(value);          Node curr=head;          if(curr == null)  //only element          {                   head=temp;                   tail=temp;          }          if(head.value <= value)  //at the begining          {                   temp.next=head;                   head.prev=temp;                   head=temp;            }

while(curr.next != null && curr.next.value > value)  //treversal          {                   curr=curr.next;          }          if(curr.next == null)  //at the end          {                   tail=temp;                   temp.prev=curr;                   curr.next=temp;          }          else  //all other          {                   temp.next=curr.next;                   temp.prev=curr;                   curr.next=temp;                   temp.next.prev=temp;          } } Analysis: ·         We need to consider only element case first. In this case, both head and tail will modify. ·         Then we need to consider the case when head will be modified when new node is added to the beginning of the list. ·         Then we need to consider general cases ·      Finally, we need to consider the case when tail will be modified. Remove Duplicate Example 7.41: Consider the list as sorted remove the repeated value nodes of the list. public void removeDuplicate() {          Node curr = head;          Node deleteMe;          while(curr!=null)     {                   if((curr.next != null) && curr.value==curr.next.value)                   {                             deleteMe=curr.next;                             curr.next=deleteMe.next;                             curr.next.prev=curr;                             if(deleteMe == tail)                                      tail = curr;                   }                   else                             curr=curr.next;          } } Analysis: ·         Remove duplicate is same as single linked list case. ·         Head can never modify only the tail can modify when the last node is removed.

Circular Linked List This type is similar to the singly linked list except that the last element points to the first node of the list. The link portion of the last node contains the address of the first node. Example 7.42: private static class Node {           private int value;           private Node next;           public Node( int v, Node n) {                    value = v;                    next = n;           }           public Node( int v) {                    value = v;                    next = null;           } } Example 7.43: public class CircularLinkedList {           private static class Node {                    // Node methods and fields.           }           private Node tail;           private int size = 0;           public int size(){                    return size;           }           public boolean isEmpty(){                    return size == 0;           }           public int peek(){                    if(isEmpty())                              throw new IllegalStateException("EmptyListException");                    return tail.next.value;           }           // Other methods } Analysis: In the circular linked list, we just need the pointer to the tail node. As head node can be

easily reached from tail node. Size(), isEmpty() and peek() functions remains the same. Insert element in front Example 7.44: public void addHead(int value){          Node temp = new Node(value, null);          if(isEmpty())          {                   tail=temp;                   temp.next = temp;          }          else            {                   temp.next = tail.next;                   tail.next = temp;          }          size++; } public class CircularLinkedListDemo {

public static void main(String[] args) {                   CircularLinkedList ll = new CircularLinkedList();                   ll.addHead(1);                   ll.addHead(2);                   ll.addHead(3);                   ll.addHead(1);                   ll.addHead(2);                   ll.addHead(3);                   ll.print();          } } Analysis: ·         First, we create node with given value and its next pointing to null. ·         If the list is empty then tail of the list will point to it. And the next of node will point to itself. ·         If the list is not empty then the next of the new node will be next of the tail. And tail next will start pointing to the new node. ·         Thus, the new node is added to the head of the list. ·         The demo program create an instance of CircularLinkedList class. Then add some value to it and finally print the content of the list. Insert element at the end Example 7.45: public void addTail(int value){          Node temp = new Node(value, null);          if(isEmpty()) {                   tail=temp;                   temp.next = temp;          }          else{                   temp.next = tail.next;                   tail.next = temp;                   tail=temp;          }          size++; } Analysis: Adding node at the end is same as adding at the beginning. Just need to modify tail reference in place of the head reference. Remove element in the front Example 7.46: public int removeHead() throws IllegalStateException {          if(isEmpty()) {                   throw new IllegalStateException("EmptyListException");          }

int value = tail.next.value;          if(tail == tail.next)    // single node case                   tail=null;          else                   tail.next = tail.next.next;          size--;          return value; } Analysis: ·         If the list is empty then exception will be thrown. Then the value stored in head is stored in local variable value. ·         If tail is equal to its next node that means there is only one node in the list so the tail will become null. ·         In all the other cases, the next of tail will point to next element of the head. ·         Finally, the value is returned. Search element in the list Example 7.47: public boolean isPresent(int data){          Node temp = tail;          for(int i=0;i<size;i++){                   if(temp.value == data)                             return true;                   temp = temp.next;          }          return false; } Analysis: Iterate through the list to find if particular value is there or not. Print the content of list Example 7.48: public void print(){          if(isEmpty()){                   return;          }          Node temp = tail.next;          while(temp != tail){                   System.out.print(temp.value+" ");                   temp = temp.next;          }          System.out.print(temp.value); } Analysis: In circular list, end of list is not there so we cannot check with null. In place of null, tail is used to check end of the list.

Delete List Example 7.49: public void freeList(){          tail = null;          size = 0; } Analysis: The reference to the list is tail. By making tail null, the whole list is deleted. Delete a node given its value Example 7.50: public boolean removeNode(int key){              if(isEmpty()){

return false;          }          Node prev = tail;          Node curr = tail.next;          Node head = tail.next;          if(curr.value == key)//head and single node case.          {                   if(curr == curr.next)//single node case                             tail=null;                   else // head case                             tail.next = tail.next.next;                   return true;          }          prev = curr;          curr =  curr.next;          while(curr != head)          {                   if(curr.value == key)                    {                             if(curr == tail)                                      tail = prev;                             prev.next = curr.next;                             return true;                   }                   prev = curr;                   curr = curr.next;               }          return false; } Analysis: Find the node that need to free. Only difference is that while traversing the list end of list is tracked by the head reference in place of null. Copy List Reversed Example 7.51: public CircularLinkedList copyListReversed() {          CircularLinkedList cl = new CircularLinkedList();          Node curr = tail.next;          Node head = curr;          if (curr != null)          {                   cl.addHead(curr.value);                   curr = curr.next;          }          while (curr != head)          {                   cl.addHead(curr.value);                   curr = curr.next;          }            return cl; }

Analysis: The list is traversed and nodes are added to new list at the beginning. There by making the new list reverse of the given list. Copy List Example 7.52: public CircularLinkedList copyList() {          CircularLinkedList cl = new CircularLinkedList();          Node curr = tail.next;          Node head = curr;          if (curr != null)          {                   cl.addTail(curr.value);                   curr = curr.next;                                  }          while (curr != head)          {                   cl.addTail(curr.value);                   curr = curr.next;          }            return cl; } Analysis: List is traversed and nodes are added to the new list at the end. There by making the list whose value are same as the input list.

Doubly Circular list 1.    For any linked list there are only three cases zero element, one element, general case 2.    To doubly linked list we have a few more things a)     null values b)     Only element (it generally introduces an if statement with null) c)     Always an “if” before “while”. Which will check from this head. d)     General case (check with the initial head kept) e)     Avoid using recursion solutions it makes life harder Example 7.53: private static class Node {           private int value;           private Node next;           private Node prev;           public Node(int v, Node nxt, Node prv) {                    value = v;                    next = nxt;                    prev = prv;           }           public Node(int v) {                    value = v;                    next = this;                    prev = this;           } } public class DoublyCircularLinkedList {           private static class Node {                    ………..           }           private Node head=null;           private Node tail=null;           private int size = 0;           public int size(){                    return size;           }

public boolean isEmpty(){                   return size == 0;          }          public int peekHead(){                   if(isEmpty())                             throw new IllegalStateException("EmptyListException");                   return head.value;          } } Search value Example 7.54: public boolean isPresent(int key){          Node temp = head;          if(head == null)                   return false;          do{                   if(temp.value == key)                             return true;                   temp = temp.next;          }while(temp != head);          return false; } Analysis: Traverse through the list and see if given key is present or not. We use do..while loop as initial state is our termination state too. Delete list Example 7.55: public void freeList(){          head = null;          tail = null;          size = 0; } Analysis: Remove the reference and list will be freed. Print List Example 7.56: public void print() {          if (isEmpty()) {                   return;          }          Node temp = head;          while (temp != tail) {

System.out.print(temp.value + " ");                   temp = temp.next;          }          System.out.print(temp.value); } Analysis: Traverse the list and print its content. Do..while is used as we want to terminate when temp is head. And want to process head node once. Insert Node at head Example 7.57: Insert value at the front of the list. public void addHead(int value) {          Node newNode = new Node(value, null, null);          if(size == 0) {                   tail = head = newNode;                   newNode.next = newNode;                   newNode.prev = newNode;          }          else {                   newNode.next = head;                   newNode.prev = head.prev;                   head.prev = newNode;                   newNode.prev.next = newNode;                   head = newNode;          }          size++; } Analysis: ·         A new node is created and if the list is empty then head and tail will point to it. The newly created newNode’s next and prev also point to newNode. ·         If the list is not empty then the pointers are adjested and a new node is added to the front of the list. Only head need to be changed in this case. ·         Size of the list is increased by one. Insert Node at tail Example 7.58: public void addTail(int value) {          Node newNode = new Node(value, null, null);          if(size == 0) {                   head = tail = newNode;                   newNode.next = newNode;                   newNode.prev = newNode;          }          else {                   newNode.next = tail.next;                   newNode.prev = tail;                   tail.next = newNode;                   newNode.next.prev = newNode;                   tail = newNode;          }

size++; } Analysis: ·         A new node is created and if the list is empty then head and tail will point to it. The newly created newNode’s next and prev also point to newNode. ·         If the list is not empty then the pointers are adjested and a new node is added to the end of the list. Only tail need to be changed in this case. ·         Size of the list is increased by one. Delete head node Example 7.59: public int removeHead(){          if(size == 0)                   throw new IllegalStateException("EmptyListException");          int value = head.value;          size--;          if(size == 0)          {                   head = null;                   tail = null;                   return value;          }          Node next = head.next;          next.prev = tail;          tail.next = next;          head = next;          return value; } Analysis: Delete node in a doubly circular linked list is just same as delete node in a circular linked list. Just few extra next reference need to be adjusted. Delete tail node Example 7.60: public int removeTail(){          if(size == 0)                   throw new IllegalStateException("EmptyListException");          int value = tail.value;          size--;          if(size == 0)          {                   head = null;                   tail = null;                   return value;

}          Node prev = tail.prev;          prev.next = head;          head.prev = prev;          tail=prev;          return value; } Analysis: Delete node in a doubly circular linked list is just same as delete node in a circular linked list. Just few extra prev reference need to be adjusted.

Introduction A stack is a basic data structure that organized items in last-in-first-out (LIFO) manner. Last element inserted in a stack will be the first to be removed from it. The real-life analogy of the stack is "chapattis in hotpot", "stack of plates". Imagine a stack of plates in a dining area everybody takes a plate at the top of the stack, thereby uncovering the next plate for the next person. Stack allow to only access the top element. The elements that are at the bottom of the stack are the one that is going to stay in the stack for the longest time. Computer science also has the common example of a stack. Function call stack is a good example of a stack. Function main() calls function foo() and then foo() calls bar(). These function calls are implemented using stack first bar() exists, then go() and then finally main(). As we navigate from web page to web page, the URL of web pages are kept in a stack, with the current page URL at the top. If we click back button, then each URL entry is popped one by one.

The Stack Abstract Data Type Stack abstract data type is defined as a class, which follows LIFO or last-in-first-out for the elements, added to it. The stack should support the following operation: 1.    Push():  which add a single element at the top of the stack 2.    Pop():  which remove a single element from the top of a stack. 3.    Top():  Reads the value of the top element of the stack (does not remove it) 4.    isEmpty(): Returns 1 if stack is empty 5.    Size(): returns the number of elements in a stack. Add n to the top of a stack void push(int  value);. Remove the top element of the stack and return it to the caller function. int  pop(); The stack can be implemented using an array or a linked list. In array case, there are two types of implementations ·         One in which array size is fixed, so it the capacity of the stack. ·         Another approach is variable size array in which memory of the array is allocated using malloc and when the array is filled the size if doubled using realloc (when the stack size decreases below half the capacity is again reduced).

In case of a linked list, there is no such limit on the number of elements it can contain. When a stack is implemented, using an array top of the stack is managed using an index variable called top. When a stack is implemented using a linked list, push() and pop() is implemented using insert at the head of the linked list and remove from the head of the linked list.

Stack using Array Implement a stack using a fixed length array. Example 8.1: public class Stack {           private static final int CAPACITY = 1000;           private int[ ] data;           private int top = -1;           public Stack() {                    this(CAPACITY);           }           public Stack(int capacity) {                    data = new int[capacity];           } } If user does not provide the max capacity of the array. Then an array of 1000 elements is created. The top is the index to the top of the stack. Number of elements in the stack is governed by the “top” index and top is initialized to -1 when a stack is initialized. Top index value of -1 indicates that the stack is empty in the beginning. public boolean isEmpty(){           return (top == -1); } isEmpty() function returns 1 if stack is empty or 0 in all other cases. By comparing the top index value with -1. public int size() {           return (top + 1); } size() function returns the number of elements in the stack. It just returns "top+1". As the top is referring the array index of the stack top variable so we need to add one to it. public void print(){           for(int i=top;i>-1;i--)                    System.out.print(" "+ data[i]); } The print function will print the elements of the array. public void push(int value) throws IllegalStateException {           if (size( ) == data.length)                    throw new IllegalStateException("StackOvarflowException");

top++;          data[top] = value; } push() function checks whether the stack has enough space to store one more element, then it increases the "top" by one. Finally sort the data in the stack "data" array. In case, stack is full then "stack overflow" message is printed and that value will not be added to the stack and will be ignored. public int pop( ) {          if (isEmpty( ))                   throw new IllegalStateException("StackEmptyException");          int topVal = data[top];          top--;          return topVal; } The pop() function is implemented, first it will check that there are some elements in the stack by checking its top index. If some element is there in the stack, then it will store the top most element value in a variable "value". The top index is reduced by one. Finally, that value is returned. public int top( ) throws IllegalStateException {          if (isEmpty( ))                   throw new IllegalStateException("StackEmptyException");          return data[top]; } top()function returns the value of stored in the top element of stack (does not remove it) public static void main(String[] args) {          Stack s = new Stack(1000);          for(int i=1;i<=100;i++)          {                   s.push(i);          }          for(int i=1;i<=50;i++)          {                   s.pop();          }          s.print(); } Analysis: ·         The user of the stack will create a stack local variable. ·         Use push() and pop() functions to add / remove variables to the stack. ·         Read the top element using the top() function call. ·         Query regarding size of the stack using size() function call ·         Query if stack is empty using isEmpty() function call

Stack using Array (Growing-Reducing capacity implementation) In the above dynamic array implementation of a stack. Make the capacity of stack variable so that when it is nearly filled, then double the capacity of the stack. Example 8.2: public class Stack {           private static final int MIN_CAPACITY = 1000;           private int[ ] data;           private int top = -1;           private int minCapacity;           private int maxCapacity;           public Stack() {                    this(MIN_CAPACITY);                    maxCapacity = minCapacity = MIN_CAPACITY;           }           public Stack(int capacity) {                    data = new int[capacity];                    maxCapacity = minCapacity = capacity;           } } public void push(int value) throws IllegalStateException {           if (size( ) == maxCapacity)           {                    System.out.println("size dubbelled");                    int[] newData = new int[maxCapacity*2];                    System.arraycopy(data,0,newData,0,maxCapacity);                          data=newData;                    maxCapacity=maxCapacity*2;           }           top++;           data[top] = value; } public int pop( ) {           if (isEmpty( ))                    throw new IllegalStateException("StackEmptyException");           int topVal = data[top];           top--;           if(size() == maxCapacity/2  &&  maxCapacity > minCapacity)           {                    System.out.println("size halfed");                    maxCapacity = maxCapacity/2;                    int[] newData = new int[maxCapacity];                    System.arraycopy(data,0,newData,0,maxCapacity);                          data=newData;           }           return topVal; }

Analysis: ·         In the push() function we double the size of the stack when stack is full. First, an array of double capacity is created. All the data of the old array is copied to the new array. Finally the reference to the array of the stack is changed to the newly array. ·         In the pop() function when size of the stack is half the max capacity of the stack and is grater then the minimum threshold then the an array of half size is created and the value of the old bigger array is copied to the newly created array. ·         Apart from push() and pop() all the other functions are same as the before fixed size implementation of stack.

Stack using linked list Example 8.3: Implement stack using a linked list. public class Stack {           private static class Node {                    private int value;                    private Node next;                    public Node( int v, Node n) {                              value = v;                              next = n;                    }           }           private Node head = null;           private int size = 0;           //Other Methods       } public int size() {           return size; } public boolean isEmpty() {           return size == 0; } public int peek() throws IllegalStateException {           if(isEmpty())                    throw new IllegalStateException("StackEmptyException");           return head.value; } public void push(int value) {           head = new Node(value, head);           size++; } public int pop() throws IllegalStateException {           if(isEmpty())                    throw new IllegalStateException("StackEmptyException");           int value = head.value;           head = head.next;           size--;           return value; } public void insertAtBottom(int value) {           if (isEmpty())                    push(value);           else           {                    int temp = pop();                    insertAtBottom(value);                    push(temAp);           } }

public void print(){          Node temp = head;          while(temp != null){                   System.out.print(temp.value +" ");                   temp = temp.next;          } } public static void main(String[] args) {          Stack s = new Stack();          for(int i=1;i<=100;i++)                   s.push(i);          for(int i=1;i<=50;i++)                   s.pop();          s.print(); } Analysis: ·         Stack implemented using a linked list is simply insertion and deletion at the head of a singly  linked list. ·         In push() function, memory is created for one node. Then the value is stored into that node. Finally, the node is inserted at the beginning of the list. ·         In pop() function, the head of the linked list starts pointing to the second node there by releasing the memory allocated to the first node (Garbage collection.).

Problems in Stack Balanced Parenthesis Example 8.4: Stacks can be used to check a program for balanced symbols (such as {}, (), []). The closing symbol should be matched with the most recently seen opening symbol. Example: {()} is legal, {() ({})} is legal, but {((} and {(}) are not legal public static boolean isBalancedParenthesis (String expn) {           ArrayDeque<Character> stk = new ArrayDeque<Character>();           for (char ch : expn.toCharArray( )) {                    switch (ch) {                    case '{':                    case '[':                    case '(':                              stk.push(ch);                              break;                    case '}':                              if (stk.pop() != '{')                                       return false;                              break;                    case ']':                              if (stk.pop() != '[')                                       return false;                              break;                    case ')':                              if (stk.pop() != '(')                                       return false;                              break;                    }           }           return stk.isEmpty(); } public static void main(String[] args) {           String expn = "{()}[";           boolean value = isBalancedParenthesis (expn);           System.out.println("Given Expn:"+expn);           System.out.println("Result after isParenthesisMatched:"+value); } Analysis: ·         Traverse the input string when we get an opening parenthesis we push it into stack. And when we get a closing parenthesis then we pop a parenthesis from the stack and compare if it is the corresponding to the one on the closing parenthesis. ·         We return false if there is a mismatch of parenthesis. ·         If at the end of the whole staring traversal, we reached to the end of the string and the stack is empty then we have balanced parenthesis.

Infix, Prefix and Postfix Expressions When we have an algebraic expression like A + B then we know that the variable is being added to variable B. This type of expression is called infix expression because the operator “+” is between operands A and operand B. Now consider another infix expression A + B * C. In the expression there is a problem that in which order + and * works. Does A and B are added first and then the result is multiplied. Or B and C are multiplied first and then the result is added to A. This makes the expression ambiguous. To deal with this ambiguity we define the precedence rule or use parentheses to remove ambiguity. So if we want to multiply B and C first and then add the result to A. Then the same expression can be written unambiguously using parentheses as A + (B * C). On the other hand, if we want to add A and B first and then the sum will be multiplied by C we will write it as (A + B) * C. So in the infix expression to make the expression unambiguous, we need parenthesis. Infix expression: In this notation, we place operator in the middle of the operands. < Operand > < operator > < operand > Prefix expressions: In this notation, we place operator at the beginning of the operands. < Operator > < operand > < operand > Postfix expression: In this notation, we place operator at the end of the operands. < Operand > < operand > < operator > Infix Expression Prefix Expression Postfix Expression A + B + A B A B + A + (B * C) + A * B C A B C * + (A + B) * C * + ABC A B + C * Now comes the most obvious question why we need so unnatural Prefix or Postfix expressions when we already have infix expressions which words just fine for us. The answer to this is that infix expressions are ambiguous and they need parenthesis to make them unambiguous. While postfix and prefix notations do not need any parenthesis. Infix-to-Postfix Conversion Example 8.5: public static char[] infixToPostfix(char[] expn)

{          ArrayDeque<Character> stk = new ArrayDeque<Character>();          String output="";          char out;          for (char ch : expn) {                   if( ch <= '9' && ch >= '0')                   {                             output = output + ch  ;                   }                   else                   {                             switch (ch)                             {                             case '+':                             case '-':                             case '*':                             case '/':                             case '%':                             case '^':                                      while (stk.isEmpty() == false && precedence(ch) <= precedence(stk.peek()))                                      {                                                out = stk.pop();                                                output = output + " " + out ;                                      }                                      stk.push(ch);                                      output = output + " ";                                      break;                             case '(':                                      stk.push(ch);                                      break;                             case ')':                                      while (stk.isEmpty() == false && (out = stk.pop()) != '(')                                      {                                                output = output + " " + out + " ";                                      }                                      break;                             }                   }          }          while(stk.isEmpty() == false)          {                   out = stk.pop();                   output = output + out + " ";          }          return output.toCharArray(); } public static String infixToPostfix(String expn) {          String output="";          char[] out = infixToPostfix(expn.toCharArray( ));          for(char ch : out)                   output = output + ch ;          return output; } public static void main(String[] args) {          String expn = "10+((3))*5/(16-4)";          String value = infixToPostfix (expn);

System.out.println("Infix Expn: "+expn);          System.out.println("Postfix Expn: "+value); } Analysis: ·         Print operands in the same order as they arrive. ·         If the stack is empty or contains a left parenthesis “(” on top, we should push the incoming operator in the stack. ·         If the incoming symbol is a left parenthesis ”(”, push left parenthesis in the stack. ·         If the incoming symbol is a right parenthesis “)”, pop from the stack and print the operators till you see a left parenthesis “)”. Discard the pair of parentheses. ·         If the precedence of incoming symbol is higher than the operator at the top of the stack, then push it to the stack. ·         If the incoming symbol has, an equal precedence compared to the top of the stack, use association. If the association is left to right, then pop and print the symbol at the top of the stack and then push the incoming operator. If the association is right to left, then push the incoming operator. ·         If the precedence of incoming symbol is lower than the operator on the top of the stack, then pop and print the top operator. Then compare the incoming operator against the new operator at the top of the stack. ·         At the end of the expression, pop and print all operators on the stack. Infix-to-Prefix Conversion Example 8.6: public static String infixToPrefix(String expn) {          char[] arr = expn.toCharArray();          reverseString(arr);          replaceParanthesis(arr);          arr=infixToPostfix(arr);          reverseString(arr);          expn = new String(arr);          return expn; } public static void replaceParanthesis(char[] a) {          int lower=0;          int upper=a.length-1;          while(lower<=upper)          {                   if(a[lower] == '(')                             a[lower] = ')';                   else if(a[lower] == ')')                             a[lower] = '(';                   lower++;          } }

public static void reverseString(char[] expn) {          int lower=0;          int upper=expn.length - 1;          char tempChar;          while(lower<upper)          {                   tempChar=expn[lower];                   expn[lower]=expn[upper];                   expn[upper]=tempChar;                   lower++;                   upper--;          } } public static void main(String[] args) {          String expn = "10+((3))*5/(16-4)";          String value = infixToPrefix (expn);          System.out.println("Infix Expn: "+expn);          System.out.println("Prefix Expn: "+value); } Analysis: 1. Reverse the given infix expression. 2. Replace '(' with ')' and ')' with '(' in the reversed expression. 3. Now apply infix to postfix subroutine already discussed. 4. Reverse the generated postfix expression and this will give required prefix expression.  Postfix Evaluate Write a postfixEvaluate() function to evaluate a postfix expression. Such as:  1 2 + 3 4 + * Example 8.7: public static int postfixEvaluate(String expn) {          ArrayDeque<Integer> stk = new ArrayDeque<Integer>();          Scanner tokens = new Scanner(expn);          while(tokens.hasNext()){                   if(tokens.hasNextInt()){                             stk.push(tokens.nextInt());                   }                   else{                             int num1 = stk.pop();                             int num2 = stk.pop();                             char op=tokens.next().charAt(0);                             switch (op)                             {                             case '+':                                      stk.push(num1 + num2);                                      break;                             case '-':                                      stk.push(num1 - num2);                                      break;                             case '*':                                      stk.push(num1 * num2);                                      break;

case '/':                                      stk.push(num1 / num2);                                      break;                             }                   }          }          tokens.close();          return stk.pop(); } public static void main3(String[] args) {          String expn = "6 5 2 3 + 8 * + 3 + *";          int value = postfixEvaluate (expn);          System.out.println("Given Postfix Expn: "+expn);          System.out.println("Result after Evaluation: "+value); } Analysis: 1) Create a stack to store values or operands. 2) Scan through the given expression and do following for each element: a)    If the element is a number, then push it into the stack. b)    If the element is an operator, then pop values from the stack. Evaluate the operator over the values and push the result into the stack. 3) When the expression is scanned completely, the number in the stack is the result. Min stack Design a stack in which get minimum value in stack should also work in O(1) Time Complexity. Hint:  Keep two stack one will be general stack, which will just keep the elements. The second will keep the min value. 1.    Push: Push an element to the top of stack1. Compare the new value with the value at the top of the stack2. If the new value is smaller, then push the new value into stack2. Or push the value at the top of the stack2 to itself once more.  2.    Pop: Pop an element from top of stack1 and return. Pop an element from top of stack2 too. 3.    Min: Read from the top of the stack2 this value will be the min. Palindrome string Find if given string is a palindrome or not using a stack. Definition of palindrome: A palindrome is a sequence of characters that is same backward or forward. Eg. “AAABBBCCCBBBAAA”, “ABA” & “ABBA” Hint:  Push characters to the stack till the half length of the string. Then pop these characters and then compare. Make sure you take care of the odd length and even length.

Reverse Stack Given a stack how to reverse the elements of the stack without using any other data-structure. You cannot use another stack too. Time Complexity and Space Complexity is wrong, it is O(n) for both cases. Hint:  Use recursion (system stack.) When you go inside the stack pop elements from stack in each subsequent call until stack is empty. Then push these elements one by one when coming out of the recursion. The elements will be reversed. Example 8.8: public static <T> void reverseStack (ArrayDeque<T> stk) {          if(stk.isEmpty())          {                   return;          }          else          {                   T value = stk.pop();                   reverseStack(stk);                   insertAtBottom(stk, value);          } } Insert At Bottom Example 8.9: public static <T> void insertAtBottom (ArrayDeque<T> stk, T value) {          if(stk.isEmpty()) {                   stk.push(value);          }          else {                   T out = stk.pop();                   insertAtBottom(stk, value);                   stk.push(out);          } } Depth-First Search with a Stack In a depth-first search, we traverse down a path until we get a dead end; then we backtrack by popping a stack to get an alternative path. ·         Create a stack ·         Create a start point ·         Push the start point onto the stack ·         While (value searching not found and the stack is not empty)

o   Pop the stack o   Find all possible points after the one which we just tried o   Push these points onto the stack Stack using a queue How to implement a stack using a queue. Analyze the running time of the stack operations. See queue chapter for this. Stock Span Problem Given a list of daily stock price in an array A[i].  Find the span of the stocks for each day. A span of stock is the maximum number of days for which the price of stock was lower than that day. Example 8.10: Approach 1 public static int[] StockSpanRange(int[] arr) {          int[] SR = new int[arr.length];          SR[0] = 1;          for (int i = 1; i < arr.length; i++) {                   SR[i] = 1;                   for (int j = i - 1; (j >= 0) && (arr[i] >= arr[j]); j--)                             SR[i]++;          }          return SR; } Example 8.11: Approach 2: int[] StockSpanRange2 (int[] arr) {          ArrayDeque<Integer> stk = new ArrayDeque<Integer>();          int[] SR = new int[arr.length];          stk.push(0);          SR[0] = 1;          for (int i = 1; i < arr.length; i++) {                   while (!stk.isEmpty() && arr[stk.peek()]<= arr[i])                             stk.pop();                   SR[i] = (stk.isEmpty()) ? (i + 1) : (i - stk.peek());                   stk.push(i);

}          return SR; } Get Max Rectangular Area in a Histogram Given a histogram of rectangle bars of each one unit wide. Find the maximum area rectangle in the histogram. Example 8.12: Approach 1 public static int GetMaxArea(int[] arr) {          int size = arr.length;          int maxArea = -1;          int currArea;          int minHeight = 0;          for (int i = 1; i < size; i++)          {                   minHeight = arr[i];                   for (int j = i - 1; j >= 0; j--)                   {                             if (minHeight > arr[j])                                      minHeight = arr[j];                             currArea = minHeight * (i - j + 1);                             if (maxArea < currArea)                                      maxArea = currArea;                   }          }          return maxArea; } Approach 2: Divide and conquer Example 8.13: Approach 3 public static int GetMaxArea2(int[] arr) {          int size=arr.length;          ArrayDeque<Integer> stk = new ArrayDeque<Integer>();

int maxArea = 0;          int top;          int topArea;          int i = 0;          while (i < size)          {                   while ( (i < size) && (stk.isEmpty() || arr[stk.peek()] <= arr[i]) )                   {                             stk.push(i);                             i++;                   }                    while ( !stk.isEmpty() && ( i == size || arr[stk.peek()] > arr[i]) )                   {                             top = stk.peek();                             stk.pop();                             topArea = arr[top] * (stk.isEmpty() ? i : i - stk.peek() - 1);                             if (maxArea < topArea)                                      maxArea = topArea;                   }          }          return maxArea; } Two stacks using single array Example 8.14: How to implement two stacks using one single array. public class TwoStack {          private final int  MAX_SIZE = 50;          int top1;          int top2;          int[] data;          public TwoStack() {                   top1 = -1;                   top2 = MAX_SIZE;                   data = new int[MAX_SIZE];          }          //Other methods. } public void StackPush1 (int value) {          if (top1 < top2 - 1)          {                   data[++top1] = value;          }          else                   System.out.print("Stack is Full!"); } public void StackPush2 (int value) {          if (top1 < top2 - 1)          {                   data[--top2] = value;          }          else                   System.out.print("Stack is Full!");

} public int StackPop1 () {          if (top1 >= 0)          {                   int value = data[top1--];                   return value;          }          else                   System.out.print("Stack Empty!");          return -999; } public int StackPop2 () {          if (top2 < MAX_SIZE)          {                    int value = data[top2++];                   return value;          }          else                   System.out.print("Stack Empty!");          return -999; } public static void main(String[] args) {          TwoStack st = new TwoStack();          for(int i=0;i<10;i++)                   st.StackPush1(i);          for(int j=0;j<10;j++)                   st.StackPush2(j+10);          for(int i=0;i<10;i++)          {                   System.out.println("stack one pop value is : " + st.StackPop1());                   System.out.println("stack two pop value is : " + st.StackPop2());          }        } Analysis: Same array is used to implement two stack. First stack is filled from the beginning of the array and second stack is filled from the end of the array. Overflow and underflow conditions need to be taken care of carefully.

Introduction A queue is a basic data structure that organized items in first-in-first-out (FIFO) manner. First element inserted into a queue will be the first to be removed. It is also known as "first-come-firstserved". The real life analogy of queue is typical lines in which we all participate time to time. ·         We wait in a line of railway reservation counter. ·         We wait in the cafeteria line (to pop a plate from “stack of plates”). ·         We wait in a queue when we call to some customer case. The elements, which are at the front of the queue, are the one that stayed in the queue for the longest time. Computer science also has common examples of queues. We issue a print command from our office to a single printer per floor, print task gets lined up in a printer queue. The print command that was issued first will be printed before the next commands in line. In addition to printing queues, operating system is also using different queues to control process scheduling. Processes are added to processing queue, which is used by an operating system for various scheduling algorithms. Soon we will be reading about graphs and will come to know about breadth-first traversal, which uses a queue.

The Queue Abstract Data Type Queue abstract data type is defined as a class whose object, follows FIFO or first-in-first-out for the elements, added to it. Queue should support the following operation: 1.    add(): Which add a single element at the back of a queue 2.    remove(): Which remove a single element from the front of a queue. 3.    isEmpty(): Returns 1 if the queue is empty 4.    size(): Returns the number of elements in a queue.

Queue Using Array Example 9.1: public class Queue {           private int size;           private int Capacity=100;           private int[] data;           int front=0;           int back=0;           public Queue() {                    size = 0;                    data = new int[100];           } } boolean isEmpty() {           return size == 0; } int size() {           return size; } public boolean add(int value) {           if (size >= Capacity )           {                    System.out.println("Queue is full.");                    return false;           }           else           {                    size++;                    data[back] = value;

back = (++back) % (Capacity - 1);          }          return true; } public int remove() {          int value;          if (size <= 0)          {                   System.out.println("Queue is empty.");                   return -999;          }          else          {                   size--;                   value = data[front];                   front = (++front) % (Capacity - 1);          }          return value; } public static void main(String[] args) {          Queue que = new Queue();          for (int i = 0; i < 20; i++)          {                   que.add(i);          }          for (int i = 0; i < 22; i++)          {                   System.out.println(que.remove());          } } Analysis: 1.    Hear queue is created from an array of size 100. 2.    The number of element in queue to zero. By assigning front, back and size of queue to zero. 3.    Add() insert one element at the back of the queue. 4.    Remove() delete one element from the front of the queue.

Queue Using linked list Example 9.2: public class Queue {           private static class Node {                    private int value;                    private Node next;                    public Node( int v, Node n) {                              value = v;                              next = n;                    }           }           private Node head = null;           private Node tail = null;           private int size = 0;           //Other Methods. } public int size() {           return size; } public boolean isEmpty(){           return size == 0; } public void print(){           Node temp = head;           while(temp != null){                    System.out.print(temp.value +" ");                    temp = temp.next;           } } public int peek() throws IllegalStateException {           if(isEmpty())                    throw new IllegalStateException("QueueEmptyException");           return head.value } Add Enqueue into a queue using linked list. Nodes are added to the end of the linked list. Below diagram indicates how a new node is added to the list. The tail is modified every time when a new value is added to the queue. However, the head is also updated in the case when there is no element in the queue and when that first element is added to the queue both head and tail will be pointing to it.

Example 9.3: public void add(int value) {          Node temp = new Node(value, null);          if(head == null)                   head = tail = temp;          else{                   tail.next = temp;                   tail = temp;          }          size++; } Analysis: add operation add one element at the end of the Queue (linked list). Remove

In this we need the tail reference as it may be the case there was only one element in the list and the tail reference will also be modified in case of the remove. Example 9.4: public int remove() throws IllegalStateException {          if(isEmpty())                   throw new IllegalStateException("QueueEmptyException");          int value = head.value;          head = head.next;          size--;          return value; } public static void main(String[] args) {          Queue q = new Queue();          for(int i=1;i<=100;i++)          {                   q.add(i);          }          for(int i=1;i<=50;i++)          {                   q.remove();          }          q.print(); }        Analysis: Remove operation removes first node from the start of the queue( linked list).

Introduction We have already read about various linear data structures like an array, linked list, stack, queue etc. Both array and linked list have a drawback of linear time required for searching an element. A tree is a nonlinear data structure, which is used to represent hierarchical relationships (parentchild relationship). Each node is connected by another node by directed edges. Example 1: Tree in organization Example 2: Tree in a file system

Terminology in tree Root: The root of the tree is the only node in the tree that has no incoming edges. It is the top node of a tree. Node: It is a fundamental element of a tree. Each node has data and two references that may point to null or its child’s. Edge: It is also a fundamental part of a tree, which is used to connect two nodes. Path: A path is an ordered list of nodes that are connected by edges. Leaf: A leaf node is a node that has no children. Height of the tree: The height of a tree is the number of edges on the longest path between the root and a leaf. The level of node: The level of a node is the number of edges on the path from the root node to that node.

Children: Nodes that have incoming edges from the same node to be said to be the children of that node. Parent: Node is a parent of all the child nodes that are linked by outgoing edges. Sibling: Nodes in the tree that are children of the same parent are said to be siblings’ Ancestor:  A node reachable by repeated moving from child to parent.

Binary Tree A binary tree is a type tree in which each node has at most two children (0, 1 or 2), which are referred to as the left child and the right child. Below is a node of the binary tree with "a" stored as data and whose left child (lChild) and whose right child (rchild) both pointing towards null. Below is a class definition used to define node. public class Tree {           private static class Node {                    private int value;                    private Node lChild;                    private Node rChild;                    public Node( int v, Node l, Node r) {                              value = v;                              lChild = l;                              rChild = r;                    }                    public Node( int v )  {                              value = v;                              lChild = null;                              rChild = null;                    }           }           private Node root;           public Tree() {                    root = null;           } } Below is a binary tree whose nodes contains data from 1 to 10

In the rest of the book binary tree will be represented as below: Properties of Binary tree are: 1.    The maximum number of nodes on level i of a binary tree is  , where i >= 1 2.    The maximum number of nodes in a binary tree of depth k is  , where k >= 1 3.    There is exactly one path from the root to any nodes in a tree. 4.    A tree with N nodes have exactly N-1 edges connecting these nodes. 5.    The height of a complete binary tree of N nodes is .

Types of Binary trees Complete binary tree In a complete binary tree, every level except the last one is completely filled. All nodes in the left are filled first, then the right one. A binary heap is an example of a complete binary tree. Full/ Strictly binary tree The full binary tree is a binary tree in which each node has exactly zero or two children. Perfect binary tree The perfect binary tree is a type of full binary tree in which each non leaf node has exactly two child nodes. All leaf nodes have identical path length and all possible node slots are occupied

Right skewed binary tree A binary tree in which each node is having either only a right child or no child (leaf) is called as right skewed binary tree Left skewed binary tree A binary tree in which each node is having either only a left child or no child (leaf) is called as Left skewed binary tree Height-balanced Binary Tree A height-balanced binary tree is a binary tree such that the left & right subtrees for any given node

differ in height by max one. Note: Each complete binary tree is a height-balanced binary tree AVL tree and RB tree are an example of height balanced tree we will discuss these trees in advance tree topic.

Problems in Binary Tree Create a Complete binary tree Create a binary tree given a list of values in an array. Solution: Since there is no order defined in a binary tree, so nodes can be inserted in any order so it can be a skewed binary tree. But it is inefficient to do anything in a skewed binary tree so we will create a Complete binary tree. At each node, the middle value stored in the array is assigned to node and left of array is passed to the left child of the node to create left sub-tree. And right portion of array is passed to right child of the node to crate right sub-tree. Example 10.1: public void levelOrderBinaryTree(int[] arr){           root = levelOrderBinaryTree(arr, 0); } public Node levelOrderBinaryTree(int[] arr, int start) {           int size = arr.length;           Node curr = new Node(arr[start]);           int left = 2 * start + 1;           int right = 2 * start + 2;           if (left < size)                    curr.lChild = levelOrderBinaryTree(arr, left);           if (right < size)                    curr.rChild = levelOrderBinaryTree(arr, right);           return curr; } public static void main(String[] args){           Tree t = new Tree();           int[] arr = {1,2,3,4,5,6,7,8,9,10};           t.levelOrderBinaryTree(arr);           t.Print(); } Complexity Analysis: This is an efficient algorithm for creating a complete binary tree. Time Complexity:                       Space Complexity:  Pre-Order Traversal Traversal is a process of visiting each node of a tree. In Pre-Order Traversal parent is visited/traversed first, then left child and right child. Pre-Order traversal is a type of depth-first traversal.

Solution: Preorder traversal is done using recursion. At each node, first the value stored in it is printed and then followed by the value of left child and right child. At each node its value is printed followed by calling printTree() function to its left and right child to print left and right sub-tree. Example 10.2: public void PrintPreOrder(){          PrintPreOrder(root); } private void  PrintPreOrder(Node node) {          if(node != null)          {                   System.out.print(" "+ node.value);                   PrintPreOrder(node.lChild);                   PrintPreOrder(node.rChild);          } } Output: 6 4 2 1 3 5 8 7 9 10 Complexity Analysis:  Time Complexity:                     Space Complexity:  Note: When there is an algorithm in which all nodes are traversed, then complexity can’t be less then . When there is a large portion of the tree, which is not traversed, then complexity reduces. Post-Order Traversal In Post-Order Traversal left child is visited/traversed first, then right child and last parent

Post-Order traversal is a type of depth-first traversal. Solution: In post order traversal, first, the left child is traversed then right child and in the end, current node value is printed to the screen. Example 10.3: public void PrintPostOrder(){          PrintPostOrder(root); } private void  PrintPostOrder(Node node)/*   post order  */ {          if(node != null)          {                   PrintPostOrder(node.lChild);                   PrintPostOrder(node.rChild);                   System.out.print(" "+ node.value);          } } Output: 1 3 2 5 4 7 10 9 8 6 Complexity Analysis: Time Complexity:                      Space Complexity:  In-Order Traversal In In-Order Traversal left child is visited/traversed first, then the parent and last right child In-Order traversal is a type of depth-first traversal. The output of In-Order traversal of BST is a sorted list. Solution: In In-Order traversal first, the value of left child is traversed, then the value of node is printed to the screen and then the value of right child is traversed.

Example 10.4: public void PrintInOrder(){          PrintPostOrder(root); } private void  PrintInOrder(Node node)/*   In order  */ {          if(node != null)          {                   PrintPostOrder(node.lChild);                   System.out.print(" "+ node.value);                            PrintPostOrder(node.rChild);          } } Output: 1 2 3 4 5 6 7 8 9 10 Complexity Analysis: Time Complexity:                      Space Complexity:  Note: Pre-Order, Post-Order, and In-Order traversal are for all binary trees. They can be used to traverse any kind of a binary tree. Level order traversal / Breadth First traversal Write code to implement level order traversal of a tree. Such that nodes at depth k is printed before nodes at depth k+1.

Solution: Level order traversal or Breadth First traversal of a tree is done using a queue. At the start, the root node reference is added to queue. The traversal of tree happens until its queue is empty. When we traverse the tree, we first remove an element from the queue, print the value stored in that node and then its left child and right child will be added to the queue. Example 10.5: public void PrintBredthFirst(){          ArrayDeque<Node> que = new ArrayDeque<Node>();          Node temp;          if(root != null)                   que.add(root);          while(que.isEmpty() == false)          {                   temp= que.remove();                   System.out.println(temp.value);                   if(temp.lChild != null)                             que.add(temp.lChild);                   if(temp.rChild != null)                             que.add(temp.rChild);          } } Complexity Analysis: Time Complexity:          Space Complexity:  Print Depth First without using the recursion / system stack. Solution:  Depth first traversal of the tree is done using recursion by using system stack. The same can be done using stack. In the start root node reference is added to the stack. The whole tree is traversed until the stack is empty. In each iteration, an element is popped from the stack its value is printed to screen. Then right child and then left child of the node is added to stack.

Example 10.6: public void PrintDepthFirst(){          ArrayDeque<Node> stk = new ArrayDeque<Node>();          Node temp;            if(root != null)                    stk.push(root);          while(stk.isEmpty() == false)          {                   temp= stk.pop();                   System.out.println(temp.value);                   if(temp.lChild != null)                             stk.push(temp.lChild);                   if(temp.rChild != null)                             stk.push(temp.rChild);          } } Complexity Analysis: Time Complexity:                      Space Complexity:  Tree Depth Solution: Depth is tree is calculated recursively by traversing left and right child of the root. At each level of traversal depth of left child is calculated and depth of right child is calculated. The greater depth among the left and right child is added by one (which is the depth of the current node) and this value is returned. Example 10.7: private int TreeDepth(Node root) {          if(root == null)                   return 0;          else          {                   int lDepth=TreeDepth(root.lChild);                   int rDepth=TreeDepth(root.rChild);                   if(lDepth>rDepth)                             return lDepth+1;                   else                             return rDepth+1;          } } public int TreeDepth(){          return TreeDepth(root); } Complexity Analysis:  Time Complexity:                     Space Complexity:  Nth Pre-Order

Solution:  We want to print the node which will be at the nth index when we print the tree in PreOrder traversal. So we keep a counter to keep track of the index. When the counter is equal to index, then we print the value and return the Nth preorder index node. Example 10.8: public void NthPreOrder(int index){          NthPreOrder(root, index, 0); } private void  NthPreOrder(Node node, int index, int counter)   /* pre order  */ {          if(node != null)          {                   counter++;                   if(counter == index)                   {                             System.out.print(" "+ node.value);                   }                   NthPreOrder(node.lChild, index, counter);                   NthPreOrder(node.rChild, index, counter);          } } Complexity Analysis:   Time Complexity:                    Space Complexity:  Nth Post Order Solution:  We want to print the node that will be at the nth index when we print the tree in post order traversal. So we keep a counter to keep track of the index, but at this time we will increment the counter after left child and right child traversal. When the counter is equal to index, then we print the value and return the nth post order index node. Example 10.9 public void NthPostOrder(int index) {          NthPostOrder(root, index, 0); } private void  NthPostOrder(Node node, int index, int counter) {          if(node != null)          {                   NthPostOrder(node.lChild,index, counter);                   NthPostOrder(node.rChild,index, counter);                   counter++;                   if(counter == index)                   {                             System.out.print(" "+ node.value);                   }          } }

Complexity Analysis: Time Complexity:                        Space Complexity:  Nth In Order Solution: We want to print the node which will be at the nth index when we print the tree in in-order traversal. So we keep a counter to keep track of the index, but at this time we will increment the counter after left child traversal but before the right child traversal. When the counter is equal to index, then we print the value and return the nth in-order index node. Example 10.10: public void NthInOrder(int index){          NthInOrder(root, index, 0); } private void  NthInOrder(Node node, int index, int counter) {          if(node != null)          {                   NthInOrder(node.lChild, index, counter);                   counter++;                   if(counter == index)                   {                             System.out.print(" "+ node.value);                   }                   NthInOrder(node.rChild, index, counter);          } } Complexity Analysis: Time Complexity:                        Space Complexity:  Copy Tree Solution: Copy tree is done by copy nodes of the input tree at each level of the traversal of the tree. At each level of the traversal of nodes of tree a new node is created and the value of the input tree node is copied to it. The left child tree is copied recursively and then reference to new subtree is returned will be assigned to the left child of the current new node. Similarly for the right child too. Finally, the tree is copied. Example 10.11: public Tree CopyTree(){          Tree tree2 = new Tree();          tree2.root = CopyTree(root);          return tree2; } private Node CopyTree(Node curr)

{          Node temp;          if(curr != null)          {                   temp = new Node(curr.value);                   temp.setLeftChild(CopyTree(curr.lChild));                   temp.setRightChild(CopyTree(curr.rChild));                   return temp;          }          else                   return null; } Complexity Analysis: Time Complexity:  Space Complexity:  Copy Mirror Tree Solution: Copy, mirror image of the tree is done same as copy tree, but in place of left child pointing to the tree formed by left child traversal of input tree, this time left child points to the tree formed by right child traversal. Similarly right child point to the traversal of the left child of the input tree. Example 10.12: public Tree CopyMirrorTree(){          Tree tree2 = new Tree();          tree2.root = CopyMirrorTree(root);            return tree2; } private Node CopyMirrorTree(Node curr) {          Node temp;          if(curr != null)          {                   temp = new Node(curr.value);                   temp.setRightChild(CopyTree(curr.lChild));                   temp.setLeftChild(CopyTree(curr.rChild));                   return temp;          }          else                   return null; } Complexity Analysis: Time Complexity:  Space Complexity:  Number of Element Solution: Number of nodes at the right child and the number of nodes at the left child is added by one and we get the total number of nodes in any tree/sub-tree.

Example 10.13: public int numNodes(){          return numNodes(root); } public int numNodes(Node curr) {          if(curr == null)                   return 0;          else                      return (1 + numNodes(curr.rChild) +  numNodes(curr.lChild) ); } Complexity Analysis:  Time Complexity:                     Space Complexity:  Number of Leaf nodes Solution: If we add the number of leaf node in the right child with the number of leaf nodes in the left child, we will get the total number of leaf node in any tree or subtree. Example 10.14: public int numLeafNodes() {          return numLeafNodes(root); } private int numLeafNodes(Node curr) {          if(curr == null)                   return 0;          if( curr.lChild == null && curr.rChild == null )                   return 1;          else                      return (numLeafNodes(curr.rChild) + numLeafNodes(curr.lChild) ); Complexity Analysis:  Time Complexity:                     Space Complexity:  Identical Solution: Two trees have identical values if at each level the value is equal. Example 10.15: public boolean isEqual(Tree T2){          return Identical(root, T2.root); } private boolean Identical(Node node1,Node node2) {          if(node1 == null && node2 == null)

return true;          else if(node1 == null || node2 == null)                   return false;          else                        return(Identical(node1.lChild, node2.lChild)                                      && Identical(node1.rChild, node2.rChild)                                                && (node1.value == node2.value)); } Complexity Analysis: Time Complexity:                      Space Complexity:  Free Tree Solution: The tree is traversed and nodes of tree are freed in such a manner such that all child nodes are freed before it. Example 10.16: public void Free(){          root = null; } Complexity Analysis: Time Complexity:                        Space Complexity:  System will do garbage collection so for user action the time complexity is 0. Tree to List Rec Solution: Tree to the list is done recursively. At each node we will suppose that the tree to list function will do its job for the left child and right child. Then we will combine the result of the left child and right child traversal. We need a head and tail reference of the left list and right list to combine them with the current node. In the process of integration the current node will be added to the tail of the left list and current node will be added to the head to the right list. Head of the left list will become the head of the newly formed list and tail of the right list will become the tail of the newly created list.  Example 10.17: private Node treeToListRec(Node curr) {          Node Head=null, Tail=null;          if(curr == null)                   return null;          if(curr.lChild == null && curr.rChild == null)          {                   curr.lChild = curr;                   curr.rChild = curr;                   return curr;          }

if(curr.lChild != null)          {                   Head = treeToListRec(curr.lChild);                   Tail = Head.lChild;                   curr.lChild = Tail;                   Tail.rChild = curr;          }          else                   Head=curr;          if(curr.rChild != null)          {                   Node tempHead = treeToListRec(curr.rChild);                   Tail = tempHead.lChild;                   curr.rChild = tempHead;                   tempHead.lChild = curr;          }          else                   Tail=curr;          Head.lChild=Tail;          Tail.rChild=Head;          return Head; } public Node treeToListRec(){          Node head = treeToListRec(root);          Node temp = head;          return temp; } Complexity Analysis: Time Complexity:                        Space Complexity:  Print all the paths Print all the paths from the roots to the leaf Solution: Whenever we traverse a node we add that node to the list. When we reach a leaf we print the whole list. When we return from a function, then we remove the element that was added to the list when we entered this function. Example 10.18:          public void printAllPath() {          ArrayDeque<Integer> stk = new ArrayDeque<Integer>();          printAllPath(root,stk); } private void printAllPath(Node curr, ArrayDeque<Integer> stk) {          if(curr == null)                   return;

stk.push(curr.value);          if(curr.lChild == null && curr.rChild == null)          {                   System.out.println(stk);                   stk.pop();                   return;          }          printAllPath(curr.rChild,stk);          printAllPath(curr.lChild,stk);          stk.pop(); } Complexity Analysis: Time Complexity:    , Space Complexity:  Least Common Ancestor Solution: We recursively traverse the nodes of a binary tree. And we find any one of the node we are searching for then we return that node. And when we get both the left and right as some valid reference location other than null we will return that node as the common ancestor. Example 10.19: public int LCA(int first, int second){          Node ans = LCA(root, first, second);          if(ans != null)                   return ans.value;          else                   return Integer.MIN_VALUE; } private Node LCA(Node curr, int first, int second) {          Node left, right;          if (curr == null)                   return null;          if (curr.value == first || curr.value == second)                   return curr;          left = LCA(curr.lChild, first, second);          right = LCA(curr.rChild, first, second);          if (left != null && right != null)                   return curr;          else if (left != null)                   return left;          else                   return right; } Complexity Analysis: Time Complexity:                      Space Complexity:

Find Max in Binary Tree Solution: We recursively traverse the nodes of a binary tree. We will find the maximum value in the left and right subtree of any node then will compare the value with the value of the current node and finally return the largest of the three values. Example 10.20: public int findMaxBT(){          int ans = findMaxBT(root);          return ans; } private int findMaxBT(Node curr) {          int left, right;          if (curr == null)                   return Integer.MIN_VALUE;          int max = curr.value;          left = findMaxBT(curr.lChild);          right = findMaxBT(curr.rChild);          if (left > max)                   max = left;          if (right > max)                   max = right;          return max; } Search value in a Binary Tree Solution: To find if some value is there in a binary tree or not is done using exhaustive search of the binary tree. First, the value of current node is compared with the value which we are looking for. Then it is compared recursively inside the left child and right child. Example 10.21: ublic boolean searchBT(Node root, int value) {          int max;          boolean left, right;          if (root == null)                   return false;          if(root.value== value)                   return true;          left = searchBT(root.lChild, value);          if (left)

return true;          right = searchBT(root.rChild, value);          if (right)                   return true;          return false; } Maximum Depth in a Binary Tree Solution: To find the maximum depth of a binary tree we need to find the depth of the left tree and depth of right tree then we need to store the value and increment it by one so that we get depth of the given node. Example 10.22: public int TreeDepth(){          return TreeDepth(root); } private int TreeDepth(Node root) {          if(root == null)                   return 0;          else          {                   int lDepth=TreeDepth(root.lChild);                   int rDepth=TreeDepth(root.rChild);                   if(lDepth>rDepth)                             return lDepth+1;                   else                             return rDepth+1;          } } Number of Full Nodes in a BT Solution: A full node is a node which have both left and right child. We will recursively travers the whole tree and will increase the count of full node as we find them. Example 10.23: public int numFullNodesBT(){          return numNodes(root); } public int numFullNodesBT(Node curr) {          int count;          if(curr == null)                   return 0;          count = numFullNodesBT(curr.rChild) +  numFullNodesBT(curr.lChild);

if(curr.rChild != null && curr.lChild != null)                   count++;          return count; } Maximum Length Path in a BT/ Diameter of BT Solution: To find the diameter of BT we need to find the depth of left child and right child then will add these two values and increment it by one so that we will get the maximum length path (diameter candidate) which contains the current node. Then we will find max length path in the left child sub-tree. And will also find the max length path in the right child sub-tree. Finally, we will compare the three values and return the maximum value out of these this will be the diameter of the Binary tree. Example 10.24: public int maxLengthPathBT() {          return maxLengthPathBT(root); } private int maxLengthPathBT(Node curr)//diameter {          int max;          int leftPath, rightPath;          int leftMax, rightMax;          if (curr == null)                   return 0;          leftPath = TreeDepth(curr.lChild);          rightPath = TreeDepth(curr.rChild);          max = leftPath + rightPath + 1;          leftMax = maxLengthPathBT(curr.lChild);          rightMax = maxLengthPathBT(curr.rChild);          if (leftMax > max)                   max = leftMax;          if (rightMax > max)                   max = rightMax;          return max; } Sum of All nodes in a BT Solution: We will find the sum of all the nodes recursively. sumAllBT() will return the sum of all the node of left and right subtree then will add the value of current node and will return the final sum.

Example 10.25: public int sumAllBT(){          return sumAllBT(root); } private int sumAllBT(Node curr) {          int sum, leftSum, rightSum;          if(curr == null)                   return 0;          rightSum = sumAllBT(curr.rChild);          leftSum = sumAllBT(curr.lChild);          sum = rightSum + leftSum + curr.value;          return sum; } Iterative Pre-order Solution: In place of using system stack in recursion, we can traverse the tree using stack data structure. Example 10.26: public void iterativePreOrder(){          ArrayDeque<Node> stk = new ArrayDeque<Node>();          Node curr;          if(root != null)                   stk.add(root);          while(stk.isEmpty() == false){                   curr = stk.pop();                   System.out.print(curr.value + " ");                   if(curr.rChild != null)                             stk.push(curr.rChild);                   if(curr.lChild != null)                             stk.push(curr.lChild);          } } Complexity Analysis: Time Complexity:                        Space Complexity:  Iterative Post-order Solution: In place of using system stack in recursion, we can traverse the tree using stack data structure. Example 10.27:

public void iterativePostOrder() { ArrayDeque<Node> stk = new ArrayDeque<Node>(); ArrayDeque<Integer> visited=new ArrayDeque<Integer>();          Node curr;          int vtd;          if (root != null) {                   stk.add(root);                   visited.add(0);          }          while (stk.isEmpty() == false) {                   curr = stk.pop();                   vtd = visited.pop();                   if (vtd == 1) {                             System.out.print(curr.value + " ");                   } else {                             stk.push(curr);                             visited.push(1);                             if (curr.rChild != null) {                                      stk.push(curr.rChild);                                      visited.push(0);                             }                             if (curr.lChild != null) {                                      stk.push(curr.lChild);                                      visited.push(0);                             }                   }          } } Complexity Analysis:  Time Complexity:                     Space Complexity:  Iterative In-order Solution: In place of using system stack in recursion, we can traverse the tree using stack data structure. Example 10.28: public void iterativeInOrder() {          ArrayDeque<Node> stk = new ArrayDeque<Node>();          ArrayDeque<Integer> visited = new ArrayDeque<Integer>();          Node curr;          int vtd;          if (root != null) {                   stk.add(root);                   visited.add(0);          }          while (stk.isEmpty() == false) {                   curr = stk.pop();                   vtd = visited.pop();                   if (vtd == 1) {                             System.out.print(curr.value + " ");                   } else {                             if (curr.rChild != null) {

stk.push(curr.rChild);                                      visited.push(0);                             }                             stk.push(curr);                             visited.push(1);                             if (curr.lChild != null) {                                      stk.push(curr.lChild);                                      visited.push(0);                             }                   }          } } Complexity Analysis:  Time Complexity:                     Space Complexity:

Binary Search Tree (BST) A binary search tree (BST) is a binary tree on which nodes are ordered in the following way: ·         The key in the left subtree is less than the key in its parent node. ·         The key in the right subtree is greater the key in its parent node. ·         No duplicate key allowed. Note: there can be two separate key and value fields in the tree node. But for simplicity, we are considering value as the key. All problems in the binary search tree are solved using this supposition that the value in the node is key for the tree. Note: Since binary search tree is a binary tree to all the above algorithm of a binary tree are applicable to a binary search tree.

Problems in Binary Search Tree (BST) All binary tree algorithms are valid for binary search tree too. Create a binary search tree from sorted array Create a binary tree given list of values in an array in sorted order Since the elements in the array are in sorted order and we want to create a binary search tree in which left subtree nodes are having values less than the current node and right subtree nodes have value greater than the value of the current node. Solution: We have to find the middle node to create a current node and send the rest of the array to construct left and right subtree. Example 10.29: public void CreateBinaryTree (int[] arr) {           root = CreateBinaryTree (arr, 0, arr.length - 1); } private Node CreateBinaryTree (int[] arr, int start, int end) {           Node curr= null;           if (start > end)                    return null;           int mid = ( start + end ) / 2;           curr = new Node(arr[mid]);           curr.lChild = CreateBinaryTree (arr, start, mid-1);           curr.rChild = CreateBinaryTree (arr, mid+1, end);           return curr; } public static void main(String[] args){           Tree t = new Tree();           int[] arr = {1,2,3,4,5,6,7,8,9,10};           t.CreateBinaryTree(arr);           t.PrintInOrder();       } Insertion Nodes with key 6,4,2,5,1,3,8,7,9,10 are inserted in a tree. Below is step by step tree after inserting nodes in the order.



Solution: Smaller values will be added to the left child sub-tree of a node and greater value will be added to the right child sub-tree of the current node. Example 10.30: public void InsertNode(int value) {          root = InsertNode(value, root);               } private Node InsertNode(int value, Node node) {          if(node==null)          {                   node=new Node(value,null,null);          }          else          {                   if(node.value > value)                             node.lChild = InsertNode(value,node.lChild);                   else                             node.rChild = InsertNode(value, node.rChild);          }          return node; } Complexity Analysis:  Time Complexity:                     Space Complexity:

Find Node Solution: The value grater then the current node value will be in the right child sub-tree and the value smaller than the current node is in the left child sub-tree. We can find a value by traversing the left or right subtree iteratively. Example 10.31: Find the node with the value given. public boolean Find(int value){          Node curr=root;          while(curr != null)          {                   if(curr.value == value)                             return true;                   else if(curr.value > value)                             curr = curr.lChild;                   else                             curr = curr.rChild;          }          return false; } Complexity Analysis: Time Complexity:                      Space Complexity:  Example 10.32: Operators are generally read from left to right public boolean Find2(int value) {          Node curr = root;          while(curr != null && curr.value != value)                   curr = (curr.value > value)? curr.lChild : curr.rChild;          return curr != null; } Complexity Analysis: Time Complexity:                        Space Complexity:  Find Min Find the node with the minimum value. Solution: left most child of the tree will be the node with the minimum value.

Example 10.33: public int FindMin() {          Node node=root;          if(node == null)          {                   return Integer.MAX_VALUE;          }          while(node.lChild != null)          {                   node = node.lChild;          }          return node.value; } Complexity Analysis: Time Complexity:                        Space Complexity:  Find Max Find the node in the tree with the maximum value. Solution: Right most node of the tree will be the node with the maximum value. Example 10.34: public int FindMax() {          Node node=root;          if(node == null) {                   return Integer.MIN_VALUE;          }          while(node.rChild != null)   {                   node = node.rChild;          }          return node.value; } Complexity Analysis:  Time Complexity:                     Space Complexity:  Is tree a BST

Solution: At each node we check, max value of left subtree is smaller than the value of current node and min value of right subtree is grater then the current node. Example 10.35: public boolean isBST(Node root) {          if(root == null)                   return true;          if(root.lChild != null && FindMax(root.lChild).value > root.value )                   return false;          if(root.rChild != null && FindMin(root.rChild).value <= root.value )                   return false;          return (isBST(root.lChild) && isBST(root.rChild)); } Complexity Analysis: Time Complexity:                        Space Complexity:  The above solution is correct but it is not efficient as same tree nodes are traversed many times. Solution: A better solution will be the one in which we will look into each node only once. This is done by narrowing the range. We will be using a isBSTUtil() function which will take the max and min range of the values of the nodes. The initial value of min and max will be INT_MIN and INT_MAX. Example 10.36: public boolean isBST(){          return isBST(root, Integer.MIN_VALUE, Integer.MAX_VALUE); } public boolean isBST(Node curr, int min, int max) {          if(curr == null)                   return true;          if( curr.value < min || curr.value > max)                   return false;          return isBST(curr.lChild, min, curr.value)                             && isBST(curr.rChild,curr.value, max); } Complexity Analysis: Time Complexity:                        Space Complexity:   for stack Solution: Above method is correct and efficient but there is an easy method to do the same. We can do in-order traversal of nodes and see if we are getting a strictly increasing sequence Example 10.37: private boolean isBST2(Node root, counter count)/*  in order  traversal */ {          boolean ret;

if(root!= null)          {                   ret = isBST2(root.lChild,count);                   if(!ret)                             return false;                   if(count.value > root.value)                             return false;                   count.value = root.value;                   ret = isBST2(root.rChild,count);                   if(!ret)                             return false;          }          return true; } class counter{          int value; } public boolean isBST2(){          counter c = new counter();          return isBST2(root,c); } Complexity Analysis: Time Complexity:                        Space Complexity:   for stack Delete Node Description: Remove the node x from the binary search tree, making the necessary, reorganize nodes of binary search tree to maintain its properties. There are three cases in delete node, let’s call the node that need to be deleted as x. Case 1: node x has no children. Just delete it (i.e. Change parent node so that it does not point to x) Case 2: node x has one child. Splice out x by linking x’s parent to x’s child Case 3: node x has two children. Splice out the x’s successor and replace x with x’s successor When the node to be deleted have no children This is a trivial case in this case we directly delete the node and return null. When the node to be deleted have only one child. In this case we save the child in a temp variable, then delete current node, and finally return the child.

We want to remove node with value 9. The node has only one child. Right child of the parent of node with value 9 that is node with value 8 will point to the node with value 10. Finally, node with value 9 is removed from the tree. When the node to be deleted has two children. We want to delete node with value 6. Which have two children.

We had found minimum value node of the right child of node with value 6. Minimum value node value is copied to the node with value 6. Delete node with minimum value 7 is called over the right child tree of the node. Finally the tree with both the children is created. Example 10.38: public void DeleteNode(int value) {          root = DeleteNode( root, value); } private Node  DeleteNode(Node node,int value) {          Node temp=null;          if(node != null)          {                   if(node.value==value)                   {                             if(node.lChild==null && node.rChild==null)                             {                                      return null;                             }                             else                             {                                      if(node.lChild==null)

{                                                temp=node.rChild;                                                return temp;                                      }                                      if(node.rChild==null)                                      {                                                temp=node.lChild;                                                return temp;                                      }                                      Node maxNode = FindMax(node.lChild);                                      int maxValue=maxNode.value;                                      node.value = maxValue;                                      node.lChild=DeleteNode(node.lChild,maxValue);                             }                   }                   else                   {                             if(node.value > value)                             {                                      node.lChild = DeleteNode(node.lChild, value);                             }                             else                             {                                      node.rChild = DeleteNode(node.rChild, value);                             }                   }          }          return node; }        Analysis:  Time Complexity:                  Space Complexity:  Least Common Ancestor In a tree T. The least common ancestor between two nodes n1 and n2 is defined as the lowest node in T that has both n1 and n2 as descendants. Example 10.39: public int LcaBST(int first, int second) {          return LcaBST(root, first, second); } private int LcaBST(Node curr, int first, int second) {          if(curr == null)          {                   return Integer.MAX_VALUE;          }          if(curr.value > first &&                             curr.value > second)          {                   return LcaBST(curr.lChild, first, second);

}          if(curr.value < first &&                             curr.value < second)          {                   return LcaBST(curr.rChild, first, second);          }          return curr.value; } Trim the Tree nodes which are Outside Range Given a range as min, max. We need to delete all the nodes of the tree that are out of this range. Solution: Traverse the tree and each node that is having value outside the range will delete itself. All the deletion will happen from inside out so we do not have to care about the children of a node as if they are out of range then they already had deleted themselves. Example 10.40: public void trimOutsideRange(int min, int max) {          trimOutsideRange(root, min, max); } private Node trimOutsideRange(Node curr, int min, int max) {          if (curr == null)                   return null;          curr.lChild=trimOutsideRange(curr.lChild, min, max);          curr.rChild=trimOutsideRange(curr.rChild, min, max);          if (curr.value < min)                   return curr.rChild;          if (curr.value > max)                   return curr.lChild;          return curr; } Print Tree nodes which are in Range Print only those nodes of the tree whose value is in the range given. Solution: Just normal inorder traversal and at the time of printing we will check if the value is inside the range provided. Example 10.41: public void printInRange(int min, int max) {          printInRange(root, min, max);

} private void printInRange(Node root, int min, int max) {          if(root == null)                   return;          printInRange(root.lChild, min, max);          if(root.value >= min && root.value <= max)                   System.out.print(root.value + " ");          printInRange(root.rChild, min, max); } Find Ceil and Floor value inside BST given key Given a tree and a value we need to find the ceil value of node in tree which is smaller than the given value and need to find the floor value of node in tree which is bigger. Our aim is to find ceil and floor value as close as possible then the given value. Example 10.42: public int FloorBST(int val) {          Node curr = root;          int floor=Integer.MAX_VALUE;          while (curr != null)          {                   if (curr.value == val)                   {                             floor = curr.value;                             break;                   }                   else if (curr.value > val)                   {                             curr = curr.lChild;                   }                   else                   {                             floor = curr.value;                             curr = curr.rChild;                   }          }          return floor; } public int CeilBST(int val) {          Node curr = root;          int ceil=Integer.MIN_VALUE;          while (curr != null)          {                   if (curr.value == val)                   {                             ceil = curr.value;                             break;                   }                   else if (curr.value > val)

{                             ceil = curr.value;                             curr = curr.lChild;                   }                   else                   {                             curr = curr.rChild;                   }          }          return ceil; }

Introduction     A Priority-Queue also knows as Binary-Heap, is a variant of queue. Items are removed from the start of the queue. However, in a Priority-Queue the logical ordering of objects is determined by their priority. The highest priority item are at the front of the Priority-Queue. When you add an item to Priority-Queue the new item can more to the front of the queue. A Priority-Queue is a very important data structure.  Priority-Queue is used in various Graph algorithms like Prim’s Algorithm and Dijkstra’s algorithm.  Priority-Queue is also used in the timer implementation etc. A Priority-Queue is implemented using a Heap (Binary Heap). A Heap data structure is an array of elements that can be observed as a complete binary tree. The tree is completely filled on all levels except possibly the lowest. And heap satisfies the heap ordering property. A heap is a complete binary tree so the height of tree with N nodes is always O(logn). A heap is not a sorted data structure and can be regarded as partially ordered. As you see from the picture, there is no relationship among nodes at any given level, even among the siblings. Heap is implemented using an array. And because heap is a complete binary tree, the left child of a parent (at position x) is the node that is found in position 2x in the array. Similarly, the right child of the parent is at position 2x+1 in the array. To find the parent of any node in the heap, we can simply division. Given the index y of a node, the parent index will by y/2.



Types of Heap There are two types of heap and the type depends on the ordering of the elements. The ordering can be done in two ways: Min-Heap and Max-Heap Max Heap Max-Heap: the value of each node is less than or equal to the value of its parent, with the largestvalue element at the root. Max Heap Operations Insert O(logn) DeleteMax O(logn) Remove O(logn) FindMax   O(1) Min Heap Min-Heap: the value of each node is greater than or equal to the value of its parent, with the minimum-value element at the root.

Use it whenever you need quick access to the smallest item, because that item will always be at the root of the tree or the first element in the array. However, the remainder of the array is kept partially sorted. Thus, instant access is only possible for the smallest item. Min Heap Operations Insert O(logn) DeleteMin O(logn) Remove O(logn) FindMin   O(1) Throughout this chapter, the word "heap" will always refer to a max-heap. The implementation of min-heap is left for the user to do it as an exercise.

Heap ADT Operations The basic operations of binary heap are as follows: Binary Heap   Create a new empty binary heap O(1) Insert Adding a new element to the heap O(logn) DeleteMax Delete the maximum element form the heap. O(logn) FindMax Find the maximum element in the heap. O(1) isEmpty return true if the heap is empty else return false O(1) Size Return the number of elements in the heap. O(1) BuildHeap Build a new heap from the array of elements O(logn)

Operation on Heap Create Heap from an array 1.     Starts by putting the elements to an array. 2.     Starting from the middle of the array move downward towards the start of the array. At each step, compare parent value with its left child and right child. And restore the heap property by shifting the parent value with its greatest-value child. Such that the parent value will always be greater than or equal to left child and right child. 3.     For all elements from middle of the array to the start of the array. We are doing comparisons and shift till we reach the leaf nodes of the heap. The Time Complexity of build heap is O(N). Given an array as input to create heap function. Value of index i is compared with value of its children nodes that is at index ( i*2 + 1 ) and ( i*2 + 2 ). Middle of array N/2 that is index 3 is comapred with index 7. If the children node value is grater then parent node then the value will be swapped. Similarly, value of index 2 is compared with index 5 and 6. The largest of the value is 7 which will be swapped with the value at the index 2.

Similarly, value of index 1 is compared with index 3 and 4 The largest of the value is 8 which will be swapped with the value at the index 1. Percolate down function is used to subsequently adjest the value replased in the previous step by comparing it with its children nodes. Now value at index 0 is comared with index 1 and 2. 8 is the largest value so it swapped with the value at index 0. Percolate down function is used to further compare the value at index 1 with its children

nodes at index 3 and 4. In the end max heap is created. Example 11.1: public class Heap {          private static final int CAPACITY = 16;          private int size;    // Number of elements in heap          private int[] arr;     // The heap array          public Heap() {                   arr = new int[CAPACITY];                    size = 0;          }          public Heap(int[] array) {                   size = array.length;                   arr = new int[array.length+1];                   //we do not use 0 index                   System.arraycopy(array, 0, arr, 1, array.length);                   //Build Heap operation over array                   for(int i=(size/2);i>0;i--)                   {                             percolateDown(i);                   }          }

//Other Methods. } private void percolateDown(int position) {          int lChild=2*position;          int rChild=lChild+1;          int small=-1;          int temp;          if( lChild <= size )                   small =lChild;          if( rChild <= size && (arr[rChild] - arr[lChild])<0 )                   small =rChild;          if( small!=-1 && (arr[small] - arr[position])<0 ) {                   temp=arr[position];                   arr[position]=arr[small];                   arr[small]=temp;                   percolateDown(small);          }        } Initializing an empty Heap Example 11.2: private void percolateUp( int position ) {          int parent=position/2;          int temp;          if(parent==0)                    return;          if((arr[parent] - arr[position])<0) {                   temp =         arr[position];                   arr[position]=arr[parent];                   arr[parent] = temp;                   percolateUp(parent);          } } Enqueue / Insert 1.    Add the new element at the end of the array. This keeps the structure as a complete binary tree, but it might no longer be a heap since the new element might have a value greater than its parent. 2.    Swap the new element with its parent until it has value greater than its parents. 3.    Step 2 will terminate when the new element reaches the root or when the new element's parent have a value greater than or equal to the new element's value. Let’s take an example of the Max heap created in the above example.

Let’s take an example by inserting element with value 9 to the heap. The element is added to the end of the heap array. Now the value will be percolate up by comparing it with the parent. The value is added to index 8 and its parent will be (N-1)/2 = index 3. Since the value 9 is grater then 4 it will be swapped with it. Percolate up is used and the value is moved up till heap property is satisfied.

Now the value at index 1 is compared with index 0 and to satisfy heap property it is further swapped. Now finally max heap is created by inserting new node. Example 11.3: public void add( int value ) {          if(size == arr.length - 1)                   doubleSize();          arr[++size]=value;            percolateUp(size); } private void doubleSize() {          int[] old = arr;          arr = new int[arr.length * 2];          System.arraycopy(old, 1, arr, 1, size); } Dequeue / Delete 1.    Copy the value at the root of the heap to the variable used to return a value. 2.    Copy the last element of the heap to the root, and then reduce the size of heap by 1. This element is called the "out-of-place" element.

3.    Restore heap property by swapping the out-of-place element with its greatest-value child. Repeat this process until the out-of-place element reaches a leaf or it has a value that is greater or equal to all its children. 4.    Return the answer that was saved in Step 1. To remove an element from heap its top value is swapped to the end of the heap array and size fo heap is reduced by 1. Since end of the heap value is copied to head of heap. Heap property is disturbed so we need to again percolate down by comparing node with its children nodes to restore heap property. Percolate down continued by comparing with its children nodes.

Percolate down Percolate down Complete Example 11.4: public int remove( ) {          if (isEmpty())                   throw new IllegalStateException();          int value=arr[1];          arr[1]=arr[size];          size--;          percolateDown(1);          return value; }

Heap-Sort 1.    Use create heap function to build a max heap from the given array of elements. This operation will take O(N) time. 2.    Dequeue the max value from the heap and store this value to the end of the array at location arr[size-1] a)    Copy the value at the root of the heap to end of the array. b)    Copy the last element of the heap to the root, and then reduce the size of heap by 1. This element is called the "out-of-place" element. c)    Restore heap property by swapping the out-of-place element with its greatest-value child. Repeat this process until the out-of-place element reaches a leaf or it has a value that is greater or equal to all its children 3.    Repeat this operation till there is just one element in the heap. Let’s take example of the heap which we had created at the start of the chapter. Heap sort is algorithm starts by creating a heap of the given array which is done in linear time. Then at each step head of the heap is swapped with the end of the heap and the heap size is reduced by 1. Then percolate down is used to restore the heap property. And this same is done multiple times till the heap contain just one element. We had started with max heap. The maximum value as the first element of the Heap array is swapped with the last element of the array. Now the largest value is at the end of the array. Then we will reduce the size of the heap by one.

Since 1 is at the top of the heap. And heap property is lost we will use Percolate down method to regain the heap property. Percolate down cont. Since heap property is regained. Then we will copy the first element of the heap array to the second last position.

Heap size is further reduced and percolate down cont. Percolate down cont. Again swap. Size of heap reduced and percolate down.

Again swap. Size of heap reduced and percolate down.

Again swap. Size of heap reduced and percolate down. Again swap. Again swap.

End. Final array which is sorted in increasing order. Example 11.5: public void print( ){          for(int i=1 ; i<=size ; i++ )                   System.out.println("value is :: " + arr[i] ); } public boolean isEmpty(){          return (size==0) ; } public int peek() {          if (isEmpty()) {                   throw new IllegalStateException();          }          return arr[1]; } public static void heapSort(int[] array){          Heap hp = new Heap(array);          for(int i=0; i < array.length; i++)                   array[i] = hp.remove(); } public static void main(String[] args) {          int[] a = {1,9,6,7,8,0,2,4,5,3};

Heap hp = new Heap(a);          hp.print();          for(int i=0;i<a.length;i++)                   System.out.println("pop value :: " + hp.remove() );          Heap.heapSort(a);          for(int i=0 ; i<a.length ; i++ )                   System.out.println("value is :: " + a[i] );  } Data structure Array Worst Case Time Complexity O(nlogn) Best Case Time Complexity O(nlogn) Average Time Complexity O(nlogn) Space Complexity O(1) Note: Heap-Sort is not a Stable sort and do not require any extra space for sorting a list.

Uses of Heap 1.    Heapsort: One of the best sorting methods being in-place and log(N) time complexity in all scenarios. 2.    Selection algorithms: Finding the min, max, both the min and max, median, or even the kth largest element can be done in linear time (often constant time) using heaps. 3.    Priority Queues: Heap Implemented priority queues are used in Graph algorithms like Prim’s Algorithm and Dijkstra’s algorithm. A heap is a useful data structure when you need to remove the object with the highest (or lowest) priority. Schedulers, timers 4.    Graph algorithms: By using heaps as internal traversal data structures, run time will be reduced by polynomial order. Examples of such problems are Prim's minimal 5.    Because of the lack of references, the operations are faster than a binary tree. Also, some more complicated heaps (such as binomial) can be merged efficiently, which isn't easy to do for a binary tree.

Problems in Heap Kth Smallest in a Min Heap Just call DeleteMin() operation K-1 times and then again call DeleteMin() this last operation will give Kth smallest value.  Time Complexity O(KlogN) Kth Largest in a Max Heap Just call DeleteMax() operation K-1 times and then again call DeleteMax () this last operation will give Kth smallest value.  Time Complexity O(KlogN) 100 Largest in a Stream There are billions of integers coming out of a stream some getInt() function is providing integers one by one. How would you determine the largest 100 numbers? Solution: Large hundred (or smallest hundred etc.) such problems are solved very easily using a Heap. In our case, we will create a min heap. 1.    First from 100 first integers builds a min heap. 2.    Then for each coming integer compare if it is greater than the top of the min heap. 3.    If not, then look for next integer. If yes, then remove the top min value from the min heap, insert the new value at the top of the heap, use procolateDown, and move it to its proper position down the heap. 4.    Every time you have largest 100 values stored in your head Merge two Heap How can we merge two heaps? Solution: There is no single solution for this. Let us suppose the size of the bigger heap is N and the size of the smaller heap is M. 1.    If both heaps are comparable size, then put both heap arrays in same bigger arrays. Or in one of the arrays if they are big enough. Then apply CreateHeap() function which will take theta(N+M) time. 2.    If M is much smaller then N then add() each element of M array one by one to N heap. This will take O(MlogN) the worst case or O(M) best case. Get Median function

Example 11.6: Give a data structure that will provide median of given values in constant time. Solution: We will be using two heap one min heap and other max heap.  First, there will be a max heap which will contain the first half of data and there will be an min heap which will contain the second half of the data. Max heap will contain the smaller half of the data and its max value that is at the top of the heap will be the median contender. Similarly, the Min heap will contain the larger values of the data and its min value that is at its top will contain the median contender. We will keep track of the size of heaps. Whenever we insert a value to heap, we will make sure that the size of two heaps differs by max one element, otherwise we will pop one element from one and insert into another to keep them balanced. import java.util.Collections; import java.util.PriorityQueue; public class MedianHeap {          PriorityQueue<Integer> minHeap;          PriorityQueue<Integer> maxHeap;          public MedianHeap()          {                   minHeap = new PriorityQueue<Integer>();                   maxHeap = new PriorityQueue<Integer> (Collections.reverseOrder());          }          //Other Methods. } public void insert(int value) {          if (maxHeap.size() == 0 || maxHeap.peek() >= value )                   maxHeap.add(value);          else                   minHeap.add(value);          //size balancing          if (maxHeap.size() > minHeap.size() + 1) {                   value = maxHeap.remove();                   minHeap.add(value);          }          if (minHeap.size() > maxHeap.size() + 1)  {                   value = minHeap.remove();                   maxHeap.add(value);          } } public int getMedian() {          if (maxHeap.size() == 0 && minHeap.size() == 0)                   return Integer.MAX_VALUE;          if (maxHeap.size() ==  minHeap.size() )                   return (maxHeap.peek() +  minHeap.peek())/2;          else if (maxHeap.size() >  minHeap.size())                   return maxHeap.peek();          else                   return minHeap.peek();

} public static void main(String[] args) {          int arr[] = { 1, 9, 2, 8, 3, 7, 4, 6, 5, 1, 9, 2, 8, 3, 7, 4, 6, 5, 10, 10 };          MedianHeap hp = new MedianHeap();          for (int i = 0; i < 20; i++)          {                   hp.insert(arr[i]);                   System.out.println("Median after insertion of " +  arr[i] + " is  " +hp.getMedian());          } } Is Min Heap Example 11.7: Given an array, find if it is a binary Heap is Min Heap boolean IsMinHeap(int[] arr, int size) {          for (int i = 0; i <= (size - 2) / 2; i++)          {                   if (2 * i + 1 < size)                   {                             if (arr[i] > arr[2 * i + 1])                                      return false;                   }                   if (2 * i + 2<size)                   {                             if (arr[i] > arr[2 * i + 2])                                      return false;                   }          }          return true; } Is Max Heap Example 11.8: Given an array find if it is a binary Heap Max heap boolean IsMaxHeap(int[] arr, int size) {          for (int i = 0; i <= (size - 2) / 2; i++)          {                   if (2 * i + 1 < size && arr[i] < arr[2 * i + 1])                             return false;                   if (2 * i + 2 < size && arr[i] < arr[2 * i + 2])                             return false;          }          return true; } Analysis: If each parent is grater then its children then heap property is true. We will start from half of the array and will reduce the size one by one thereby comparing the value of index node with its left child and right child node.

Traversal in Heap Heaps are not designed to traverse to find some element they are made to get min or max element fast. Still if you want to traverse a heap just traverse the array sequentially. This traversal will be level order traversal. This traversal will have linear Time Complexity. Deleting Arbiter element from Min Heap Again, heap is not designed to delete an arbitrary element, but still if you want to do so. Find the element by linear search in the heap array. Replace it with the value stored at the end of the Heap value. Reduce the size of the heap by one.  Compare the new inserted value with its parent. If its value is smaller than the parent value, then percolate up. Else if its value is greater than its left and right child then percolate down. Time Complexity is O(logn) Deleting Kth element from Min Heap Again, heap is not designed to delete an arbitrary element, but still if you want to do so. Replace the kth value with the value stored at the end of the Heap value. Reduce the size of the heap by one.  Compare the new inserted value with its parent. If its value is smaller than the parent value, then percolate up. Else if its value is greater than its left and right child then percolate down. Time Complexity is O(logn) Print value in Range in Min Heap Linearly traverse through the heap and print the value that are in the given range.

Introduction In the previous chapter, we have looked into various searching techniques. Consider a problem of searching a value in an array. If the array is not sorted then we have no other option but to look into each and every element one by one so the searching Time Complexity will be O(n). If the array is sorted then we can search the value we are looking for in O(logn) logarithmic time using binary search. What if we have a function that can tell us the location/index of the value we are looking for in the array? We can directly go into that location and tell whether our object we are searching for is present or not in just O(1) constant time. Such a function is called a Hash function. In real life when a letter is handed over to a postman, by looking at the address on the letter, postman precisely knows to which house this letter needs to be delivered. He is not going to ask for a person door to door.  The process of storing objects using a hash function is as follows: 1.    Create an array of size M to store objects; this array is called Hash-Table. 2.    Find a hash code of an object by passing it through the hash function. 3.    Take module of hash code by the size of Hashtable to get the index of the table where objects will be stored. 4.    Finally store these objects in the designated index. The process of searching objects in Hash-Table using a hash function is as follows: 1.    Find a hash code of the object we are searching for by passing it through the hash function. 2.    Take module of hash code by the size of Hashtable to get the index of the table where objects are stored.

3.    Finally, retrieve the object from the designated index.

Hash-Table A Hash-Table is a data structure that maps keys to values. Each position of the Hash-Table is called a slot. The Hash-Table uses a hash function to calculate an index of an array of slots. We use the Hash-Table when the number of keys actually stored is small relatively to the number of possible keys. Hash-Table Abstract Data Type (ADT) ADT of Hash-Table contains the following functions: 1.    Insert(x), add object x to the data set. 2.    Delete(x), delete object x from the data set. 3.    Search(x), search object x in data set. Hash Function A hash function is a function that generates an index in a table for a given object. An ideal hash function should generate a unique index for each and every object is called the perfect hash function. Example 12.1: Most simple hash function private int ComputeHash(int key)//division method {           int hashValue = 0;           hashValue = key;           return hashValue % tableSize; } There are many hash functions, but this is the minimum that a hash function should do. Various hash generation logics will be added to this function to generate a better hash. Properties of good hash function: 1.    It should provide a uniform distribution of hash values. A non-uniform distribution increased the number of collisions and the cost of resolving them. 2.    Choose a hash function which can be computed quickly and returns values within the range of the Hash-Table. 3.    Chose a hash function with a good collision resolution algorithm which can be used to compute alternative index if the collision occurs. 4.    Choose a hash function which uses the necessary information provided in the key. 5.    It should have high load factor for a given set of keys.

Load Factor Load factor = Number of elements in Hash-Table / Hash-Table size Based on the above definition, Load factor tells whether the hash function is distributing the keys uniformly or not. So it helps in determining the efficiency of the hashing function. It also works as decision parameter when we want to expand or rehash the existing Hash-Table entries. Collisions When a hash function generates the same index for the two or more different objects, the problem known as the collision. Ideally, hash function should return a unique address for each key, but practically it is not possible. Collision Resolution Techniques Hash collisions are practically unavoidable when hashing large number of objects. Techniques that are used to find the alternate location in the Hash-Table is called collision resolution. There are a number of collision resolution techniques to handle the collision in hashing. Most common and widely used techniques are: ·         Open addressing ·         Separate chaining

Hashing with Open Addressing When using linear open addressing, the Hash-Table is represented by a one-dimensional array with indices that range from 0 to the desired table size-1. One method of resolving collision is the look into a Hash-Table and find another free slot the hold the object that have caused the collision. A simple way is to move from one slot to another in some sequential order until we find a free space. This collision resolution process is called Open Addressing. Linear Probing In Linear Probing, we try to resolve the collision of an index of a Hash-Table by sequentially searching the Hash-Table free location. Let us suppose, if k is the index retrieved from the hash function. If the kth index is already filled then we will look for (k+1) %M, then (k+2) %M and so on. When we get a free slot, we will insert the object into that free slot. Example 12.2: The resolver function of linear probing int resolverFun(int i) {           return i; } Quadratic Probing In Quadratic Probing, we try to resolve the collision of the index of a Hash-Table by quadratic ally increasing the search index free location. Let us suppose, if k is the index retrieved from the hash function. If the kth index is already filled then we will look for (k+1^2) %M, then (k+2^2) %M and so on. When we get a free slot, we will insert the object into that free slot. Example 12.3: The resolver function of quadratic probing int resolverFun(int i) {           return i * i; } Table size should be a prime number to prevent early looping should not be too close to 2powN Linear Probing implementation Example 12.4: Below is a linear probing collision resolution Hash-Table implementation. public class HashTable {           private static int EMPTY_NODE = -1;

private static int  LAZY_DELETED = -2;          private static int  FILLED_NODE = 0;          private int tableSize;          int[] Arr;          int[] Flag;          public HashTable(int tSize) {                   tableSize = tSize;                   Arr = new int[tSize + 1];                   Flag = new int[tSize + 1];                   for(int i=0;i<=tSize;i++)                             Flag[i]=EMPTY_NODE;                           } } Table array size will be 50 and we have defined two constant values EMPTY_NODE and LAZY_DELETED. int ComputeHash(int key) {                 return key%tableSize; } This is the most simple hash generation function which does nothing but just take the modulus of the key. int resolverFun(int index) {          return index; } When the hash index is already occupied by some element the value will be placed in some other location to find that new location resolver function is used. Hash-Table has two component one is table size and other is reference to array. Example 12.5: boolean InsertNode(int value) {          int hashValue = ComputeHash(value);          for (int i = 0; i < tableSize; i++)          {                   if (Flag[hashValue] == EMPTY_NODE || Flag[hashValue] == LAZY_DELETED)                   {                             Arr[hashValue] = value;                             Flag[hashValue] = FILLED_NODE;                             return true;                   }                   hashValue +=  resolverFun(i);                   hashValue %= tableSize;          }                 return false; }

An insert node function is used to add values to the array. First hash is calculated. Then we try to place that value in the Hash-Table. We look for empty node or lazy deleted node to insert value. In case insert did not success, we try new location using a resolver function. Example 12.6: boolean FindNode(int value) {          int hashValue = ComputeHash(value);          for (int i = 0; i < tableSize; i++)          {                   if (Flag[hashValue] == EMPTY_NODE)                             return false;                   if (Flag[hashValue] == FILLED_NODE && Arr[hashValue] == value)                             return true;                   hashValue += resolverFun(i);                   hashValue %= tableSize;          }          return false; } Find node function is used to search values in the array. First hash is calculated. Then we try to find that value in the Hash-Table. We look for over desired value or empty node. In case we find the value we are looking for then we return that value or in case we don’t we return -1. We use a resolver function to find the next probable index to search. Example 12.7: boolean DeleteNode(int value) {          int hashValue = ComputeHash(value);          for (int i = 0; i < tableSize; i++)          {                   if (Flag[hashValue] == EMPTY_NODE)                             return false;                   if (Flag[hashValue] == FILLED_NODE && Arr[hashValue] == value)                   {                             Flag[hashValue] = LAZY_DELETED;                             return true;                   }                   hashValue += resolverFun(i);                   hashValue %= tableSize;          }          return false; } Delete node function is used to delete values from a Hashtable. We do not actually delete the value we just mark that value as LAZY_DELETED. Same as the insert and search we use resolverFun to find the next probable location of the key. Example 12.8:

void Print() {          for (int i = 0; i < tableSize; i++)          {                   if (Flag[i] == FILLED_NODE)                             System.out.println("Node at index [" +i+ " ] :: "+Arr[i] );          } } Print method print the content of hash table. public static void main(String[] args) {          HashTable ht = new HashTable(1000);          ht.InsertNode(89);          ht.InsertNode(18);          ht.InsertNode(49);          ht.InsertNode(58);          ht.InsertNode(69);          ht.InsertNode(89);          ht.InsertNode(18);          ht.InsertNode(49);          ht.InsertNode(58);          ht.InsertNode(69);          ht.Print();          System.out.println("");          ht.DeleteNode(89);          ht.DeleteNode(18);          ht.DeleteNode(49);          ht.DeleteNode(58);          ht.DeleteNode(69);          ht.Print(); } Main function demonstrating how to use hash table. Quadratic Probing implementation. Everything will be same as linear probing implementation only resolver function will be changed. int resolverFun(int index) {          return index * index; }

Hashing with separate chaining Another method for collision resolution is based on an idea of putting the keys that collide in a linked list. This method is called separate chaining. To speed up search we use Insertion-Sort or keeping the linked list sorted. Separate Chaining implementation ‘Example 12.9: Below is separate chaining implementation of hash tables. public class HashTableSC {           private class Node{                    private int value;                    private Node next;                    public Node(int v, Node n){                              value = v;                              next = n;                    }           };           private int tableSize;           Node[] listArray;      //double pointer           public HashTableSC() {                    tableSize=512;                    listArray = new Node[tableSize];                    for (int i = 0; i<tableSize; i++)                              listArray[i] = null;           } } private int ComputeHash(int key)           //division method {           int hashValue = 0;           hashValue = key;           return hashValue % tableSize;

} public void insert(int value) {          int index = ComputeHash(value);          listArray[index]=new Node(value,listArray[index]); } public boolean find(int value) {          int index = ComputeHash(value);          Node head = listArray[index];          while (head!=null){                   if(head.value == value)                             return true;                   head = head.next;          }          return false; } public boolean delete(int value) {          int index = ComputeHash(value);          Node nextNode, head = listArray[index];          if (head != null && head.value == value)          {                   listArray[index] = head.next;                   return true;          }          while (head!=null)          {                   nextNode = head.next;                   if (nextNode != null&& nextNode.value == value)                   {                             head.next = nextNode.next;                             return true;                   }                   else                   {                             head = nextNode;                   }          }          return false; } public void print() {          for (int i = 0; i<tableSize; i++)          {                   System.out.println("Printing for index value :: " + i + "List of value printing :: ");                   Node head = listArray[i];                   while (head!=null)                   {                             System.out.println(head.value);                             head = head.next;                   }          } } public static void main(String[] args) {

HashTableSC ht = new HashTableSC();          for (int i = 100; i < 110; i++)                   ht.insert(i);          System.out.println("search 100 :: "+ ht.find(100));          System.out.println("remove 100 :: "+ ht.delete( 100));          System.out.println("search 100 :: "+ ht.find( 100));          System.out.println("remove 100 :: "+ ht.delete( 100)); } Note: It is important to note that the size of the “skip” must be such that all the slots in the table will eventually be occupied. Otherwise, part of the table will be unused. To ensure this, it is often suggested that the table size being a prime number. This is the reason we have been using 11 in our examples.

Count Map Below is CountMap<T> implementation over java collection HashMap<T>, this interface is important if you have repeted values can come and you want to keep track of there count. CountMap<T> is our implementation that we will use to solve many problems. Example 12.10: import java.util.HashMap; public class CountMap<T> {           HashMap<T, Integer> hm = new HashMap<T, Integer>();           public void add(T key)           {                    if(hm.containsKey(key))                    {                              int count = hm.get(key);                              hm.put(key, count+1);                    }                    else                    {                              hm.put(key, 1);                    }           } } public void remove(T key) {           if(hm.containsKey(key))           {                    if(hm.get(key)==1)                               hm.remove(key);                    else{                              int count = hm.get(key);                              hm.put(key, count-1);                    }           } } public int get(T key) {           if(hm.containsKey(key))                    return hm.get(key);           return 0; } public boolean containsKey(T key) {           return hm.containsKey(key); } public int size() {           return hm.size(); }

public static void main(String[] args) {          CountMap<Integer> cm = new CountMap<Integer>();          cm.add(2);cm.add(2);          cm.remove(2);          System.out.println("count is : " + cm.get(2));          System.out.println("count is : " + cm.get(3)); } Output: count is : 1 count is : 0

Problems in Hashing Anagram solver An anagram is a word or phrase formed by reordering the letters of another word or phrase. Example 12.11: Two words are anagram if they are of same size and there chracters are same. public static boolean isAnagram(char[] str1, char[] str2) {           int size1 = str1.length;           int size2 = str2.length;           if (size1 != size2)                    return false;           CountMap<Character> cm = new CountMap<Character>();           for (char ch : str1)                    cm.add(ch);           for (char ch : str2)                    cm.remove(ch);           return (cm.size() == 0); } Remove Duplicate Remove duplicates in an array of numbers. Solution: We can use a second array or the same array, as the output array. In the below example Hash-Table is used to solve this problem. Example 12.12: public static void removeDuplicate(char[] str) {           int index = 0;           HashSet<Character> hs= new HashSet<Character>();           for(char ch : str)           {                    if (hs.contains(ch) == false)                    {                              str[index++] = ch;                              hs.add(ch);                    }           }           str[index] = '\0'; } Find Missing Example 12.13: There is a list of integers we need to find the missing number in the list.

public static int findMissing(int[] arr, int start, int end) {          HashSet<Integer> hs = new HashSet<Integer>();          for (int i : arr)          {                   hs.add(i);          }          for(int curr = start; curr <= end; curr++)          {                   if (hs.contains(curr) == false)                             return curr;          }          return Integer.MAX_VALUE; } All the element in the list is added to hashtable and then the missing element is found by searching into hashtable and final missing value is returned. Print Repeating Example 12.14: Print the repeating integer in a list of integers. public static void printRepeating(int[] arr) {          HashSet<Integer> hs = new HashSet<Integer>();          System.out.print("Repeating elements are:");          for (int val : arr)          {                   if (hs.contains(val))                             System.out.print("  " + val);                   else                             hs.add(val);          } } All the values to the hash table when some value came which is already in the hash table then that is the repeted value. Print First Repeating Example 12.15: Same as the above problem in this we need to print the first repeating number. Caution should be taken to find the first repeating number. It should be the one number that is repeating. For example, 1, 2, 3, 2,1. The answer should be 1 as it is the first number which is repeating. public static void printFirstRepeating(int[] arr) {          int i;          int size = arr.length;          CountMap<Integer> hs = new CountMap<Integer>();

for (i = 0; i < size; i++)          {                   hs.add(arr[i]);          }          for (i = 0; i < size; i++)          {                   hs.remove(arr[i]);                   if (hs.containsKey(arr[i]))                   {                             System.out.println("First Repeating number is : " + arr[i]);                             return;                   }          } } Add values to the count map the one that is repeting will have multiple count. Now traverse the array again and see if the count is more then one. So that is the first repeting.

Introduction In this chapter, we will study about Graphs. Graphs can be used to represent many interesting things in the real world. Flights from cities to cities, rods connecting various town and cities. Even the sequence of steps that we take to become ready for jobs daily, or even a sequence of classes that we take to become a graduate in computer science. Once we have a good representation of the map, then we use a standard graph algorithms to solve many interesting problems of real life. The flight connection between major cities of India can also be represented by the below graph. Each node is a city and each edge is a straight flight path from one city to another. You may want to go from Delhi to Chennai, if given this data in good representation to a computer, through graph algorithms the computer may propose shortest, quickest or cheapest path from soured to destination. Google map that we use is also a big graph of lots of nodes and edges. And suggest shortest and quickest path to the user. Graph Definitions A Graph is represented by G where G = (V, E), where V is a finite set of points called Vertices and E is a finite set of Edges. Each edge is a tuple (u, v) where u, v ∈ V. There can be a third component weight to the tuple. Weight is cost to go from one vertex to another. Edge in a graph can be directed or undirected. If the edges of graph are one way, it is called Directed graph or Digraph. The graph whose edges are two ways are called Undirected graph or just graph.

A Path is a sequence of edges between two vertices. The length of a path is defined as the sum of the weight of all the edges in the path. Two vertices u and v are adjacent if there is an edge whose endpoints are u and v. In the below graph: V = { V1, V2, V3, V4, V5, V6, V7, V8, V9 }  , E =  The in-degree of a vertex v, denoted by indeg(v) is the number of incoming edges to the vertex v. The out-degree of a vertex v, denoted by outdeg(v) is the number of outgoing edges of a vertex v. The degree of a vertex v, denoted by deg(v) is the total number of edges whose one endpoint is v. deg(v) = Indeg (v) + outdeg (v) In the above graph deg(V4)=3,  indeg(V4)=2 and outdeg(V4)=1 A Cycle is a path that starts and ends at the same vertex and include at least one vertex. An edge is a Self-Loop if two if its two endpoints coincide. This is a form of a cycle. A vertex v is Reachable from vertex u or “u reaches v” if there is a path from u to v. In an undirected graph if v is reachable from u then u is reachable from v. But in a directed graph it is possible that u reaches v but there is no path from v to u. A graph is Connected if for any two vertices there is a path between them.

A Forest is a graph without cycles. A Sub-Graph of a graph G is a graph whose vertices and edges are a subset of the vertices and edges of G. A Spanning Sub-Graph of G is a graph that connects all the vertices of G. A tree is an acyclic connected graph. A Spanning tree of a graph is a spanning sub-graph that is also a tree that means, a connected graph which connects all the vertices of graph and that does not have a cycle.

Graph Representation In this section, we introduce the data structure for representing a graph. In the below representations we maintain a collection to store edges and vertices of the graph.

Adjacency Matrix One of the ways to represent a graph is to use two-dimensional matrix. Each combination of row and column represent a vertex in the graph. The value stored at the location row v and column w is the edge from vertex v to vertex w. The nodes that are connected by an edge are called adjacent nodes. This matrix is used to store adjacent relation so it is called the Adjacency Matrix. In the below diagram, we have a graph and its Adjacency matrix. In the above graph, each node has weight 1 so the adjacency matrix has just 1s or 0s. If the edges are of different, weights that that weight will be filled in the matrix. Pros: Adjacency matrix implementation is simple. Adding/Removing an edge between two vertices is just O(1). Query if there is an edge between two vertices is also O(1) Cons: It always consumes O(V^2) space, which is an inefficient way to store when a graph is a sparse. Sparse Matrix: In a huge graph, each node is connected with fewer nodes. So most of the places in adjacency matrix are empty. Such matrix is called sparse matrix. In most of the real world problems adjacency matrix is not a good choice for sore graph data.

Adjacency List A more space efficient way of storing graph is adjacency list. In adjacency list of references to a linked list node. Each reference corresponds to vertices in a graph.  Each reference will then point to the vertices that are connected to it and store this as a list. In the below diagram node 2 is connected to 1, 3 and 4. So the reference at location 2 is pointing to a list which contain 1, 3 and 4. The adjacency list helps us to compactly represent a sparse graph. An adjacency list representation also allows us to find all the vertices that are directly connected to any vertices by just one link list scan. In all our programs, we are going to use the adjacency list to store the graph. Below is the code for adjacency list representation of an undirected graph: Example 13.1: public class Graph {           private static class AdjNode {                    private int source;                    private int destination;                    private int cost;                    private AdjNode next;                    public AdjNode(int src, int dst, int cst) {                              source = src;                              destination = dst;                              cost = cst;                              next = null;                    }                    public AdjNode(int src, int dst) {                              this(src, dst, 1);                    }                  }           private class static AdjList {                    private AdjNode head;           }

int count;          AdjList[] array;          public Graph(int cnt) {                   count = cnt;                   array = new AdjList[cnt];                   for(int i=0; i<cnt; i++){                             array[i]=new AdjList();                             array[i].head = null;                   }          }          public void AddEdge(int source, int destination, int cost){                   AdjNode node = new AdjNode(source, destination, cost);                   node.next = array[source].head;                   array[source].head =node;          }          public void AddEdge(int source, int destination){                   AddEdge(source, destination, 1);          }          //bi directional edge          public void AddBiEdge(int source, int destination, int cost)          {                   AddEdge(source,destination,cost);                   AddEdge(destination,source,cost);          }          public void AddBiEdge(int source, int destination)//bi directional edge          {                   AddBiEdge(source,destination,1);          }            public void Print(){                   AdjNode ad;                   for(int i=0;i<count;i++){                             ad = array[i].head;                             if(ad != null){                                      System.out.print("Vertex " + i+ " is connected to:");                                      while(ad != null)                                      {                                                System.out.print(ad.destination+ " ");                                                ad = ad.next;                                      }                                      System.out.println("");                             }                   }          } }

Graph traversals The Depth first search (DFS) and Breadth first search (BFS) are the two algorithms used to traverse a graph. These same algorithms can also be used to find some node in the graph, find if a node is reachable etc. Traversal is the process of exploring a graph by examining all its edges and vertices. A list of some of the problems that are solved using graph traversal are: 1.    Determining a path from vertex u to vertex v, or report an error if there is no such path. 2.    Given a starting vertex s, finding the minimum number of edges from vertex s to all the other vertices of the graph. 3.    Testing of a graph G is connected. 4.    Finding a spanning tree of a Graph. 5.    Finding if there is some cycle in the graph.

Depth First Traversal The DFS algorithm we start from starting point and go into depth of graph until we reach a dead end and then move up to parent node (Backtrack).  In DFS we use stack to get the next vertex to start a search. Or we can use recursion (system stack) to do the same. Algorithm steps for DFS 1.    Push the starting node in the stack. 2.    Loop until the stack is empty. 3.    Pop the node from the stack inside loop call this node current. 4.    Process the current node. //Print, etc. 5.    Traverse all the child nodes of the current node and push them into stack. 6.    Repeat steps 3 to 5 until the stack is empty. Stack based implementation of DFS Example 13.2: public void DFSStack(Graph gph, int start) {           int count = gph.count;           int[] visited = new int[count];           int curr;           Stack<Integer> stk = new Stack<Integer>();           for (int i = 0; i < count; i++)                    visited[i] = 0;           visited[start] = 1;           stk.push(start);           while (stk.isEmpty() == false) {                    curr = stk.pop();                    AdjNode head = gph.array[curr].head;                    while(head != null) {                              if(visited[head.destination] == 0) {                                       visited[head.destination] = 1;                                       stk.push(head.destination);

}                             head = head.next;                   }          } } Recursion based implementation of DFS Example 13.3: public static void DFSRec(Graph gph, int start, int[] visited) {          AdjNode head = gph.array[start].head;          while(head != null)          {                   if(visited[head.destination] == 0)                   {                             visited[head.destination] = 1;                             DFSRec( gph, head.destination, visited);                   }                   head = head.next;          } }

Breadth First Traversal In BFS algorithm, a graph is traversed in layer-by-layer fashion. The graph is traversed closer to the starting point. The queue is used to implement BFS. Algorithm steps for BFS 1.    Push the starting node into the Queue. 2.    Loop until the Queue is empty. 3.    Remove a node from the Queue inside loop, call this node current. 4.    Process the current node.//print etc. 5.    Traverse all the child nodes of the current node and push them into Queue. 6.    Repeat steps 3 to 5 until Queue is empty. Example 13.4: Public void BFSQueue(Graph gph, int start, int[] visited) {           int curr;           LinkedList<Integer> que= new LinkedList<Integer>();           visited[start] = 1;           que.add(start);           while( que.isEmpty() == false)           {                    curr=que.remove();                    AdjNode head = gph.array[curr].head;                    while(head != null)                    {                              if(visited[head.destination] == 0)                              {                                       visited[head.destination] = 1;                                       que.add(head.destination);                              }                              head = head.next;                    }

} } A runtime analysis of DFS and BFS traversal is O(n+m) time, where n is the number of edges reachable from source node and m is the number of edges incident on s. The following problems have O(m+n) time performance: 1.    Determining a path from vertex u to vertex v, or report an error if there is no such path. 2.    Given a starting vertex s, finding the minimum number of edges from vertex s to all the other vertices of the graph. 3.    Testing of a graph G is connected. 4.    Finding a spanning tree of a Graph. 5.    Finding if there is some cycle in the graph.

Problems in Graph Determining a path from vertex u to vertex v IF there is a path from u to v and we are doing DFS from u then v must be visited. And if there is no path them report an error. Example 13.5: public static int PathExist(Graph gph, int source, int destination) {           int count = gph.count;           int[] visited = new int[count];           for (int i = 0; i < count; i++)                    visited[i] = 0;           visited[source] = 1;           DFSRec (gph, source, visited);           return visited[destination]; } Given a starting vertex s, finding the minimum number of edges from vertex s to all the other vertices of the graph Look for single source shortest path algorithm for each edge cost as 1 unit. Testing of a graph G is connected. We simply start from any arbitrary vertex and do DFS search and should see if there is some vertex which is not visited. If all the vertices are visited then it is a connected graph. Example 13.6: public boolean isConnected(Graph gph) {           int count = gph.count;           int[] visited = new int[count];           for (int i = 0; i < count; i++)                    visited[i] = 0;           visited[0] = 1;           DFSRec(gph,0, visited);           for (int i = 0; i < count; i++)                    if(visited[i] == 0)                              return false;           return true; } Finding if there is some cycle in the graph. Modify DFS problem and get this done.



Directed Acyclic Graph A Directed Acyclic Graph (DAG) is a directed graph with no cycle. A DAG represent relationship which is more general than a tree. Below is an example of DAG, this is how someone becomes ready for work. There are N other real life examples of DAG such as coerces selection to being graduated from college

Topological Sort A topological sort is a method of ordering the nodes of a directed graph in which nodes represent activities and the edges represent dependency among those tasks. For topological sorting to work it is required that the graph should be a DAG which means it should not have any cycle. Just use DFS to get topological sorting. Example 13.7: public static void TopologicalSort(Graph gph) {           Stack<Integer> stk = new Stack<Integer>();           int count = gph.count;           int[] visited = new int[count];           for (int i = 0; i < count; i++)                    visited[i] = 0;           for (int i = 0; i < count; i++)           {                    if (visited[i] == 0)                    {                              visited[i] = 1;                              TopologicalSortDFS(gph, i, visited, stk);                    }           }           while (stk.isEmpty() != true)                    System.out.print(" " + stk.pop()); } private static void TopologicalSortDFS(Graph gph, int index, int[] visited, Stack<Integer> stk) {           AdjNode head = gph.array[index].head;           while(head != null)           {                    if(visited[head.destination] == 0)                    {                              visited[head.destination] = 1;                              TopologicalSortDFS( gph, head.destination, visited, stk);                    }                    head = head.next;           }           stk.push(index); } Topology sort is DFS traversal of topology graph. First the children of node are added to the stack then only the current node is added. So the sorting order is maintained. Reader is requested to run some examples to fully understand this algo.

Minimum Spanning Trees (MST) A Spanning Tree of a graph G is a tree which contains all the edges of the Graph G. A Minimum Spanning Tree is a tree whose sum of length/weight of edges is minimum as possible. For example, if you want to setup communication between a set of cities, then you may want to use the least amount of wire as possible. MST can be used to find the network path and wire cost estimate. Prim’s Algorithm for MST        Prim’s algorithm grows a single tree T, one edge at a time, until it becomes a spanning tree. We initialize T with zero edges. And U with single node. Where T is spanning tree edges set and U is spanning tree vertex set. At each step, Prim’s algorithm adds the smallest value edge with one endpoint in U and other not in us. Since each edge adds one new vertex to U, after n − 1 additions, U contain all the vertices of the spanning tree and T becomes a spanning tree. Example 13.8: // Returns the MST by Prim’s Algorithm // Input: A weighted connected graph G = (V, E) // Output: Set of edges comprising a MST Algorithm Prim(G)           T = {}           Let r be any vertex in G           U = {r}           for i = 1 to |V| - 1 do                    e = minimum-weight edge (u, v)                              With u in U and v in V-U                    U = U + {v}                    T = T + {e}           return T Prim’s Algorithm using a priority queue (min heap) to get the closest fringe vertex

Time Complexity will be O(m log n) where n vertices and m edges of the MST. Example 13.9: public static void Prims(Graph gph) {          int[] previous = new int[gph.count];          int[] dist = new int[gph.count];          int source = 1;          for (int i = 0; i < gph.count; i++) {                   previous[i] = -1;                   dist[i] = Integer.MAX_VALUE;          }          dist[source] = 0;          previous[source] = -1;          AdjNodeComparator comp = new AdjNodeComparator();          PriorityQueue<AdjNode> queue= new PriorityQueue<AdjNode>(100,comp);          AdjNode node = new AdjNode(source, source, 0);          queue.add(node);          while (queue.isEmpty() != true) {                   node = queue.peek();                   queue.remove();                   if(dist[node.destination] < node.cost)                             continue;                   dist[node.destination] = node.cost;                   previous[node.destination] = node.source;                   AdjList adl = gph.array[node.destination];                   AdjNode adn = adl.head;                   while (adn != null) {                             if (previous[adn.destination]==-1)                             {                                      node = new AdjNode(adn.source, adn.destination, adn.cost);                             queue.add(node);                             }                             adn = adn.next;                   }          }          AdjNodeComparator comp = new AdjNodeComparator();          PriorityQueue<AdjNode> queue = new PriorityQueue<AdjNode>(100,comp);          AdjNode node = new AdjNode(source, source, 0);          queue.add(node);          while (queue.isEmpty()!= true)          {                   node = queue.peek();                   queue.remove();                   AdjList adl = gph.array[node.destination];                   AdjNode adn = adl.head;                   while(adn != null)                   {                             int alt = adn.cost;

if(alt < dist[adn.destination]){                                       dist[adn.destination]= alt;                                      previous[adn.destination]=adn.source;                                      node = new AdjNode(adn.source, adn.destination, alt);                                      queue.add(node);                             }                             adn = adn.next;                   }          }          // Printing result.          int count = gph.count;          for (int i = 0; i < count; i++)          {                   if(dist[i] == Integer.MAX_VALUE){                             System.out.println(" node id " + i + "  prev " +  previous[i] + " distance : Unreachable");                           }                    else{                             System.out.println(" node id " + i + "  prev " +  previous[i] + " distance : " + dist[i] );                   }          } } public static void main(String[] args) {          Graph gph = new Graph(9);          gph.AddBiEdge( 0, 2, 1);          gph.AddBiEdge( 1, 2, 5);          gph.AddBiEdge( 1, 3, 7);          gph.AddBiEdge( 1, 4, 9);          gph.AddBiEdge( 3, 2, 2);          gph.AddBiEdge( 3, 5, 4);          gph.AddBiEdge( 4, 5, 6);          gph.AddBiEdge( 4, 6, 3);          gph.AddBiEdge( 5, 7, 5);          gph.AddBiEdge( 6, 7, 7);          gph.AddBiEdge( 7, 8, 17);          //Dijkstra(gph,1);          Prims(gph); } Kruskal’s Algorithm Kruskal’s Algorithm repeatedly chooses the smallest-weight edge that does not form a cycle. Sort the edges in non-decreasing order of cost: c (e1) ≤ c (e2) ≤ · · · ≤ c (em). Set T to be the empty tree. Add edges to tree one by one if it does not create a cycle. Example 13.10: // Returns the MST by Kruskal’s Algorithm // Input: A weighted connected graph G = (V, E) // Output: Set of edges comprising a MST Algorithm Kruskal(G)          Sort the edges E by their weights          T = {}

while |T | + 1 < |V | do                   e = next edge in E                   if T + {e} does not have a cycle then                             T = T + {e}          return T Kruskal’s Algorithm is O(E log V) using efficient cycle detection.

Shortest Path Algorithms in Graph Single Source Shortest Path For a graph G= (V, E), the single source shortest path problem is to find the shortest path from a given source vertex s to all the vertices of V. Single Source Shortest Path for unweighted Graph. Find single source shortest path for unweighted graph or a graph whose all the vertices have same weight. Example 13.11: public void ShortestPath(Graph gph, int source)// unweighted graph {           int curr;           int count = gph.count;           int[] distance= new int[count];           int[] path= new int[count];           Queue<Integer> que= new LinkedList<Integer>();           for (int i = 0; i < count; i++)                    distance[i] = -1;           que.add(source);           distance[source]=0;           while(que.isEmpty() == false)           {                    curr=que.remove();                    AdjNode head = gph.array[curr].head;                         while(head != null)                    {                              if(distance[head.destination] == -1)                              {                                       distance[head.destination] = distance[curr] + 1;                                       path[head.destination]=curr;                                       que.add(head.destination);                              }                              head = head.next;                    }           }           for(int i=0;i<count;i++)                    System.out.println(path[i] + " to " + i +" weight " + distance[i]); } Dijkstra’s algorithm Dijkstra’s algorithm for single-source shortest path problem for weighted edges with no negative weight. Given a weighted connected graph G, find shortest paths from the source vertex s to each

of the other vertices. Dijkstra’s algorithm is similar to prims algorithm. It maintains a set of nodes for which shortest path is known. The algorithm starts by keeping track of the distance of each node and its parents. All the distance is set to infinite in the beginning as we don’t know the actual path to the nodes and parents of all the vertices are set to null. All the vertices are added to a priority queue (min heap implementation) At each step algorithm takes one vertex from the priority queue (which will be the source vertex in the beginning). Then update the distance array corresponding to all the adjacent vertices. When the queue is empty, then we will have the distance and parent array fully populated. Example 13.12: // Solves SSSP by Dijkstra’s Algorithm // Input: A weighted connected graph G = (V, E) // with no negative weights, and source vertex v // Output: The length and path from s to every v Algorithm Dijkstra(G, s) for each v in V do          D[v] = infinite // Unknown distance          P[v] = null     //unknown previous node          add v to PQ    //adding all nodes to priority queue D[source] = 0 // Distance from source to source while (PQ is not empty)          u = vertex from PQ with smallest D[u]          remove u from PQ          for each v adjacent from u do                   alt = D[u] + length ( u , v)                   if  alt < D[v] then                             D[v] = alt                             P[v] = u Return D[] , P[] Time Complexity will be O(|E|log|V|) Note: Dijkstra’s algorithm does not work for graphs with negative edges weight. Note: Dijkstra’s algorithm is applicable to both undirected and directed graphs.

Example 13.13: public static void Dijkstra(Graph gph, int source) {          class AdjNodeComparator implements Comparator<AdjNode>{                   public int compare(AdjNode x, AdjNode y){                             if(x.cost < y.cost                                      return -1;                             if (x.cost > y.cost)                                      return 1;                             return 0;                   }          };          int[] previous=new int[gph.count];          int[] dist=new int[gph.count];          for(int i=0;i<gph.count;i++)          {                   previous[i]=-1;                   dist[i]=Integer.MAX_VALUE; //infinite          }          dist[source] = 0;          previous[source]= -1;          AdjNodeComparator comp = new AdjNodeComparator();          PriorityQueue<AdjNode> queue = new PriorityQueue<AdjNode> (100,comp);          AdjNode node = new AdjNode(source, source, 0);          queue.add(node);          while (queue.isEmpty()!= true)          {                   node = queue.peek();                   queue.remove();                   AdjList adl = gph.array[node.destination];                   AdjNode adn = adl.head;                   while(adn != null)                   {                             int alt = adn.cost + dist[adn.source];                             if(alt < dist[adn.destination]){                                      dist[adn.destination]= alt;                                      previous[adn.destination]=adn.source;                                      node = new AdjNode(adn.source, adn.destination, alt);                                      queue.add(node);                             }                             adn = adn.next;                   }          }            int count = gph.count;          for (int i = 0; i < count; i++)          {                   if(dist[i] == Integer.MAX_VALUE){                             System.out.println(" node id " + i + "  prev " +  previous[i]  + " distance : Unreachable" );                          }                   else{                             System.out.println(" node id " + i + "  prev " +  previous[i] + " distance : " + dist[i] );                   }          } }

Bellman Ford Shortest Path The bellman ford algorithm works even when there are negative weight edges in the graph. It does not work if there is some cycle in the graph whose total weight is negative. Example 13.14: public void BellmanFordShortestPath(Graph gph, int source) {          int count = gph.count;          int[] distance= new int[count];          int[] path= new int[count];          for (int i = 0; i < count; i++)                   distance[i] = Integer.MAX_VALUE;          distance[source]=0;          for (int i = 0; i < count -1; i++)          {                   for (int j = 0; j < count; j++)                   {                             AdjNode head = gph.array[j].head;                             while(head!= null)                             {                                      int newDistance = distance[j]+ head.cost;                                      if(distance[head.destination] > newDistance)                                      {                                                distance[head.destination] = newDistance;                                                path[head.destination]=j;                                      }                                      head = head.next;                             }                   }          }          for(int i=0;i<count;i++)                   System.out.println(path[i] + " to " + i +" weight " + distance[i]); } All Pairs Shortest Paths Given a weighted graph G(V, E), the all pair shortest path problem is to find the shortest path between all pairs of vertices u, v є V. Execute n instances of single source shortest path algorithm for each vertex of the graph. The complexity of this algorithm will be

